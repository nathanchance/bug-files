

struct ftrace_branch_data {
	const char *func;
	const char *file;
	unsigned line;
	union {
		struct {
			unsigned long correct;
			unsigned long incorrect;
		};
		struct {
			unsigned long miss;
			unsigned long hit;
		};
		unsigned long miss_hit[2];
	};
};

struct ftrace_likely_data {
	struct ftrace_branch_data data;
	unsigned long constant;
};

typedef __signed__ char __s8;
typedef unsigned char __u8;

typedef __signed__ short __s16;
typedef unsigned short __u16;

typedef __signed__ int __s32;
typedef unsigned int __u32;

__extension__ typedef __signed__ long long __s64;
__extension__ typedef unsigned long long __u64;

typedef __s8 s8;
typedef __u8 u8;
typedef __s16 s16;
typedef __u16 u16;
typedef __s32 s32;
typedef __u32 u32;
typedef __s64 s64;
typedef __u64 u64;

enum { false = 0, true = 1 };
typedef struct {
	unsigned long fds_bits[1024 / (8 * sizeof(long))];
} __kernel_fd_set;

typedef void (*__kernel_sighandler_t)(int);

typedef int __kernel_key_t;
typedef int __kernel_mqd_t;

typedef unsigned short __kernel_old_uid_t;
typedef unsigned short __kernel_old_gid_t;

typedef unsigned long __kernel_old_dev_t;

typedef long __kernel_long_t;
typedef unsigned long __kernel_ulong_t;

typedef __kernel_ulong_t __kernel_ino_t;

typedef unsigned int __kernel_mode_t;

typedef int __kernel_pid_t;

typedef int __kernel_ipc_pid_t;

typedef unsigned int __kernel_uid_t;
typedef unsigned int __kernel_gid_t;

typedef __kernel_long_t __kernel_suseconds_t;

typedef int __kernel_daddr_t;

typedef unsigned int __kernel_uid32_t;
typedef unsigned int __kernel_gid32_t;
typedef __kernel_ulong_t __kernel_size_t;
typedef __kernel_long_t __kernel_ssize_t;
typedef __kernel_long_t __kernel_ptrdiff_t;

typedef struct {
	int val[2];
} __kernel_fsid_t;

typedef __kernel_long_t __kernel_off_t;
typedef long long __kernel_loff_t;
typedef __kernel_long_t __kernel_old_time_t;

typedef long long __kernel_time64_t;
typedef __kernel_long_t __kernel_clock_t;
typedef int __kernel_timer_t;
typedef int __kernel_clockid_t;
typedef char *__kernel_caddr_t;
typedef unsigned short __kernel_uid16_t;
typedef unsigned short __kernel_gid16_t;

typedef __signed__ __int128 __s128 __attribute__((aligned(16)));
typedef unsigned __int128 __u128 __attribute__((aligned(16)));
typedef __u16 __le16;
typedef __u16 __be16;
typedef __u32 __le32;
typedef __u32 __be32;
typedef __u64 __le64;
typedef __u64 __be64;

typedef __u16 __sum16;
typedef __u32 __wsum;
typedef unsigned __poll_t;

typedef __s128 s128;
typedef __u128 u128;

typedef u32 __kernel_dev_t;

typedef __kernel_fd_set fd_set;
typedef __kernel_dev_t dev_t;
typedef __kernel_ulong_t ino_t;
typedef __kernel_mode_t mode_t;
typedef unsigned short umode_t;
typedef u32 nlink_t;
typedef __kernel_off_t off_t;
typedef __kernel_pid_t pid_t;
typedef __kernel_daddr_t daddr_t;
typedef __kernel_key_t key_t;
typedef __kernel_suseconds_t suseconds_t;
typedef __kernel_timer_t timer_t;
typedef __kernel_clockid_t clockid_t;
typedef __kernel_mqd_t mqd_t;

typedef _Bool bool;

typedef __kernel_uid32_t uid_t;
typedef __kernel_gid32_t gid_t;
typedef __kernel_uid16_t uid16_t;
typedef __kernel_gid16_t gid16_t;

typedef unsigned long uintptr_t;
typedef long intptr_t;

typedef __kernel_old_uid_t old_uid_t;
typedef __kernel_old_gid_t old_gid_t;

typedef __kernel_loff_t loff_t;
typedef __kernel_size_t size_t;

typedef __kernel_ssize_t ssize_t;

typedef __kernel_ptrdiff_t ptrdiff_t;

typedef __kernel_clock_t clock_t;

typedef __kernel_caddr_t caddr_t;

typedef unsigned char u_char;
typedef unsigned short u_short;
typedef unsigned int u_int;
typedef unsigned long u_long;

typedef unsigned char unchar;
typedef unsigned short ushort;
typedef unsigned int uint;
typedef unsigned long ulong;

typedef u8 u_int8_t;
typedef s8 int8_t;
typedef u16 u_int16_t;
typedef s16 int16_t;
typedef u32 u_int32_t;
typedef s32 int32_t;

typedef u8 uint8_t;
typedef u16 uint16_t;
typedef u32 uint32_t;

typedef u64 uint64_t;
typedef u64 u_int64_t;
typedef s64 int64_t;
typedef s64 ktime_t;
typedef u64 sector_t;
typedef u64 blkcnt_t;
typedef u64 dma_addr_t;

typedef unsigned int gfp_t;
typedef unsigned int slab_flags_t;
typedef unsigned int fmode_t;

typedef u64 phys_addr_t;

typedef phys_addr_t resource_size_t;

typedef unsigned long irq_hw_number_t;

typedef struct {
	int counter;
} atomic_t;

typedef struct {
	s64 counter;
} atomic64_t;

typedef struct {
	atomic_t refcnt;
} rcuref_t;

struct list_head {
	struct list_head *next, *prev;
};

struct hlist_head {
	struct hlist_node *first;
};

struct hlist_node {
	struct hlist_node *next, **pprev;
};

struct ustat {
	__kernel_daddr_t f_tfree;

	unsigned long f_tinode;

	char f_fname[6];
	char f_fpack[6];
};
struct callback_head {
	struct callback_head *next;
	void (*func)(struct callback_head *head);
} __attribute__((aligned(sizeof(void *))));

typedef void (*rcu_callback_t)(struct callback_head *head);
typedef void (*call_rcu_func_t)(struct callback_head *head,
				rcu_callback_t func);

typedef void (*swap_r_func_t)(void *a, void *b, int size, const void *priv);
typedef void (*swap_func_t)(void *a, void *b, int size);

typedef int (*cmp_r_func_t)(const void *a, const void *b, const void *priv);
typedef int (*cmp_func_t)(const void *a, const void *b);

struct __una_u16 {
	u16 x;
} __attribute__((__packed__));
struct __una_u32 {
	u32 x;
} __attribute__((__packed__));
struct __una_u64 {
	u64 x;
} __attribute__((__packed__));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u16
__get_unaligned_cpu16(const void *p)
{
	const struct __una_u16 *ptr = (const struct __una_u16 *)p;
	return ptr->x;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u32
__get_unaligned_cpu32(const void *p)
{
	const struct __una_u32 *ptr = (const struct __una_u32 *)p;
	return ptr->x;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u64
__get_unaligned_cpu64(const void *p)
{
	const struct __una_u64 *ptr = (const struct __una_u64 *)p;
	return ptr->x;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__put_unaligned_cpu16(u16 val, void *p)
{
	struct __una_u16 *ptr = (struct __una_u16 *)p;
	ptr->x = val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__put_unaligned_cpu32(u32 val, void *p)
{
	struct __una_u32 *ptr = (struct __una_u32 *)p;
	ptr->x = val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__put_unaligned_cpu64(u64 val, void *p)
{
	struct __una_u64 *ptr = (struct __una_u64 *)p;
	ptr->x = val;
}

void ftrace_likely_update(struct ftrace_likely_data *f, int val, int expect,
			  int is_constant);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void *
offset_to_ptr(const int *off)
{
	return (void *)((unsigned long)off + *off);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) bool
__kasan_check_read(const volatile void *p, unsigned int size)
{
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) bool
__kasan_check_write(const volatile void *p, unsigned int size)
{
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) bool
kasan_check_read(const volatile void *p, unsigned int size)
{
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) bool
kasan_check_write(const volatile void *p, unsigned int size)
{
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__kcsan_check_access(const volatile void *ptr, size_t size, int type)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__kcsan_mb(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__kcsan_wmb(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__kcsan_rmb(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__kcsan_release(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
kcsan_disable_current(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
kcsan_enable_current(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
kcsan_enable_current_nowarn(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
kcsan_nestable_atomic_begin(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
kcsan_nestable_atomic_end(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
kcsan_flat_atomic_begin(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
kcsan_flat_atomic_end(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
kcsan_atomic_next(int n)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
kcsan_set_access_mask(unsigned long mask)
{
}

struct kcsan_scoped_access {};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) struct kcsan_scoped_access *
kcsan_begin_scoped_access(const volatile void *ptr, size_t size, int type,
			  struct kcsan_scoped_access *sa)
{
	return sa;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
kcsan_end_scoped_access(struct kcsan_scoped_access *sa)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
kcsan_check_access(const volatile void *ptr, size_t size, int type)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__kcsan_enable_current(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__kcsan_disable_current(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) unsigned long
__read_once_word_nocheck(const void *addr)
{
	return (*(
		const volatile typeof(_Generic((*(unsigned long *)addr),
					      char: (char)0,
					      unsigned char: (unsigned char)0,
					      signed char: (signed char)0,
					      unsigned short: (unsigned short)0,
					      signed short: (signed short)0,
					      unsigned int: (unsigned int)0,
					      signed int: (signed int)0,
					      unsigned long: (unsigned long)0,
					      signed long: (signed long)0,
					      unsigned long long: (
						       unsigned long long)0,
					      signed long long: (
						       signed long long)0,
					      default: (*(unsigned long *)addr)))
			*)&(*(unsigned long *)addr));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) unsigned long
read_word_at_a_time(const void *addr)
{
	kasan_check_read(addr, 1);
	return *(unsigned long *)addr;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) __attribute__((__const__)) __u32
__arch_swab32(__u32 val)
{
	asm("bswapl %0" : "=r"(val) : "0"(val));
	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) __attribute__((__const__)) __u64
__arch_swab64(__u64 val)
{
	asm("bswapq %0" : "=r"(val) : "0"(val));
	return val;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) __attribute__((__const__)) __u16
__fswab16(__u16 val)
{
	return ((__u16)((((__u16)(val) & (__u16)0x00ffU) << 8) |
			(((__u16)(val) & (__u16)0xff00U) >> 8)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) __attribute__((__const__)) __u32
__fswab32(__u32 val)
{
	return __arch_swab32(val);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) __attribute__((__const__)) __u64
__fswab64(__u64 val)
{
	return __arch_swab64(val);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) __attribute__((__const__)) __u32
__fswahw32(__u32 val)
{
	return ((__u32)((((__u32)(val) & (__u32)0x0000ffffUL) << 16) |
			(((__u32)(val) & (__u32)0xffff0000UL) >> 16)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) __attribute__((__const__)) __u32
__fswahb32(__u32 val)
{
	return ((__u32)((((__u32)(val) & (__u32)0x00ff00ffUL) << 8) |
			(((__u32)(val) & (__u32)0xff00ff00UL) >> 8)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) unsigned long
__swab(const unsigned long y)
{
	return (__u64)__builtin_bswap64((__u64)(y));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __u16
__swab16p(const __u16 *p)
{
	return (__u16)__builtin_bswap16((__u16)(*p));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __u32
__swab32p(const __u32 *p)
{
	return (__u32)__builtin_bswap32((__u32)(*p));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __u64
__swab64p(const __u64 *p)
{
	return (__u64)__builtin_bswap64((__u64)(*p));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) __u32
__swahw32p(const __u32 *p)
{
	return (__builtin_constant_p((__u32)(*p)) ?
			((__u32)((((__u32)(*p) & (__u32)0x0000ffffUL) << 16) |
				 (((__u32)(*p) & (__u32)0xffff0000UL) >> 16))) :
			__fswahw32(*p));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) __u32
__swahb32p(const __u32 *p)
{
	return (__builtin_constant_p((__u32)(*p)) ?
			((__u32)((((__u32)(*p) & (__u32)0x00ff00ffUL) << 8) |
				 (((__u32)(*p) & (__u32)0xff00ff00UL) >> 8))) :
			__fswahb32(*p));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__swab16s(__u16 *p)
{
	*p = __swab16p(p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) void
__swab32s(__u32 *p)
{
	*p = __swab32p(p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) void
__swab64s(__u64 *p)
{
	*p = __swab64p(p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__swahw32s(__u32 *p)
{
	*p = __swahw32p(p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__swahb32s(__u32 *p)
{
	*p = __swahb32p(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
swab16_array(u16 *buf, unsigned int words)
{
	while (words--) {
		__swab16s(buf);
		buf++;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
swab32_array(u32 *buf, unsigned int words)
{
	while (words--) {
		__swab32s(buf);
		buf++;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
swab64_array(u64 *buf, unsigned int words)
{
	while (words--) {
		__swab64s(buf);
		buf++;
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __le64
__cpu_to_le64p(const __u64 *p)
{
	return (__le64)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __u64
__le64_to_cpup(const __le64 *p)
{
	return (__u64)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __le32
__cpu_to_le32p(const __u32 *p)
{
	return (__le32)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __u32
__le32_to_cpup(const __le32 *p)
{
	return (__u32)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __le16
__cpu_to_le16p(const __u16 *p)
{
	return (__le16)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __u16
__le16_to_cpup(const __le16 *p)
{
	return (__u16)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __be64
__cpu_to_be64p(const __u64 *p)
{
	return (__be64)__swab64p(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __u64
__be64_to_cpup(const __be64 *p)
{
	return __swab64p((__u64 *)p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __be32
__cpu_to_be32p(const __u32 *p)
{
	return (__be32)__swab32p(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __u32
__be32_to_cpup(const __be32 *p)
{
	return __swab32p((__u32 *)p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __be16
__cpu_to_be16p(const __u16 *p)
{
	return (__be16)__swab16p(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__))
__attribute__((__always_inline__)) __u16
__be16_to_cpup(const __be16 *p)
{
	return __swab16p((__u16 *)p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
le16_add_cpu(__le16 *var, u16 val)
{
	*var = ((__le16)(__u16)(((__u16)(__le16)(*var)) + val));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
le32_add_cpu(__le32 *var, u32 val)
{
	*var = ((__le32)(__u32)(((__u32)(__le32)(*var)) + val));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
le64_add_cpu(__le64 *var, u64 val)
{
	*var = ((__le64)(__u64)(((__u64)(__le64)(*var)) + val));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
le32_to_cpu_array(u32 *buf, unsigned int words)
{
	while (words--) {
		do {
			(void)(buf);
		} while (0);
		buf++;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
cpu_to_le32_array(u32 *buf, unsigned int words)
{
	while (words--) {
		do {
			(void)(buf);
		} while (0);
		buf++;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
be16_add_cpu(__be16 *var, u16 val)
{
	*var = ((__be16)(__u16)__builtin_bswap16((__u16)((
		(__u16)__builtin_bswap16((__u16)((__u16)(__be16)(*var))) +
		val))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
be32_add_cpu(__be32 *var, u32 val)
{
	*var = ((__be32)(__u32)__builtin_bswap32((__u32)((
		(__u32)__builtin_bswap32((__u32)((__u32)(__be32)(*var))) +
		val))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
be64_add_cpu(__be64 *var, u64 val)
{
	*var = ((__be64)(__u64)__builtin_bswap64((__u64)((
		(__u64)__builtin_bswap64((__u64)((__u64)(__be64)(*var))) +
		val))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
cpu_to_be32_array(__be32 *dst, const u32 *src, size_t len)
{
	size_t i;

	for (i = 0; i < len; i++)
		dst[i] = ((__be32)(__u32)__builtin_bswap32((__u32)((src[i]))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
be32_to_cpu_array(u32 *dst, const __be32 *src, size_t len)
{
	size_t i;

	for (i = 0; i < len; i++)
		dst[i] = (__u32)__builtin_bswap32(
			(__u32)((__u32)(__be32)(src[i])));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u16
get_unaligned_le16(const void *p)
{
	return ((__u16)(__le16)(({
		const struct {
			__le16 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x;
	})));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u32
get_unaligned_le32(const void *p)
{
	return ((__u32)(__le32)(({
		const struct {
			__le32 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x;
	})));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u64
get_unaligned_le64(const void *p)
{
	return ((__u64)(__le64)(({
		const struct {
			__le64 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x;
	})));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
put_unaligned_le16(u16 val, void *p)
{
	do {
		struct {
			__le16 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x = (((__le16)(__u16)(val)));
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
put_unaligned_le32(u32 val, void *p)
{
	do {
		struct {
			__le32 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x = (((__le32)(__u32)(val)));
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
put_unaligned_le64(u64 val, void *p)
{
	do {
		struct {
			__le64 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x = (((__le64)(__u64)(val)));
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u16
get_unaligned_be16(const void *p)
{
	return (__u16)__builtin_bswap16((__u16)((__u16)(__be16)(({
		const struct {
			__be16 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x;
	}))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u32
get_unaligned_be32(const void *p)
{
	return (__u32)__builtin_bswap32((__u32)((__u32)(__be32)(({
		const struct {
			__be32 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x;
	}))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u64
get_unaligned_be64(const void *p)
{
	return (__u64)__builtin_bswap64((__u64)((__u64)(__be64)(({
		const struct {
			__be64 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x;
	}))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
put_unaligned_be16(u16 val, void *p)
{
	do {
		struct {
			__be16 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x =
			(((__be16)(__u16)__builtin_bswap16((__u16)((val)))));
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
put_unaligned_be32(u32 val, void *p)
{
	do {
		struct {
			__be32 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x =
			(((__be32)(__u32)__builtin_bswap32((__u32)((val)))));
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
put_unaligned_be64(u64 val, void *p)
{
	do {
		struct {
			__be64 x;
		} __attribute__((__packed__)) *__pptr = (typeof(__pptr))(p);
		__pptr->x =
			(((__be64)(__u64)__builtin_bswap64((__u64)((val)))));
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u32
__get_unaligned_be24(const u8 *p)
{
	return p[0] << 16 | p[1] << 8 | p[2];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u32
get_unaligned_be24(const void *p)
{
	return __get_unaligned_be24(p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u32
__get_unaligned_le24(const u8 *p)
{
	return p[0] | p[1] << 8 | p[2] << 16;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u32
get_unaligned_le24(const void *p)
{
	return __get_unaligned_le24(p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__put_unaligned_be24(const u32 val, u8 *p)
{
	*p++ = (val >> 16) & 0xff;
	*p++ = (val >> 8) & 0xff;
	*p++ = val & 0xff;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
put_unaligned_be24(const u32 val, void *p)
{
	__put_unaligned_be24(val, p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__put_unaligned_le24(const u32 val, u8 *p)
{
	*p++ = val & 0xff;
	*p++ = (val >> 8) & 0xff;
	*p++ = (val >> 16) & 0xff;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
put_unaligned_le24(const u32 val, void *p)
{
	__put_unaligned_le24(val, p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
__put_unaligned_be48(const u64 val, u8 *p)
{
	*p++ = (val >> 40) & 0xff;
	*p++ = (val >> 32) & 0xff;
	*p++ = (val >> 24) & 0xff;
	*p++ = (val >> 16) & 0xff;
	*p++ = (val >> 8) & 0xff;
	*p++ = val & 0xff;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
put_unaligned_be48(const u64 val, void *p)
{
	__put_unaligned_be48(val, p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u64
__get_unaligned_be48(const u8 *p)
{
	return (u64)p[0] << 40 | (u64)p[1] << 32 | (u64)p[2] << 24 |
	       p[3] << 16 | p[4] << 8 | p[5];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) u64
get_unaligned_be48(const void *p)
{
	return __get_unaligned_be48(p);
}
void __crypto_xor(u8 *dst, const u8 *src1, const u8 *src2, unsigned int size);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
crypto_xor(u8 *dst, const u8 *src, unsigned int size)
{
	if (1 && __builtin_constant_p(size) &&
	    (size % sizeof(unsigned long)) == 0) {
		unsigned long *d = (unsigned long *)dst;
		unsigned long *s = (unsigned long *)src;
		unsigned long l;

		while (size > 0) {
			l = ({
				    const struct {
					    typeof(*(d)) x;
				    } __attribute__((__packed__)) *__pptr =
					    (typeof(__pptr))((d));
				    __pptr->x;
			    }) ^
			    ({
				    const struct {
					    typeof(*(s++)) x;
				    } __attribute__((__packed__)) *__pptr =
					    (typeof(__pptr))((s++));
				    __pptr->x;
			    });
			do {
				struct {
					typeof(*(d++)) x;
				} __attribute__((__packed__)) *__pptr =
					(typeof(__pptr))((d++));
				__pptr->x = ((l));
			} while (0);
			size -= sizeof(unsigned long);
		}
	} else {
		__crypto_xor(dst, dst, src, size);
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) void
crypto_xor_cpy(u8 *dst, const u8 *src1, const u8 *src2, unsigned int size)
{
	if (1 && __builtin_constant_p(size) &&
	    (size % sizeof(unsigned long)) == 0) {
		unsigned long *d = (unsigned long *)dst;
		unsigned long *s1 = (unsigned long *)src1;
		unsigned long *s2 = (unsigned long *)src2;
		unsigned long l;

		while (size > 0) {
			l = ({
				    const struct {
					    typeof(*(s1++)) x;
				    } __attribute__((__packed__)) *__pptr =
					    (typeof(__pptr))((s1++));
				    __pptr->x;
			    }) ^
			    ({
				    const struct {
					    typeof(*(s2++)) x;
				    } __attribute__((__packed__)) *__pptr =
					    (typeof(__pptr))((s2++));
				    __pptr->x;
			    });
			do {
				struct {
					typeof(*(d++)) x;
				} __attribute__((__packed__)) *__pptr =
					(typeof(__pptr))((d++));
				__pptr->x = ((l));
			} while (0);
			size -= sizeof(unsigned long);
		}
	} else {
		__crypto_xor(dst, src1, src2, size);
	}
}

__attribute__((__noinline__)) unsigned long
__crypto_memneq(const void *a, const void *b, size_t size);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) int
crypto_memneq(const void *a, const void *b, size_t size)
{
	return __crypto_memneq(a, b, size) != 0UL ? 1 : 0;
}

struct sysinfo {
	__kernel_long_t uptime;
	__kernel_ulong_t loads[3];
	__kernel_ulong_t totalram;
	__kernel_ulong_t freeram;
	__kernel_ulong_t sharedram;
	__kernel_ulong_t bufferram;
	__kernel_ulong_t totalswap;
	__kernel_ulong_t freeswap;
	__u16 procs;
	__u16 pad;
	__kernel_ulong_t totalhigh;
	__kernel_ulong_t freehigh;
	__u32 mem_unit;
	char _f[20 - 2 * sizeof(__kernel_ulong_t) - sizeof(__u32)];
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) __attribute__((__const__)) u32
gen_endbr(void)
{
	u32 endbr;

	asm("mov $~0xfa1e0ff3, %[endbr]\n\t"
	    "not %[endbr]\n\t"
	    : [endbr] "=&r"(endbr));

	return endbr;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) __attribute__((__const__)) u32
gen_endbr_poison(void)
{
	return 0x001f0f66;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((__no_instrument_function__)) bool
is_endbr(u32 val)
{
	if (val == gen_endbr_poison())
		return true;

	val &= ~0x01000000U;
	return val == gen_endbr();
}

extern __attribute__((nocf_check)) u64 ibt_save(bool disable);
extern __attribute__((nocf_check)) void ibt_restore(u64 save);
struct cacheline_padding {
	char x[0];
} __attribute__((__aligned__(1 << (6))));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__pure__)) void *
rip_rel_ptr(void *p)
{
	asm("leaq %c1(%%rip), %0" : "=r"(p) : "i"(p));

	return p;
}
register unsigned long current_stack_pointer asm("rsp");
struct alt_instr {
	s32 instr_offset;
	s32 repl_offset;

	union {
		struct {
			u32 cpuid : 16;
			u32 flags : 16;
		};
		u32 ft_flags;
	};

	u8 instrlen;
	u8 replacementlen;
} __attribute__((__packed__));

extern struct alt_instr __alt_instructions[], __alt_instructions_end[];

extern int alternatives_patched;

extern void alternative_instructions(void);
extern void apply_alternatives(struct alt_instr *start, struct alt_instr *end);
extern void apply_retpolines(s32 *start, s32 *end);
extern void apply_returns(s32 *start, s32 *end);
extern void apply_seal_endbr(s32 *start, s32 *end);
extern void apply_fineibt(s32 *start_retpoline, s32 *end_retpoine,
			  s32 *start_cfi, s32 *end_cfi);

struct module;

struct callthunk_sites {
	s32 *call_start, *call_end;
	struct alt_instr *alt_start, *alt_end;
};

extern void callthunks_patch_builtin_calls(void);
extern void callthunks_patch_module_calls(struct callthunk_sites *sites,
					  struct module *mod);
extern void *callthunks_translate_call_dest(void *dest);
extern int x86_call_depth_emit_accounting(u8 **pprog, void *func, void *ip);
extern void alternatives_smp_module_add(struct module *mod, char *name,
					void *locks, void *locks_end,
					void *text, void *text_end);
extern void alternatives_smp_module_del(struct module *mod);
extern void alternatives_enable_smp(void);
extern int alternatives_text_reserved(void *start, void *end);
extern bool skip_smp_alternatives;
void BUG_func(void);
void nop_func(void);
extern const unsigned char *const x86_nops[];
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
INIT_LIST_HEAD(struct list_head *list)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_0(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(list->next) == sizeof(char) ||
			       sizeof(list->next) == sizeof(short) ||
			       sizeof(list->next) == sizeof(int) ||
			       sizeof(list->next) == sizeof(long)) ||
			      sizeof(list->next) == sizeof(long long)))
				__compiletime_assert_0();
		} while (0);
		do {
			*(volatile typeof(list->next) *)&(list->next) = (list);
		} while (0);
	} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_1(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(list->prev) == sizeof(char) ||
			       sizeof(list->prev) == sizeof(short) ||
			       sizeof(list->prev) == sizeof(int) ||
			       sizeof(list->prev) == sizeof(long)) ||
			      sizeof(list->prev) == sizeof(long long)))
				__compiletime_assert_1();
		} while (0);
		do {
			*(volatile typeof(list->prev) *)&(list->prev) = (list);
		} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__list_add_valid(struct list_head *new, struct list_head *prev,
		 struct list_head *next)
{
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__list_del_entry_valid(struct list_head *entry)
{
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__list_add(struct list_head *new, struct list_head *prev,
	   struct list_head *next)
{
	if (!__list_add_valid(new, prev, next))
		return;

	next->prev = new;
	new->next = next;
	new->prev = prev;
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_2(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(prev->next) == sizeof(char) ||
			       sizeof(prev->next) == sizeof(short) ||
			       sizeof(prev->next) == sizeof(int) ||
			       sizeof(prev->next) == sizeof(long)) ||
			      sizeof(prev->next) == sizeof(long long)))
				__compiletime_assert_2();
		} while (0);
		do {
			*(volatile typeof(prev->next) *)&(prev->next) = (new);
		} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_add(struct list_head *new, struct list_head *head)
{
	__list_add(new, head, head->next);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__list_del(struct list_head *prev, struct list_head *next)
{
	next->prev = prev;
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_3(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(prev->next) == sizeof(char) ||
			       sizeof(prev->next) == sizeof(short) ||
			       sizeof(prev->next) == sizeof(int) ||
			       sizeof(prev->next) == sizeof(long)) ||
			      sizeof(prev->next) == sizeof(long long)))
				__compiletime_assert_3();
		} while (0);
		do {
			*(volatile typeof(prev->next) *)&(prev->next) = (next);
		} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__list_del_clearprev(struct list_head *entry)
{
	__list_del(entry->prev, entry->next);
	entry->prev = ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__list_del_entry(struct list_head *entry)
{
	if (!__list_del_entry_valid(entry))
		return;

	__list_del(entry->prev, entry->next);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_del(struct list_head *entry)
{
	__list_del_entry(entry);
	entry->next = ((void *)0x100 + (0xdead000000000000UL));
	entry->prev = ((void *)0x122 + (0xdead000000000000UL));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_replace(struct list_head *old, struct list_head *new)
{
	new->next = old->next;
	new->next->prev = new;
	new->prev = old->prev;
	new->prev->next = new;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_replace_init(struct list_head *old, struct list_head *new)
{
	list_replace(old, new);
	INIT_LIST_HEAD(old);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_swap(struct list_head *entry1, struct list_head *entry2)
{
	struct list_head *pos = entry2->prev;

	list_del(entry2);
	list_replace(entry1, entry2);
	if (pos == entry1)
		pos = entry2;
	list_add(entry1, pos);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_del_init(struct list_head *entry)
{
	__list_del_entry(entry);
	INIT_LIST_HEAD(entry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_move(struct list_head *list, struct list_head *head)
{
	__list_del_entry(list);
	list_add(list, head);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_move_tail(struct list_head *list, struct list_head *head)
{
	__list_del_entry(list);
	list_add_tail(list, head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_bulk_move_tail(struct list_head *head, struct list_head *first,
		    struct list_head *last)
{
	first->prev->next = last->next;
	last->next->prev = first->prev;

	head->prev->next = first;
	first->prev = head->prev;

	last->next = head;
	head->prev = last;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
list_is_first(const struct list_head *list, const struct list_head *head)
{
	return list->prev == head;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
list_is_last(const struct list_head *list, const struct list_head *head)
{
	return list->next == head;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
list_is_head(const struct list_head *list, const struct list_head *head)
{
	return list == head;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
list_empty(const struct list_head *head)
{
	return ({
		       do {
			       __attribute__((__noreturn__)) extern void
			       __compiletime_assert_4(void) __attribute__((__error__(
				       "Unsupported access size for {READ,WRITE}_ONCE().")));
			       if (!((sizeof(head->next) == sizeof(char) ||
				      sizeof(head->next) == sizeof(short) ||
				      sizeof(head->next) == sizeof(int) ||
				      sizeof(head->next) == sizeof(long)) ||
				     sizeof(head->next) == sizeof(long long)))
				       __compiletime_assert_4();
		       } while (0);
		       (*(const volatile typeof(_Generic(
			       (head->next),
							char: (char)0,
							unsigned char: (
								unsigned char)0,
							signed char: (
								signed char)0,
							unsigned short: (
								unsigned short)0,
							signed short: (
								signed short)0,
							unsigned int: (
								unsigned int)0,
							signed int: (
								signed int)0,
							unsigned long: (
								unsigned long)0,
							signed long: (
								signed long)0,
							unsigned long long: (
								unsigned long long)0,
							signed long long: (
								signed long long)0,
							default: (head->next)))
				  *)&(head->next));
	       }) == head;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_del_init_careful(struct list_head *entry)
{
	__list_del_entry(entry);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_5(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(entry->prev) == sizeof(char) ||
			       sizeof(entry->prev) == sizeof(short) ||
			       sizeof(entry->prev) == sizeof(int) ||
			       sizeof(entry->prev) == sizeof(long)) ||
			      sizeof(entry->prev) == sizeof(long long)))
				__compiletime_assert_5();
		} while (0);
		do {
			*(volatile typeof(entry->prev) *)&(entry->prev) =
				(entry);
		} while (0);
	} while (0);
	do {
		do {
		} while (0);
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_6(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*&entry->next) == sizeof(char) ||
				       sizeof(*&entry->next) == sizeof(short) ||
				       sizeof(*&entry->next) == sizeof(int) ||
				       sizeof(*&entry->next) == sizeof(long))))
					__compiletime_assert_6();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_7(void) __attribute__((__error__(
						"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*&entry->next) ==
						       sizeof(char) ||
					       sizeof(*&entry->next) ==
						       sizeof(short) ||
					       sizeof(*&entry->next) ==
						       sizeof(int) ||
					       sizeof(*&entry->next) ==
						       sizeof(long)) ||
					      sizeof(*&entry->next) ==
						      sizeof(long long)))
						__compiletime_assert_7();
				} while (0);
				do {
					*(volatile typeof(*&entry->next) *)&(
						*&entry->next) = (entry);
				} while (0);
			} while (0);
		} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
list_empty_careful(const struct list_head *head)
{
	struct list_head *next = ({
		typeof(*&head->next) ___p1 = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_8(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*&head->next) == sizeof(char) ||
				       sizeof(*&head->next) == sizeof(short) ||
				       sizeof(*&head->next) == sizeof(int) ||
				       sizeof(*&head->next) == sizeof(long)) ||
				      sizeof(*&head->next) ==
					      sizeof(long long)))
					__compiletime_assert_8();
			} while (0);
			(*(const volatile typeof(_Generic(
				(*&head->next),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 *&head->next)))
				   *)&(*&head->next));
		});
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_9(void) __attribute__((__error__(
				"Need native word sized stores/loads for atomicity.")));
			if (!((sizeof(*&head->next) == sizeof(char) ||
			       sizeof(*&head->next) == sizeof(short) ||
			       sizeof(*&head->next) == sizeof(int) ||
			       sizeof(*&head->next) == sizeof(long))))
				__compiletime_assert_9();
		} while (0);
		__asm__ __volatile__("" : : : "memory");
		___p1;
	});
	return list_is_head(next, head) &&
	       (next == ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_10(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(head->prev) == sizeof(char) ||
				       sizeof(head->prev) == sizeof(short) ||
				       sizeof(head->prev) == sizeof(int) ||
				       sizeof(head->prev) == sizeof(long)) ||
				      sizeof(head->prev) == sizeof(long long)))
					__compiletime_assert_10();
			} while (0);
			(*(const volatile typeof(_Generic(
				(head->prev),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (head->prev)))
				   *)&(head->prev));
		}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_rotate_left(struct list_head *head)
{
	struct list_head *first;

	if (!list_empty(head)) {
		first = head->next;
		list_move_tail(first, head);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_rotate_to_front(struct list_head *list, struct list_head *head)
{
	list_move_tail(head, list);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
list_is_singular(const struct list_head *head)
{
	return !list_empty(head) && (head->next == head->prev);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__list_cut_position(struct list_head *list, struct list_head *head,
		    struct list_head *entry)
{
	struct list_head *new_first = entry->next;
	list->next = head->next;
	list->next->prev = list;
	list->prev = entry;
	entry->next = list;
	head->next = new_first;
	new_first->prev = head;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_cut_position(struct list_head *list, struct list_head *head,
		  struct list_head *entry)
{
	if (list_empty(head))
		return;
	if (list_is_singular(head) && !list_is_head(entry, head) &&
	    (entry != head->next))
		return;
	if (list_is_head(entry, head))
		INIT_LIST_HEAD(list);
	else
		__list_cut_position(list, head, entry);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_cut_before(struct list_head *list, struct list_head *head,
		struct list_head *entry)
{
	if (head->next == entry) {
		INIT_LIST_HEAD(list);
		return;
	}
	list->next = head->next;
	list->next->prev = list;
	list->prev = entry->prev;
	list->prev->next = list;
	head->next = entry;
	entry->prev = head;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__list_splice(const struct list_head *list, struct list_head *prev,
	      struct list_head *next)
{
	struct list_head *first = list->next;
	struct list_head *last = list->prev;

	first->prev = prev;
	prev->next = first;

	last->next = next;
	next->prev = last;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_splice(const struct list_head *list, struct list_head *head)
{
	if (!list_empty(list))
		__list_splice(list, head, head->next);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_splice_tail(struct list_head *list, struct list_head *head)
{
	if (!list_empty(list))
		__list_splice(list, head->prev, head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_splice_init(struct list_head *list, struct list_head *head)
{
	if (!list_empty(list)) {
		__list_splice(list, head, head->next);
		INIT_LIST_HEAD(list);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_splice_tail_init(struct list_head *list, struct list_head *head)
{
	if (!list_empty(list)) {
		__list_splice(list, head->prev, head);
		INIT_LIST_HEAD(list);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
list_count_nodes(struct list_head *head)
{
	struct list_head *pos;
	size_t count = 0;

	for (pos = (head)->next; !list_is_head(pos, (head)); pos = pos->next)
		count++;

	return count;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
INIT_HLIST_NODE(struct hlist_node *h)
{
	h->next = ((void *)0);
	h->pprev = ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
hlist_unhashed(const struct hlist_node *h)
{
	return !h->pprev;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
hlist_unhashed_lockless(const struct hlist_node *h)
{
	return !({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_11(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(h->pprev) == sizeof(char) ||
			       sizeof(h->pprev) == sizeof(short) ||
			       sizeof(h->pprev) == sizeof(int) ||
			       sizeof(h->pprev) == sizeof(long)) ||
			      sizeof(h->pprev) == sizeof(long long)))
				__compiletime_assert_11();
		} while (0);
		(*(const volatile typeof(_Generic((h->pprev),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (h->pprev)))
			   *)&(h->pprev));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
hlist_empty(const struct hlist_head *h)
{
	return !({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_12(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(h->first) == sizeof(char) ||
			       sizeof(h->first) == sizeof(short) ||
			       sizeof(h->first) == sizeof(int) ||
			       sizeof(h->first) == sizeof(long)) ||
			      sizeof(h->first) == sizeof(long long)))
				__compiletime_assert_12();
		} while (0);
		(*(const volatile typeof(_Generic((h->first),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (h->first)))
			   *)&(h->first));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__hlist_del(struct hlist_node *n)
{
	struct hlist_node *next = n->next;
	struct hlist_node **pprev = n->pprev;

	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_13(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*pprev) == sizeof(char) ||
			       sizeof(*pprev) == sizeof(short) ||
			       sizeof(*pprev) == sizeof(int) ||
			       sizeof(*pprev) == sizeof(long)) ||
			      sizeof(*pprev) == sizeof(long long)))
				__compiletime_assert_13();
		} while (0);
		do {
			*(volatile typeof(*pprev) *)&(*pprev) = (next);
		} while (0);
	} while (0);
	if (next)
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_14(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(next->pprev) == sizeof(char) ||
				       sizeof(next->pprev) == sizeof(short) ||
				       sizeof(next->pprev) == sizeof(int) ||
				       sizeof(next->pprev) == sizeof(long)) ||
				      sizeof(next->pprev) == sizeof(long long)))
					__compiletime_assert_14();
			} while (0);
			do {
				*(volatile typeof(next->pprev) *)&(
					next->pprev) = (pprev);
			} while (0);
		} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_del(struct hlist_node *n)
{
	__hlist_del(n);
	n->next = ((void *)0x100 + (0xdead000000000000UL));
	n->pprev = ((void *)0x122 + (0xdead000000000000UL));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_del_init(struct hlist_node *n)
{
	if (!hlist_unhashed(n)) {
		__hlist_del(n);
		INIT_HLIST_NODE(n);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_add_head(struct hlist_node *n, struct hlist_head *h)
{
	struct hlist_node *first = h->first;
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_15(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->next) == sizeof(char) ||
			       sizeof(n->next) == sizeof(short) ||
			       sizeof(n->next) == sizeof(int) ||
			       sizeof(n->next) == sizeof(long)) ||
			      sizeof(n->next) == sizeof(long long)))
				__compiletime_assert_15();
		} while (0);
		do {
			*(volatile typeof(n->next) *)&(n->next) = (first);
		} while (0);
	} while (0);
	if (first)
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_16(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(first->pprev) == sizeof(char) ||
				       sizeof(first->pprev) == sizeof(short) ||
				       sizeof(first->pprev) == sizeof(int) ||
				       sizeof(first->pprev) == sizeof(long)) ||
				      sizeof(first->pprev) ==
					      sizeof(long long)))
					__compiletime_assert_16();
			} while (0);
			do {
				*(volatile typeof(first->pprev) *)&(
					first->pprev) = (&n->next);
			} while (0);
		} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_17(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(h->first) == sizeof(char) ||
			       sizeof(h->first) == sizeof(short) ||
			       sizeof(h->first) == sizeof(int) ||
			       sizeof(h->first) == sizeof(long)) ||
			      sizeof(h->first) == sizeof(long long)))
				__compiletime_assert_17();
		} while (0);
		do {
			*(volatile typeof(h->first) *)&(h->first) = (n);
		} while (0);
	} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_18(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->pprev) == sizeof(char) ||
			       sizeof(n->pprev) == sizeof(short) ||
			       sizeof(n->pprev) == sizeof(int) ||
			       sizeof(n->pprev) == sizeof(long)) ||
			      sizeof(n->pprev) == sizeof(long long)))
				__compiletime_assert_18();
		} while (0);
		do {
			*(volatile typeof(n->pprev) *)&(n->pprev) = (&h->first);
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_add_before(struct hlist_node *n, struct hlist_node *next)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_19(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->pprev) == sizeof(char) ||
			       sizeof(n->pprev) == sizeof(short) ||
			       sizeof(n->pprev) == sizeof(int) ||
			       sizeof(n->pprev) == sizeof(long)) ||
			      sizeof(n->pprev) == sizeof(long long)))
				__compiletime_assert_19();
		} while (0);
		do {
			*(volatile typeof(n->pprev) *)&(n->pprev) =
				(next->pprev);
		} while (0);
	} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_20(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->next) == sizeof(char) ||
			       sizeof(n->next) == sizeof(short) ||
			       sizeof(n->next) == sizeof(int) ||
			       sizeof(n->next) == sizeof(long)) ||
			      sizeof(n->next) == sizeof(long long)))
				__compiletime_assert_20();
		} while (0);
		do {
			*(volatile typeof(n->next) *)&(n->next) = (next);
		} while (0);
	} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_21(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(next->pprev) == sizeof(char) ||
			       sizeof(next->pprev) == sizeof(short) ||
			       sizeof(next->pprev) == sizeof(int) ||
			       sizeof(next->pprev) == sizeof(long)) ||
			      sizeof(next->pprev) == sizeof(long long)))
				__compiletime_assert_21();
		} while (0);
		do {
			*(volatile typeof(next->pprev) *)&(next->pprev) =
				(&n->next);
		} while (0);
	} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_22(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*(n->pprev)) == sizeof(char) ||
			       sizeof(*(n->pprev)) == sizeof(short) ||
			       sizeof(*(n->pprev)) == sizeof(int) ||
			       sizeof(*(n->pprev)) == sizeof(long)) ||
			      sizeof(*(n->pprev)) == sizeof(long long)))
				__compiletime_assert_22();
		} while (0);
		do {
			*(volatile typeof(*(n->pprev)) *)&(*(n->pprev)) = (n);
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_add_behind(struct hlist_node *n, struct hlist_node *prev)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_23(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->next) == sizeof(char) ||
			       sizeof(n->next) == sizeof(short) ||
			       sizeof(n->next) == sizeof(int) ||
			       sizeof(n->next) == sizeof(long)) ||
			      sizeof(n->next) == sizeof(long long)))
				__compiletime_assert_23();
		} while (0);
		do {
			*(volatile typeof(n->next) *)&(n->next) = (prev->next);
		} while (0);
	} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_24(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(prev->next) == sizeof(char) ||
			       sizeof(prev->next) == sizeof(short) ||
			       sizeof(prev->next) == sizeof(int) ||
			       sizeof(prev->next) == sizeof(long)) ||
			      sizeof(prev->next) == sizeof(long long)))
				__compiletime_assert_24();
		} while (0);
		do {
			*(volatile typeof(prev->next) *)&(prev->next) = (n);
		} while (0);
	} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_25(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->pprev) == sizeof(char) ||
			       sizeof(n->pprev) == sizeof(short) ||
			       sizeof(n->pprev) == sizeof(int) ||
			       sizeof(n->pprev) == sizeof(long)) ||
			      sizeof(n->pprev) == sizeof(long long)))
				__compiletime_assert_25();
		} while (0);
		do {
			*(volatile typeof(n->pprev) *)&(n->pprev) =
				(&prev->next);
		} while (0);
	} while (0);

	if (n->next)
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_26(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(n->next->pprev) == sizeof(char) ||
				       sizeof(n->next->pprev) == sizeof(short) ||
				       sizeof(n->next->pprev) == sizeof(int) ||
				       sizeof(n->next->pprev) == sizeof(long)) ||
				      sizeof(n->next->pprev) ==
					      sizeof(long long)))
					__compiletime_assert_26();
			} while (0);
			do {
				*(volatile typeof(n->next->pprev) *)&(
					n->next->pprev) = (&n->next);
			} while (0);
		} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_add_fake(struct hlist_node *n)
{
	n->pprev = &n->next;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
hlist_fake(struct hlist_node *h)
{
	return h->pprev == &h->next;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
hlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)
{
	return !n->next && n->pprev == &h->first;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_move_list(struct hlist_head *old, struct hlist_head *new)
{
	new->first = old->first;
	if (new->first)
		new->first->pprev = &new->first;
	old->first = ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_splice_init(struct hlist_head *from, struct hlist_node *last,
		  struct hlist_head *to)
{
	if (to->first)
		to->first->pprev = &last->next;
	last->next = to->first;
	to->first = from->first;
	from->first->pprev = &to->first;
	from->first = ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
hlist_count_nodes(struct hlist_head *head)
{
	struct hlist_node *pos;
	size_t count = 0;

	for (pos = (head)->first; pos; pos = pos->next)
		count++;

	return count;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__warn_unused_result__))
const volatile void *
__must_check_fn(const volatile void *val)
{
	return val;
}

extern void __bad_size_call_parameter(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__this_cpu_preempt_check(const char *op)
{
}
extern unsigned long __per_cpu_offset[64];
extern void setup_per_cpu_areas(void);

extern __attribute__((
	section(".data..percpu"
		"..read_mostly"))) __typeof__(unsigned long) this_cpu_off;
struct task_struct;

struct pcpu_hot {
	union {
		struct {
			struct task_struct *current_task;
			int preempt_count;
			int cpu_number;

			u64 call_depth;

			unsigned long top_of_stack;
			void *hardirq_stack_ptr;
			u16 softirq_pending;

			bool hardirq_stack_inuse;
		};
		u8 pad[64];
	};
};
_Static_assert(sizeof(struct pcpu_hot) == 64, "sizeof(struct pcpu_hot) == 64");

extern __attribute__((
	section(".data..percpu"
		"..shared_aligned"))) __typeof__(struct pcpu_hot) pcpu_hot
	__attribute__((__aligned__((1 << (6)))));

extern __attribute__((section(
	".data..percpu"
	"..shared_aligned"))) __typeof__(const struct pcpu_hot) const_pcpu_hot
	__attribute__((__aligned__((1 << (6)))));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct task_struct *
get_current(void)
{
	if (0)
		return ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_27(void) __attribute__((
					__error__("BUILD_BUG failed")));
				if (!(!(1)))
					__compiletime_assert_27();
			} while (0);
			(typeof(const_pcpu_hot.current_task))0;
		});

	return ({
		typeof(pcpu_hot.current_task) pscr_ret__;
		do {
			const void *__vpp_verify =
				(typeof((&(pcpu_hot.current_task)) +
					0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		switch (sizeof(pcpu_hot.current_task)) {
		case 1:
			pscr_ret__ = ({
				u8 pfo_val__;
				asm("mov"
				    "b "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "a[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "q"(pfo_val__)
				    : [var] "i"(&(pcpu_hot.current_task)));
				(typeof(pcpu_hot.current_task))(unsigned long)
					pfo_val__;
			});
			break;
		case 2:
			pscr_ret__ = ({
				u16 pfo_val__;
				asm("mov"
				    "w "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "a[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "r"(pfo_val__)
				    : [var] "i"(&(pcpu_hot.current_task)));
				(typeof(pcpu_hot.current_task))(unsigned long)
					pfo_val__;
			});
			break;
		case 4:
			pscr_ret__ = ({
				u32 pfo_val__;
				asm("mov"
				    "l "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "a[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "r"(pfo_val__)
				    : [var] "i"(&(pcpu_hot.current_task)));
				(typeof(pcpu_hot.current_task))(unsigned long)
					pfo_val__;
			});
			break;
		case 8:
			pscr_ret__ = ({
				u64 pfo_val__;
				asm("mov"
				    "q "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "a[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "r"(pfo_val__)
				    : [var] "i"(&(pcpu_hot.current_task)));
				(typeof(pcpu_hot.current_task))(unsigned long)
					pfo_val__;
			});
			break;
		default:
			__bad_size_call_parameter();
			break;
		}
		pscr_ret__;
	});
}

struct static_call_site {
	s32 addr;
	s32 key;
};
struct static_call_key {
	void *func;
	union {
		unsigned long type;
		struct static_call_mod *mods;
		struct static_call_site *sites;
	};
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
preempt_count(void)
{
	return ({
		       u32 pfo_val__;
		       asm("mov"
			   "l "
			   "%%"
			   "gs"
			   ":"
			   "%"
			   "[var]"
			   ", "
			   "%[val]"
			   : [val] "="
				   "r"(pfo_val__)
			   : [var] "m"((*(typeof(*(&(pcpu_hot.preempt_count)))
						  *)(uintptr_t)(&(
				   pcpu_hot.preempt_count)))));
		       (typeof(pcpu_hot.preempt_count))(unsigned long)pfo_val__;
	       }) &
	       ~0x80000000;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
preempt_count_set(int pc)
{
	int old, new;

	old = ({
		u32 pfo_val__;
		asm("mov"
		    "l "
		    "%%"
		    "gs"
		    ":"
		    "%"
		    "[var]"
		    ", "
		    "%[val]"
		    : [val] "="
			    "r"(pfo_val__)
		    : [var] "m"((*(
			    typeof(*(&(pcpu_hot.preempt_count))) *)(uintptr_t)(&(
			    pcpu_hot.preempt_count)))));
		(typeof(pcpu_hot.preempt_count))(unsigned long)pfo_val__;
	});
	do {
		new = (old & 0x80000000) | (pc & ~0x80000000);
	} while (!({
		bool success;
		u32 *pco_oval__ = (u32 *)(&old);
		u32 pco_old__ = *pco_oval__;
		u32 pco_new__ = ((u32)(((unsigned long)new) & 0xffffffff));
		asm("cmpxchg"
		    "l "
		    "%[nval]"
		    ", "
		    "%%"
		    "gs"
		    ":"
		    "%"
		    "[var]"
		    "\n\t/* output condition code "
		    "z"
		    "*/\n"
		    : "=@cc"
		      "z"(success),
		      [oval] "+a"(pco_old__),
		      [var] "+m"((*(
			      typeof(*(&(pcpu_hot.preempt_count)))
				      *)(uintptr_t)(&(pcpu_hot.preempt_count))))
		    : [nval] "r"(pco_new__)
		    : "memory");
		if (__builtin_expect(!!(!success), 0))
			*pco_oval__ = pco_old__;
		__builtin_expect(!!(success), 1);
	}));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
set_preempt_need_resched(void)
{
	do {
		u32 pto_val__ =
			((u32)(((unsigned long)~0x80000000) & 0xffffffff));
		if (0) {
			typeof((pcpu_hot.preempt_count)) pto_tmp__;
			pto_tmp__ = (~0x80000000);
			(void)pto_tmp__;
		}
		asm("and"
		    "l "
		    "%[val]"
		    ", "
		    "%%"
		    "gs"
		    ":"
		    "%"
		    "[var]"
		    : [var] "+m"((*(
			    typeof(*(&((pcpu_hot.preempt_count))))
				    *)(uintptr_t)(&((pcpu_hot.preempt_count)))))
		    : [val] "ri"(pto_val__));
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
clear_preempt_need_resched(void)
{
	do {
		u32 pto_val__ =
			((u32)(((unsigned long)0x80000000) & 0xffffffff));
		if (0) {
			typeof((pcpu_hot.preempt_count)) pto_tmp__;
			pto_tmp__ = (0x80000000);
			(void)pto_tmp__;
		}
		asm("or"
		    "l "
		    "%[val]"
		    ", "
		    "%%"
		    "gs"
		    ":"
		    "%"
		    "[var]"
		    : [var] "+m"((*(
			    typeof(*(&((pcpu_hot.preempt_count))))
				    *)(uintptr_t)(&((pcpu_hot.preempt_count)))))
		    : [val] "ri"(pto_val__));
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
test_preempt_need_resched(void)
{
	return !(({
			 u32 pfo_val__;
			 asm("mov"
			     "l "
			     "%%"
			     "gs"
			     ":"
			     "%"
			     "[var]"
			     ", "
			     "%[val]"
			     : [val] "="
				     "r"(pfo_val__)
			     : [var] "m"((*(typeof(*(&(pcpu_hot.preempt_count)))
						    *)(uintptr_t)(&(
				     pcpu_hot.preempt_count)))));
			 (typeof(pcpu_hot.preempt_count))(unsigned long)
				 pfo_val__;
		 }) &
		 0x80000000);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__preempt_count_add(int val)
{
	do {
		const int pao_ID__ = (__builtin_constant_p(val) &&
				      ((val) == 1 || (val) == -1)) ?
					     (int)(val) :
					     0;
		if (0) {
			typeof((pcpu_hot.preempt_count)) pao_tmp__;
			pao_tmp__ = (val);
			(void)pao_tmp__;
		}
		if (pao_ID__ == 1)
			({
				asm("inc"
				    "l "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "[var]"
				    : [var] "+m"((*(typeof(*(
					    &((pcpu_hot.preempt_count))))
							    *)(uintptr_t)(&(
					    (pcpu_hot.preempt_count))))));
			});
		else if (pao_ID__ == -1)
			({
				asm("dec"
				    "l "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "[var]"
				    : [var] "+m"((*(typeof(*(
					    &((pcpu_hot.preempt_count))))
							    *)(uintptr_t)(&(
					    (pcpu_hot.preempt_count))))));
			});
		else
			do {
				u32 pto_val__ = ((u32)(((unsigned long)val) &
						       0xffffffff));
				if (0) {
					typeof((pcpu_hot.preempt_count))
						pto_tmp__;
					pto_tmp__ = (val);
					(void)pto_tmp__;
				}
				asm("add"
				    "l "
				    "%[val]"
				    ", "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "[var]"
				    : [var] "+m"((*(typeof(*(
					    &((pcpu_hot.preempt_count))))
							    *)(uintptr_t)(&(
					    (pcpu_hot.preempt_count)))))
				    : [val] "ri"(pto_val__));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__preempt_count_sub(int val)
{
	do {
		const int pao_ID__ = (__builtin_constant_p(-val) &&
				      ((-val) == 1 || (-val) == -1)) ?
					     (int)(-val) :
					     0;
		if (0) {
			typeof((pcpu_hot.preempt_count)) pao_tmp__;
			pao_tmp__ = (-val);
			(void)pao_tmp__;
		}
		if (pao_ID__ == 1)
			({
				asm("inc"
				    "l "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "[var]"
				    : [var] "+m"((*(typeof(*(
					    &((pcpu_hot.preempt_count))))
							    *)(uintptr_t)(&(
					    (pcpu_hot.preempt_count))))));
			});
		else if (pao_ID__ == -1)
			({
				asm("dec"
				    "l "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "[var]"
				    : [var] "+m"((*(typeof(*(
					    &((pcpu_hot.preempt_count))))
							    *)(uintptr_t)(&(
					    (pcpu_hot.preempt_count))))));
			});
		else
			do {
				u32 pto_val__ = ((u32)(((unsigned long)-val) &
						       0xffffffff));
				if (0) {
					typeof((pcpu_hot.preempt_count))
						pto_tmp__;
					pto_tmp__ = (-val);
					(void)pto_tmp__;
				}
				asm("add"
				    "l "
				    "%[val]"
				    ", "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "[var]"
				    : [var] "+m"((*(typeof(*(
					    &((pcpu_hot.preempt_count))))
							    *)(uintptr_t)(&(
					    (pcpu_hot.preempt_count)))))
				    : [val] "ri"(pto_val__));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__preempt_count_dec_and_test(void)
{
	return ({
		bool c;
		asm volatile(
			"decl"
			" "
			"%%"
			"gs"
			":"
			"%"
			"[var]"
			"\n\t/* output condition code "
			"e"
			"*/\n"
			: [var] "+m"((*(typeof(*(&(pcpu_hot.preempt_count)))
						*)(uintptr_t)(&(
				  pcpu_hot.preempt_count)))),
			  "=@cc"
			  "e"(c)
			:
			: "memory");
		c;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
should_resched(int preempt_offset)
{
	return __builtin_expect(
		!!(({
			   u32 pfo_val__;
			   asm("mov"
			       "l "
			       "%%"
			       "gs"
			       ":"
			       "%"
			       "[var]"
			       ", "
			       "%[val]"
			       : [val] "="
				       "r"(pfo_val__)
			       : [var] "m"(
				       (*(typeof(*(&(pcpu_hot.preempt_count)))
						  *)(uintptr_t)(&(
					       pcpu_hot.preempt_count)))));
			   (typeof(pcpu_hot.preempt_count))(unsigned long)
				   pfo_val__;
		   }) == preempt_offset),
		0);
}

extern void preempt_schedule(void);
extern void preempt_schedule_thunk(void);

extern void preempt_schedule_notrace(void);
extern void preempt_schedule_notrace_thunk(void);

extern struct static_call_key __SCK__preempt_schedule;
extern typeof(preempt_schedule_thunk) __SCT__preempt_schedule;
;

extern struct static_call_key __SCK__preempt_schedule_notrace;
extern typeof(preempt_schedule_notrace_thunk) __SCT__preempt_schedule_notrace;
;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned char
interrupt_context_level(void)
{
	unsigned long pc = preempt_count();
	unsigned char level = 0;

	level += !!(pc & ((((1UL << (4)) - 1) << (((0 + 8) + 8) + 4))));
	level += !!(pc & ((((1UL << (4)) - 1) << (((0 + 8) + 8) + 4)) |
			  (((1UL << (4)) - 1) << ((0 + 8) + 8))));
	level += !!(pc &
		    ((((1UL << (4)) - 1) << (((0 + 8) + 8) + 4)) |
		     (((1UL << (4)) - 1) << ((0 + 8) + 8)) | (1UL << (0 + 8))));

	return level;
}
extern void migrate_disable(void);
extern void migrate_enable(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
preempt_enable_nested(void)
{
	if (0)
		do {
			__asm__ __volatile__("" : : : "memory");
			if (__builtin_expect(!!(__preempt_count_dec_and_test()),
					     0))
				do {
					static void *__attribute__((__used__))
					__attribute__((__section__(
						".discard.addressable")))
					__UNIQUE_ID___addressable___SCK__preempt_schedule28 =
						(void *)(uintptr_t)&__SCK__preempt_schedule;
					;
					asm volatile(
						"call "
						"__SCT__preempt_schedule"
						: "+r"(current_stack_pointer));
				} while (0);
		} while (0);
}

typedef struct {
	void *lock;
	;
} class_preempt_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_preempt_destructor(class_preempt_t *_T)
{
	if (_T->lock) {
		do {
			__asm__ __volatile__("" : : : "memory");
			if (__builtin_expect(!!(__preempt_count_dec_and_test()),
					     0))
				do {
					static void *__attribute__((__used__))
					__attribute__((__section__(
						".discard.addressable")))
					__UNIQUE_ID___addressable___SCK__preempt_schedule29 =
						(void *)(uintptr_t)&__SCK__preempt_schedule;
					;
					asm volatile(
						"call "
						"__SCT__preempt_schedule"
						: "+r"(current_stack_pointer));
				} while (0);
		} while (0);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_preempt_lock_ptr(class_preempt_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_preempt_t
class_preempt_constructor(void)
{
	class_preempt_t _t = { .lock = (void *)1 },
			*_T __attribute__((__unused__)) = &_t;
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	return _t;
}
typedef struct {
	void *lock;
	;
} class_preempt_notrace_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_preempt_notrace_destructor(class_preempt_notrace_t *_T)
{
	if (_T->lock) {
		do {
			__asm__ __volatile__("" : : : "memory");
			if (__builtin_expect(!!(__preempt_count_dec_and_test()),
					     0))
				do {
					static void *__attribute__((__used__))
					__attribute__((__section__(
						".discard.addressable")))
					__UNIQUE_ID___addressable___SCK__preempt_schedule_notrace30 =
						(void *)(uintptr_t)&__SCK__preempt_schedule_notrace;
					;
					asm volatile(
						"call "
						"__SCT__preempt_schedule_notrace"
						: "+r"(current_stack_pointer));
				} while (0);
		} while (0);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_preempt_notrace_lock_ptr(class_preempt_notrace_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_preempt_notrace_t
class_preempt_notrace_constructor(void)
{
	class_preempt_notrace_t _t = { .lock = (void *)1 },
				*_T __attribute__((__unused__)) = &_t;
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	return _t;
}
typedef struct {
	void *lock;
	;
} class_migrate_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_migrate_destructor(class_migrate_t *_T)
{
	if (_T->lock) {
		migrate_enable();
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_migrate_lock_ptr(class_migrate_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_migrate_t
class_migrate_constructor(void)
{
	class_migrate_t _t = { .lock = (void *)1 },
			*_T __attribute__((__unused__)) = &_t;
	migrate_disable();
	return _t;
}

extern bool preempt_model_none(void);
extern bool preempt_model_voluntary(void);
extern bool preempt_model_full(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
preempt_model_rt(void)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
preempt_model_preemptible(void)
{
	return preempt_model_full() || preempt_model_rt();
}

typedef int (*initcall_t)(void);
typedef void (*exitcall_t)(void);

typedef int initcall_entry_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) initcall_t
initcall_from_entry(initcall_entry_t *entry)
{
	return offset_to_ptr(entry);
}
extern initcall_entry_t __con_initcall_start[], __con_initcall_end[];

typedef void (*ctor_fn_t)(void);

struct file_system_type;

extern int do_one_initcall(initcall_t fn);
extern char __attribute__((__section__(".init.data"))) boot_command_line[];
extern char *saved_command_line;
extern unsigned int saved_command_line_len;
extern unsigned int reset_devices;

void setup_arch(char **);
void prepare_namespace(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
init_rootfs(void);

void init_IRQ(void);
void time_init(void);
void poking_init(void);
void pgtable_cache_init(void);

extern initcall_entry_t __initcall_start[];
extern initcall_entry_t __initcall0_start[];
extern initcall_entry_t __initcall1_start[];
extern initcall_entry_t __initcall2_start[];
extern initcall_entry_t __initcall3_start[];
extern initcall_entry_t __initcall4_start[];
extern initcall_entry_t __initcall5_start[];
extern initcall_entry_t __initcall6_start[];
extern initcall_entry_t __initcall7_start[];
extern initcall_entry_t __initcall_end[];

extern struct file_system_type rootfs_fs_type;

extern bool rodata_enabled;
void mark_rodata_ro(void);

extern void (*late_time_init)(void);

extern bool initcall_debug;
struct obs_kernel_param {
	const char *str;
	int (*setup_func)(char *);
	int early;
};

extern const struct obs_kernel_param __setup_start[], __setup_end[];
void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
parse_early_param(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
parse_early_options(char *cmdline);
enum cc_attr {
	CC_ATTR_MEM_ENCRYPT,
	CC_ATTR_HOST_MEM_ENCRYPT,
	CC_ATTR_GUEST_MEM_ENCRYPT,
	CC_ATTR_GUEST_STATE_ENCRYPT,
	CC_ATTR_GUEST_UNROLL_STRING_IO,

	CC_ATTR_GUEST_SEV_SNP,

	CC_ATTR_HOST_SEV_SNP,
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
cc_platform_has(enum cc_attr attr)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cc_platform_set(enum cc_attr attr)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cc_platform_clear(enum cc_attr attr)
{
}

struct boot_params;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mem_encrypt_init(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
	__attribute__((__section__(".init.text"))) __attribute__((__cold__))
	mem_encrypt_setup_arch(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
	__attribute__((__section__(".init.text"))) __attribute__((__cold__))
	sme_early_encrypt(resource_size_t paddr, unsigned long size)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
	__attribute__((__section__(".init.text"))) __attribute__((__cold__))
	sme_early_decrypt(resource_size_t paddr, unsigned long size)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
	__attribute__((__section__(".init.text"))) __attribute__((__cold__))
	sme_map_bootdata(char *real_mode_data)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
	__attribute__((__section__(".init.text"))) __attribute__((__cold__))
	sme_unmap_bootdata(char *real_mode_data)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
	__attribute__((__section__(".init.text"))) __attribute__((__cold__))
	sme_early_init(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sme_encrypt_kernel(struct boot_params *bp)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sme_enable(struct boot_params *bp)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sev_es_init_vc_handling(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__section__(".init.text"))) __attribute__((__cold__))
	early_set_memory_decrypted(unsigned long vaddr, unsigned long size)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__section__(".init.text"))) __attribute__((__cold__))
	early_set_memory_encrypted(unsigned long vaddr, unsigned long size)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
	__attribute__((__section__(".init.text"))) __attribute__((__cold__))
	early_set_mem_enc_dec_hypercall(unsigned long vaddr, unsigned long size,
					bool enc)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mem_encrypt_free_decrypted_mem(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
sme_get_me_mask(void)
{
	return 0;
}

void add_encrypt_protection_map(void);
extern char __start_bss_decrypted[], __end_bss_decrypted[],
	__start_bss_decrypted_unused[];

extern bool static_key_initialized;

struct static_key {
	atomic_t enabled;
	union {
		unsigned long type;
		struct jump_entry *entries;
		struct static_key_mod *next;
	};
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_static_branch(struct static_key *key, bool branch)
{
	asm goto("1:"
		 "jmp %l[l_yes] # objtool NOPs this \n\t"
		 ".pushsection __jump_table,  \"aw\" \n\t"
		 " "
		 ".balign 8"
		 " "
		 "\n\t"
		 ".long 1b - . \n\t"
		 ".long %l[l_yes] - . \n\t"
		 " "
		 ".quad"
		 " "
		 "%c0 + %c1 - .\n\t"
		 ".popsection \n\t"
		 :
		 : "i"(key), "i"(2 | branch)
		 :
		 : l_yes);

	return false;
l_yes:
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_static_branch_jump(struct static_key *const key, const bool branch)
{
	asm goto("1:"
		 "jmp %l[l_yes]\n\t"
		 ".pushsection __jump_table,  \"aw\" \n\t"
		 " "
		 ".balign 8"
		 " "
		 "\n\t"
		 ".long 1b - . \n\t"
		 ".long %l[l_yes] - . \n\t"
		 " "
		 ".quad"
		 " "
		 "%c0 + %c1 - .\n\t"
		 ".popsection \n\t"
		 :
		 : "i"(key), "i"(branch)
		 :
		 : l_yes);

	return false;
l_yes:
	return true;
}

extern int arch_jump_entry_size(struct jump_entry *entry);

struct jump_entry {
	s32 code;
	s32 target;
	long key;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
jump_entry_code(const struct jump_entry *entry)
{
	return (unsigned long)&entry->code + entry->code;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
jump_entry_target(const struct jump_entry *entry)
{
	return (unsigned long)&entry->target + entry->target;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct static_key *
jump_entry_key(const struct jump_entry *entry)
{
	long offset = entry->key & ~3L;

	return (struct static_key *)((unsigned long)&entry->key + offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
jump_entry_is_branch(const struct jump_entry *entry)
{
	return (unsigned long)entry->key & 1UL;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
jump_entry_is_init(const struct jump_entry *entry)
{
	return (unsigned long)entry->key & 2UL;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
jump_entry_set_init(struct jump_entry *entry, bool set)
{
	if (set)
		entry->key |= 2;
	else
		entry->key &= ~2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
jump_entry_size(struct jump_entry *entry)
{
	return arch_jump_entry_size(entry);
}

enum jump_label_type {
	JUMP_LABEL_NOP = 0,
	JUMP_LABEL_JMP,
};

struct module;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
static_key_false(struct static_key *key)
{
	return arch_static_branch(key, false);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
static_key_true(struct static_key *key)
{
	return !arch_static_branch(key, true);
}

extern struct jump_entry __start___jump_table[];
extern struct jump_entry __stop___jump_table[];

extern void jump_label_init(void);
extern void jump_label_init_ro(void);
extern void jump_label_lock(void);
extern void jump_label_unlock(void);
extern void arch_jump_label_transform(struct jump_entry *entry,
				      enum jump_label_type type);
extern bool arch_jump_label_transform_queue(struct jump_entry *entry,
					    enum jump_label_type type);
extern void arch_jump_label_transform_apply(void);
extern int jump_label_text_reserved(void *start, void *end);
extern bool static_key_slow_inc(struct static_key *key);
extern bool static_key_fast_inc_not_disabled(struct static_key *key);
extern void static_key_slow_dec(struct static_key *key);
extern bool static_key_slow_inc_cpuslocked(struct static_key *key);
extern void static_key_slow_dec_cpuslocked(struct static_key *key);
extern int static_key_count(struct static_key *key);
extern void static_key_enable(struct static_key *key);
extern void static_key_disable(struct static_key *key);
extern void static_key_enable_cpuslocked(struct static_key *key);
extern void static_key_disable_cpuslocked(struct static_key *key);
extern enum jump_label_type jump_label_init_type(struct jump_entry *entry);
struct static_key_true {
	struct static_key key;
};

struct static_key_false {
	struct static_key key;
};
extern bool ____wrong_branch_error(void);

struct unwind_hint {
	u32 ip;
	s16 sp_offset;
	u8 sp_reg;
	u8 type;
	u8 signal;
};

struct orc_entry {
	s16 sp_offset;
	s16 bp_offset;

	unsigned sp_reg : 4;
	unsigned bp_reg : 4;
	unsigned type : 3;
	unsigned signal : 1;

} __attribute__((__packed__));
typedef u8 retpoline_thunk_t[32];
extern retpoline_thunk_t __x86_indirect_thunk_array[];
extern retpoline_thunk_t __x86_indirect_call_thunk_array[];
extern retpoline_thunk_t __x86_indirect_jump_thunk_array[];

extern void __x86_return_thunk(void);

extern void retbleed_return_thunk(void);

extern void srso_alias_untrain_ret(void);

extern void srso_return_thunk(void);
extern void srso_alias_return_thunk(void);

extern void retbleed_return_thunk(void);
extern void srso_return_thunk(void);
extern void srso_alias_return_thunk(void);

extern void entry_untrain_ret(void);
extern void entry_ibpb(void);

extern void clear_bhb_loop(void);

extern void (*x86_return_thunk)(void);

extern void __warn_thunk(void);

extern void call_depth_return_thunk(void);

extern retpoline_thunk_t __x86_indirect_thunk_rax;
extern retpoline_thunk_t __x86_indirect_thunk_rcx;
extern retpoline_thunk_t __x86_indirect_thunk_rdx;
extern retpoline_thunk_t __x86_indirect_thunk_rbx;
extern retpoline_thunk_t __x86_indirect_thunk_rsp;
extern retpoline_thunk_t __x86_indirect_thunk_rbp;
extern retpoline_thunk_t __x86_indirect_thunk_rsi;
extern retpoline_thunk_t __x86_indirect_thunk_rdi;
extern retpoline_thunk_t __x86_indirect_thunk_r8;
extern retpoline_thunk_t __x86_indirect_thunk_r9;
extern retpoline_thunk_t __x86_indirect_thunk_r10;
extern retpoline_thunk_t __x86_indirect_thunk_r11;
extern retpoline_thunk_t __x86_indirect_thunk_r12;
extern retpoline_thunk_t __x86_indirect_thunk_r13;
extern retpoline_thunk_t __x86_indirect_thunk_r14;
extern retpoline_thunk_t __x86_indirect_thunk_r15;

extern retpoline_thunk_t __x86_indirect_call_thunk_rax;
extern retpoline_thunk_t __x86_indirect_call_thunk_rcx;
extern retpoline_thunk_t __x86_indirect_call_thunk_rdx;
extern retpoline_thunk_t __x86_indirect_call_thunk_rbx;
extern retpoline_thunk_t __x86_indirect_call_thunk_rsp;
extern retpoline_thunk_t __x86_indirect_call_thunk_rbp;
extern retpoline_thunk_t __x86_indirect_call_thunk_rsi;
extern retpoline_thunk_t __x86_indirect_call_thunk_rdi;
extern retpoline_thunk_t __x86_indirect_call_thunk_r8;
extern retpoline_thunk_t __x86_indirect_call_thunk_r9;
extern retpoline_thunk_t __x86_indirect_call_thunk_r10;
extern retpoline_thunk_t __x86_indirect_call_thunk_r11;
extern retpoline_thunk_t __x86_indirect_call_thunk_r12;
extern retpoline_thunk_t __x86_indirect_call_thunk_r13;
extern retpoline_thunk_t __x86_indirect_call_thunk_r14;
extern retpoline_thunk_t __x86_indirect_call_thunk_r15;

extern retpoline_thunk_t __x86_indirect_jump_thunk_rax;
extern retpoline_thunk_t __x86_indirect_jump_thunk_rcx;
extern retpoline_thunk_t __x86_indirect_jump_thunk_rdx;
extern retpoline_thunk_t __x86_indirect_jump_thunk_rbx;
extern retpoline_thunk_t __x86_indirect_jump_thunk_rsp;
extern retpoline_thunk_t __x86_indirect_jump_thunk_rbp;
extern retpoline_thunk_t __x86_indirect_jump_thunk_rsi;
extern retpoline_thunk_t __x86_indirect_jump_thunk_rdi;
extern retpoline_thunk_t __x86_indirect_jump_thunk_r8;
extern retpoline_thunk_t __x86_indirect_jump_thunk_r9;
extern retpoline_thunk_t __x86_indirect_jump_thunk_r10;
extern retpoline_thunk_t __x86_indirect_jump_thunk_r11;
extern retpoline_thunk_t __x86_indirect_jump_thunk_r12;
extern retpoline_thunk_t __x86_indirect_jump_thunk_r13;
extern retpoline_thunk_t __x86_indirect_jump_thunk_r14;
extern retpoline_thunk_t __x86_indirect_jump_thunk_r15;
enum spectre_v2_mitigation {
	SPECTRE_V2_NONE,
	SPECTRE_V2_RETPOLINE,
	SPECTRE_V2_LFENCE,
	SPECTRE_V2_EIBRS,
	SPECTRE_V2_EIBRS_RETPOLINE,
	SPECTRE_V2_EIBRS_LFENCE,
	SPECTRE_V2_IBRS,
};

enum spectre_v2_user_mitigation {
	SPECTRE_V2_USER_NONE,
	SPECTRE_V2_USER_STRICT,
	SPECTRE_V2_USER_STRICT_PREFERRED,
	SPECTRE_V2_USER_PRCTL,
	SPECTRE_V2_USER_SECCOMP,
};

enum ssb_mitigation {
	SPEC_STORE_BYPASS_NONE,
	SPEC_STORE_BYPASS_DISABLE,
	SPEC_STORE_BYPASS_PRCTL,
	SPEC_STORE_BYPASS_SECCOMP,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
alternative_msr_write(unsigned int msr, u64 val, unsigned int feature)
{
	asm volatile("# ALT: oldinstr\n"
		     "771:\n\t"
		     ""
		     "\n772:\n"
		     "# ALT: padding\n"
		     ".skip -((("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")) > 0) * "
		     "(("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")),0x90\n"
		     "773:\n"
		     ".pushsection .altinstructions,\"a\"\n"
		     " .long 771b - .\n"
		     " .long 774f - .\n"
		     " .4byte "
		     "%c[feature]"
		     "\n"
		     " .byte "
		     "773b-771b"
		     "\n"
		     " .byte "
		     "775f-774f"
		     "\n"
		     ".popsection\n"
		     ".pushsection .altinstr_replacement, \"ax\"\n"
		     "# ALT: replacement\n"
		     "774:\n\t"
		     "wrmsr"
		     "\n775:\n"
		     ".popsection\n"
		     :
		     : "c"(msr), "a"((u32)val),
		       "d"((u32)(val >> 32)), [feature] "i"(feature)
		     : "memory");
}

extern u64 x86_pred_cmd;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
indirect_branch_prediction_barrier(void)
{
	alternative_msr_write(0x00000049, x86_pred_cmd, (7 * 32 + 21));
}

extern u64 x86_spec_ctrl_base;
extern __attribute__((section(".data..percpu"
			      ""))) __typeof__(u64) x86_spec_ctrl_current;
extern void update_spec_ctrl_cond(u64 val);
extern u64 spec_ctrl_current(void);
extern struct static_key_false switch_to_cond_stibp;
extern struct static_key_false switch_mm_cond_ibpb;
extern struct static_key_false switch_mm_always_ibpb;

extern struct static_key_false mds_idle_clear;

extern struct static_key_false switch_mm_cond_l1d_flush;

extern struct static_key_false mmio_stale_data_clear;

extern u16 mds_verw_sel;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
vdso_encode_cpunode(int cpu, unsigned long node)
{
	return (node << 12) | cpu;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
vdso_read_cpunode(unsigned *cpu, unsigned *node)
{
	unsigned int p;
	asm __inline volatile("# ALT: oldinstr\n"
			      "771:\n\t"
			      "lsl %[seg],%[p]"
			      "\n772:\n"
			      "# ALT: padding\n"
			      ".skip -((("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")) > 0) * "
			      "(("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")),0x90\n"
			      "773:\n"
			      ".pushsection .altinstructions,\"a\"\n"
			      " .long 771b - .\n"
			      " .long 774f - .\n"
			      " .4byte "
			      "(16*32+22)"
			      "\n"
			      " .byte "
			      "773b-771b"
			      "\n"
			      " .byte "
			      "775f-774f"
			      "\n"
			      ".popsection\n"
			      ".pushsection .altinstr_replacement, \"ax\"\n"
			      "# ALT: replacement\n"
			      "774:\n\t"
			      ".byte 0xf3,0x0f,0xc7,0xf8"
			      "\n775:\n"
			      ".popsection\n"
			      : [p] "=a"(p)
			      : "i"(0), [seg] "r"((15 * 8 + 3)));

	if (cpu)
		*cpu = (p & 0xfff);
	if (node)
		*node = (p >> 12);
}
extern const char early_idt_handler_array[32][(9 + (4 * 1))];
extern void early_ignore_irq(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__loadsegment_fs(unsigned short value)
{
	asm volatile("						\n"
		     "1:	movw %0, %%fs			\n"
		     "2:					\n"

		     " .pushsection \"__ex_table\",\"a\"\n"
		     " .balign 4\n"
		     " .long ("
		     "1b"
		     ") - .\n"
		     " .long ("
		     "2b"
		     ") - .\n"
		     " .long "
		     "5"
		     " \n"
		     " .popsection\n"

		     :
		     : "rm"(value)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
mds_clear_cpu_buffers(void)
{
	static const u16 ds = (3 * 8);
	asm volatile("verw %[ds]" : : [ds] "m"(ds) : "cc");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
mds_idle_clear_cpu_buffers(void)
{
	if (({
		    bool branch;
		    if (__builtin_types_compatible_p(typeof(*&mds_idle_clear),
						     struct static_key_true))
			    branch = !arch_static_branch(
				    &(&mds_idle_clear)->key, true);
		    else if (__builtin_types_compatible_p(
				     typeof(*&mds_idle_clear),
				     struct static_key_false))
			    branch = !arch_static_branch_jump(
				    &(&mds_idle_clear)->key, true);
		    else
			    branch = ____wrong_branch_error();
		    __builtin_expect(!!(branch), 1);
	    }))
		mds_clear_cpu_buffers();
}

extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
native_save_fl(void);
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
native_save_fl(void)
{
	unsigned long flags;

	asm volatile("# __raw_save_flags\n\t"
		     "pushf ; pop %0"
		     : "=rm"(flags)
		     :
		     : "memory");

	return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
native_irq_disable(void)
{
	asm volatile("cli" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
native_irq_enable(void)
{
	asm volatile("sti" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
native_safe_halt(void)
{
	mds_idle_clear_cpu_buffers();
	asm volatile("sti; hlt" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
native_halt(void)
{
	mds_idle_clear_cpu_buffers();
	asm volatile("hlt" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
native_irqs_disabled_flags(unsigned long flags)
{
	return !(flags & (((1UL)) << (9)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
native_local_irq_save(void)
{
	unsigned long flags = native_save_fl();

	native_irq_disable();

	return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
native_local_irq_restore(unsigned long flags)
{
	if (!native_irqs_disabled_flags(flags))
		native_irq_enable();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
arch_local_save_flags(void)
{
	return native_save_fl();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_local_irq_disable(void)
{
	native_irq_disable();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_local_irq_enable(void)
{
	native_irq_enable();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_safe_halt(void)
{
	native_safe_halt();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
halt(void)
{
	native_halt();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
arch_local_irq_save(void)
{
	unsigned long flags = arch_local_save_flags();
	arch_local_irq_disable();
	return flags;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
arch_irqs_disabled_flags(unsigned long flags)
{
	return !(flags & (((1UL)) << (9)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
arch_irqs_disabled(void)
{
	unsigned long flags = arch_local_save_flags();

	return arch_irqs_disabled_flags(flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_local_irq_restore(unsigned long flags)
{
	if (!arch_irqs_disabled_flags(flags))
		arch_local_irq_enable();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_softirqs_on(unsigned long ip)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_softirqs_off(unsigned long ip)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_hardirqs_on_prepare(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_hardirqs_on(unsigned long ip)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_hardirqs_off(unsigned long ip)
{
}
typedef struct {
	void *lock;
	;
} class_irq_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_irq_destructor(class_irq_t *_T)
{
	if (_T->lock) {
		do {
			arch_local_irq_enable();
		} while (0);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_irq_lock_ptr(class_irq_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_irq_t
class_irq_constructor(void)
{
	class_irq_t _t = { .lock = (void *)1 },
		    *_T __attribute__((__unused__)) = &_t;
	do {
		arch_local_irq_disable();
	} while (0);
	return _t;
}
typedef struct {
	void *lock;
	unsigned long flags;
} class_irqsave_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_irqsave_destructor(class_irqsave_t *_T)
{
	if (_T->lock) {
		do {
			do {
				({
					unsigned long __dummy;
					typeof(_T->flags) __dummy2;
					(void)(&__dummy == &__dummy2);
					1;
				});
				do {
				} while (0);
				arch_local_irq_restore(_T->flags);
			} while (0);
		} while (0);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_irqsave_lock_ptr(class_irqsave_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_irqsave_t
class_irqsave_constructor(void)
{
	class_irqsave_t _t = { .lock = (void *)1 },
			*_T __attribute__((__unused__)) = &_t;
	do {
		do {
			({
				unsigned long __dummy;
				typeof(_T->flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			_T->flags = arch_local_irq_save();
		} while (0);
	} while (0);
	return _t;
}

struct pt_regs;

extern long (*panic_blink)(int state);
__attribute__((__format__(printf, 1, 2))) void panic(const char *fmt, ...)
	__attribute__((__noreturn__)) __attribute__((__cold__));
void nmi_panic(struct pt_regs *regs, const char *msg);
void check_panic_on_warn(const char *origin);
extern void oops_enter(void);
extern void oops_exit(void);
extern bool oops_may_print(void);

extern bool panic_triggering_all_cpu_backtrace;
extern int panic_timeout;
extern unsigned long panic_print;
extern int panic_on_oops;
extern int panic_on_unrecovered_nmi;
extern int panic_on_io_nmi;
extern int panic_on_warn;

extern unsigned long panic_on_taint;
extern bool panic_on_taint_nousertaint;

extern int sysctl_panic_on_rcu_stall;
extern int sysctl_max_rcu_stall_to_panic;
extern int sysctl_panic_on_stackoverflow;

extern bool crash_kexec_post_notifiers;

extern void __stack_chk_fail(void);
void abort(void);

extern atomic_t panic_cpu;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_arch_panic_timeout(int timeout, int arch_default_timeout)
{
	if (panic_timeout == arch_default_timeout)
		panic_timeout = timeout;
}
struct taint_flag {
	char c_true;
	char c_false;
	bool module;
	const char *desc;
};

extern const struct taint_flag taint_flags[19];

enum lockdep_ok {
	LOCKDEP_STILL_OK,
	LOCKDEP_NOW_UNRELIABLE,
};

extern const char *print_tainted(void);
extern const char *print_tainted_verbose(void);
extern void add_taint(unsigned flag, enum lockdep_ok);
extern int test_taint(unsigned flag);
extern unsigned long get_taint(void);

typedef __builtin_va_list va_list;

typedef struct qspinlock {
	union {
		atomic_t val;

		struct {
			u8 locked;
			u8 pending;
		};
		struct {
			u16 locked_pending;
			u16 tail;
		};
	};
} arch_spinlock_t;

typedef struct qrwlock {
	union {
		atomic_t cnts;
		struct {
			u8 wlocked;
			u8 __lstate[3];
		};
	};
	arch_spinlock_t wait_lock;
} arch_rwlock_t;

enum lockdep_wait_type {
	LD_WAIT_INV = 0,

	LD_WAIT_FREE,
	LD_WAIT_SPIN,

	LD_WAIT_CONFIG = LD_WAIT_SPIN,

	LD_WAIT_SLEEP,

	LD_WAIT_MAX,
};

enum lockdep_lock_type {
	LD_LOCK_NORMAL = 0,
	LD_LOCK_PERCPU,
	LD_LOCK_WAIT_OVERRIDE,
	LD_LOCK_MAX,
};
struct lock_class_key {};

struct lockdep_map {};

struct pin_cookie {};

typedef struct raw_spinlock {
	arch_spinlock_t raw_lock;

} raw_spinlock_t;

struct ratelimit_state {
	raw_spinlock_t lock;

	int interval;
	int burst;
	int printed;
	int missed;
	unsigned int flags;
	unsigned long begin;
};
extern int ___ratelimit(struct ratelimit_state *rs, const char *func);

struct console;

extern const char linux_banner[];
extern const char linux_proc_banner[];

extern int oops_in_progress;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
printk_get_level(const char *buffer)
{
	if (buffer[0] == '\001' && buffer[1]) {
		switch (buffer[1]) {
		case '0' ... '7':
		case 'c':
			return buffer[1];
		}
	}
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const char *
printk_skip_level(const char *buffer)
{
	if (printk_get_level(buffer))
		return buffer + 2;

	return buffer;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const char *
printk_skip_headers(const char *buffer)
{
	while (printk_get_level(buffer))
		buffer = printk_skip_level(buffer);

	return buffer;
}
int match_devname_and_update_preferred_console(const char *match,
					       const char *name,
					       const short idx);

extern int console_printk[];

extern void console_verbose(void);

extern char devkmsg_log_str[10];
struct ctl_table;

extern int suppress_printk;

struct va_format {
	const char *fmt;
	va_list *va;
};
extern __attribute__((__format__(printf, 1, 2))) void
early_printk(const char *fmt, ...);

struct dev_printk_info;

__attribute__((__format__(printf, 4, 0))) int
vprintk_emit(int facility, int level, const struct dev_printk_info *dev_info,
	     const char *fmt, va_list args);

__attribute__((__format__(printf, 1, 0))) int vprintk(const char *fmt,
						      va_list args);

__attribute__((__format__(printf, 1, 2))) __attribute__((__cold__)) int
_printk(const char *fmt, ...);

__attribute__((__format__(printf, 1, 2))) __attribute__((__cold__)) int
_printk_deferred(const char *fmt, ...);

extern void __printk_deferred_enter(void);
extern void __printk_deferred_exit(void);
extern int __printk_ratelimit(const char *func);

extern bool printk_timed_ratelimit(unsigned long *caller_jiffies,
				   unsigned int interval_msec);

extern int printk_delay_msec;
extern int dmesg_restrict;

extern void wake_up_klogd(void);

char *log_buf_addr_get(void);
u32 log_buf_len_get(void);
void log_buf_vmcoreinfo_setup(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
setup_log_buf(int early);
__attribute__((__format__(printf, 1, 2))) void
dump_stack_set_arch_desc(const char *fmt, ...);
void dump_stack_print_info(const char *log_lvl);
void show_regs_print_info(const char *log_lvl);
extern void dump_stack_lvl(const char *log_lvl) __attribute__((__cold__));
extern void dump_stack(void) __attribute__((__cold__));
void printk_trigger_flush(void);
void console_try_replay_all(void);
void printk_legacy_allow_panic_sync(void);
extern bool nbcon_device_try_acquire(struct console *con);
extern void nbcon_device_release(struct console *con);
void nbcon_atomic_flush_unsafe(void);
bool this_cpu_in_panic(void);

extern int __printk_cpu_sync_try_get(void);
extern void __printk_cpu_sync_wait(void);
extern void __printk_cpu_sync_put(void);
extern int kptr_restrict;
struct module;
extern const struct file_operations kmsg_fops;

enum { DUMP_PREFIX_NONE, DUMP_PREFIX_ADDRESS, DUMP_PREFIX_OFFSET };
extern int hex_dump_to_buffer(const void *buf, size_t len, int rowsize,
			      int groupsize, char *linebuf, size_t linebuflen,
			      bool ascii);

extern void print_hex_dump(const char *level, const char *prefix_str,
			   int prefix_type, int rowsize, int groupsize,
			   const void *buf, size_t len, bool ascii);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
print_hex_dump_debug(const char *prefix_str, int prefix_type, int rowsize,
		     int groupsize, const void *buf, size_t len, bool ascii)
{
}

struct warn_args;
struct pt_regs;

void __warn(const char *file, int line, void *caller, unsigned taint,
	    struct pt_regs *regs, struct warn_args *args);

struct bug_entry {
	signed int bug_addr_disp;

	signed int file_disp;

	unsigned short line;

	unsigned short flags;
};
extern __attribute__((__format__(printf, 4, 5))) void
warn_slowpath_fmt(const char *file, const int line, unsigned taint,
		  const char *fmt, ...);
extern __attribute__((__format__(printf, 1, 2))) void
__warn_printk(const char *fmt, ...);

enum bug_trap_type {
	BUG_TRAP_TYPE_NONE = 0,
	BUG_TRAP_TYPE_WARN = 1,
	BUG_TRAP_TYPE_BUG = 2,
};

struct pt_regs;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
is_warning_bug(const struct bug_entry *bug)
{
	return bug->flags & (1 << 0);
}

void bug_get_file_line(struct bug_entry *bug, const char **file,
		       unsigned int *line);

struct bug_entry *find_bug(unsigned long bugaddr);

enum bug_trap_type report_bug(unsigned long bug_addr, struct pt_regs *regs);

int is_valid_bugaddr(unsigned long addr);

void generic_bug_clear_once(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) bool
check_data_corruption(bool v)
{
	return v;
}
struct __kernel_timespec;
struct timespec;
struct old_timespec32;
struct pollfd;

enum timespec_type {
	TT_NONE = 0,
	TT_NATIVE = 1,
	TT_COMPAT = 2,
};

struct restart_block {
	unsigned long arch_data;
	long (*fn)(struct restart_block *);
	union {
		struct {
			u32 *uaddr;
			u32 val;
			u32 flags;
			u32 bitset;
			u64 time;
			u32 *uaddr2;
		} futex;

		struct {
			clockid_t clockid;
			enum timespec_type type;
			union {
				struct __kernel_timespec *rmtp;
				struct old_timespec32 *compat_rmtp;
			};
			u64 expires;
		} nanosleep;

		struct {
			struct pollfd *ufds;
			int nfds;
			int has_timeout;
			unsigned long tv_sec;
			unsigned long tv_nsec;
		} poll;
	};
};

extern long do_no_restart_syscall(struct restart_block *parm);

extern unsigned int __sw_hweight8(unsigned int w);
extern unsigned int __sw_hweight16(unsigned int w);
extern unsigned int __sw_hweight32(unsigned int w);
extern unsigned long __sw_hweight64(__u64 w);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
generic___set_bit(unsigned long nr, volatile unsigned long *addr)
{
	unsigned long mask = ((((1UL))) << ((nr) % 64));
	unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);

	*p |= mask;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
generic___clear_bit(unsigned long nr, volatile unsigned long *addr)
{
	unsigned long mask = ((((1UL))) << ((nr) % 64));
	unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);

	*p &= ~mask;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
generic___change_bit(unsigned long nr, volatile unsigned long *addr)
{
	unsigned long mask = ((((1UL))) << ((nr) % 64));
	unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);

	*p ^= mask;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
generic___test_and_set_bit(unsigned long nr, volatile unsigned long *addr)
{
	unsigned long mask = ((((1UL))) << ((nr) % 64));
	unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
	unsigned long old = *p;

	*p = old | mask;
	return (old & mask) != 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
generic___test_and_clear_bit(unsigned long nr, volatile unsigned long *addr)
{
	unsigned long mask = ((((1UL))) << ((nr) % 64));
	unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
	unsigned long old = *p;

	*p = old & ~mask;
	return (old & mask) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
generic___test_and_change_bit(unsigned long nr, volatile unsigned long *addr)
{
	unsigned long mask = ((((1UL))) << ((nr) % 64));
	unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
	unsigned long old = *p;

	*p = old ^ mask;
	return (old & mask) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
generic_test_bit(unsigned long nr, const volatile unsigned long *addr)
{
	return 1UL & (addr[((nr) / 64)] >> (nr & (64 - 1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
generic_test_bit_acquire(unsigned long nr, const volatile unsigned long *addr)
{
	unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
	return 1UL &
	       (({
			typeof(*p) ___p1 = ({
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_31(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*p) == sizeof(char) ||
					       sizeof(*p) == sizeof(short) ||
					       sizeof(*p) == sizeof(int) ||
					       sizeof(*p) == sizeof(long)) ||
					      sizeof(*p) == sizeof(long long)))
						__compiletime_assert_31();
				} while (0);
				(*(const volatile typeof(_Generic(
					(*p),
								 char: (char)0,
								 unsigned char: (
									 unsigned char)0,
								 signed char: (
									 signed char)0,
								 unsigned short: (
									 unsigned short)0,
								 signed short: (
									 signed short)0,
								 unsigned int: (
									 unsigned int)0,
								 signed int: (
									 signed int)0,
								 unsigned long: (
									 unsigned long)0,
								 signed long: (
									 signed long)0,
								 unsigned long long: (
									 unsigned long long)0,
								 signed long long: (
									 signed long long)0,
								 default: (*p)))
					   *)&(*p));
			});
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_32(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*p) == sizeof(char) ||
				       sizeof(*p) == sizeof(short) ||
				       sizeof(*p) == sizeof(int) ||
				       sizeof(*p) == sizeof(long))))
					__compiletime_assert_32();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			___p1;
		}) >>
		(nr & (64 - 1)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
const_test_bit(unsigned long nr, const volatile unsigned long *addr)
{
	const unsigned long *p = (const unsigned long *)addr + ((nr) / 64);
	unsigned long mask = ((((1UL))) << ((nr) % 64));
	unsigned long val = *p;

	return !!(val & mask);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_set_bit(long nr, volatile unsigned long *addr)
{
	if (__builtin_constant_p(nr)) {
		asm volatile(
			".pushsection .smp_locks,\"a\"\n"
			".balign 4\n"
			".long 671f - .\n"
			".popsection\n"
			"671:"
			"\n\tlock; "
			"orb %b1,%0"
			: "+m"(*(volatile char *)((void *)(addr) + ((nr) >> 3)))
			: "iq"((1 << ((nr) & 7)))
			: "memory");
	} else {
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     " "
			     "btsq"
			     " "
			     " %1,%0"
			     :
			     : "m"(*(volatile long *)(addr)), "Ir"(nr)
			     : "memory");
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch___set_bit(unsigned long nr, volatile unsigned long *addr)
{
	asm volatile(" "
		     "btsq"
		     " "
		     " %1,%0"
		     :
		     : "m"(*(volatile long *)(addr)), "Ir"(nr)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_clear_bit(long nr, volatile unsigned long *addr)
{
	if (__builtin_constant_p(nr)) {
		asm volatile(
			".pushsection .smp_locks,\"a\"\n"
			".balign 4\n"
			".long 671f - .\n"
			".popsection\n"
			"671:"
			"\n\tlock; "
			"andb %b1,%0"
			: "+m"(*(volatile char *)((void *)(addr) + ((nr) >> 3)))
			: "iq"(~(1 << ((nr) & 7))));
	} else {
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     " "
			     "btrq"
			     " "
			     " %1,%0"
			     :
			     : "m"(*(volatile long *)(addr)), "Ir"(nr)
			     : "memory");
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_clear_bit_unlock(long nr, volatile unsigned long *addr)
{
	__asm__ __volatile__("" : : : "memory");
	arch_clear_bit(nr, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch___clear_bit(unsigned long nr, volatile unsigned long *addr)
{
	asm volatile(" "
		     "btrq"
		     " "
		     " %1,%0"
		     :
		     : "m"(*(volatile long *)(addr)), "Ir"(nr)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_xor_unlock_is_negative_byte(unsigned long mask,
				 volatile unsigned long *addr)
{
	bool negative;
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "xorb %2,%1"
		     "\n\t/* output condition code "
		     "s"
		     "*/\n"
		     : "=@cc"
		       "s"(negative),
		       "+m"(*(volatile char *)(addr))
		     : "iq"((char)mask)
		     : "memory");
	return negative;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch___clear_bit_unlock(long nr, volatile unsigned long *addr)
{
	arch___clear_bit(nr, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch___change_bit(unsigned long nr, volatile unsigned long *addr)
{
	asm volatile(" "
		     "btcq"
		     " "
		     " %1,%0"
		     :
		     : "m"(*(volatile long *)(addr)), "Ir"(nr)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_change_bit(long nr, volatile unsigned long *addr)
{
	if (__builtin_constant_p(nr)) {
		asm volatile(
			".pushsection .smp_locks,\"a\"\n"
			".balign 4\n"
			".long 671f - .\n"
			".popsection\n"
			"671:"
			"\n\tlock; "
			"xorb %b1,%0"
			: "+m"(*(volatile char *)((void *)(addr) + ((nr) >> 3)))
			: "iq"((1 << ((nr) & 7))));
	} else {
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     " "
			     "btcq"
			     " "
			     " %1,%0"
			     :
			     : "m"(*(volatile long *)(addr)), "Ir"(nr)
			     : "memory");
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_test_and_set_bit(long nr, volatile unsigned long *addr)
{
	return ({
		bool c;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     " "
			     "btsq"
			     " "
			     " %[val], "
			     "%[var]"
			     "\n\t/* output condition code "
			     "c"
			     "*/\n"
			     : [var] "+m"(*addr), "=@cc"
						  "c"(c)
			     : [val] "Ir"(nr)
			     : "memory");
		c;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_test_and_set_bit_lock(long nr, volatile unsigned long *addr)
{
	return arch_test_and_set_bit(nr, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch___test_and_set_bit(unsigned long nr, volatile unsigned long *addr)
{
	bool oldbit;

	asm(" "
	    "btsq"
	    " "
	    " %2,%1"
	    "\n\t/* output condition code "
	    "c"
	    "*/\n"
	    : "=@cc"
	      "c"(oldbit)
	    : "m"(*(volatile long *)(addr)), "Ir"(nr)
	    : "memory");
	return oldbit;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_test_and_clear_bit(long nr, volatile unsigned long *addr)
{
	return ({
		bool c;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     " "
			     "btrq"
			     " "
			     " %[val], "
			     "%[var]"
			     "\n\t/* output condition code "
			     "c"
			     "*/\n"
			     : [var] "+m"(*addr), "=@cc"
						  "c"(c)
			     : [val] "Ir"(nr)
			     : "memory");
		c;
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch___test_and_clear_bit(unsigned long nr, volatile unsigned long *addr)
{
	bool oldbit;

	asm volatile(" "
		     "btrq"
		     " "
		     " %2,%1"
		     "\n\t/* output condition code "
		     "c"
		     "*/\n"
		     : "=@cc"
		       "c"(oldbit)
		     : "m"(*(volatile long *)(addr)), "Ir"(nr)
		     : "memory");
	return oldbit;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch___test_and_change_bit(unsigned long nr, volatile unsigned long *addr)
{
	bool oldbit;

	asm volatile(" "
		     "btcq"
		     " "
		     " %2,%1"
		     "\n\t/* output condition code "
		     "c"
		     "*/\n"
		     : "=@cc"
		       "c"(oldbit)
		     : "m"(*(volatile long *)(addr)), "Ir"(nr)
		     : "memory");

	return oldbit;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_test_and_change_bit(long nr, volatile unsigned long *addr)
{
	return ({
		bool c;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     " "
			     "btcq"
			     " "
			     " %[val], "
			     "%[var]"
			     "\n\t/* output condition code "
			     "c"
			     "*/\n"
			     : [var] "+m"(*addr), "=@cc"
						  "c"(c)
			     : [val] "Ir"(nr)
			     : "memory");
		c;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (64 - 1))) & (addr[nr >> 6])) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
constant_test_bit_acquire(long nr, const volatile unsigned long *addr)
{
	bool oldbit;

	asm volatile("testb %2,%1"
		     "\n\t/* output condition code "
		     "nz"
		     "*/\n"
		     : "=@cc"
		       "nz"(oldbit)
		     : "m"(((unsigned char *)addr)[nr >> 3]), "i"(1 << (nr & 7))
		     : "memory");

	return oldbit;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
variable_test_bit(long nr, volatile const unsigned long *addr)
{
	bool oldbit;

	asm volatile(" "
		     "btq"
		     " "
		     " %2,%1"
		     "\n\t/* output condition code "
		     "c"
		     "*/\n"
		     : "=@cc"
		       "c"(oldbit)
		     : "m"(*(unsigned long *)addr), "Ir"(nr)
		     : "memory");

	return oldbit;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_test_bit(unsigned long nr, const volatile unsigned long *addr)
{
	return __builtin_constant_p(nr) ? constant_test_bit(nr, addr) :
					  variable_test_bit(nr, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_test_bit_acquire(unsigned long nr, const volatile unsigned long *addr)
{
	return __builtin_constant_p(nr) ? constant_test_bit_acquire(nr, addr) :
					  variable_test_bit(nr, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
variable__ffs(unsigned long word)
{
	asm("rep; bsf %1,%0" : "=r"(word) : "r"(word));
	return word;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
variable_ffz(unsigned long word)
{
	asm("rep; bsf %1,%0" : "=r"(word) : "r"(~word));
	return word;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
__fls(unsigned long word)
{
	if (__builtin_constant_p(word))
		return 64 - 1 - __builtin_clzl(word);

	asm("bsr %1,%0" : "=r"(word) : "r"(word));
	return word;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
variable_ffs(int x)
{
	int r;
	asm("bsfl %1,%0" : "=r"(r) : "r"(x), "0"(-1));
	return r + 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
fls(unsigned int x)
{
	int r;

	if (__builtin_constant_p(x))
		return x ? 32 - __builtin_clz(x) : 0;
	asm("bsrl %1,%0" : "=r"(r) : "r"(x), "0"(-1));
	return r + 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
fls64(__u64 x)
{
	int bitpos = -1;

	if (__builtin_constant_p(x))
		return x ? 64 - __builtin_clzll(x) : 0;

	asm("bsrq %1,%q0" : "+r"(bitpos) : "r"(x));
	return bitpos + 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sched_find_first_bit(const unsigned long *b)
{
	if (b[0])
		return (__builtin_constant_p(b[0]) ?
				(unsigned long)__builtin_ctzl(b[0]) :
				variable__ffs(b[0]));
	return (__builtin_constant_p(b[1]) ?
			(unsigned long)__builtin_ctzl(b[1]) :
			variable__ffs(b[1])) +
	       64;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
__arch_hweight32(unsigned int w)
{
	unsigned int res;

	asm("# ALT: oldinstr\n"
	    "771:\n\t"
	    "call __sw_hweight32"
	    "\n772:\n"
	    "# ALT: padding\n"
	    ".skip -((("
	    "775f-774f"
	    ")-("
	    "772b-771b"
	    ")) > 0) * "
	    "(("
	    "775f-774f"
	    ")-("
	    "772b-771b"
	    ")),0x90\n"
	    "773:\n"
	    ".pushsection .altinstructions,\"a\"\n"
	    " .long 771b - .\n"
	    " .long 774f - .\n"
	    " .4byte "
	    "( 4*32+23)"
	    "\n"
	    " .byte "
	    "773b-771b"
	    "\n"
	    " .byte "
	    "775f-774f"
	    "\n"
	    ".popsection\n"
	    ".pushsection .altinstr_replacement, \"ax\"\n"
	    "# ALT: replacement\n"
	    "774:\n\t"
	    "popcntl %1, %0"
	    "\n775:\n"
	    ".popsection\n"
	    : "="
	      "a"(res)
	    : "D"(w));

	return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
__arch_hweight16(unsigned int w)
{
	return __arch_hweight32(w & 0xffff);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
__arch_hweight8(unsigned int w)
{
	return __arch_hweight32(w & 0xff);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
__arch_hweight64(__u64 w)
{
	unsigned long res;

	asm("# ALT: oldinstr\n"
	    "771:\n\t"
	    "call __sw_hweight64"
	    "\n772:\n"
	    "# ALT: padding\n"
	    ".skip -((("
	    "775f-774f"
	    ")-("
	    "772b-771b"
	    ")) > 0) * "
	    "(("
	    "775f-774f"
	    ")-("
	    "772b-771b"
	    ")),0x90\n"
	    "773:\n"
	    ".pushsection .altinstructions,\"a\"\n"
	    " .long 771b - .\n"
	    " .long 774f - .\n"
	    " .4byte "
	    "( 4*32+23)"
	    "\n"
	    " .byte "
	    "773b-771b"
	    "\n"
	    " .byte "
	    "775f-774f"
	    "\n"
	    ".popsection\n"
	    ".pushsection .altinstr_replacement, \"ax\"\n"
	    "# ALT: replacement\n"
	    "774:\n\t"
	    "popcntq %1, %0"
	    "\n775:\n"
	    ".popsection\n"
	    : "="
	      "a"(res)
	    : "D"(w));

	return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kmsan_poison_memory(const void *address, size_t size, gfp_t flags)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kmsan_unpoison_memory(const void *address, size_t size)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kmsan_check_memory(const void *address, size_t size)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kmsan_copy_to_user(void *to, const void *from, size_t to_copy, size_t left)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kmsan_memmove(void *to, const void *from, size_t to_copy)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
instrument_read(const volatile void *v, size_t size)
{
	kasan_check_read(v, size);
	kcsan_check_access(v, size, 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
instrument_write(const volatile void *v, size_t size)
{
	kasan_check_write(v, size);
	kcsan_check_access(v, size, (1 << 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
instrument_read_write(const volatile void *v, size_t size)
{
	kasan_check_write(v, size);
	kcsan_check_access(v, size, (1 << 1) | (1 << 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
instrument_atomic_read(const volatile void *v, size_t size)
{
	kasan_check_read(v, size);
	kcsan_check_access(v, size, (1 << 2));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
instrument_atomic_write(const volatile void *v, size_t size)
{
	kasan_check_write(v, size);
	kcsan_check_access(v, size, (1 << 2) | (1 << 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
instrument_atomic_read_write(const volatile void *v, size_t size)
{
	kasan_check_write(v, size);
	kcsan_check_access(v, size, (1 << 2) | (1 << 0) | (1 << 1));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
instrument_copy_to_user(void *to, const void *from, unsigned long n)
{
	kasan_check_read(from, n);
	kcsan_check_access(from, n, 0);
	kmsan_copy_to_user(to, from, n, 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
instrument_copy_from_user_before(const void *to, const void *from,
				 unsigned long n)
{
	kasan_check_write(to, n);
	kcsan_check_access(to, n, (1 << 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
instrument_copy_from_user_after(const void *to, const void *from,
				unsigned long n, unsigned long left)
{
	kmsan_unpoison_memory(to, n - left);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
instrument_memcpy_before(void *to, const void *from, unsigned long n)
{
	kasan_check_write(to, n);
	kasan_check_read(from, n);
	kcsan_check_access(to, n, (1 << 0));
	kcsan_check_access(from, n, 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
instrument_memcpy_after(void *to, const void *from, unsigned long n,
			unsigned long left)
{
	kmsan_memmove(to, from, n - left);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
set_bit(long nr, volatile unsigned long *addr)
{
	instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
	arch_set_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
clear_bit(long nr, volatile unsigned long *addr)
{
	instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
	arch_clear_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
change_bit(long nr, volatile unsigned long *addr)
{
	instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
	arch_change_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
test_and_set_bit(long nr, volatile unsigned long *addr)
{
	do {
	} while (0);
	instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
	return arch_test_and_set_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
test_and_clear_bit(long nr, volatile unsigned long *addr)
{
	do {
	} while (0);
	instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
	return arch_test_and_clear_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
test_and_change_bit(long nr, volatile unsigned long *addr)
{
	do {
	} while (0);
	instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
	return arch_test_and_change_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
___set_bit(unsigned long nr, volatile unsigned long *addr)
{
	instrument_write(addr + ((nr) / 64), sizeof(long));
	arch___set_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
___clear_bit(unsigned long nr, volatile unsigned long *addr)
{
	instrument_write(addr + ((nr) / 64), sizeof(long));
	arch___clear_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
___change_bit(unsigned long nr, volatile unsigned long *addr)
{
	instrument_write(addr + ((nr) / 64), sizeof(long));
	arch___change_bit(nr, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__instrument_read_write_bitop(long nr, volatile unsigned long *addr)
{
	if (0) {
		kcsan_check_access(addr + ((nr) / 64), sizeof(long), 0);

		instrument_write(addr + ((nr) / 64), sizeof(long));
	} else {
		instrument_read_write(addr + ((nr) / 64), sizeof(long));
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
___test_and_set_bit(unsigned long nr, volatile unsigned long *addr)
{
	__instrument_read_write_bitop(nr, addr);
	return arch___test_and_set_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
___test_and_clear_bit(unsigned long nr, volatile unsigned long *addr)
{
	__instrument_read_write_bitop(nr, addr);
	return arch___test_and_clear_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
___test_and_change_bit(unsigned long nr, volatile unsigned long *addr)
{
	__instrument_read_write_bitop(nr, addr);
	return arch___test_and_change_bit(nr, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
_test_bit(unsigned long nr, const volatile unsigned long *addr)
{
	instrument_atomic_read(addr + ((nr) / 64), sizeof(long));
	return arch_test_bit(nr, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
_test_bit_acquire(unsigned long nr, const volatile unsigned long *addr)
{
	instrument_atomic_read(addr + ((nr) / 64), sizeof(long));
	return arch_test_bit_acquire(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_bit_unlock(long nr, volatile unsigned long *addr)
{
	do {
	} while (0);
	instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
	arch_clear_bit_unlock(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__clear_bit_unlock(long nr, volatile unsigned long *addr)
{
	do {
	} while (0);
	instrument_write(addr + ((nr) / 64), sizeof(long));
	arch___clear_bit_unlock(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
test_and_set_bit_lock(long nr, volatile unsigned long *addr)
{
	instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
	return arch_test_and_set_bit_lock(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xor_unlock_is_negative_byte(unsigned long mask, volatile unsigned long *addr)
{
	do {
	} while (0);
	instrument_atomic_write(addr, sizeof(long));
	return arch_xor_unlock_is_negative_byte(mask, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
test_bit_le(int nr, const void *addr)
{
	return ((__builtin_constant_p(nr ^ 0) &&
		 __builtin_constant_p((uintptr_t)(addr) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(addr) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(const unsigned long *)(addr))) ?
			const_test_bit(nr ^ 0, addr) :
			_test_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_bit_le(int nr, void *addr)
{
	set_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_bit_le(int nr, void *addr)
{
	clear_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__set_bit_le(int nr, void *addr)
{
	((__builtin_constant_p(nr ^ 0) &&
	  __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(addr) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(*(const unsigned long *)(addr))) ?
		 generic___set_bit(nr ^ 0, addr) :
		 ___set_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__clear_bit_le(int nr, void *addr)
{
	((__builtin_constant_p(nr ^ 0) &&
	  __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(addr) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(*(const unsigned long *)(addr))) ?
		 generic___clear_bit(nr ^ 0, addr) :
		 ___clear_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
test_and_set_bit_le(int nr, void *addr)
{
	return test_and_set_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
test_and_clear_bit_le(int nr, void *addr)
{
	return test_and_clear_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__test_and_set_bit_le(int nr, void *addr)
{
	return ((__builtin_constant_p(nr ^ 0) &&
		 __builtin_constant_p((uintptr_t)(addr) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(addr) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(const unsigned long *)(addr))) ?
			generic___test_and_set_bit(nr ^ 0, addr) :
			___test_and_set_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__test_and_clear_bit_le(int nr, void *addr)
{
	return ((__builtin_constant_p(nr ^ 0) &&
		 __builtin_constant_p((uintptr_t)(addr) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(addr) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(const unsigned long *)(addr))) ?
			generic___test_and_clear_bit(nr ^ 0, addr) :
			___test_and_clear_bit(nr ^ 0, addr));
}

_Static_assert(
	__builtin_types_compatible_p(typeof(arch___set_bit),
				     typeof(generic___set_bit)) &&
		__builtin_types_compatible_p(typeof(generic___set_bit),
					     typeof(generic___set_bit)) &&
		__builtin_types_compatible_p(typeof(___set_bit),
					     typeof(generic___set_bit)),
	"__same_type(arch___set_bit, generic___set_bit) && __same_type(const___set_bit, generic___set_bit) && __same_type(___set_bit, generic___set_bit)");
_Static_assert(
	__builtin_types_compatible_p(typeof(arch___clear_bit),
				     typeof(generic___clear_bit)) &&
		__builtin_types_compatible_p(typeof(generic___clear_bit),
					     typeof(generic___clear_bit)) &&
		__builtin_types_compatible_p(typeof(___clear_bit),
					     typeof(generic___clear_bit)),
	"__same_type(arch___clear_bit, generic___clear_bit) && __same_type(const___clear_bit, generic___clear_bit) && __same_type(___clear_bit, generic___clear_bit)");
_Static_assert(
	__builtin_types_compatible_p(typeof(arch___change_bit),
				     typeof(generic___change_bit)) &&
		__builtin_types_compatible_p(typeof(generic___change_bit),
					     typeof(generic___change_bit)) &&
		__builtin_types_compatible_p(typeof(___change_bit),
					     typeof(generic___change_bit)),
	"__same_type(arch___change_bit, generic___change_bit) && __same_type(const___change_bit, generic___change_bit) && __same_type(___change_bit, generic___change_bit)");
_Static_assert(
	__builtin_types_compatible_p(typeof(arch___test_and_set_bit),
				     typeof(generic___test_and_set_bit)) &&
		__builtin_types_compatible_p(
			typeof(generic___test_and_set_bit),
			typeof(generic___test_and_set_bit)) &&
		__builtin_types_compatible_p(typeof(___test_and_set_bit),
					     typeof(generic___test_and_set_bit)),
	"__same_type(arch___test_and_set_bit, generic___test_and_set_bit) && __same_type(const___test_and_set_bit, generic___test_and_set_bit) && __same_type(___test_and_set_bit, generic___test_and_set_bit)");
_Static_assert(
	__builtin_types_compatible_p(typeof(arch___test_and_clear_bit),
				     typeof(generic___test_and_clear_bit)) &&
		__builtin_types_compatible_p(
			typeof(generic___test_and_clear_bit),
			typeof(generic___test_and_clear_bit)) &&
		__builtin_types_compatible_p(
			typeof(___test_and_clear_bit),
			typeof(generic___test_and_clear_bit)),
	"__same_type(arch___test_and_clear_bit, generic___test_and_clear_bit) && __same_type(const___test_and_clear_bit, generic___test_and_clear_bit) && __same_type(___test_and_clear_bit, generic___test_and_clear_bit)");
_Static_assert(
	__builtin_types_compatible_p(typeof(arch___test_and_change_bit),
				     typeof(generic___test_and_change_bit)) &&
		__builtin_types_compatible_p(
			typeof(generic___test_and_change_bit),
			typeof(generic___test_and_change_bit)) &&
		__builtin_types_compatible_p(
			typeof(___test_and_change_bit),
			typeof(generic___test_and_change_bit)),
	"__same_type(arch___test_and_change_bit, generic___test_and_change_bit) && __same_type(const___test_and_change_bit, generic___test_and_change_bit) && __same_type(___test_and_change_bit, generic___test_and_change_bit)");
_Static_assert(
	__builtin_types_compatible_p(typeof(arch_test_bit),
				     typeof(generic_test_bit)) &&
		__builtin_types_compatible_p(typeof(const_test_bit),
					     typeof(generic_test_bit)) &&
		__builtin_types_compatible_p(typeof(_test_bit),
					     typeof(generic_test_bit)),
	"__same_type(arch_test_bit, generic_test_bit) && __same_type(const_test_bit, generic_test_bit) && __same_type(_test_bit, generic_test_bit)");
_Static_assert(
	__builtin_types_compatible_p(typeof(arch_test_bit_acquire),
				     typeof(generic_test_bit_acquire)) &&
		__builtin_types_compatible_p(typeof(generic_test_bit_acquire),
					     typeof(generic_test_bit_acquire)) &&
		__builtin_types_compatible_p(typeof(_test_bit_acquire),
					     typeof(generic_test_bit_acquire)),
	"__same_type(arch_test_bit_acquire, generic_test_bit_acquire) && __same_type(const_test_bit_acquire, generic_test_bit_acquire) && __same_type(_test_bit_acquire, generic_test_bit_acquire)");

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_bitmask_order(unsigned int count)
{
	int order;

	order = fls(count);
	return order;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
hweight_long(unsigned long w)
{
	return sizeof(w) == 4 ?
		       (__builtin_constant_p(w) ?
				((((unsigned int)((!!((w) & (1ULL << 0))) +
						  (!!((w) & (1ULL << 1))) +
						  (!!((w) & (1ULL << 2))) +
						  (!!((w) & (1ULL << 3))) +
						  (!!((w) & (1ULL << 4))) +
						  (!!((w) & (1ULL << 5))) +
						  (!!((w) & (1ULL << 6))) +
						  (!!((w) & (1ULL << 7))))) +
				  ((unsigned int)((!!(((w) >> 8) &
						      (1ULL << 0))) +
						  (!!(((w) >> 8) &
						      (1ULL << 1))) +
						  (!!(((w) >> 8) &
						      (1ULL << 2))) +
						  (!!(((w) >> 8) &
						      (1ULL << 3))) +
						  (!!(((w) >> 8) &
						      (1ULL << 4))) +
						  (!!(((w) >> 8) &
						      (1ULL << 5))) +
						  (!!(((w) >> 8) &
						      (1ULL << 6))) +
						  (!!(((w) >> 8) &
						      (1ULL << 7)))))) +
				 (((unsigned int)((!!(((w) >> 16) &
						      (1ULL << 0))) +
						  (!!(((w) >> 16) &
						      (1ULL << 1))) +
						  (!!(((w) >> 16) &
						      (1ULL << 2))) +
						  (!!(((w) >> 16) &
						      (1ULL << 3))) +
						  (!!(((w) >> 16) &
						      (1ULL << 4))) +
						  (!!(((w) >> 16) &
						      (1ULL << 5))) +
						  (!!(((w) >> 16) &
						      (1ULL << 6))) +
						  (!!(((w) >> 16) &
						      (1ULL << 7))))) +
				  ((unsigned int)((!!((((w) >> 16) >> 8) &
						      (1ULL << 0))) +
						  (!!((((w) >> 16) >> 8) &
						      (1ULL << 1))) +
						  (!!((((w) >> 16) >> 8) &
						      (1ULL << 2))) +
						  (!!((((w) >> 16) >> 8) &
						      (1ULL << 3))) +
						  (!!((((w) >> 16) >> 8) &
						      (1ULL << 4))) +
						  (!!((((w) >> 16) >> 8) &
						      (1ULL << 5))) +
						  (!!((((w) >> 16) >> 8) &
						      (1ULL << 6))) +
						  (!!((((w) >> 16) >> 8) &
						      (1ULL << 7))))))) :
				__arch_hweight32(w)) :
		       (__builtin_constant_p((__u64)w) ?
				(((((unsigned int)((!!(((__u64)w) &
						       (1ULL << 0))) +
						   (!!(((__u64)w) &
						       (1ULL << 1))) +
						   (!!(((__u64)w) &
						       (1ULL << 2))) +
						   (!!(((__u64)w) &
						       (1ULL << 3))) +
						   (!!(((__u64)w) &
						       (1ULL << 4))) +
						   (!!(((__u64)w) &
						       (1ULL << 5))) +
						   (!!(((__u64)w) &
						       (1ULL << 6))) +
						   (!!(((__u64)w) &
						       (1ULL << 7))))) +
				   ((unsigned int)((!!((((__u64)w) >> 8) &
						       (1ULL << 0))) +
						   (!!((((__u64)w) >> 8) &
						       (1ULL << 1))) +
						   (!!((((__u64)w) >> 8) &
						       (1ULL << 2))) +
						   (!!((((__u64)w) >> 8) &
						       (1ULL << 3))) +
						   (!!((((__u64)w) >> 8) &
						       (1ULL << 4))) +
						   (!!((((__u64)w) >> 8) &
						       (1ULL << 5))) +
						   (!!((((__u64)w) >> 8) &
						       (1ULL << 6))) +
						   (!!((((__u64)w) >> 8) &
						       (1ULL << 7)))))) +
				  (((unsigned int)((!!((((__u64)w) >> 16) &
						       (1ULL << 0))) +
						   (!!((((__u64)w) >> 16) &
						       (1ULL << 1))) +
						   (!!((((__u64)w) >> 16) &
						       (1ULL << 2))) +
						   (!!((((__u64)w) >> 16) &
						       (1ULL << 3))) +
						   (!!((((__u64)w) >> 16) &
						       (1ULL << 4))) +
						   (!!((((__u64)w) >> 16) &
						       (1ULL << 5))) +
						   (!!((((__u64)w) >> 16) &
						       (1ULL << 6))) +
						   (!!((((__u64)w) >> 16) &
						       (1ULL << 7))))) +
				   ((unsigned int)((!!(((((__u64)w) >> 16) >> 8) &
						       (1ULL << 0))) +
						   (!!(((((__u64)w) >> 16) >> 8) &
						       (1ULL << 1))) +
						   (!!(((((__u64)w) >> 16) >> 8) &
						       (1ULL << 2))) +
						   (!!(((((__u64)w) >> 16) >> 8) &
						       (1ULL << 3))) +
						   (!!(((((__u64)w) >> 16) >> 8) &
						       (1ULL << 4))) +
						   (!!(((((__u64)w) >> 16) >> 8) &
						       (1ULL << 5))) +
						   (!!(((((__u64)w) >> 16) >> 8) &
						       (1ULL << 6))) +
						   (!!(((((__u64)w) >> 16) >> 8) &
						       (1ULL << 7))))))) +
				 ((((unsigned int)((!!((((__u64)w) >> 32) &
						       (1ULL << 0))) +
						   (!!((((__u64)w) >> 32) &
						       (1ULL << 1))) +
						   (!!((((__u64)w) >> 32) &
						       (1ULL << 2))) +
						   (!!((((__u64)w) >> 32) &
						       (1ULL << 3))) +
						   (!!((((__u64)w) >> 32) &
						       (1ULL << 4))) +
						   (!!((((__u64)w) >> 32) &
						       (1ULL << 5))) +
						   (!!((((__u64)w) >> 32) &
						       (1ULL << 6))) +
						   (!!((((__u64)w) >> 32) &
						       (1ULL << 7))))) +
				   ((unsigned int)((!!(((((__u64)w) >> 32) >> 8) &
						       (1ULL << 0))) +
						   (!!(((((__u64)w) >> 32) >> 8) &
						       (1ULL << 1))) +
						   (!!(((((__u64)w) >> 32) >> 8) &
						       (1ULL << 2))) +
						   (!!(((((__u64)w) >> 32) >> 8) &
						       (1ULL << 3))) +
						   (!!(((((__u64)w) >> 32) >> 8) &
						       (1ULL << 4))) +
						   (!!(((((__u64)w) >> 32) >> 8) &
						       (1ULL << 5))) +
						   (!!(((((__u64)w) >> 32) >> 8) &
						       (1ULL << 6))) +
						   (!!(((((__u64)w) >> 32) >> 8) &
						       (1ULL << 7)))))) +
				  (((unsigned int)((!!(((((__u64)w) >> 32) >>
							16) &
						       (1ULL << 0))) +
						   (!!(((((__u64)w) >> 32) >>
							16) &
						       (1ULL << 1))) +
						   (!!(((((__u64)w) >> 32) >>
							16) &
						       (1ULL << 2))) +
						   (!!(((((__u64)w) >> 32) >>
							16) &
						       (1ULL << 3))) +
						   (!!(((((__u64)w) >> 32) >>
							16) &
						       (1ULL << 4))) +
						   (!!(((((__u64)w) >> 32) >>
							16) &
						       (1ULL << 5))) +
						   (!!(((((__u64)w) >> 32) >>
							16) &
						       (1ULL << 6))) +
						   (!!(((((__u64)w) >> 32) >>
							16) &
						       (1ULL << 7))))) +
				   ((unsigned int)((!!((((((__u64)w) >> 32) >>
							 16) >>
							8) &
						       (1ULL << 0))) +
						   (!!((((((__u64)w) >> 32) >>
							 16) >>
							8) &
						       (1ULL << 1))) +
						   (!!((((((__u64)w) >> 32) >>
							 16) >>
							8) &
						       (1ULL << 2))) +
						   (!!((((((__u64)w) >> 32) >>
							 16) >>
							8) &
						       (1ULL << 3))) +
						   (!!((((((__u64)w) >> 32) >>
							 16) >>
							8) &
						       (1ULL << 4))) +
						   (!!((((((__u64)w) >> 32) >>
							 16) >>
							8) &
						       (1ULL << 5))) +
						   (!!((((((__u64)w) >> 32) >>
							 16) >>
							8) &
						       (1ULL << 6))) +
						   (!!((((((__u64)w) >> 32) >>
							 16) >>
							8) &
						       (1ULL << 7)))))))) :
				__arch_hweight64((__u64)w));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __u64
rol64(__u64 word, unsigned int shift)
{
	return (word << (shift & 63)) | (word >> ((-shift) & 63));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __u64
ror64(__u64 word, unsigned int shift)
{
	return (word >> (shift & 63)) | (word << ((-shift) & 63));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __u32
rol32(__u32 word, unsigned int shift)
{
	return (word << (shift & 31)) | (word >> ((-shift) & 31));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __u32
ror32(__u32 word, unsigned int shift)
{
	return (word >> (shift & 31)) | (word << ((-shift) & 31));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __u16
rol16(__u16 word, unsigned int shift)
{
	return (word << (shift & 15)) | (word >> ((-shift) & 15));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __u16
ror16(__u16 word, unsigned int shift)
{
	return (word >> (shift & 15)) | (word << ((-shift) & 15));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __u8
rol8(__u8 word, unsigned int shift)
{
	return (word << (shift & 7)) | (word >> ((-shift) & 7));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __u8
ror8(__u8 word, unsigned int shift)
{
	return (word >> (shift & 7)) | (word << ((-shift) & 7));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) __s32
sign_extend32(__u32 value, int index)
{
	__u8 shift = 31 - index;
	return (__s32)(value << shift) >> shift;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) __s64
sign_extend64(__u64 value, int index)
{
	__u8 shift = 63 - index;
	return (__s64)(value << shift) >> shift;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
fls_long(unsigned long l)
{
	if (sizeof(l) == 4)
		return fls(l);
	return fls64(l);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_count_order(unsigned int count)
{
	if (count == 0)
		return -1;

	return fls(--count);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_count_order_long(unsigned long l)
{
	if (l == 0UL)
		return -1;
	return (int)fls_long(--l);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
__ffs64(u64 word)
{
	return (__builtin_constant_p((unsigned long)word) ?
			(unsigned long)__builtin_ctzl((unsigned long)word) :
			variable__ffs((unsigned long)word));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
fns(unsigned long word, unsigned int n)
{
	while (word && n--)
		word &= word - 1;

	return word ? (__builtin_constant_p(word) ?
			       (unsigned long)__builtin_ctzl(word) :
			       variable__ffs(word)) :
		      64;
}

enum {
	BAD_STACK = -1,
	NOT_STACK = 0,
	GOOD_FRAME,
	GOOD_STACK,
};

enum syscall_work_bit {
	SYSCALL_WORK_BIT_SECCOMP,
	SYSCALL_WORK_BIT_SYSCALL_TRACEPOINT,
	SYSCALL_WORK_BIT_SYSCALL_TRACE,
	SYSCALL_WORK_BIT_SYSCALL_EMU,
	SYSCALL_WORK_BIT_SYSCALL_AUDIT,
	SYSCALL_WORK_BIT_SYSCALL_USER_DISPATCH,
	SYSCALL_WORK_BIT_SYSCALL_EXIT_TRAP,
};

unsigned long kaslr_get_random_long(const char *purpose);

void kernel_randomize_memory(void);
void init_trampoline_kaslr(void);
extern int devmem_is_allowed(unsigned long pagenr);

extern unsigned long max_low_pfn_mapped;
extern unsigned long max_pfn_mapped;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) phys_addr_t
get_max_mapped(void)
{
	return (phys_addr_t)max_pfn_mapped << 12;
}

bool pfn_range_is_mapped(unsigned long start_pfn, unsigned long end_pfn);

extern void initmem_init(void);

extern unsigned long max_pfn;
extern unsigned long phys_base;

extern unsigned long page_offset_base;
extern unsigned long vmalloc_base;
extern unsigned long vmemmap_base;
extern unsigned long physmem_end;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
__phys_addr_nodebug(unsigned long x)
{
	unsigned long y = x - (0xffffffff80000000UL);

	x = y + ((x > y) ? phys_base :
			   ((0xffffffff80000000UL) -
			    ((unsigned long)page_offset_base)));

	return x;
}
void clear_page_orig(void *page);
void clear_page_rep(void *page);
void clear_page_erms(void *page);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_page(void *page)
{
	kmsan_unpoison_memory(page, ((1UL) << 12));
	asm __inline volatile(
		"# ALT: oldinstr\n"
		"771:\n\t"
		"# ALT: oldinstr\n"
		"771:\n\t"
		"call %c[old]"
		"\n772:\n"
		"# ALT: padding\n"
		".skip -((("
		"775f-774f"
		")-("
		"772b-771b"
		")) > 0) * "
		"(("
		"775f-774f"
		")-("
		"772b-771b"
		")),0x90\n"
		"773:\n"
		".pushsection .altinstructions,\"a\"\n"
		" .long 771b - .\n"
		" .long 774f - .\n"
		" .4byte "
		"( 3*32+16)"
		"\n"
		" .byte "
		"773b-771b"
		"\n"
		" .byte "
		"775f-774f"
		"\n"
		".popsection\n"
		".pushsection .altinstr_replacement, \"ax\"\n"
		"# ALT: replacement\n"
		"774:\n\t"
		"call %c[new1]"
		"\n775:\n"
		".popsection\n"
		"\n772:\n"
		"# ALT: padding\n"
		".skip -((("
		"775f-774f"
		")-("
		"772b-771b"
		")) > 0) * "
		"(("
		"775f-774f"
		")-("
		"772b-771b"
		")),0x90\n"
		"773:\n"
		".pushsection .altinstructions,\"a\"\n"
		" .long 771b - .\n"
		" .long 774f - .\n"
		" .4byte "
		"( 9*32+ 9)"
		"\n"
		" .byte "
		"773b-771b"
		"\n"
		" .byte "
		"775f-774f"
		"\n"
		".popsection\n"
		".pushsection .altinstr_replacement, \"ax\"\n"
		"# ALT: replacement\n"
		"774:\n\t"
		"call %c[new2]"
		"\n775:\n"
		".popsection\n"
		: "+r"(current_stack_pointer), "=D"(page)
		: [old] "i"(clear_page_orig), [new1] "i"(clear_page_rep),
		  [new2] "i"(clear_page_erms), "D"(page)
		: "cc", "memory", "rax", "rcx");
}

void copy_page(void *to, void *from);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
task_size_max(void)
{
	unsigned long ret;

	asm __inline volatile(
		"# ALT: oldinstr\n"
		"771:\n\t"
		"movq %[small],%0"
		"\n772:\n"
		"# ALT: padding\n"
		".skip -((("
		"775f-774f"
		")-("
		"772b-771b"
		")) > 0) * "
		"(("
		"775f-774f"
		")-("
		"772b-771b"
		")),0x90\n"
		"773:\n"
		".pushsection .altinstructions,\"a\"\n"
		" .long 771b - .\n"
		" .long 774f - .\n"
		" .4byte "
		"(16*32+16)"
		"\n"
		" .byte "
		"773b-771b"
		"\n"
		" .byte "
		"775f-774f"
		"\n"
		".popsection\n"
		".pushsection .altinstr_replacement, \"ax\"\n"
		"# ALT: replacement\n"
		"774:\n\t"
		"movq %[large],%0"
		"\n775:\n"
		".popsection\n"
		: "=r"(ret)
		: "i"(0), [small] "i"((1ul << 47) - ((1UL) << 12)),
		  [large] "i"((1ul << 56) - ((1UL) << 12)));

	return ret;
}

struct page;

struct range {
	u64 start;
	u64 end;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
range_len(const struct range *range)
{
	return range->end - range->start + 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
range_contains(struct range *r1, struct range *r2)
{
	return r1->start <= r2->start && r1->end >= r2->end;
}

int add_range(struct range *range, int az, int nr_range, u64 start, u64 end);

int add_range_with_merge(struct range *range, int az, int nr_range, u64 start,
			 u64 end);

void subtract_range(struct range *range, int az, u64 start, u64 end);

int clean_sort_range(struct range *range, int az);

void sort_range(struct range *range, int nr_range);
extern struct range pfn_mapped[];
extern int nr_pfn_mapped;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_user_page(void *page, unsigned long vaddr, struct page *pg)
{
	clear_page(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
copy_user_page(void *to, void *from, unsigned long vaddr, struct page *topage)
{
	copy_page(to, from);
}
extern bool __virt_addr_valid(unsigned long kaddr);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) void *
pfn_to_kaddr(unsigned long pfn)
{
	return ((void *)((unsigned long)(pfn << 12) +
			 ((unsigned long)page_offset_base)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
__canonical_address(u64 vaddr, u8 vaddr_bits)
{
	return ((s64)vaddr << (64 - vaddr_bits)) >> (64 - vaddr_bits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
__is_canonical_address(u64 vaddr, u8 vaddr_bits)
{
	return __canonical_address(vaddr, vaddr_bits) == vaddr;
}

typedef struct {
	u64 val;
} pfn_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((const)) int
__ilog2_u32(u32 n)
{
	return fls(n) - 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((const)) int
__ilog2_u64(u64 n)
{
	return fls64(n) - 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((const)) bool
is_power_of_2(unsigned long n)
{
	return (n != 0 && ((n & (n - 1)) == 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((const)) unsigned long
__roundup_pow_of_two(unsigned long n)
{
	return 1UL << fls_long(n - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((const)) unsigned long
__rounddown_pow_of_two(unsigned long n)
{
	return 1UL << (fls_long(n) - 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__const__)) int
__order_base_2(unsigned long n)
{
	return n > 1 ? (__builtin_constant_p(n - 1) ?
				((n - 1) < 2 ? 0 :
					       63 - __builtin_clzll(n - 1)) :
			(sizeof(n - 1) <= 4) ? __ilog2_u32(n - 1) :
					       __ilog2_u64(n - 1)) +
			       1 :
		       0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((const)) int
__bits_per(unsigned long n)
{
	if (n < 2)
		return 1;
	if (is_power_of_2(n))
		return (__builtin_constant_p(n) ?
				(((n) == 0 || (n) == 1) ?
					 0 :
					 (__builtin_constant_p((n)-1) ?
						  (((n)-1) < 2 ?
							   0 :
							   63 - __builtin_clzll(
									(n)-1)) :
					  (sizeof((n)-1) <= 4) ?
						  __ilog2_u32((n)-1) :
						  __ilog2_u64((n)-1)) +
						 1) :
				__order_base_2(n)) +
		       1;
	return (__builtin_constant_p(n) ?
			(((n) == 0 || (n) == 1) ?
				 0 :
				 (__builtin_constant_p((n)-1) ?
					  (((n)-1) < 2 ? 0 :
							 63 - __builtin_clzll(
								      (n)-1)) :
				  (sizeof((n)-1) <= 4) ? __ilog2_u32((n)-1) :
							 __ilog2_u64((n)-1)) +
					 1) :
			__order_base_2(n));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__const__)) int
get_order(unsigned long size)
{
	if (__builtin_constant_p(size)) {
		if (!size)
			return 64 - 12;

		if (size < (1UL << 12))
			return 0;

		return (__builtin_constant_p((size)-1) ?
				(((size)-1) < 2 ?
					 0 :
					 63 - __builtin_clzll((size)-1)) :
			(sizeof((size)-1) <= 4) ? __ilog2_u32((size)-1) :
						  __ilog2_u64((size)-1)) -
		       12 + 1;
	}

	size--;
	size >>= 12;

	return fls64(size);
}

struct task_struct;

struct task_struct;
struct mm_struct;
struct io_bitmap;
struct vm86;

struct fred_cs {
	u64 cs : 16,

		sl : 2,

		wfe : 1, : 45;
};

struct fred_ss {
	u64 ss : 16,

		sti : 1,

		swevent : 1,

		nmi : 1, : 13,

		vector : 8, : 8,

		type : 4, : 4,

		enclave : 1,

		lm : 1,

		nested : 1, : 1,

		insnlen : 4;
};

struct pt_regs {
	unsigned long r15;
	unsigned long r14;
	unsigned long r13;
	unsigned long r12;
	unsigned long bp;
	unsigned long bx;

	unsigned long r11;
	unsigned long r10;
	unsigned long r9;
	unsigned long r8;
	unsigned long ax;
	unsigned long cx;
	unsigned long dx;
	unsigned long si;
	unsigned long di;
	unsigned long orig_ax;

	unsigned long ip;

	union {
		u16 cs;

		u64 csx;

		struct fred_cs fred_cs;
	};

	unsigned long flags;
	unsigned long sp;

	union {
		u16 ss;

		u64 ssx;

		struct fred_ss fred_ss;
	};
};

struct desc_struct {
	u16 limit0;
	u16 base0;
	u16 base1 : 8, type : 4, s : 1, dpl : 2, p : 1;
	u16 limit1 : 4, avl : 1, l : 1, d : 1, g : 1, base2 : 8;
} __attribute__((packed));
enum {
	GATE_INTERRUPT = 0xE,
	GATE_TRAP = 0xF,
	GATE_CALL = 0xC,
	GATE_TASK = 0x5,
};

enum {
	DESC_TSS = 0x9,
	DESC_LDT = 0x2,
	DESCTYPE_S = 0x10,
};

struct ldttss_desc {
	u16 limit0;
	u16 base0;

	u16 base1 : 8, type : 5, dpl : 2, p : 1;
	u16 limit1 : 4, zero0 : 3, g : 1, base2 : 8;

	u32 base3;
	u32 zero1;

} __attribute__((packed));

typedef struct ldttss_desc ldt_desc;
typedef struct ldttss_desc tss_desc;

struct idt_bits {
	u16 ist : 3, zero : 5, type : 5, dpl : 2, p : 1;
} __attribute__((packed));

struct idt_data {
	unsigned int vector;
	unsigned int segment;
	struct idt_bits bits;
	const void *addr;
};

struct gate_struct {
	u16 offset_low;
	u16 segment;
	struct idt_bits bits;
	u16 offset_middle;

	u32 offset_high;
	u32 reserved;

} __attribute__((packed));

typedef struct gate_struct gate_desc;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
gate_offset(const gate_desc *g)
{
	return g->offset_low | ((unsigned long)g->offset_middle << 16) |
	       ((unsigned long)g->offset_high << 32);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
gate_segment(const gate_desc *g)
{
	return g->segment;
}

struct desc_ptr {
	unsigned short size;
	unsigned long address;
} __attribute__((packed));
enum page_cache_mode {
	_PAGE_CACHE_MODE_WB = 0,
	_PAGE_CACHE_MODE_WC = 1,
	_PAGE_CACHE_MODE_UC_MINUS = 2,
	_PAGE_CACHE_MODE_UC = 3,
	_PAGE_CACHE_MODE_WT = 4,
	_PAGE_CACHE_MODE_WP = 5,

	_PAGE_CACHE_MODE_NUM = 8
};

typedef unsigned long pteval_t;
typedef unsigned long pmdval_t;
typedef unsigned long pudval_t;
typedef unsigned long p4dval_t;
typedef unsigned long pgdval_t;
typedef unsigned long pgprotval_t;

typedef struct {
	pteval_t pte;
} pte_t;
typedef struct {
	pmdval_t pmd;
} pmd_t;

extern unsigned int __pgtable_l5_enabled;
extern unsigned int pgdir_shift;
extern unsigned int ptrs_per_p4d;
typedef struct pgprot {
	pgprotval_t pgprot;
} pgprot_t;

typedef struct {
	pgdval_t pgd;
} pgd_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgprot_t
pgprot_nx(pgprot_t prot)
{
	return ((pgprot_t){ (((prot).pgprot) | (((pteval_t)(1)) << 63)) });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgd_t
native_make_pgd(pgdval_t val)
{
	return (pgd_t){ val & (~0ULL) };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgdval_t
native_pgd_val(pgd_t pgd)
{
	return pgd.pgd & (~0ULL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgdval_t
pgd_flags(pgd_t pgd)
{
	return native_pgd_val(pgd) &
	       (~((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			     ((phys_addr_t)((1ULL << 52) - 1)))));
}

typedef struct {
	p4dval_t p4d;
} p4d_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) p4d_t
native_make_p4d(pudval_t val)
{
	return (p4d_t){ val };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) p4dval_t
native_p4d_val(p4d_t p4d)
{
	return p4d.p4d;
}
typedef struct {
	pudval_t pud;
} pud_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
native_make_pud(pmdval_t val)
{
	return (pud_t){ val };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pudval_t
native_pud_val(pud_t pud)
{
	return pud.pud;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
native_make_pmd(pmdval_t val)
{
	return (pmd_t){ .pmd = val };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmdval_t
native_pmd_val(pmd_t pmd)
{
	return pmd.pmd;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) p4dval_t
p4d_pfn_mask(p4d_t p4d)
{
	return ((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			   ((phys_addr_t)((1ULL << 52) - 1))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) p4dval_t
p4d_flags_mask(p4d_t p4d)
{
	return ~p4d_pfn_mask(p4d);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) p4dval_t
p4d_flags(p4d_t p4d)
{
	return native_p4d_val(p4d) & p4d_flags_mask(p4d);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pudval_t
pud_pfn_mask(pud_t pud)
{
	if (native_pud_val(pud) & (((pteval_t)(1)) << 7))
		return (((signed long)(~(((1UL) << 30) - 1))) &
			((phys_addr_t)((1ULL << 52) - 1)));
	else
		return ((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
				   ((phys_addr_t)((1ULL << 52) - 1))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pudval_t
pud_flags_mask(pud_t pud)
{
	return ~pud_pfn_mask(pud);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pudval_t
pud_flags(pud_t pud)
{
	return native_pud_val(pud) & pud_flags_mask(pud);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmdval_t
pmd_pfn_mask(pmd_t pmd)
{
	if (native_pmd_val(pmd) & (((pteval_t)(1)) << 7))
		return (((signed long)(~(((1UL) << 21) - 1))) &
			((phys_addr_t)((1ULL << 52) - 1)));
	else
		return ((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
				   ((phys_addr_t)((1ULL << 52) - 1))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmdval_t
pmd_flags_mask(pmd_t pmd)
{
	return ~pmd_pfn_mask(pmd);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmdval_t
pmd_flags(pmd_t pmd)
{
	return native_pmd_val(pmd) & pmd_flags_mask(pmd);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
native_make_pte(pteval_t val)
{
	return (pte_t){ .pte = val };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pteval_t
native_pte_val(pte_t pte)
{
	return pte.pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pteval_t
pte_flags(pte_t pte)
{
	return native_pte_val(pte) &
	       (~((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			     ((phys_addr_t)((1ULL << 52) - 1)))));
}
unsigned long cachemode2protval(enum page_cache_mode pcm);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgprotval_t
protval_4k_2_large(pgprotval_t val)
{
	return (val & ~((((pteval_t)(1)) << 7) | (((pteval_t)(1)) << 12))) |
	       ((val & (((pteval_t)(1)) << 7)) << (12 - 7));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgprot_t
pgprot_4k_2_large(pgprot_t pgprot)
{
	return ((pgprot_t){ (protval_4k_2_large(((pgprot).pgprot))) });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgprotval_t
protval_large_2_4k(pgprotval_t val)
{
	return (val & ~((((pteval_t)(1)) << 7) | (((pteval_t)(1)) << 12))) |
	       ((val & (((pteval_t)(1)) << 12)) >> (12 - 7));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgprot_t
pgprot_large_2_4k(pgprot_t pgprot)
{
	return ((pgprot_t){ (protval_large_2_4k(((pgprot).pgprot))) });
}

typedef struct page *pgtable_t;

extern pteval_t __supported_pte_mask;
extern pteval_t __default_kernel_pte_mask;

extern pgprot_t pgprot_writecombine(pgprot_t prot);

extern pgprot_t pgprot_writethrough(pgprot_t prot);

struct file;
pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
			      unsigned long size, pgprot_t vma_prot);

void set_pte_vaddr(unsigned long vaddr, pte_t pte);

enum pg_level {
	PG_LEVEL_NONE,
	PG_LEVEL_4K,
	PG_LEVEL_2M,
	PG_LEVEL_1G,
	PG_LEVEL_512G,
	PG_LEVEL_256T,
	PG_LEVEL_NUM
};

extern void update_page_count(int level, unsigned long pages);
extern pte_t *lookup_address(unsigned long address, unsigned int *level);
extern pte_t *lookup_address_in_pgd(pgd_t *pgd, unsigned long address,
				    unsigned int *level);
pte_t *lookup_address_in_pgd_attr(pgd_t *pgd, unsigned long address,
				  unsigned int *level, bool *nx, bool *rw);
extern pmd_t *lookup_pmd_address(unsigned long address);
extern phys_addr_t slow_virt_to_phys(void *__address);
extern int __attribute__((__section__(".init.text"))) __attribute__((__cold__))
kernel_map_pages_in_pgd(pgd_t *pgd, u64 pfn, unsigned long address,
			unsigned numpages, unsigned long page_flags);
extern int __attribute__((__section__(".init.text"))) __attribute__((__cold__))
kernel_unmap_pages_in_pgd(pgd_t *pgd, unsigned long address,
			  unsigned long numpages);

struct page;
struct thread_struct;
struct desc_ptr;
struct tss_struct;
struct mm_struct;
struct desc_struct;
struct task_struct;
struct cpumask;
struct flush_tlb_info;
struct mmu_gather;
struct vm_area_struct;

struct paravirt_callee_save {
	void *func;
};

struct pv_info {
	const char *name;
};
struct pv_cpu_ops {
	void (*io_delay)(void);
};

struct pv_irq_ops {};

struct pv_mmu_ops {
	void (*flush_tlb_user)(void);
	void (*flush_tlb_kernel)(void);
	void (*flush_tlb_one_user)(unsigned long addr);
	void (*flush_tlb_multi)(const struct cpumask *cpus,
				const struct flush_tlb_info *info);

	void (*tlb_remove_table)(struct mmu_gather *tlb, void *table);

	void (*exit_mmap)(struct mm_struct *mm);
	void (*notify_page_enc_status_changed)(unsigned long pfn, int npages,
					       bool enc);
};

struct arch_spinlock;

struct qspinlock;

struct pv_lock_ops {
	void (*queued_spin_lock_slowpath)(struct qspinlock *lock, u32 val);
	struct paravirt_callee_save queued_spin_unlock;

	void (*wait)(u8 *ptr, u8 val);
	void (*kick)(int cpu);

	struct paravirt_callee_save vcpu_is_preempted;
};

struct paravirt_patch_template {
	struct pv_cpu_ops cpu;
	struct pv_irq_ops irq;
	struct pv_mmu_ops mmu;
	struct pv_lock_ops lock;
};

extern struct pv_info pv_info;
extern struct paravirt_patch_template pv_ops;

int paravirt_disable_iospace(void);
unsigned long paravirt_ret0(void);

struct user_desc {
	unsigned int entry_number;
	unsigned int base_addr;
	unsigned int limit;
	unsigned int seg_32bit : 1;
	unsigned int contents : 2;
	unsigned int read_exec_only : 1;
	unsigned int limit_in_pages : 1;
	unsigned int seg_not_present : 1;
	unsigned int useable : 1;
	unsigned int lm : 1;
};

struct task_struct;

void syscall_init(void);

void entry_SYSCALL_64(void);
void entry_SYSCALL_64_safe_stack(void);
void entry_SYSRETQ_unsafe_stack(void);
void entry_SYSRETQ_end(void);
long do_arch_prctl_64(struct task_struct *task, int option, unsigned long arg2);
void entry_SYSENTER_compat(void);
void __end_entry_SYSENTER_compat(void);
void entry_SYSCALL_compat(void);
void entry_SYSCALL_compat_safe_stack(void);
void entry_SYSRETL_compat_unsafe_stack(void);
void entry_SYSRETL_compat_end(void);

void x86_configure_nx(void);

extern int reboot_force;

long do_arch_prctl_common(int option, unsigned long arg2);

struct cpuinfo_x86;
struct task_struct;

extern unsigned long profile_pc(struct pt_regs *regs);

extern unsigned long convert_ip_to_linear(struct task_struct *child,
					  struct pt_regs *regs);
extern void send_sigtrap(struct pt_regs *regs, int error_code, int si_code);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
regs_return_value(struct pt_regs *regs)
{
	return regs->ax;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
regs_set_return_value(struct pt_regs *regs, unsigned long rc)
{
	regs->ax = rc;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
user_mode(struct pt_regs *regs)
{
	return !!(regs->cs & 3);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
v8086_mode(struct pt_regs *regs)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
user_64bit_mode(struct pt_regs *regs)
{
	return regs->cs == (6 * 8 + 3);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
any_64bit_mode(struct pt_regs *regs)
{
	return !user_mode(regs) || user_64bit_mode(regs);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
ip_within_syscall_gap(struct pt_regs *regs)
{
	bool ret = (regs->ip >= (unsigned long)entry_SYSCALL_64 &&
		    regs->ip < (unsigned long)entry_SYSCALL_64_safe_stack);

	ret = ret || (regs->ip >= (unsigned long)entry_SYSRETQ_unsafe_stack &&
		      regs->ip < (unsigned long)entry_SYSRETQ_end);

	ret = ret ||
	      (regs->ip >= (unsigned long)entry_SYSCALL_compat &&
	       regs->ip < (unsigned long)entry_SYSCALL_compat_safe_stack);
	ret = ret ||
	      (regs->ip >= (unsigned long)entry_SYSRETL_compat_unsafe_stack &&
	       regs->ip < (unsigned long)entry_SYSRETL_compat_end);

	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
kernel_stack_pointer(struct pt_regs *regs)
{
	return regs->sp;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
instruction_pointer(struct pt_regs *regs)
{
	return regs->ip;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
instruction_pointer_set(struct pt_regs *regs, unsigned long val)
{
	regs->ip = val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
frame_pointer(struct pt_regs *regs)
{
	return regs->bp;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
user_stack_pointer(struct pt_regs *regs)
{
	return regs->sp;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
user_stack_pointer_set(struct pt_regs *regs, unsigned long val)
{
	regs->sp = val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
regs_irqs_disabled(struct pt_regs *regs)
{
	return !(regs->flags & (((1UL)) << (9)));
}

extern int regs_query_register_offset(const char *name);
extern const char *regs_query_register_name(unsigned int offset);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
regs_get_register(struct pt_regs *regs, unsigned int offset)
{
	if (__builtin_expect(
		    !!(offset > (__builtin_offsetof(struct pt_regs, ss))), 0))
		return 0;
	return *(unsigned long *)((unsigned long)regs + offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr)
{
	return ((addr & ~((((1UL) << 12) << (2 + 0)) - 1)) ==
		(regs->sp & ~((((1UL) << 12) << (2 + 0)) - 1)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long *
regs_get_kernel_stack_nth_addr(struct pt_regs *regs, unsigned int n)
{
	unsigned long *addr = (unsigned long *)regs->sp;

	addr += n;
	if (regs_within_kernel_stack(regs, (unsigned long)addr))
		return addr;
	else
		return ((void *)0);
}

extern long copy_from_kernel_nofault(void *dst, const void *src, size_t size);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
regs_get_kernel_stack_nth(struct pt_regs *regs, unsigned int n)
{
	unsigned long *addr;
	unsigned long val;
	long ret;

	addr = regs_get_kernel_stack_nth_addr(regs, n);
	if (addr) {
		ret = copy_from_kernel_nofault(&val, addr, sizeof(val));
		if (!ret)
			return val;
	}
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
regs_get_kernel_argument(struct pt_regs *regs, unsigned int n)
{
	static const unsigned int argument_offs[] = {

		__builtin_offsetof(struct pt_regs, di),
		__builtin_offsetof(struct pt_regs, si),
		__builtin_offsetof(struct pt_regs, dx),
		__builtin_offsetof(struct pt_regs, cx),
		__builtin_offsetof(struct pt_regs, r8),
		__builtin_offsetof(struct pt_regs, r9),

	};

	if (n >= 6) {
		n -= 6 - 1;
		return regs_get_kernel_stack_nth(regs, n);
	} else
		return regs_get_register(regs, argument_offs[n]);
}
struct user_desc;
extern int do_get_thread_area(struct task_struct *p, int idx,
			      struct user_desc *info);
extern int do_set_thread_area(struct task_struct *p, int idx,
			      struct user_desc *info, int can_allocate);

struct math_emu_info {
	long ___orig_eip;
	struct pt_regs *regs;
};

struct _fpx_sw_bytes {
	__u32 magic1;
	__u32 extended_size;

	__u64 xfeatures;

	__u32 xstate_size;

	__u32 padding[7];
};
struct _fpreg {
	__u16 significand[4];
	__u16 exponent;
};

struct _fpxreg {
	__u16 significand[4];
	__u16 exponent;
	__u16 padding[3];
};

struct _xmmreg {
	__u32 element[4];
};

struct _fpstate_32 {
	__u32 cw;
	__u32 sw;
	__u32 tag;
	__u32 ipoff;
	__u32 cssel;
	__u32 dataoff;
	__u32 datasel;
	struct _fpreg _st[8];
	__u16 status;
	__u16 magic;

	__u32 _fxsr_env[6];
	__u32 mxcsr;
	__u32 reserved;
	struct _fpxreg _fxsr_st[8];
	struct _xmmreg _xmm[8];
	union {
		__u32 padding1[44];
		__u32 padding[44];
	};

	union {
		__u32 padding2[12];
		struct _fpx_sw_bytes sw_reserved;
	};
};
struct _fpstate_64 {
	__u16 cwd;
	__u16 swd;

	__u16 twd;
	__u16 fop;
	__u64 rip;
	__u64 rdp;
	__u32 mxcsr;
	__u32 mxcsr_mask;
	__u32 st_space[32];
	__u32 xmm_space[64];
	__u32 reserved2[12];
	union {
		__u32 reserved3[12];
		struct _fpx_sw_bytes sw_reserved;
	};
};

struct _header {
	__u64 xfeatures;
	__u64 reserved1[2];
	__u64 reserved2[5];
};

struct _ymmh_state {
	__u32 ymmh_space[64];
};
struct _xstate {
	struct _fpstate_64 fpstate;
	struct _header xstate_hdr;
	struct _ymmh_state ymmh;
};

struct sigcontext_32 {
	__u16 gs, __gsh;
	__u16 fs, __fsh;
	__u16 es, __esh;
	__u16 ds, __dsh;
	__u32 di;
	__u32 si;
	__u32 bp;
	__u32 sp;
	__u32 bx;
	__u32 dx;
	__u32 cx;
	__u32 ax;
	__u32 trapno;
	__u32 err;
	__u32 ip;
	__u16 cs, __csh;
	__u32 flags;
	__u32 sp_at_signal;
	__u16 ss, __ssh;
	__u32 fpstate;
	__u32 oldmask;
	__u32 cr2;
};

struct sigcontext_64 {
	__u64 r8;
	__u64 r9;
	__u64 r10;
	__u64 r11;
	__u64 r12;
	__u64 r13;
	__u64 r14;
	__u64 r15;
	__u64 di;
	__u64 si;
	__u64 bp;
	__u64 bx;
	__u64 dx;
	__u64 ax;
	__u64 cx;
	__u64 sp;
	__u64 ip;
	__u64 flags;
	__u16 cs;
	__u16 gs;
	__u16 fs;
	__u16 ss;
	__u64 err;
	__u64 trapno;
	__u64 oldmask;
	__u64 cr2;
	__u64 fpstate;
	__u64 reserved1[8];
};

extern void *memcpy(void *to, const void *from, size_t len);
extern void *__memcpy(void *to, const void *from, size_t len);

void *memset(void *s, int c, size_t n);
void *__memset(void *s, int c, size_t n);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
memset16(uint16_t *s, uint16_t v, size_t n)
{
	const __auto_type s0 = s;
	asm volatile("rep stosw" : "+D"(s), "+c"(n) : "a"(v) : "memory");
	return s0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
memset32(uint32_t *s, uint32_t v, size_t n)
{
	const __auto_type s0 = s;
	asm volatile("rep stosl" : "+D"(s), "+c"(n) : "a"(v) : "memory");
	return s0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
memset64(uint64_t *s, uint64_t v, size_t n)
{
	const __auto_type s0 = s;
	asm volatile("rep stosq" : "+D"(s), "+c"(n) : "a"(v) : "memory");
	return s0;
}

void *memmove(void *dest, const void *src, size_t count);
void *__memmove(void *dest, const void *src, size_t count);

int memcmp(const void *cs, const void *ct, size_t count);
size_t strlen(const char *s);
char *strcpy(char *dest, const char *src);
char *strcat(char *dest, const char *src);
int strcmp(const char *cs, const char *ct);

void __memcpy_flushcache(void *dst, const void *src, size_t cnt);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
memcpy_flushcache(void *dst, const void *src, size_t cnt)
{
	if (__builtin_constant_p(cnt)) {
		switch (cnt) {
		case 4:
			asm("movntil %1, %0"
			    : "=m"(*(u32 *)dst)
			    : "r"(*(u32 *)src));
			return;
		case 8:
			asm("movntiq %1, %0"
			    : "=m"(*(u64 *)dst)
			    : "r"(*(u64 *)src));
			return;
		case 16:
			asm("movntiq %1, %0"
			    : "=m"(*(u64 *)dst)
			    : "r"(*(u64 *)src));
			asm("movntiq %1, %0"
			    : "=m"(*(u64 *)(dst + 8))
			    : "r"(*(u64 *)(src + 8)));
			return;
		}
	}
	__memcpy_flushcache(dst, src, cnt);
}

struct cpuid_regs {
	u32 eax, ebx, ecx, edx;
};

enum cpuid_regs_idx {
	CPUID_EAX = 0,
	CPUID_EBX,
	CPUID_ECX,
	CPUID_EDX,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
have_cpuid_p(void)
{
	return 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_cpuid(unsigned int *eax, unsigned int *ebx, unsigned int *ecx,
	     unsigned int *edx)
{
	asm volatile("cpuid"
		     : "=a"(*eax), "=b"(*ebx), "=c"(*ecx), "=d"(*edx)
		     : "0"(*eax), "2"(*ecx)
		     : "memory");
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
native_cpuid_eax(unsigned int op)
{
	unsigned int eax = op, ebx, ecx = 0, edx;
	native_cpuid(&eax, &ebx, &ecx, &edx);
	return eax;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
native_cpuid_ebx(unsigned int op)
{
	unsigned int eax = op, ebx, ecx = 0, edx;
	native_cpuid(&eax, &ebx, &ecx, &edx);
	return ebx;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
native_cpuid_ecx(unsigned int op)
{
	unsigned int eax = op, ebx, ecx = 0, edx;
	native_cpuid(&eax, &ebx, &ecx, &edx);
	return ecx;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
native_cpuid_edx(unsigned int op)
{
	unsigned int eax = op, ebx, ecx = 0, edx;
	native_cpuid(&eax, &ebx, &ecx, &edx);
	return edx;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cpuid(unsigned int op, unsigned int *eax, unsigned int *ebx, unsigned int *ecx,
      unsigned int *edx)
{
	*eax = op;
	*ecx = 0;
	native_cpuid(eax, ebx, ecx, edx);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cpuid_count(unsigned int op, int count, unsigned int *eax, unsigned int *ebx,
	    unsigned int *ecx, unsigned int *edx)
{
	*eax = op;
	*ecx = count;
	native_cpuid(eax, ebx, ecx, edx);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
cpuid_eax(unsigned int op)
{
	unsigned int eax, ebx, ecx, edx;

	cpuid(op, &eax, &ebx, &ecx, &edx);

	return eax;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
cpuid_ebx(unsigned int op)
{
	unsigned int eax, ebx, ecx, edx;

	cpuid(op, &eax, &ebx, &ecx, &edx);

	return ebx;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
cpuid_ecx(unsigned int op)
{
	unsigned int eax, ebx, ecx, edx;

	cpuid(op, &eax, &ebx, &ecx, &edx);

	return ecx;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
cpuid_edx(unsigned int op)
{
	unsigned int eax, ebx, ecx, edx;

	cpuid(op, &eax, &ebx, &ecx, &edx);

	return edx;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__cpuid_read(unsigned int leaf, unsigned int subleaf, u32 *regs)
{
	regs[CPUID_EAX] = leaf;
	regs[CPUID_ECX] = subleaf;
	native_cpuid(regs + CPUID_EAX, regs + CPUID_EBX, regs + CPUID_ECX,
		     regs + CPUID_EDX);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__cpuid_read_reg(unsigned int leaf, unsigned int subleaf,
		 enum cpuid_regs_idx regidx, u32 *reg)
{
	u32 regs[4];

	__cpuid_read(leaf, subleaf, regs);
	*reg = regs[regidx];
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpuid_function_is_indexed(u32 function)
{
	switch (function) {
	case 4:
	case 7:
	case 0xb:
	case 0xd:
	case 0xf:
	case 0x10:
	case 0x12:
	case 0x14:
	case 0x17:
	case 0x18:
	case 0x1d:
	case 0x1e:
	case 0x1f:
	case 0x24:
	case 0x8000001d:
		return true;
	}

	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) uint32_t
hypervisor_cpuid_base(const char *sig, uint32_t leaves)
{
	uint32_t base, eax, signature[3];

	for (base = 0x40000000; base < 0x40010000; base += 0x100) {
		cpuid(base, &eax, &signature[0], &signature[1], &signature[2]);

		if (!__builtin_memcmp(sig, signature, 12) &&
		    (leaves == 0 || ((eax - base) >= leaves)))
			return base;
	}

	return 0;
}

void native_write_cr0(unsigned long val);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
native_read_cr0(void)
{
	unsigned long val;
	asm volatile("mov %%cr0,%0\n\t"
		     : "=r"(val)
		     : "m"(*(unsigned int *)0x1000UL));
	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
native_read_cr2(void)
{
	unsigned long val;
	asm volatile("mov %%cr2,%0\n\t"
		     : "=r"(val)
		     : "m"(*(unsigned int *)0x1000UL));
	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
native_write_cr2(unsigned long val)
{
	asm volatile("mov %0,%%cr2" : : "r"(val) : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
__native_read_cr3(void)
{
	unsigned long val;
	asm volatile("mov %%cr3,%0\n\t"
		     : "=r"(val)
		     : "m"(*(unsigned int *)0x1000UL));
	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_write_cr3(unsigned long val)
{
	asm volatile("mov %0,%%cr3" : : "r"(val) : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
native_read_cr4(void)
{
	unsigned long val;
	asm volatile("mov %%cr4,%0\n\t"
		     : "=r"(val)
		     : "m"(*(unsigned int *)0x1000UL));

	return val;
}

void native_write_cr4(unsigned long val);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
rdpkru(void)
{
	u32 ecx = 0;
	u32 edx, pkru;

	asm volatile(".byte 0x0f,0x01,0xee\n\t"
		     : "=a"(pkru), "=d"(edx)
		     : "c"(ecx));
	return pkru;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
wrpkru(u32 pkru)
{
	u32 ecx = 0, edx = 0;

	asm volatile(".byte 0x0f,0x01,0xef\n\t"
		     :
		     : "a"(pkru), "c"(ecx), "d"(edx));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
native_wbinvd(void)
{
	asm volatile("wbinvd" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
__read_cr4(void)
{
	return native_read_cr4();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
read_cr0(void)
{
	return native_read_cr0();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
write_cr0(unsigned long x)
{
	native_write_cr0(x);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
read_cr2(void)
{
	return native_read_cr2();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
write_cr2(unsigned long x)
{
	native_write_cr2(x);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
__read_cr3(void)
{
	return __native_read_cr3();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
write_cr3(unsigned long x)
{
	native_write_cr3(x);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__write_cr4(unsigned long x)
{
	native_write_cr4(x);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
wbinvd(void)
{
	native_wbinvd();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
clflush(volatile void *__p)
{
	asm volatile("clflush %0" : "+m"(*(volatile char *)__p));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clflushopt(volatile void *__p)
{
	asm __inline volatile("# ALT: oldinstr\n"
			      "771:\n\t"
			      ".byte 0x3e; clflush %0"
			      "\n772:\n"
			      "# ALT: padding\n"
			      ".skip -((("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")) > 0) * "
			      "(("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")),0x90\n"
			      "773:\n"
			      ".pushsection .altinstructions,\"a\"\n"
			      " .long 771b - .\n"
			      " .long 774f - .\n"
			      " .4byte "
			      "( 9*32+23)"
			      "\n"
			      " .byte "
			      "773b-771b"
			      "\n"
			      " .byte "
			      "775f-774f"
			      "\n"
			      ".popsection\n"
			      ".pushsection .altinstr_replacement, \"ax\"\n"
			      "# ALT: replacement\n"
			      "774:\n\t"
			      ".byte 0x66; clflush %0"
			      "\n775:\n"
			      ".popsection\n"
			      : "+m"(*(volatile char *)__p)
			      : "i"(0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clwb(volatile void *__p)
{
	volatile struct {
		char x[64];
	} *p = __p;

	asm volatile("# ALT: oldinstr\n"
		     "771:\n\t"
		     "# ALT: oldinstr\n"
		     "771:\n\t"
		     ".byte 0x3e; clflush (%[pax])"
		     "\n772:\n"
		     "# ALT: padding\n"
		     ".skip -((("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")) > 0) * "
		     "(("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")),0x90\n"
		     "773:\n"
		     ".pushsection .altinstructions,\"a\"\n"
		     " .long 771b - .\n"
		     " .long 774f - .\n"
		     " .4byte "
		     "( 9*32+23)"
		     "\n"
		     " .byte "
		     "773b-771b"
		     "\n"
		     " .byte "
		     "775f-774f"
		     "\n"
		     ".popsection\n"
		     ".pushsection .altinstr_replacement, \"ax\"\n"
		     "# ALT: replacement\n"
		     "774:\n\t"
		     ".byte 0x66; clflush (%[pax])"
		     "\n775:\n"
		     ".popsection\n"
		     "\n772:\n"
		     "# ALT: padding\n"
		     ".skip -((("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")) > 0) * "
		     "(("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")),0x90\n"
		     "773:\n"
		     ".pushsection .altinstructions,\"a\"\n"
		     " .long 771b - .\n"
		     " .long 774f - .\n"
		     " .4byte "
		     "( 9*32+24)"
		     "\n"
		     " .byte "
		     "773b-771b"
		     "\n"
		     " .byte "
		     "775f-774f"
		     "\n"
		     ".popsection\n"
		     ".pushsection .altinstr_replacement, \"ax\"\n"
		     "# ALT: replacement\n"
		     "774:\n\t"
		     ".byte 0x66, 0x0f, 0xae, 0x30"
		     "\n775:\n"
		     ".popsection\n"

		     : [p] "+m"(*p)
		     : [pax] "a"(p));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
serialize(void)
{
	asm volatile(".byte 0xf, 0x1, 0xe8" ::: "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
movdir64b(void *dst, const void *src)
{
	const struct {
		char _[64];
	} *__src = src;
	struct {
		char _[64];
	} *__dst = dst;
	asm volatile(".byte 0x66, 0x0f, 0x38, 0xf8, 0x02"
		     : "+m"(*__dst)
		     : "m"(*__src), "a"(__dst), "d"(__src));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
movdir64b_io(void *dst, const void *src)
{
	movdir64b((void *)dst, src);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
enqcmds(void *dst, const void *src)
{
	const struct {
		char _[64];
	} *__src = src;
	struct {
		char _[64];
	} *__dst = dst;
	bool zf;

	asm volatile(".byte 0xf3, 0x0f, 0x38, 0xf8, 0x02, 0x66, 0x90"
		     "\n\t/* output condition code "
		     "z"
		     "*/\n"
		     : "=@cc"
		       "z"(zf),
		       "+m"(*__dst)
		     : "m"(*__src), "a"(__dst), "d"(__src));

	if (zf)
		return -11;

	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
tile_release(void)
{
	asm volatile(".byte 0xc4, 0xe2, 0x78, 0x49, 0xc0");
}
struct fregs_state {
	u32 cwd;
	u32 swd;
	u32 twd;
	u32 fip;
	u32 fcs;
	u32 foo;
	u32 fos;

	u32 st_space[20];

	u32 status;
};

struct fxregs_state {
	u16 cwd;
	u16 swd;
	u16 twd;
	u16 fop;
	union {
		struct {
			u64 rip;
			u64 rdp;
		};
		struct {
			u32 fip;
			u32 fcs;
			u32 foo;
			u32 fos;
		};
	};
	u32 mxcsr;
	u32 mxcsr_mask;

	u32 st_space[32];

	u32 xmm_space[64];

	u32 padding[12];

	union {
		u32 padding1[12];
		u32 sw_reserved[12];
	};

} __attribute__((aligned(16)));
struct swregs_state {
	u32 cwd;
	u32 swd;
	u32 twd;
	u32 fip;
	u32 fcs;
	u32 foo;
	u32 fos;

	u32 st_space[20];
	u8 ftop;
	u8 changed;
	u8 lookahead;
	u8 no_update;
	u8 rm;
	u8 alimit;
	struct math_emu_info *info;
	u32 entry_eip;
};

enum xfeature {
	XFEATURE_FP,
	XFEATURE_SSE,

	XFEATURE_YMM,
	XFEATURE_BNDREGS,
	XFEATURE_BNDCSR,
	XFEATURE_OPMASK,
	XFEATURE_ZMM_Hi256,
	XFEATURE_Hi16_ZMM,
	XFEATURE_PT_UNIMPLEMENTED_SO_FAR,
	XFEATURE_PKRU,
	XFEATURE_PASID,
	XFEATURE_CET_USER,
	XFEATURE_CET_KERNEL_UNUSED,
	XFEATURE_RSRVD_COMP_13,
	XFEATURE_RSRVD_COMP_14,
	XFEATURE_LBR,
	XFEATURE_RSRVD_COMP_16,
	XFEATURE_XTILE_CFG,
	XFEATURE_XTILE_DATA,

	XFEATURE_MAX,
};
struct reg_128_bit {
	u8 regbytes[128 / 8];
};
struct reg_256_bit {
	u8 regbytes[256 / 8];
};
struct reg_512_bit {
	u8 regbytes[512 / 8];
};
struct reg_1024_byte {
	u8 regbytes[1024];
};
struct ymmh_struct {
	struct reg_128_bit hi_ymm[16];
} __attribute__((__packed__));

struct mpx_bndreg {
	u64 lower_bound;
	u64 upper_bound;
} __attribute__((__packed__));

struct mpx_bndreg_state {
	struct mpx_bndreg bndreg[4];
} __attribute__((__packed__));

struct mpx_bndcsr {
	u64 bndcfgu;
	u64 bndstatus;
} __attribute__((__packed__));

struct mpx_bndcsr_state {
	union {
		struct mpx_bndcsr bndcsr;
		u8 pad_to_64_bytes[64];
	};
} __attribute__((__packed__));

struct avx_512_opmask_state {
	u64 opmask_reg[8];
} __attribute__((__packed__));

struct avx_512_zmm_uppers_state {
	struct reg_256_bit zmm_upper[16];
} __attribute__((__packed__));

struct avx_512_hi16_state {
	struct reg_512_bit hi16_zmm[16];
} __attribute__((__packed__));

struct pkru_state {
	u32 pkru;
	u32 pad;
} __attribute__((__packed__));

struct cet_user_state {
	u64 user_cet;

	u64 user_ssp;
};

struct lbr_entry {
	u64 from;
	u64 to;
	u64 info;
};

struct arch_lbr_state {
	u64 lbr_ctl;
	u64 lbr_depth;
	u64 ler_from;
	u64 ler_to;
	u64 ler_info;
	struct lbr_entry entries[];
};

struct xtile_cfg {
	u64 tcfg[8];
} __attribute__((__packed__));

struct xtile_data {
	struct reg_1024_byte tmm;
} __attribute__((__packed__));

struct ia32_pasid_state {
	u64 pasid;
} __attribute__((__packed__));

struct xstate_header {
	u64 xfeatures;
	u64 xcomp_bv;
	u64 reserved[6];
} __attribute__((packed));
struct xregs_state {
	struct fxregs_state i387;
	struct xstate_header header;
	u8 extended_state_area[];
} __attribute__((packed, aligned(64)));
union fpregs_state {
	struct fregs_state fsave;
	struct fxregs_state fxsave;
	struct swregs_state soft;
	struct xregs_state xsave;
	u8 __padding[((1UL) << 12)];
};

struct fpstate {
	unsigned int size;

	unsigned int user_size;

	u64 xfeatures;

	u64 user_xfeatures;

	u64 xfd;

	unsigned int is_valloc : 1;

	unsigned int is_guest : 1;
	unsigned int is_confidential : 1;

	unsigned int in_use : 1;

	union fpregs_state regs;

} __attribute__((__aligned__(64)));

struct fpu_state_perm {
	u64 __state_perm;

	unsigned int __state_size;

	unsigned int __user_state_size;
};

struct fpu {
	unsigned int last_cpu;

	unsigned long avx512_timestamp;

	struct fpstate *fpstate;

	struct fpstate *__task_fpstate;

	struct fpu_state_perm perm;

	struct fpu_state_perm guest_perm;
	struct fpstate __fpstate;
};

struct fpu_guest {
	u64 xfeatures;

	u64 perm;

	u64 xfd_err;

	unsigned int uabi_size;

	struct fpstate *fpstate;
};

struct fpu_state_config {
	unsigned int max_size;
	unsigned int default_size;

	u64 max_features;
	u64 default_features;

	u64 legacy_features;

	u64 independent_features;
};

extern struct fpu_state_config fpu_kernel_cfg, fpu_user_cfg;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
rep_nop(void)
{
	asm volatile("rep; nop" ::: "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpu_relax(void)
{
	rep_nop();
}

struct getcpu_cache;

__attribute__((no_instrument_function)) long
__vdso_getcpu(unsigned *cpu, unsigned *node, struct getcpu_cache *unused);

struct task_struct;
struct ksignal;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) long
shstk_prctl(struct task_struct *task, int option, unsigned long arg2)
{
	return -22;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
reset_thread_features(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
shstk_alloc_thread_stack(struct task_struct *p, unsigned long clone_flags,
			 unsigned long stack_size)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
shstk_free(struct task_struct *p)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
setup_signal_shadow_stack(struct ksignal *ksig)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
restore_signal_shadow_stack(void)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
shstk_update_last_frame(unsigned long val)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
shstk_is_enabled(void)
{
	return false;
}

enum {
	UNAME26 = 0x0020000,
	ADDR_NO_RANDOMIZE = 0x0040000,
	FDPIC_FUNCPTRS = 0x0080000,

	MMAP_PAGE_ZERO = 0x0100000,
	ADDR_COMPAT_LAYOUT = 0x0200000,
	READ_IMPLIES_EXEC = 0x0400000,
	ADDR_LIMIT_32BIT = 0x0800000,
	SHORT_INODE = 0x1000000,
	WHOLE_SECONDS = 0x2000000,
	STICKY_TIMEOUTS = 0x4000000,
	ADDR_LIMIT_3GB = 0x8000000,
};
enum {
	PER_LINUX = 0x0000,
	PER_LINUX_32BIT = 0x0000 | ADDR_LIMIT_32BIT,
	PER_LINUX_FDPIC = 0x0000 | FDPIC_FUNCPTRS,
	PER_SVR4 = 0x0001 | STICKY_TIMEOUTS | MMAP_PAGE_ZERO,
	PER_SVR3 = 0x0002 | STICKY_TIMEOUTS | SHORT_INODE,
	PER_SCOSVR3 = 0x0003 | STICKY_TIMEOUTS | WHOLE_SECONDS | SHORT_INODE,
	PER_OSR5 = 0x0003 | STICKY_TIMEOUTS | WHOLE_SECONDS,
	PER_WYSEV386 = 0x0004 | STICKY_TIMEOUTS | SHORT_INODE,
	PER_ISCR4 = 0x0005 | STICKY_TIMEOUTS,
	PER_BSD = 0x0006,
	PER_SUNOS = 0x0006 | STICKY_TIMEOUTS,
	PER_XENIX = 0x0007 | STICKY_TIMEOUTS | SHORT_INODE,
	PER_LINUX32 = 0x0008,
	PER_LINUX32_3GB = 0x0008 | ADDR_LIMIT_3GB,
	PER_IRIX32 = 0x0009 | STICKY_TIMEOUTS,
	PER_IRIXN32 = 0x000a | STICKY_TIMEOUTS,
	PER_IRIX64 = 0x000b | STICKY_TIMEOUTS,
	PER_RISCOS = 0x000c,
	PER_SOLARIS = 0x000d | STICKY_TIMEOUTS,
	PER_UW7 = 0x000e | STICKY_TIMEOUTS | MMAP_PAGE_ZERO,
	PER_OSF4 = 0x000f,
	PER_HPUX = 0x0010,
	PER_MASK = 0x00ff,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
mul_u64_u64_div_u64(u64 a, u64 mul, u64 div)
{
	u64 q;

	asm("mulq %2; divq %3"
	    : "=a"(q)
	    : "a"(a), "rm"(mul), "rm"(div)
	    : "rdx");

	return q;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
mul_u64_u32_div(u64 a, u32 mul, u32 div)
{
	return mul_u64_u64_div_u64(a, mul, div);
}
struct s8_fract {
	__s8 numerator;
	__s8 denominator;
};
struct u8_fract {
	__u8 numerator;
	__u8 denominator;
};
struct s16_fract {
	__s16 numerator;
	__s16 denominator;
};
struct u16_fract {
	__u16 numerator;
	__u16 denominator;
};
struct s32_fract {
	__s32 numerator;
	__s32 denominator;
};
struct u32_fract {
	__u32 numerator;
	__u32 denominator;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
reciprocal_scale(u32 val, u32 ep_ro)
{
	return (u32)(((u64)val * ep_ro) >> 32);
}

u64 int_pow(u64 base, unsigned int exp);
unsigned long int_sqrt(unsigned long);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
int_sqrt64(u64 x)
{
	return (u32)int_sqrt(x);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u32
__iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder)
{
	u32 ret = 0;

	while (dividend >= divisor) {
		asm("" : "+rm"(dividend));

		dividend -= divisor;
		ret++;
	}

	*remainder = dividend;

	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
mul_u64_u32_add_u64_shr(u64 a, u32 mul, u64 b, unsigned int shift)
{
	return (u64)((((unsigned __int128)a * mul) + b) >> shift);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
{
	*remainder = dividend % divisor;
	return dividend / divisor;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
div_s64_rem(s64 dividend, s32 divisor, s32 *remainder)
{
	*remainder = dividend % divisor;
	return dividend / divisor;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
div64_u64_rem(u64 dividend, u64 divisor, u64 *remainder)
{
	*remainder = dividend % divisor;
	return dividend / divisor;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
div64_u64(u64 dividend, u64 divisor)
{
	return dividend / divisor;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
div64_s64(s64 dividend, s64 divisor)
{
	return dividend / divisor;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
div_u64(u64 dividend, u32 divisor)
{
	u32 remainder;
	return div_u64_rem(dividend, divisor, &remainder);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
div_s64(s64 dividend, s32 divisor)
{
	s32 remainder;
	return div_s64_rem(dividend, divisor, &remainder);
}

u32 iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
mul_u32_u32(u32 a, u32 b)
{
	return (u64)a * b;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
mul_u64_u32_shr(u64 a, u32 mul, unsigned int shift)
{
	return (u64)(((unsigned __int128)a * mul) >> shift);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
mul_u64_u64_shr(u64 a, u64 mul, unsigned int shift)
{
	return (u64)(((unsigned __int128)a * mul) >> shift);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
mul_s64_u64_shr(s64 a, u64 b, unsigned int shift)
{
	u64 ret;

	ret = mul_u64_u64_shr(
		__builtin_choose_expr(
			__builtin_types_compatible_p(typeof(a),
						     signed long long) ||
				__builtin_types_compatible_p(
					typeof(a), unsigned long long),
			({
				signed long long __x = (a);
				__x < 0 ? -__x : __x;
			}),
			__builtin_choose_expr(
				__builtin_types_compatible_p(typeof(a),
							     signed long) ||
					__builtin_types_compatible_p(
						typeof(a), unsigned long),
				({
					signed long __x = (a);
					__x < 0 ? -__x : __x;
				}),
				__builtin_choose_expr(
					__builtin_types_compatible_p(
						typeof(a), signed int) ||
						__builtin_types_compatible_p(
							typeof(a), unsigned int),
					({
						signed int __x = (a);
						__x < 0 ? -__x : __x;
					}),
					__builtin_choose_expr(
						__builtin_types_compatible_p(
							typeof(a),
							signed short) ||
							__builtin_types_compatible_p(
								typeof(a),
								unsigned short),
						({
							signed short __x = (a);
							__x < 0 ? -__x : __x;
						}),
						__builtin_choose_expr(
							__builtin_types_compatible_p(
								typeof(a),
								signed char) ||
								__builtin_types_compatible_p(
									typeof(a),
									unsigned char),
							({
								signed char __x =
									(a);
								__x < 0 ? -__x :
									  __x;
							}),
							__builtin_choose_expr(
								__builtin_types_compatible_p(
									typeof(a),
									char),
								(char)({
									signed char __x =
										(a);
									__x < 0 ?
										-__x :
										__x;
								}),
								((void)0))))))),
		b, shift);

	if (a < 0)
		ret = -((s64)ret);

	return ret;
}
u64 mul_u64_u64_div_u64(u64 a, u64 mul, u64 div);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
roundup_u64(u64 x, u32 y)
{
	return ({
		       u32 _tmp = (y);
		       div_u64((x) + _tmp - 1, _tmp);
	       }) *
	       y;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *__attribute__((
	__warn_unused_result__))
ERR_PTR(long error)
{
	return (void *)error;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) long
	__attribute__((__warn_unused_result__))
	PTR_ERR(const void *ptr)
{
	return (long)ptr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
	__attribute__((__warn_unused_result__))
	IS_ERR(const void *ptr)
{
	return __builtin_expect(
		!!((unsigned long)(void *)((unsigned long)ptr) >=
		   (unsigned long)-4095),
		0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
	__attribute__((__warn_unused_result__))
	IS_ERR_OR_NULL(const void *ptr)
{
	return __builtin_expect(!!(!ptr), 0) ||
	       __builtin_expect(
		       !!((unsigned long)(void *)((unsigned long)ptr) >=
			  (unsigned long)-4095),
		       0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *__attribute__((
	__warn_unused_result__))
ERR_CAST(const void *ptr)
{
	return (void *)ptr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	PTR_ERR_OR_ZERO(const void *ptr)
{
	if (IS_ERR(ptr))
		return PTR_ERR(ptr);
	else
		return 0;
}
enum tlb_infos { ENTRIES, NR_INFO };

extern u16
	__attribute__((__section__(".data..read_mostly"))) tlb_lli_4k[NR_INFO];
extern u16
	__attribute__((__section__(".data..read_mostly"))) tlb_lli_2m[NR_INFO];
extern u16
	__attribute__((__section__(".data..read_mostly"))) tlb_lli_4m[NR_INFO];
extern u16
	__attribute__((__section__(".data..read_mostly"))) tlb_lld_4k[NR_INFO];
extern u16
	__attribute__((__section__(".data..read_mostly"))) tlb_lld_2m[NR_INFO];
extern u16
	__attribute__((__section__(".data..read_mostly"))) tlb_lld_4m[NR_INFO];
extern u16
	__attribute__((__section__(".data..read_mostly"))) tlb_lld_1g[NR_INFO];

struct cpuinfo_topology {
	u32 apicid;

	u32 initial_apicid;

	u32 pkg_id;

	u32 die_id;

	u32 cu_id;

	u32 core_id;

	u32 logical_pkg_id;
	u32 logical_die_id;

	u32 amd_node_id;

	u32 llc_id;
	u32 l2c_id;
};

struct cpuinfo_x86 {
	union {
		struct {
			__u8 x86_model;

			__u8 x86;

			__u8 x86_vendor;
			__u8 x86_reserved;
		};

		__u32 x86_vfm;
	};
	__u8 x86_stepping;

	int x86_tlbsize;

	__u32 vmx_capability[5];

	__u8 x86_virt_bits;
	__u8 x86_phys_bits;

	__u32 extended_cpuid_level;

	int cpuid_level;

	union {
		__u32 x86_capability[22 + 2];
		unsigned long x86_capability_alignment;
	};
	char x86_vendor_id[16];
	char x86_model_id[64];
	struct cpuinfo_topology topo;

	unsigned int x86_cache_size;
	int x86_cache_alignment;

	int x86_cache_max_rmid;
	int x86_cache_occ_scale;
	int x86_cache_mbm_width_offset;
	int x86_power;
	unsigned long loops_per_jiffy;

	u64 ppin;
	u16 x86_clflush_size;

	u16 booted_cores;

	u16 cpu_index;

	bool smt_active;
	u32 microcode;

	u8 x86_cache_bits;
	unsigned initialized : 1;
};
extern struct cpuinfo_x86 boot_cpu_data;
extern struct cpuinfo_x86 new_cpu_data;

extern __u32 cpu_caps_cleared[22 + 2];
extern __u32 cpu_caps_set[22 + 2];

extern __attribute__((
	section(".data..percpu"
		"..read_mostly"))) __typeof__(struct cpuinfo_x86) cpu_info;

extern const struct seq_operations cpuinfo_op;

extern void cpu_detect(struct cpuinfo_x86 *c);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long long
l1tf_pfn_limit(void)
{
	return ((((1ULL))) << (boot_cpu_data.x86_cache_bits - 1 - 12));
}

extern void early_cpu_init(void);
extern void identify_secondary_cpu(struct cpuinfo_x86 *);
extern void print_cpu_info(struct cpuinfo_x86 *);
void print_cpu_msr(struct cpuinfo_x86 *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
read_cr3_pa(void)
{
	return __read_cr3() & ((((signed long)(~(((1UL) << 12) - 1))) &
				((phys_addr_t)((1ULL << 52) - 1))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
native_read_cr3_pa(void)
{
	return __native_read_cr3() & ((((signed long)(~(((1UL) << 12) - 1))) &
				       ((phys_addr_t)((1ULL << 52) - 1))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
load_cr3(pgd_t *pgdir)
{
	write_cr3((__phys_addr_nodebug((unsigned long)(pgdir)) | 0ULL));
}
struct x86_hw_tss {
	u32 reserved1;
	u64 sp0;
	u64 sp1;

	u64 sp2;

	u64 reserved2;
	u64 ist[7];
	u32 reserved3;
	u32 reserved4;
	u16 reserved5;
	u16 io_bitmap_base;

} __attribute__((packed));
struct entry_stack {
	char stack[((1UL) << 12)];
};

struct entry_stack_page {
	struct entry_stack stack;
} __attribute__((__aligned__(((1UL) << 12))));

struct x86_io_bitmap {
	u64 prev_sequence;
	unsigned int prev_max;

	unsigned long bitmap[((65536 / 8) / sizeof(long)) + 1];

	unsigned long mapall[((65536 / 8) / sizeof(long)) + 1];
};

struct tss_struct {
	struct x86_hw_tss x86_tss;

	struct x86_io_bitmap io_bitmap;
} __attribute__((__aligned__(((1UL) << 12))));

extern __attribute__((
	section(".data..percpu"
		"..page_aligned"))) __typeof__(struct tss_struct) cpu_tss_rw
	__attribute__((__aligned__(((1UL) << 12))));

struct irq_stack {
	char stack[(((1UL) << 12) << (2 + 0))];
} __attribute__((__aligned__((((1UL) << 12) << (2 + 0)))));

struct fixed_percpu_data {
	char gs_base[40];
	unsigned long stack_canary;
};

extern __attribute__((section(
	".data..percpu"
	"..first"))) __typeof__(struct fixed_percpu_data) fixed_percpu_data;
extern typeof(fixed_percpu_data) init_per_cpu__fixed_percpu_data;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
cpu_kernelmode_gs_base(int cpu)
{
	return (unsigned long)(*({
		do {
			const void *__vpp_verify =
				(typeof((&(fixed_percpu_data.gs_base)) +
					0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((
				typeof(*((&(fixed_percpu_data.gs_base)))) *)((
				&(fixed_percpu_data.gs_base))));
			(typeof((typeof(*((&(fixed_percpu_data.gs_base)))) *)((
				&(fixed_percpu_data
					  .gs_base)))))(__ptr +
							(((__per_cpu_offset[(
								cpu)]))));
		});
	}));
}

extern void entry_SYSCALL32_ignore(void);

void current_save_fsgs(void);

struct perf_event;

struct thread_struct {
	struct desc_struct tls_array[3];

	unsigned long sp;

	unsigned short es;
	unsigned short ds;
	unsigned short fsindex;
	unsigned short gsindex;

	unsigned long fsbase;
	unsigned long gsbase;
	struct perf_event *ptrace_bps[4];

	unsigned long virtual_dr6;

	unsigned long ptrace_dr7;

	unsigned long cr2;
	unsigned long trap_nr;
	unsigned long error_code;

	struct io_bitmap *io_bitmap;

	unsigned long iopl_emul;

	unsigned int iopl_warn : 1;
	u32 pkru;
	struct fpu fpu;
};

extern void fpu_thread_struct_whitelist(unsigned long *offset,
					unsigned long *size);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
arch_thread_struct_whitelist(unsigned long *offset, unsigned long *size)
{
	fpu_thread_struct_whitelist(offset, size);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_load_sp0(unsigned long sp0)
{
	do {
		do {
			const void *__vpp_verify =
				(typeof((&(cpu_tss_rw.x86_tss.sp0)) +
					0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		switch (sizeof(cpu_tss_rw.x86_tss.sp0)) {
		case 1:
			do {
				u8 pto_val__ =
					((u8)(((unsigned long)sp0) & 0xff));
				if (0) {
					typeof(cpu_tss_rw.x86_tss.sp0) pto_tmp__;
					pto_tmp__ = (sp0);
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"b "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((*(typeof(*(
						&(cpu_tss_rw.x86_tss.sp0)))
								*)(uintptr_t)(&(
						cpu_tss_rw.x86_tss.sp0))))
					: [val] "qi"(pto_val__));
			} while (0);
			break;
		case 2:
			do {
				u16 pto_val__ =
					((u16)(((unsigned long)sp0) & 0xffff));
				if (0) {
					typeof(cpu_tss_rw.x86_tss.sp0) pto_tmp__;
					pto_tmp__ = (sp0);
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"w "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((*(typeof(*(
						&(cpu_tss_rw.x86_tss.sp0)))
								*)(uintptr_t)(&(
						cpu_tss_rw.x86_tss.sp0))))
					: [val] "ri"(pto_val__));
			} while (0);
			break;
		case 4:
			do {
				u32 pto_val__ = ((u32)(((unsigned long)sp0) &
						       0xffffffff));
				if (0) {
					typeof(cpu_tss_rw.x86_tss.sp0) pto_tmp__;
					pto_tmp__ = (sp0);
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"l "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((*(typeof(*(
						&(cpu_tss_rw.x86_tss.sp0)))
								*)(uintptr_t)(&(
						cpu_tss_rw.x86_tss.sp0))))
					: [val] "ri"(pto_val__));
			} while (0);
			break;
		case 8:
			do {
				u64 pto_val__ = ((u64)(sp0));
				if (0) {
					typeof(cpu_tss_rw.x86_tss.sp0) pto_tmp__;
					pto_tmp__ = (sp0);
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"q "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((*(typeof(*(
						&(cpu_tss_rw.x86_tss.sp0)))
								*)(uintptr_t)(&(
						cpu_tss_rw.x86_tss.sp0))))
					: [val] "re"(pto_val__));
			} while (0);
			break;
		default:
			__bad_size_call_parameter();
			break;
		}
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
native_swapgs(void)
{
	asm volatile("swapgs" ::: "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
current_top_of_stack(void)
{
	if (0)
		return ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_33(void) __attribute__((
					__error__("BUILD_BUG failed")));
				if (!(!(1)))
					__compiletime_assert_33();
			} while (0);
			(typeof(const_pcpu_hot.top_of_stack))0;
		});

	return ({
		typeof(pcpu_hot.top_of_stack) pscr_ret__;
		do {
			const void *__vpp_verify =
				(typeof((&(pcpu_hot.top_of_stack)) +
					0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		switch (sizeof(pcpu_hot.top_of_stack)) {
		case 1:
			pscr_ret__ = ({
				u8 pfo_val__;
				asm("mov"
				    "b "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "a[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "q"(pfo_val__)
				    : [var] "i"(&(pcpu_hot.top_of_stack)));
				(typeof(pcpu_hot.top_of_stack))(unsigned long)
					pfo_val__;
			});
			break;
		case 2:
			pscr_ret__ = ({
				u16 pfo_val__;
				asm("mov"
				    "w "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "a[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "r"(pfo_val__)
				    : [var] "i"(&(pcpu_hot.top_of_stack)));
				(typeof(pcpu_hot.top_of_stack))(unsigned long)
					pfo_val__;
			});
			break;
		case 4:
			pscr_ret__ = ({
				u32 pfo_val__;
				asm("mov"
				    "l "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "a[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "r"(pfo_val__)
				    : [var] "i"(&(pcpu_hot.top_of_stack)));
				(typeof(pcpu_hot.top_of_stack))(unsigned long)
					pfo_val__;
			});
			break;
		case 8:
			pscr_ret__ = ({
				u64 pfo_val__;
				asm("mov"
				    "q "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "a[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "r"(pfo_val__)
				    : [var] "i"(&(pcpu_hot.top_of_stack)));
				(typeof(pcpu_hot.top_of_stack))(unsigned long)
					pfo_val__;
			});
			break;
		default:
			__bad_size_call_parameter();
			break;
		}
		pscr_ret__;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
on_thread_stack(void)
{
	return (unsigned long)(current_top_of_stack() - current_stack_pointer) <
	       (((1UL) << 12) << (2 + 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
load_sp0(unsigned long sp0)
{
	native_load_sp0(sp0);
}

unsigned long __get_wchan(struct task_struct *p);

extern void select_idle_routine(void);
extern void amd_e400_c1e_apic_setup(void);

extern unsigned long boot_option_idle_override;

enum idle_boot_override {
	IDLE_NO_OVERRIDE = 0,
	IDLE_HALT,
	IDLE_NOMWAIT,
	IDLE_POLL
};

extern void enable_sep_cpu(void);

extern struct desc_ptr early_gdt_descr;

extern void switch_gdt_and_percpu_base(int);
extern void load_direct_gdt(int);
extern void load_fixmap_gdt(int);
extern void cpu_init(void);
extern void cpu_init_exception_handling(bool boot_cpu);
extern void cpu_init_replace_early_idt(void);
extern void cr4_init(void);

extern void set_task_blockstep(struct task_struct *task, bool on);

extern int bootloader_type;
extern int bootloader_version;

extern char ignore_fpu_irq;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
prefetch(const void *x)
{
	asm __inline volatile("# ALT: oldinstr\n"
			      "771:\n\t"
			      "prefetcht0 %1"
			      "\n772:\n"
			      "# ALT: padding\n"
			      ".skip -((("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")) > 0) * "
			      "(("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")),0x90\n"
			      "773:\n"
			      ".pushsection .altinstructions,\"a\"\n"
			      " .long 771b - .\n"
			      " .long 774f - .\n"
			      " .4byte "
			      "( 0*32+25)"
			      "\n"
			      " .byte "
			      "773b-771b"
			      "\n"
			      " .byte "
			      "775f-774f"
			      "\n"
			      ".popsection\n"
			      ".pushsection .altinstr_replacement, \"ax\"\n"
			      "# ALT: replacement\n"
			      "774:\n\t"
			      "prefetchnta %1"
			      "\n775:\n"
			      ".popsection\n"
			      :
			      : "i"(0), "m"(*(const char *)x));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
prefetchw(const void *x)
{
	asm __inline volatile("# ALT: oldinstr\n"
			      "771:\n\t"
			      "prefetcht0 %1"
			      "\n772:\n"
			      "# ALT: padding\n"
			      ".skip -((("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")) > 0) * "
			      "(("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")),0x90\n"
			      "773:\n"
			      ".pushsection .altinstructions,\"a\"\n"
			      " .long 771b - .\n"
			      " .long 774f - .\n"
			      " .4byte "
			      "( 6*32+ 8)"
			      "\n"
			      " .byte "
			      "773b-771b"
			      "\n"
			      " .byte "
			      "775f-774f"
			      "\n"
			      ".popsection\n"
			      ".pushsection .altinstr_replacement, \"ax\"\n"
			      "# ALT: replacement\n"
			      "774:\n\t"
			      "prefetchw %1"
			      "\n775:\n"
			      ".popsection\n"
			      :
			      : "i"(0), "m"(*(const char *)x));
}
extern unsigned long __top_init_kernel_stack[];

extern unsigned long KSTK_ESP(struct task_struct *task);

extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
			 unsigned long new_sp);
extern int get_tsc_mode(unsigned long adr);
extern int set_tsc_mode(unsigned int val);

extern __attribute__((section(".data..percpu"
			      ""))) __typeof__(u64) msr_misc_features_shadow;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
per_cpu_llc_id(unsigned int cpu)
{
	return (*({
		do {
			const void *__vpp_verify =
				(typeof((&(cpu_info.topo.llc_id)) +
					0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((
				typeof(*((&(cpu_info.topo.llc_id)))) *)((
				&(cpu_info.topo.llc_id))));
			(typeof((typeof(*((&(cpu_info.topo.llc_id)))) *)((&(
				cpu_info.topo.llc_id)))))(__ptr +
							  (((__per_cpu_offset[(
								  cpu)]))));
		});
	}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
per_cpu_l2c_id(unsigned int cpu)
{
	return (*({
		do {
			const void *__vpp_verify =
				(typeof((&(cpu_info.topo.l2c_id)) +
					0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((
				typeof(*((&(cpu_info.topo.l2c_id)))) *)((
				&(cpu_info.topo.l2c_id))));
			(typeof((typeof(*((&(cpu_info.topo.l2c_id)))) *)((&(
				cpu_info.topo.l2c_id)))))(__ptr +
							  (((__per_cpu_offset[(
								  cpu)]))));
		});
	}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
amd_clear_divider(void)
{
	asm volatile("# ALT: oldinstr\n"
		     "771:\n\t"
		     ""
		     "\n772:\n"
		     "# ALT: padding\n"
		     ".skip -((("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")) > 0) * "
		     "(("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")),0x90\n"
		     "773:\n"
		     ".pushsection .altinstructions,\"a\"\n"
		     " .long 771b - .\n"
		     " .long 774f - .\n"
		     " .4byte "
		     "(22*32 + (1*32 + 1))"
		     "\n"
		     " .byte "
		     "773b-771b"
		     "\n"
		     " .byte "
		     "775f-774f"
		     "\n"
		     ".popsection\n"
		     ".pushsection .altinstr_replacement, \"ax\"\n"
		     "# ALT: replacement\n"
		     "774:\n\t"
		     "div %2\n\t"
		     "\n775:\n"
		     ".popsection\n" ::"a"(0),
		     "d"(0), "r"(1));
}

extern void amd_check_microcode(void);

extern unsigned long arch_align_stack(unsigned long sp);
void free_init_pages(const char *what, unsigned long begin, unsigned long end);
extern void free_kernel_image_pages(const char *what, void *begin, void *end);

void default_idle(void);

void __attribute__((__noreturn__)) stop_this_cpu(void *dummy);
void microcode_check(struct cpuinfo_x86 *prev_info);
void store_cpu_caps(struct cpuinfo_x86 *info);

enum l1tf_mitigations {
	L1TF_MITIGATION_OFF,
	L1TF_MITIGATION_FLUSH_NOWARN,
	L1TF_MITIGATION_FLUSH,
	L1TF_MITIGATION_FLUSH_NOSMT,
	L1TF_MITIGATION_FULL,
	L1TF_MITIGATION_FULL_FORCE
};

extern enum l1tf_mitigations l1tf_mitigation;

enum mds_mitigations {
	MDS_MITIGATION_OFF,
	MDS_MITIGATION_FULL,
	MDS_MITIGATION_VMWERV,
};

extern bool gds_ucode_mitigated(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
weak_wrmsr_fence(void)
{
	asm __inline volatile("# ALT: oldinstr\n"
			      "771:\n\t"
			      "mfence; lfence"
			      "\n772:\n"
			      "# ALT: padding\n"
			      ".skip -((("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")) > 0) * "
			      "(("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")),0x90\n"
			      "773:\n"
			      ".pushsection .altinstructions,\"a\"\n"
			      " .long 771b - .\n"
			      " .long 774f - .\n"
			      " .4byte "
			      "(((1 << 0) << 16) | ((11*32+27)))"
			      "\n"
			      " .byte "
			      "773b-771b"
			      "\n"
			      " .byte "
			      "775f-774f"
			      "\n"
			      ".popsection\n"
			      ".pushsection .altinstr_replacement, \"ax\"\n"
			      "# ALT: replacement\n"
			      "774:\n\t"
			      ""
			      "\n775:\n"
			      ".popsection\n"
			      :
			      :
			      : "memory");
}

enum cpuid_leafs {
	CPUID_1_EDX = 0,
	CPUID_8000_0001_EDX,
	CPUID_8086_0001_EDX,
	CPUID_LNX_1,
	CPUID_1_ECX,
	CPUID_C000_0001_EDX,
	CPUID_8000_0001_ECX,
	CPUID_LNX_2,
	CPUID_LNX_3,
	CPUID_7_0_EBX,
	CPUID_D_1_EAX,
	CPUID_LNX_4,
	CPUID_7_1_EAX,
	CPUID_8000_0008_EBX,
	CPUID_6_EAX,
	CPUID_8000_000A_EDX,
	CPUID_7_ECX,
	CPUID_8000_0007_EBX,
	CPUID_7_EDX,
	CPUID_8000_001F_EAX,
	CPUID_8000_0021_EAX,
	CPUID_LNX_5,
	NR_CPUID_WORDS,
};

extern const char *const x86_cap_flags[22 * 32];
extern const char *const x86_power_flags[32];

extern const char *const x86_bug_flags[2 * 32];
extern void setup_clear_cpu_cap(unsigned int bit);
extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
_static_cpu_has(u16 bit)
{
	asm goto("# ALT: oldinstr\n"
		 "771:\n\t"
		 "# ALT: oldinstr\n"
		 "771:\n\t"
		 "jmp 6f"
		 "\n772:\n"
		 "# ALT: padding\n"
		 ".skip -((("
		 "775f-774f"
		 ")-("
		 "772b-771b"
		 ")) > 0) * "
		 "(("
		 "775f-774f"
		 ")-("
		 "772b-771b"
		 ")),0x90\n"
		 "773:\n"
		 ".pushsection .altinstructions,\"a\"\n"
		 " .long 771b - .\n"
		 " .long 774f - .\n"
		 " .4byte "
		 "( 3*32+21)"
		 "\n"
		 " .byte "
		 "773b-771b"
		 "\n"
		 " .byte "
		 "775f-774f"
		 "\n"
		 ".popsection\n"
		 ".pushsection .altinstr_replacement, \"ax\"\n"
		 "# ALT: replacement\n"
		 "774:\n\t"
		 "jmp %l[t_no]"
		 "\n775:\n"
		 ".popsection\n"
		 "\n772:\n"
		 "# ALT: padding\n"
		 ".skip -((("
		 "775f-774f"
		 ")-("
		 "772b-771b"
		 ")) > 0) * "
		 "(("
		 "775f-774f"
		 ")-("
		 "772b-771b"
		 ")),0x90\n"
		 "773:\n"
		 ".pushsection .altinstructions,\"a\"\n"
		 " .long 771b - .\n"
		 " .long 774f - .\n"
		 " .4byte "
		 "%c[feature]"
		 "\n"
		 " .byte "
		 "773b-771b"
		 "\n"
		 " .byte "
		 "775f-774f"
		 "\n"
		 ".popsection\n"
		 ".pushsection .altinstr_replacement, \"ax\"\n"
		 "# ALT: replacement\n"
		 "774:\n\t"
		 ""
		 "\n775:\n"
		 ".popsection\n"
		 ".pushsection .altinstr_aux,\"ax\"\n"
		 "6:\n"
		 " testb %[bitnum], %a[cap_byte]\n"
		 " jnz %l[t_yes]\n"
		 " jmp %l[t_no]\n"
		 ".popsection\n"
		 :
		 : [feature] "i"(bit), [bitnum] "i"(1 << (bit & 7)),
		   [cap_byte] "i"(
			   &((const char *)boot_cpu_data.x86_capability)[bit >>
									 3])
		 :
		 : t_yes, t_no);
t_yes:
	return true;
t_no:
	return false;
}

extern void __xchg_wrong_size(void)
	__attribute__((__error__("Bad argument size for xchg")));
extern void __cmpxchg_wrong_size(void)
	__attribute__((__error__("Bad argument size for cmpxchg")));
extern void __xadd_wrong_size(void)
	__attribute__((__error__("Bad argument size for xadd")));
extern void __add_wrong_size(void)
	__attribute__((__error__("Bad argument size for add")));
union __u128_halves {
	u128 full;
	struct {
		u64 low, high;
	};
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u128
arch_cmpxchg128(volatile u128 *ptr, u128 old, u128 new)
{
	return ({
		union __u128_halves o = { .full = (old), }, n = { .full = (new), };
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     "cmpxchg16b %[ptr]"
			     : [ptr] "+m"(*(ptr)), "+a"(o.low), "+d"(o.high)
			     : "b"(n.low), "c"(n.high)
			     : "memory");
		o.full;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u128
arch_cmpxchg128_local(volatile u128 *ptr, u128 old, u128 new)
{
	return ({
		union __u128_halves o = { .full = (old), }, n = { .full = (new), };
		asm volatile("cmpxchg16b %[ptr]"
			     : [ptr] "+m"(*(ptr)), "+a"(o.low), "+d"(o.high)
			     : "b"(n.low), "c"(n.high)
			     : "memory");
		o.full;
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_try_cmpxchg128(volatile u128 *ptr, u128 *oldp, u128 new)
{
	return ({
		union __u128_halves o = { .full = *(oldp), }, n = { .full = (new), };
		bool ret;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     "cmpxchg16b %[ptr]"
			     "\n\t/* output condition code "
			     "e"
			     "*/\n"
			     : "=@cc"
			       "e"(ret),
			       [ptr] "+m"(*(ptr)), "+a"(o.low), "+d"(o.high)
			     : "b"(n.low), "c"(n.high)
			     : "memory");
		if (__builtin_expect(!!(!ret), 0))
			*(oldp) = o.full;
		__builtin_expect(!!(ret), 1);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_try_cmpxchg128_local(volatile u128 *ptr, u128 *oldp, u128 new)
{
	return ({
		union __u128_halves o = { .full = *(oldp), }, n = { .full = (new), };
		bool ret;
		asm volatile("cmpxchg16b %[ptr]"
			     "\n\t/* output condition code "
			     "e"
			     "*/\n"
			     : "=@cc"
			       "e"(ret),
			       [ptr] "+m"(*(ptr)), "+a"(o.low), "+d"(o.high)
			     : "b"(n.low), "c"(n.high)
			     : "memory");
		if (__builtin_expect(!!(!ret), 0))
			*(oldp) = o.full;
		__builtin_expect(!!(ret), 1);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
arch_atomic_read(const atomic_t *v)
{
	return (*(
		const volatile typeof(_Generic(((v)->counter),
					      char: (char)0,
					      unsigned char: (unsigned char)0,
					      signed char: (signed char)0,
					      unsigned short: (unsigned short)0,
					      signed short: (signed short)0,
					      unsigned int: (unsigned int)0,
					      signed int: (signed int)0,
					      unsigned long: (unsigned long)0,
					      signed long: (signed long)0,
					      unsigned long long: (
						       unsigned long long)0,
					      signed long long: (
						       signed long long)0,
					      default: ((v)->counter)))
			*)&((v)->counter));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic_set(atomic_t *v, int i)
{
	do {
		*(volatile typeof(v->counter) *)&(v->counter) = (i);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic_add(int i, atomic_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "addl %1,%0"
		     : "+m"(v->counter)
		     : "ir"(i)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic_sub(int i, atomic_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "subl %1,%0"
		     : "+m"(v->counter)
		     : "ir"(i)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_atomic_sub_and_test(int i, atomic_t *v)
{
	return ({
		bool c;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     "subl"
			     " %[val], "
			     "%[var]"
			     "\n\t/* output condition code "
			     "e"
			     "*/\n"
			     : [var] "+m"(v->counter), "=@cc"
						       "e"(c)
			     : [val] "er"(i)
			     : "memory");
		c;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic_inc(atomic_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "incl %0"
		     : "+m"(v->counter)::"memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic_dec(atomic_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "decl %0"
		     : "+m"(v->counter)::"memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_atomic_dec_and_test(atomic_t *v)
{
	return ({
		bool c;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     "decl"
			     " "
			     "%[var]"
			     "\n\t/* output condition code "
			     "e"
			     "*/\n"
			     : [var] "+m"(v->counter), "=@cc"
						       "e"(c)
			     :
			     : "memory");
		c;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_atomic_inc_and_test(atomic_t *v)
{
	return ({
		bool c;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     "incl"
			     " "
			     "%[var]"
			     "\n\t/* output condition code "
			     "e"
			     "*/\n"
			     : [var] "+m"(v->counter), "=@cc"
						       "e"(c)
			     :
			     : "memory");
		c;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_atomic_add_negative(int i, atomic_t *v)
{
	return ({
		bool c;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     "addl"
			     " %[val], "
			     "%[var]"
			     "\n\t/* output condition code "
			     "s"
			     "*/\n"
			     : [var] "+m"(v->counter), "=@cc"
						       "s"(c)
			     : [val] "er"(i)
			     : "memory");
		c;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
arch_atomic_add_return(int i, atomic_t *v)
{
	return i + ({
		       __typeof__(*(((&v->counter)))) __ret = (((i)));
		       switch (sizeof(*(((&v->counter))))) {
		       case 1:
			       asm volatile(".pushsection .smp_locks,\"a\"\n"
					    ".balign 4\n"
					    ".long 671f - .\n"
					    ".popsection\n"
					    "671:"
					    "\n\tlock; "
					    "xadd"
					    "b %b0, %1\n"
					    : "+q"(__ret),
					      "+m"(*(((&v->counter))))
					    :
					    : "memory", "cc");
			       break;
		       case 2:
			       asm volatile(".pushsection .smp_locks,\"a\"\n"
					    ".balign 4\n"
					    ".long 671f - .\n"
					    ".popsection\n"
					    "671:"
					    "\n\tlock; "
					    "xadd"
					    "w %w0, %1\n"
					    : "+r"(__ret),
					      "+m"(*(((&v->counter))))
					    :
					    : "memory", "cc");
			       break;
		       case 4:
			       asm volatile(".pushsection .smp_locks,\"a\"\n"
					    ".balign 4\n"
					    ".long 671f - .\n"
					    ".popsection\n"
					    "671:"
					    "\n\tlock; "
					    "xadd"
					    "l %0, %1\n"
					    : "+r"(__ret),
					      "+m"(*(((&v->counter))))
					    :
					    : "memory", "cc");
			       break;
		       case 8:
			       asm volatile(".pushsection .smp_locks,\"a\"\n"
					    ".balign 4\n"
					    ".long 671f - .\n"
					    ".popsection\n"
					    "671:"
					    "\n\tlock; "
					    "xadd"
					    "q %q0, %1\n"
					    : "+r"(__ret),
					      "+m"(*(((&v->counter))))
					    :
					    : "memory", "cc");
			       break;
		       default:
			       __xadd_wrong_size();
		       }
		       __ret;
	       });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
arch_atomic_fetch_add(int i, atomic_t *v)
{
	return ({
		__typeof__(*(((&v->counter)))) __ret = (((i)));
		switch (sizeof(*(((&v->counter))))) {
		case 1:
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "xadd"
				     "b %b0, %1\n"
				     : "+q"(__ret), "+m"(*(((&v->counter))))
				     :
				     : "memory", "cc");
			break;
		case 2:
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "xadd"
				     "w %w0, %1\n"
				     : "+r"(__ret), "+m"(*(((&v->counter))))
				     :
				     : "memory", "cc");
			break;
		case 4:
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "xadd"
				     "l %0, %1\n"
				     : "+r"(__ret), "+m"(*(((&v->counter))))
				     :
				     : "memory", "cc");
			break;
		case 8:
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "xadd"
				     "q %q0, %1\n"
				     : "+r"(__ret), "+m"(*(((&v->counter))))
				     :
				     : "memory", "cc");
			break;
		default:
			__xadd_wrong_size();
		}
		__ret;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
arch_atomic_cmpxchg(atomic_t *v, int old, int new)
{
	return ({
		__typeof__(*((&v->counter))) __ret;
		__typeof__(*((&v->counter))) __old = ((old));
		__typeof__(*((&v->counter))) __new = ((new));
		switch ((sizeof(*(&v->counter)))) {
		case 1: {
			volatile u8 *__ptr = (volatile u8 *)((&v->counter));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgb %2,%1"
				     : "=a"(__ret), "+m"(*__ptr)
				     : "q"(__new), "0"(__old)
				     : "memory");
			break;
		}
		case 2: {
			volatile u16 *__ptr = (volatile u16 *)((&v->counter));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgw %2,%1"
				     : "=a"(__ret), "+m"(*__ptr)
				     : "r"(__new), "0"(__old)
				     : "memory");
			break;
		}
		case 4: {
			volatile u32 *__ptr = (volatile u32 *)((&v->counter));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgl %2,%1"
				     : "=a"(__ret), "+m"(*__ptr)
				     : "r"(__new), "0"(__old)
				     : "memory");
			break;
		}
		case 8: {
			volatile u64 *__ptr = (volatile u64 *)((&v->counter));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgq %2,%1"
				     : "=a"(__ret), "+m"(*__ptr)
				     : "r"(__new), "0"(__old)
				     : "memory");
			break;
		}
		default:
			__cmpxchg_wrong_size();
		}
		__ret;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_atomic_try_cmpxchg(atomic_t *v, int *old, int new)
{
	return ({
		bool success;
		__typeof__(((&v->counter))) _old =
			(__typeof__(((&v->counter))))(((old)));
		__typeof__(*(((&v->counter)))) __old = *_old;
		__typeof__(*(((&v->counter)))) __new = (((new)));
		switch ((sizeof(*(&v->counter)))) {
		case 1: {
			volatile u8 *__ptr = (volatile u8 *)(((&v->counter)));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgb %[new], %[ptr]"
				     "\n\t/* output condition code "
				     "z"
				     "*/\n"
				     : "=@cc"
				       "z"(success),
				       [ptr] "+m"(*__ptr), [old] "+a"(__old)
				     : [new] "q"(__new)
				     : "memory");
			break;
		}
		case 2: {
			volatile u16 *__ptr = (volatile u16 *)(((&v->counter)));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgw %[new], %[ptr]"
				     "\n\t/* output condition code "
				     "z"
				     "*/\n"
				     : "=@cc"
				       "z"(success),
				       [ptr] "+m"(*__ptr), [old] "+a"(__old)
				     : [new] "r"(__new)
				     : "memory");
			break;
		}
		case 4: {
			volatile u32 *__ptr = (volatile u32 *)(((&v->counter)));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgl %[new], %[ptr]"
				     "\n\t/* output condition code "
				     "z"
				     "*/\n"
				     : "=@cc"
				       "z"(success),
				       [ptr] "+m"(*__ptr), [old] "+a"(__old)
				     : [new] "r"(__new)
				     : "memory");
			break;
		}
		case 8: {
			volatile u64 *__ptr = (volatile u64 *)(((&v->counter)));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgq %[new], %[ptr]"
				     "\n\t/* output condition code "
				     "z"
				     "*/\n"
				     : "=@cc"
				       "z"(success),
				       [ptr] "+m"(*__ptr), [old] "+a"(__old)
				     : [new] "r"(__new)
				     : "memory");
			break;
		}
		default:
			__cmpxchg_wrong_size();
		}
		if (__builtin_expect(!!(!success), 0))
			*_old = __old;
		__builtin_expect(!!(success), 1);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
arch_atomic_xchg(atomic_t *v, int new)
{
	return ({
		__typeof__(*((&v->counter))) __ret = ((new));
		switch (sizeof(*((&v->counter)))) {
		case 1:
			asm volatile(""
				     "xchg"
				     "b %b0, %1\n"
				     : "+q"(__ret), "+m"(*((&v->counter)))
				     :
				     : "memory", "cc");
			break;
		case 2:
			asm volatile(""
				     "xchg"
				     "w %w0, %1\n"
				     : "+r"(__ret), "+m"(*((&v->counter)))
				     :
				     : "memory", "cc");
			break;
		case 4:
			asm volatile(""
				     "xchg"
				     "l %0, %1\n"
				     : "+r"(__ret), "+m"(*((&v->counter)))
				     :
				     : "memory", "cc");
			break;
		case 8:
			asm volatile(""
				     "xchg"
				     "q %q0, %1\n"
				     : "+r"(__ret), "+m"(*((&v->counter)))
				     :
				     : "memory", "cc");
			break;
		default:
			__xchg_wrong_size();
		}
		__ret;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic_and(int i, atomic_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "andl %1,%0"
		     : "+m"(v->counter)
		     : "ir"(i)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
arch_atomic_fetch_and(int i, atomic_t *v)
{
	int val = arch_atomic_read(v);

	do {
	} while (!arch_atomic_try_cmpxchg(v, &val, val & i));

	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic_or(int i, atomic_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "orl %1,%0"
		     : "+m"(v->counter)
		     : "ir"(i)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
arch_atomic_fetch_or(int i, atomic_t *v)
{
	int val = arch_atomic_read(v);

	do {
	} while (!arch_atomic_try_cmpxchg(v, &val, val | i));

	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic_xor(int i, atomic_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "xorl %1,%0"
		     : "+m"(v->counter)
		     : "ir"(i)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
arch_atomic_fetch_xor(int i, atomic_t *v)
{
	int val = arch_atomic_read(v);

	do {
	} while (!arch_atomic_try_cmpxchg(v, &val, val ^ i));

	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
arch_atomic64_read(const atomic64_t *v)
{
	return (*(
		const volatile typeof(_Generic(((v)->counter),
					      char: (char)0,
					      unsigned char: (unsigned char)0,
					      signed char: (signed char)0,
					      unsigned short: (unsigned short)0,
					      signed short: (signed short)0,
					      unsigned int: (unsigned int)0,
					      signed int: (signed int)0,
					      unsigned long: (unsigned long)0,
					      signed long: (signed long)0,
					      unsigned long long: (
						       unsigned long long)0,
					      signed long long: (
						       signed long long)0,
					      default: ((v)->counter)))
			*)&((v)->counter));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic64_set(atomic64_t *v, s64 i)
{
	do {
		*(volatile typeof(v->counter) *)&(v->counter) = (i);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic64_add(s64 i, atomic64_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "addq %1,%0"
		     : "=m"(v->counter)
		     : "er"(i), "m"(v->counter)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic64_sub(s64 i, atomic64_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "subq %1,%0"
		     : "=m"(v->counter)
		     : "er"(i), "m"(v->counter)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_atomic64_sub_and_test(s64 i, atomic64_t *v)
{
	return ({
		bool c;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     "subq"
			     " %[val], "
			     "%[var]"
			     "\n\t/* output condition code "
			     "e"
			     "*/\n"
			     : [var] "+m"(v->counter), "=@cc"
						       "e"(c)
			     : [val] "er"(i)
			     : "memory");
		c;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic64_inc(atomic64_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "incq %0"
		     : "=m"(v->counter)
		     : "m"(v->counter)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic64_dec(atomic64_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "decq %0"
		     : "=m"(v->counter)
		     : "m"(v->counter)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_atomic64_dec_and_test(atomic64_t *v)
{
	return ({
		bool c;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     "decq"
			     " "
			     "%[var]"
			     "\n\t/* output condition code "
			     "e"
			     "*/\n"
			     : [var] "+m"(v->counter), "=@cc"
						       "e"(c)
			     :
			     : "memory");
		c;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_atomic64_inc_and_test(atomic64_t *v)
{
	return ({
		bool c;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     "incq"
			     " "
			     "%[var]"
			     "\n\t/* output condition code "
			     "e"
			     "*/\n"
			     : [var] "+m"(v->counter), "=@cc"
						       "e"(c)
			     :
			     : "memory");
		c;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_atomic64_add_negative(s64 i, atomic64_t *v)
{
	return ({
		bool c;
		asm volatile(".pushsection .smp_locks,\"a\"\n"
			     ".balign 4\n"
			     ".long 671f - .\n"
			     ".popsection\n"
			     "671:"
			     "\n\tlock; "
			     "addq"
			     " %[val], "
			     "%[var]"
			     "\n\t/* output condition code "
			     "s"
			     "*/\n"
			     : [var] "+m"(v->counter), "=@cc"
						       "s"(c)
			     : [val] "er"(i)
			     : "memory");
		c;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
arch_atomic64_add_return(s64 i, atomic64_t *v)
{
	return i + ({
		       __typeof__(*(((&v->counter)))) __ret = (((i)));
		       switch (sizeof(*(((&v->counter))))) {
		       case 1:
			       asm volatile(".pushsection .smp_locks,\"a\"\n"
					    ".balign 4\n"
					    ".long 671f - .\n"
					    ".popsection\n"
					    "671:"
					    "\n\tlock; "
					    "xadd"
					    "b %b0, %1\n"
					    : "+q"(__ret),
					      "+m"(*(((&v->counter))))
					    :
					    : "memory", "cc");
			       break;
		       case 2:
			       asm volatile(".pushsection .smp_locks,\"a\"\n"
					    ".balign 4\n"
					    ".long 671f - .\n"
					    ".popsection\n"
					    "671:"
					    "\n\tlock; "
					    "xadd"
					    "w %w0, %1\n"
					    : "+r"(__ret),
					      "+m"(*(((&v->counter))))
					    :
					    : "memory", "cc");
			       break;
		       case 4:
			       asm volatile(".pushsection .smp_locks,\"a\"\n"
					    ".balign 4\n"
					    ".long 671f - .\n"
					    ".popsection\n"
					    "671:"
					    "\n\tlock; "
					    "xadd"
					    "l %0, %1\n"
					    : "+r"(__ret),
					      "+m"(*(((&v->counter))))
					    :
					    : "memory", "cc");
			       break;
		       case 8:
			       asm volatile(".pushsection .smp_locks,\"a\"\n"
					    ".balign 4\n"
					    ".long 671f - .\n"
					    ".popsection\n"
					    "671:"
					    "\n\tlock; "
					    "xadd"
					    "q %q0, %1\n"
					    : "+r"(__ret),
					      "+m"(*(((&v->counter))))
					    :
					    : "memory", "cc");
			       break;
		       default:
			       __xadd_wrong_size();
		       }
		       __ret;
	       });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_add(s64 i, atomic64_t *v)
{
	return ({
		__typeof__(*(((&v->counter)))) __ret = (((i)));
		switch (sizeof(*(((&v->counter))))) {
		case 1:
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "xadd"
				     "b %b0, %1\n"
				     : "+q"(__ret), "+m"(*(((&v->counter))))
				     :
				     : "memory", "cc");
			break;
		case 2:
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "xadd"
				     "w %w0, %1\n"
				     : "+r"(__ret), "+m"(*(((&v->counter))))
				     :
				     : "memory", "cc");
			break;
		case 4:
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "xadd"
				     "l %0, %1\n"
				     : "+r"(__ret), "+m"(*(((&v->counter))))
				     :
				     : "memory", "cc");
			break;
		case 8:
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "xadd"
				     "q %q0, %1\n"
				     : "+r"(__ret), "+m"(*(((&v->counter))))
				     :
				     : "memory", "cc");
			break;
		default:
			__xadd_wrong_size();
		}
		__ret;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
arch_atomic64_cmpxchg(atomic64_t *v, s64 old, s64 new)
{
	return ({
		__typeof__(*((&v->counter))) __ret;
		__typeof__(*((&v->counter))) __old = ((old));
		__typeof__(*((&v->counter))) __new = ((new));
		switch ((sizeof(*(&v->counter)))) {
		case 1: {
			volatile u8 *__ptr = (volatile u8 *)((&v->counter));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgb %2,%1"
				     : "=a"(__ret), "+m"(*__ptr)
				     : "q"(__new), "0"(__old)
				     : "memory");
			break;
		}
		case 2: {
			volatile u16 *__ptr = (volatile u16 *)((&v->counter));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgw %2,%1"
				     : "=a"(__ret), "+m"(*__ptr)
				     : "r"(__new), "0"(__old)
				     : "memory");
			break;
		}
		case 4: {
			volatile u32 *__ptr = (volatile u32 *)((&v->counter));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgl %2,%1"
				     : "=a"(__ret), "+m"(*__ptr)
				     : "r"(__new), "0"(__old)
				     : "memory");
			break;
		}
		case 8: {
			volatile u64 *__ptr = (volatile u64 *)((&v->counter));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgq %2,%1"
				     : "=a"(__ret), "+m"(*__ptr)
				     : "r"(__new), "0"(__old)
				     : "memory");
			break;
		}
		default:
			__cmpxchg_wrong_size();
		}
		__ret;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_atomic64_try_cmpxchg(atomic64_t *v, s64 *old, s64 new)
{
	return ({
		bool success;
		__typeof__(((&v->counter))) _old =
			(__typeof__(((&v->counter))))(((old)));
		__typeof__(*(((&v->counter)))) __old = *_old;
		__typeof__(*(((&v->counter)))) __new = (((new)));
		switch ((sizeof(*(&v->counter)))) {
		case 1: {
			volatile u8 *__ptr = (volatile u8 *)(((&v->counter)));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgb %[new], %[ptr]"
				     "\n\t/* output condition code "
				     "z"
				     "*/\n"
				     : "=@cc"
				       "z"(success),
				       [ptr] "+m"(*__ptr), [old] "+a"(__old)
				     : [new] "q"(__new)
				     : "memory");
			break;
		}
		case 2: {
			volatile u16 *__ptr = (volatile u16 *)(((&v->counter)));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgw %[new], %[ptr]"
				     "\n\t/* output condition code "
				     "z"
				     "*/\n"
				     : "=@cc"
				       "z"(success),
				       [ptr] "+m"(*__ptr), [old] "+a"(__old)
				     : [new] "r"(__new)
				     : "memory");
			break;
		}
		case 4: {
			volatile u32 *__ptr = (volatile u32 *)(((&v->counter)));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgl %[new], %[ptr]"
				     "\n\t/* output condition code "
				     "z"
				     "*/\n"
				     : "=@cc"
				       "z"(success),
				       [ptr] "+m"(*__ptr), [old] "+a"(__old)
				     : [new] "r"(__new)
				     : "memory");
			break;
		}
		case 8: {
			volatile u64 *__ptr = (volatile u64 *)(((&v->counter)));
			asm volatile(".pushsection .smp_locks,\"a\"\n"
				     ".balign 4\n"
				     ".long 671f - .\n"
				     ".popsection\n"
				     "671:"
				     "\n\tlock; "
				     "cmpxchgq %[new], %[ptr]"
				     "\n\t/* output condition code "
				     "z"
				     "*/\n"
				     : "=@cc"
				       "z"(success),
				       [ptr] "+m"(*__ptr), [old] "+a"(__old)
				     : [new] "r"(__new)
				     : "memory");
			break;
		}
		default:
			__cmpxchg_wrong_size();
		}
		if (__builtin_expect(!!(!success), 0))
			*_old = __old;
		__builtin_expect(!!(success), 1);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
arch_atomic64_xchg(atomic64_t *v, s64 new)
{
	return ({
		__typeof__(*((&v->counter))) __ret = ((new));
		switch (sizeof(*((&v->counter)))) {
		case 1:
			asm volatile(""
				     "xchg"
				     "b %b0, %1\n"
				     : "+q"(__ret), "+m"(*((&v->counter)))
				     :
				     : "memory", "cc");
			break;
		case 2:
			asm volatile(""
				     "xchg"
				     "w %w0, %1\n"
				     : "+r"(__ret), "+m"(*((&v->counter)))
				     :
				     : "memory", "cc");
			break;
		case 4:
			asm volatile(""
				     "xchg"
				     "l %0, %1\n"
				     : "+r"(__ret), "+m"(*((&v->counter)))
				     :
				     : "memory", "cc");
			break;
		case 8:
			asm volatile(""
				     "xchg"
				     "q %q0, %1\n"
				     : "+r"(__ret), "+m"(*((&v->counter)))
				     :
				     : "memory", "cc");
			break;
		default:
			__xchg_wrong_size();
		}
		__ret;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic64_and(s64 i, atomic64_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "andq %1,%0"
		     : "+m"(v->counter)
		     : "er"(i)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_and(s64 i, atomic64_t *v)
{
	s64 val = arch_atomic64_read(v);

	do {
	} while (!arch_atomic64_try_cmpxchg(v, &val, val & i));
	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic64_or(s64 i, atomic64_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "orq %1,%0"
		     : "+m"(v->counter)
		     : "er"(i)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_or(s64 i, atomic64_t *v)
{
	s64 val = arch_atomic64_read(v);

	do {
	} while (!arch_atomic64_try_cmpxchg(v, &val, val | i));
	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_atomic64_xor(s64 i, atomic64_t *v)
{
	asm volatile(".pushsection .smp_locks,\"a\"\n"
		     ".balign 4\n"
		     ".long 671f - .\n"
		     ".popsection\n"
		     "671:"
		     "\n\tlock; "
		     "xorq %1,%0"
		     : "+m"(v->counter)
		     : "er"(i)
		     : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
arch_atomic64_fetch_xor(s64 i, atomic64_t *v)
{
	s64 val = arch_atomic64_read(v);

	do {
	} while (!arch_atomic64_try_cmpxchg(v, &val, val ^ i));
	return val;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_read(const atomic_t *v)
{
	return arch_atomic_read(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_read_acquire(const atomic_t *v)
{
	int ret;

	if ((sizeof(atomic_t) == sizeof(char) ||
	     sizeof(atomic_t) == sizeof(short) ||
	     sizeof(atomic_t) == sizeof(int) ||
	     sizeof(atomic_t) == sizeof(long))) {
		ret = ({
			typeof(*&(v)->counter) ___p1 = ({
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_34(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*&(v)->counter) ==
						       sizeof(char) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(short) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(int) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(long)) ||
					      sizeof(*&(v)->counter) ==
						      sizeof(long long)))
						__compiletime_assert_34();
				} while (0);
				(*(const volatile typeof(_Generic(
					(*&(v)->counter),
								 char: (char)0,
								 unsigned char: (
									 unsigned char)0,
								 signed char: (
									 signed char)0,
								 unsigned short: (
									 unsigned short)0,
								 signed short: (
									 signed short)0,
								 unsigned int: (
									 unsigned int)0,
								 signed int: (
									 signed int)0,
								 unsigned long: (
									 unsigned long)0,
								 signed long: (
									 signed long)0,
								 unsigned long long: (
									 unsigned long long)0,
								 signed long long: (
									 signed long long)0,
								 default: (
									 *&(v)->counter)))
					   *)&(*&(v)->counter));
			});
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_35(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*&(v)->counter) == sizeof(char) ||
				       sizeof(*&(v)->counter) == sizeof(short) ||
				       sizeof(*&(v)->counter) == sizeof(int) ||
				       sizeof(*&(v)->counter) == sizeof(long))))
					__compiletime_assert_35();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			___p1;
		});
	} else {
		ret = raw_atomic_read(v);
		do {
			do {
			} while (0);
			do {
			} while (0);
		} while (0);
	}

	return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_set(atomic_t *v, int i)
{
	arch_atomic_set(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_set_release(atomic_t *v, int i)
{
	if ((sizeof(atomic_t) == sizeof(char) ||
	     sizeof(atomic_t) == sizeof(short) ||
	     sizeof(atomic_t) == sizeof(int) ||
	     sizeof(atomic_t) == sizeof(long))) {
		do {
			do {
			} while (0);
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_36(void)
						__attribute__((__error__(
							"Need native word sized stores/loads for atomicity.")));
					if (!((sizeof(*&(v)->counter) ==
						       sizeof(char) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(short) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(int) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(long))))
						__compiletime_assert_36();
				} while (0);
				__asm__ __volatile__("" : : : "memory");
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_37(void)
							__attribute__((__error__(
								"Unsupported access size for {READ,WRITE}_ONCE().")));
						if (!((sizeof(*&(v)->counter) ==
							       sizeof(char) ||
						       sizeof(*&(v)->counter) ==
							       sizeof(short) ||
						       sizeof(*&(v)->counter) ==
							       sizeof(int) ||
						       sizeof(*&(v)->counter) ==
							       sizeof(long)) ||
						      sizeof(*&(v)->counter) ==
							      sizeof(long long)))
							__compiletime_assert_37();
					} while (0);
					do {
						*(volatile typeof(*&(v)->counter)
							  *)&(*&(v)->counter) =
							(i);
					} while (0);
				} while (0);
			} while (0);
		} while (0);
	} else {
		do {
			do {
			} while (0);
			do {
			} while (0);
		} while (0);
		raw_atomic_set(v, i);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_add(int i, atomic_t *v)
{
	arch_atomic_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_add_return(int i, atomic_t *v)
{
	return arch_atomic_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_add_return_acquire(int i, atomic_t *v)
{
	return arch_atomic_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_add_return_release(int i, atomic_t *v)
{
	return arch_atomic_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_add_return_relaxed(int i, atomic_t *v)
{
	return arch_atomic_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_add(int i, atomic_t *v)
{
	return arch_atomic_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_add_acquire(int i, atomic_t *v)
{
	return arch_atomic_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_add_release(int i, atomic_t *v)
{
	return arch_atomic_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_add_relaxed(int i, atomic_t *v)
{
	return arch_atomic_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_sub(int i, atomic_t *v)
{
	arch_atomic_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_sub_return(int i, atomic_t *v)
{
	return arch_atomic_add_return(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_sub_return_acquire(int i, atomic_t *v)
{
	return arch_atomic_add_return(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_sub_return_release(int i, atomic_t *v)
{
	return arch_atomic_add_return(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_sub_return_relaxed(int i, atomic_t *v)
{
	return arch_atomic_add_return(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_sub(int i, atomic_t *v)
{
	return arch_atomic_fetch_add(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_sub_acquire(int i, atomic_t *v)
{
	return arch_atomic_fetch_add(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_sub_release(int i, atomic_t *v)
{
	return arch_atomic_fetch_add(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_sub_relaxed(int i, atomic_t *v)
{
	return arch_atomic_fetch_add(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_inc(atomic_t *v)
{
	arch_atomic_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_inc_return(atomic_t *v)
{
	return raw_atomic_add_return(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_inc_return_acquire(atomic_t *v)
{
	return raw_atomic_add_return_acquire(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_inc_return_release(atomic_t *v)
{
	return raw_atomic_add_return_release(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_inc_return_relaxed(atomic_t *v)
{
	return raw_atomic_add_return_relaxed(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_inc(atomic_t *v)
{
	return raw_atomic_fetch_add(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_inc_acquire(atomic_t *v)
{
	return raw_atomic_fetch_add_acquire(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_inc_release(atomic_t *v)
{
	return raw_atomic_fetch_add_release(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_inc_relaxed(atomic_t *v)
{
	return raw_atomic_fetch_add_relaxed(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_dec(atomic_t *v)
{
	arch_atomic_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_dec_return(atomic_t *v)
{
	return raw_atomic_sub_return(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_dec_return_acquire(atomic_t *v)
{
	return raw_atomic_sub_return_acquire(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_dec_return_release(atomic_t *v)
{
	return raw_atomic_sub_return_release(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_dec_return_relaxed(atomic_t *v)
{
	return raw_atomic_sub_return_relaxed(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_dec(atomic_t *v)
{
	return raw_atomic_fetch_sub(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_dec_acquire(atomic_t *v)
{
	return raw_atomic_fetch_sub_acquire(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_dec_release(atomic_t *v)
{
	return raw_atomic_fetch_sub_release(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_dec_relaxed(atomic_t *v)
{
	return raw_atomic_fetch_sub_relaxed(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_and(int i, atomic_t *v)
{
	arch_atomic_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_and(int i, atomic_t *v)
{
	return arch_atomic_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_and_acquire(int i, atomic_t *v)
{
	return arch_atomic_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_and_release(int i, atomic_t *v)
{
	return arch_atomic_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_and_relaxed(int i, atomic_t *v)
{
	return arch_atomic_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_andnot(int i, atomic_t *v)
{
	raw_atomic_and(~i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_andnot(int i, atomic_t *v)
{
	return raw_atomic_fetch_and(~i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_andnot_acquire(int i, atomic_t *v)
{
	return raw_atomic_fetch_and_acquire(~i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_andnot_release(int i, atomic_t *v)
{
	return raw_atomic_fetch_and_release(~i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_andnot_relaxed(int i, atomic_t *v)
{
	return raw_atomic_fetch_and_relaxed(~i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_or(int i, atomic_t *v)
{
	arch_atomic_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_or(int i, atomic_t *v)
{
	return arch_atomic_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_or_acquire(int i, atomic_t *v)
{
	return arch_atomic_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_or_release(int i, atomic_t *v)
{
	return arch_atomic_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_or_relaxed(int i, atomic_t *v)
{
	return arch_atomic_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_xor(int i, atomic_t *v)
{
	arch_atomic_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_xor(int i, atomic_t *v)
{
	return arch_atomic_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_xor_acquire(int i, atomic_t *v)
{
	return arch_atomic_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_xor_release(int i, atomic_t *v)
{
	return arch_atomic_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_xor_relaxed(int i, atomic_t *v)
{
	return arch_atomic_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_xchg(atomic_t *v, int new)
{
	return arch_atomic_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_xchg_acquire(atomic_t *v, int new)
{
	return arch_atomic_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_xchg_release(atomic_t *v, int new)
{
	return arch_atomic_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_xchg_relaxed(atomic_t *v, int new)
{
	return arch_atomic_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_cmpxchg(atomic_t *v, int old, int new)
{
	return arch_atomic_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_cmpxchg_acquire(atomic_t *v, int old, int new)
{
	return arch_atomic_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_cmpxchg_release(atomic_t *v, int old, int new)
{
	return arch_atomic_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_cmpxchg_relaxed(atomic_t *v, int old, int new)
{
	return arch_atomic_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_try_cmpxchg(atomic_t *v, int *old, int new)
{
	return arch_atomic_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_try_cmpxchg_acquire(atomic_t *v, int *old, int new)
{
	return arch_atomic_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_try_cmpxchg_release(atomic_t *v, int *old, int new)
{
	return arch_atomic_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_try_cmpxchg_relaxed(atomic_t *v, int *old, int new)
{
	return arch_atomic_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_sub_and_test(int i, atomic_t *v)
{
	return arch_atomic_sub_and_test(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_dec_and_test(atomic_t *v)
{
	return arch_atomic_dec_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_inc_and_test(atomic_t *v)
{
	return arch_atomic_inc_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_add_negative(int i, atomic_t *v)
{
	return arch_atomic_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_add_negative_acquire(int i, atomic_t *v)
{
	return arch_atomic_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_add_negative_release(int i, atomic_t *v)
{
	return arch_atomic_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_add_negative_relaxed(int i, atomic_t *v)
{
	return arch_atomic_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_fetch_add_unless(atomic_t *v, int a, int u)
{
	int c = raw_atomic_read(v);

	do {
		if (__builtin_expect(!!(c == u), 0))
			break;
	} while (!raw_atomic_try_cmpxchg(v, &c, c + a));

	return c;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_add_unless(atomic_t *v, int a, int u)
{
	return raw_atomic_fetch_add_unless(v, a, u) != u;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_inc_not_zero(atomic_t *v)
{
	return raw_atomic_add_unless(v, 1, 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_inc_unless_negative(atomic_t *v)
{
	int c = raw_atomic_read(v);

	do {
		if (__builtin_expect(!!(c < 0), 0))
			return false;
	} while (!raw_atomic_try_cmpxchg(v, &c, c + 1));

	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_dec_unless_positive(atomic_t *v)
{
	int c = raw_atomic_read(v);

	do {
		if (__builtin_expect(!!(c > 0), 0))
			return false;
	} while (!raw_atomic_try_cmpxchg(v, &c, c - 1));

	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_atomic_dec_if_positive(atomic_t *v)
{
	int dec, c = raw_atomic_read(v);

	do {
		dec = c - 1;
		if (__builtin_expect(!!(dec < 0), 0))
			break;
	} while (!raw_atomic_try_cmpxchg(v, &c, dec));

	return dec;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_read(const atomic64_t *v)
{
	return arch_atomic64_read(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_read_acquire(const atomic64_t *v)
{
	s64 ret;

	if ((sizeof(atomic64_t) == sizeof(char) ||
	     sizeof(atomic64_t) == sizeof(short) ||
	     sizeof(atomic64_t) == sizeof(int) ||
	     sizeof(atomic64_t) == sizeof(long))) {
		ret = ({
			typeof(*&(v)->counter) ___p1 = ({
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_38(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*&(v)->counter) ==
						       sizeof(char) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(short) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(int) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(long)) ||
					      sizeof(*&(v)->counter) ==
						      sizeof(long long)))
						__compiletime_assert_38();
				} while (0);
				(*(const volatile typeof(_Generic(
					(*&(v)->counter),
								 char: (char)0,
								 unsigned char: (
									 unsigned char)0,
								 signed char: (
									 signed char)0,
								 unsigned short: (
									 unsigned short)0,
								 signed short: (
									 signed short)0,
								 unsigned int: (
									 unsigned int)0,
								 signed int: (
									 signed int)0,
								 unsigned long: (
									 unsigned long)0,
								 signed long: (
									 signed long)0,
								 unsigned long long: (
									 unsigned long long)0,
								 signed long long: (
									 signed long long)0,
								 default: (
									 *&(v)->counter)))
					   *)&(*&(v)->counter));
			});
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_39(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*&(v)->counter) == sizeof(char) ||
				       sizeof(*&(v)->counter) == sizeof(short) ||
				       sizeof(*&(v)->counter) == sizeof(int) ||
				       sizeof(*&(v)->counter) == sizeof(long))))
					__compiletime_assert_39();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			___p1;
		});
	} else {
		ret = raw_atomic64_read(v);
		do {
			do {
			} while (0);
			do {
			} while (0);
		} while (0);
	}

	return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic64_set(atomic64_t *v, s64 i)
{
	arch_atomic64_set(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic64_set_release(atomic64_t *v, s64 i)
{
	if ((sizeof(atomic64_t) == sizeof(char) ||
	     sizeof(atomic64_t) == sizeof(short) ||
	     sizeof(atomic64_t) == sizeof(int) ||
	     sizeof(atomic64_t) == sizeof(long))) {
		do {
			do {
			} while (0);
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_40(void)
						__attribute__((__error__(
							"Need native word sized stores/loads for atomicity.")));
					if (!((sizeof(*&(v)->counter) ==
						       sizeof(char) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(short) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(int) ||
					       sizeof(*&(v)->counter) ==
						       sizeof(long))))
						__compiletime_assert_40();
				} while (0);
				__asm__ __volatile__("" : : : "memory");
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_41(void)
							__attribute__((__error__(
								"Unsupported access size for {READ,WRITE}_ONCE().")));
						if (!((sizeof(*&(v)->counter) ==
							       sizeof(char) ||
						       sizeof(*&(v)->counter) ==
							       sizeof(short) ||
						       sizeof(*&(v)->counter) ==
							       sizeof(int) ||
						       sizeof(*&(v)->counter) ==
							       sizeof(long)) ||
						      sizeof(*&(v)->counter) ==
							      sizeof(long long)))
							__compiletime_assert_41();
					} while (0);
					do {
						*(volatile typeof(*&(v)->counter)
							  *)&(*&(v)->counter) =
							(i);
					} while (0);
				} while (0);
			} while (0);
		} while (0);
	} else {
		do {
			do {
			} while (0);
			do {
			} while (0);
		} while (0);
		raw_atomic64_set(v, i);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic64_add(s64 i, atomic64_t *v)
{
	arch_atomic64_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_add_return(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_add_return_acquire(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_add_return_release(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_add_return_relaxed(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_add(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_add_acquire(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_add_release(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_add_relaxed(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic64_sub(s64 i, atomic64_t *v)
{
	arch_atomic64_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_sub_return(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_return(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_sub_return_acquire(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_return(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_sub_return_release(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_return(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_sub_return_relaxed(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_return(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_sub(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_add(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_sub_acquire(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_add(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_sub_release(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_add(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_sub_relaxed(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_add(-(i), v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic64_inc(atomic64_t *v)
{
	arch_atomic64_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_inc_return(atomic64_t *v)
{
	return raw_atomic64_add_return(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_inc_return_acquire(atomic64_t *v)
{
	return raw_atomic64_add_return_acquire(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_inc_return_release(atomic64_t *v)
{
	return raw_atomic64_add_return_release(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_inc_return_relaxed(atomic64_t *v)
{
	return raw_atomic64_add_return_relaxed(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_inc(atomic64_t *v)
{
	return raw_atomic64_fetch_add(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_inc_acquire(atomic64_t *v)
{
	return raw_atomic64_fetch_add_acquire(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_inc_release(atomic64_t *v)
{
	return raw_atomic64_fetch_add_release(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_inc_relaxed(atomic64_t *v)
{
	return raw_atomic64_fetch_add_relaxed(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic64_dec(atomic64_t *v)
{
	arch_atomic64_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_dec_return(atomic64_t *v)
{
	return raw_atomic64_sub_return(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_dec_return_acquire(atomic64_t *v)
{
	return raw_atomic64_sub_return_acquire(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_dec_return_release(atomic64_t *v)
{
	return raw_atomic64_sub_return_release(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_dec_return_relaxed(atomic64_t *v)
{
	return raw_atomic64_sub_return_relaxed(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_dec(atomic64_t *v)
{
	return raw_atomic64_fetch_sub(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_dec_acquire(atomic64_t *v)
{
	return raw_atomic64_fetch_sub_acquire(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_dec_release(atomic64_t *v)
{
	return raw_atomic64_fetch_sub_release(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_dec_relaxed(atomic64_t *v)
{
	return raw_atomic64_fetch_sub_relaxed(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic64_and(s64 i, atomic64_t *v)
{
	arch_atomic64_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_and(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_and_acquire(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_and_release(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_and_relaxed(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic64_andnot(s64 i, atomic64_t *v)
{
	raw_atomic64_and(~i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_andnot(s64 i, atomic64_t *v)
{
	return raw_atomic64_fetch_and(~i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_andnot_acquire(s64 i, atomic64_t *v)
{
	return raw_atomic64_fetch_and_acquire(~i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_andnot_release(s64 i, atomic64_t *v)
{
	return raw_atomic64_fetch_and_release(~i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_andnot_relaxed(s64 i, atomic64_t *v)
{
	return raw_atomic64_fetch_and_relaxed(~i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic64_or(s64 i, atomic64_t *v)
{
	arch_atomic64_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_or(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_or_acquire(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_or_release(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_or_relaxed(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic64_xor(s64 i, atomic64_t *v)
{
	arch_atomic64_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_xor(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_xor_acquire(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_xor_release(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_xor_relaxed(s64 i, atomic64_t *v)
{
	return arch_atomic64_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_xchg(atomic64_t *v, s64 new)
{
	return arch_atomic64_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_xchg_acquire(atomic64_t *v, s64 new)
{
	return arch_atomic64_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_xchg_release(atomic64_t *v, s64 new)
{
	return arch_atomic64_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_xchg_relaxed(atomic64_t *v, s64 new)
{
	return arch_atomic64_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_cmpxchg(atomic64_t *v, s64 old, s64 new)
{
	return arch_atomic64_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_cmpxchg_acquire(atomic64_t *v, s64 old, s64 new)
{
	return arch_atomic64_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_cmpxchg_release(atomic64_t *v, s64 old, s64 new)
{
	return arch_atomic64_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_cmpxchg_relaxed(atomic64_t *v, s64 old, s64 new)
{
	return arch_atomic64_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_try_cmpxchg(atomic64_t *v, s64 *old, s64 new)
{
	return arch_atomic64_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_try_cmpxchg_acquire(atomic64_t *v, s64 *old, s64 new)
{
	return arch_atomic64_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_try_cmpxchg_release(atomic64_t *v, s64 *old, s64 new)
{
	return arch_atomic64_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_try_cmpxchg_relaxed(atomic64_t *v, s64 *old, s64 new)
{
	return arch_atomic64_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_sub_and_test(s64 i, atomic64_t *v)
{
	return arch_atomic64_sub_and_test(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_dec_and_test(atomic64_t *v)
{
	return arch_atomic64_dec_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_inc_and_test(atomic64_t *v)
{
	return arch_atomic64_inc_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_add_negative(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_add_negative_acquire(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_add_negative_release(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_add_negative_relaxed(s64 i, atomic64_t *v)
{
	return arch_atomic64_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
{
	s64 c = raw_atomic64_read(v);

	do {
		if (__builtin_expect(!!(c == u), 0))
			break;
	} while (!raw_atomic64_try_cmpxchg(v, &c, c + a));

	return c;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_add_unless(atomic64_t *v, s64 a, s64 u)
{
	return raw_atomic64_fetch_add_unless(v, a, u) != u;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_inc_not_zero(atomic64_t *v)
{
	return raw_atomic64_add_unless(v, 1, 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_inc_unless_negative(atomic64_t *v)
{
	s64 c = raw_atomic64_read(v);

	do {
		if (__builtin_expect(!!(c < 0), 0))
			return false;
	} while (!raw_atomic64_try_cmpxchg(v, &c, c + 1));

	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic64_dec_unless_positive(atomic64_t *v)
{
	s64 c = raw_atomic64_read(v);

	do {
		if (__builtin_expect(!!(c > 0), 0))
			return false;
	} while (!raw_atomic64_try_cmpxchg(v, &c, c - 1));

	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
raw_atomic64_dec_if_positive(atomic64_t *v)
{
	s64 dec, c = raw_atomic64_read(v);

	do {
		dec = c - 1;
		if (__builtin_expect(!!(dec < 0), 0))
			break;
	} while (!raw_atomic64_try_cmpxchg(v, &c, dec));

	return dec;
}

typedef atomic64_t atomic_long_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_read(const atomic_long_t *v)
{
	return raw_atomic64_read(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_read_acquire(const atomic_long_t *v)
{
	return raw_atomic64_read_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_long_set(atomic_long_t *v, long i)
{
	raw_atomic64_set(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_long_set_release(atomic_long_t *v, long i)
{
	raw_atomic64_set_release(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_long_add(long i, atomic_long_t *v)
{
	raw_atomic64_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_add_return(long i, atomic_long_t *v)
{
	return raw_atomic64_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_add_return_acquire(long i, atomic_long_t *v)
{
	return raw_atomic64_add_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_add_return_release(long i, atomic_long_t *v)
{
	return raw_atomic64_add_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_add_return_relaxed(long i, atomic_long_t *v)
{
	return raw_atomic64_add_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_add(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_add_acquire(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_add_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_add_release(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_add_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_add_relaxed(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_add_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_long_sub(long i, atomic_long_t *v)
{
	raw_atomic64_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_sub_return(long i, atomic_long_t *v)
{
	return raw_atomic64_sub_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_sub_return_acquire(long i, atomic_long_t *v)
{
	return raw_atomic64_sub_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_sub_return_release(long i, atomic_long_t *v)
{
	return raw_atomic64_sub_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_sub_return_relaxed(long i, atomic_long_t *v)
{
	return raw_atomic64_sub_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_sub(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_sub_acquire(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_sub_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_sub_release(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_sub_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_sub_relaxed(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_sub_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_long_inc(atomic_long_t *v)
{
	raw_atomic64_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_inc_return(atomic_long_t *v)
{
	return raw_atomic64_inc_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_inc_return_acquire(atomic_long_t *v)
{
	return raw_atomic64_inc_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_inc_return_release(atomic_long_t *v)
{
	return raw_atomic64_inc_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_inc_return_relaxed(atomic_long_t *v)
{
	return raw_atomic64_inc_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_inc(atomic_long_t *v)
{
	return raw_atomic64_fetch_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_inc_acquire(atomic_long_t *v)
{
	return raw_atomic64_fetch_inc_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_inc_release(atomic_long_t *v)
{
	return raw_atomic64_fetch_inc_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_inc_relaxed(atomic_long_t *v)
{
	return raw_atomic64_fetch_inc_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_long_dec(atomic_long_t *v)
{
	raw_atomic64_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_dec_return(atomic_long_t *v)
{
	return raw_atomic64_dec_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_dec_return_acquire(atomic_long_t *v)
{
	return raw_atomic64_dec_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_dec_return_release(atomic_long_t *v)
{
	return raw_atomic64_dec_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_dec_return_relaxed(atomic_long_t *v)
{
	return raw_atomic64_dec_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_dec(atomic_long_t *v)
{
	return raw_atomic64_fetch_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_dec_acquire(atomic_long_t *v)
{
	return raw_atomic64_fetch_dec_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_dec_release(atomic_long_t *v)
{
	return raw_atomic64_fetch_dec_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_dec_relaxed(atomic_long_t *v)
{
	return raw_atomic64_fetch_dec_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_long_and(long i, atomic_long_t *v)
{
	raw_atomic64_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_and(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_and_acquire(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_and_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_and_release(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_and_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_and_relaxed(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_and_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_long_andnot(long i, atomic_long_t *v)
{
	raw_atomic64_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_andnot(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_andnot_acquire(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_andnot_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_andnot_release(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_andnot_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_andnot_relaxed(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_andnot_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_long_or(long i, atomic_long_t *v)
{
	raw_atomic64_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_or(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_or_acquire(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_or_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_or_release(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_or_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_or_relaxed(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_or_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
raw_atomic_long_xor(long i, atomic_long_t *v)
{
	raw_atomic64_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_xor(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_xor_acquire(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_xor_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_xor_release(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_xor_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_xor_relaxed(long i, atomic_long_t *v)
{
	return raw_atomic64_fetch_xor_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_xchg(atomic_long_t *v, long new)
{
	return raw_atomic64_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_xchg_acquire(atomic_long_t *v, long new)
{
	return raw_atomic64_xchg_acquire(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_xchg_release(atomic_long_t *v, long new)
{
	return raw_atomic64_xchg_release(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_xchg_relaxed(atomic_long_t *v, long new)
{
	return raw_atomic64_xchg_relaxed(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_cmpxchg(atomic_long_t *v, long old, long new)
{
	return raw_atomic64_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_cmpxchg_acquire(atomic_long_t *v, long old, long new)
{
	return raw_atomic64_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_cmpxchg_release(atomic_long_t *v, long old, long new)
{
	return raw_atomic64_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_cmpxchg_relaxed(atomic_long_t *v, long old, long new)
{
	return raw_atomic64_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_try_cmpxchg(atomic_long_t *v, long *old, long new)
{
	return raw_atomic64_try_cmpxchg(v, (s64 *)old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_try_cmpxchg_acquire(atomic_long_t *v, long *old, long new)
{
	return raw_atomic64_try_cmpxchg_acquire(v, (s64 *)old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_try_cmpxchg_release(atomic_long_t *v, long *old, long new)
{
	return raw_atomic64_try_cmpxchg_release(v, (s64 *)old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_try_cmpxchg_relaxed(atomic_long_t *v, long *old, long new)
{
	return raw_atomic64_try_cmpxchg_relaxed(v, (s64 *)old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_sub_and_test(long i, atomic_long_t *v)
{
	return raw_atomic64_sub_and_test(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_dec_and_test(atomic_long_t *v)
{
	return raw_atomic64_dec_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_inc_and_test(atomic_long_t *v)
{
	return raw_atomic64_inc_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_add_negative(long i, atomic_long_t *v)
{
	return raw_atomic64_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_add_negative_acquire(long i, atomic_long_t *v)
{
	return raw_atomic64_add_negative_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_add_negative_release(long i, atomic_long_t *v)
{
	return raw_atomic64_add_negative_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_add_negative_relaxed(long i, atomic_long_t *v)
{
	return raw_atomic64_add_negative_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_add_unless(atomic_long_t *v, long a, long u)
{
	return raw_atomic64_fetch_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_add_unless(atomic_long_t *v, long a, long u)
{
	return raw_atomic64_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_inc_not_zero(atomic_long_t *v)
{
	return raw_atomic64_inc_not_zero(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_inc_unless_negative(atomic_long_t *v)
{
	return raw_atomic64_inc_unless_negative(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
raw_atomic_long_dec_unless_positive(atomic_long_t *v)
{
	return raw_atomic64_dec_unless_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
raw_atomic_long_dec_if_positive(atomic_long_t *v)
{
	return raw_atomic64_dec_if_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_read(const atomic_t *v)
{
	instrument_atomic_read(v, sizeof(*v));
	return raw_atomic_read(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_read_acquire(const atomic_t *v)
{
	instrument_atomic_read(v, sizeof(*v));
	return raw_atomic_read_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_set(atomic_t *v, int i)
{
	instrument_atomic_write(v, sizeof(*v));
	raw_atomic_set(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_set_release(atomic_t *v, int i)
{
	do {
	} while (0);
	instrument_atomic_write(v, sizeof(*v));
	raw_atomic_set_release(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_add(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_add_return(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_add_return_acquire(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_add_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_add_return_release(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_add_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_add_return_relaxed(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_add_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_add(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_add_acquire(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_add_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_add_release(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_add_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_add_relaxed(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_add_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_sub(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_sub_return(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_sub_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_sub_return_acquire(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_sub_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_sub_return_release(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_sub_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_sub_return_relaxed(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_sub_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_sub(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_sub_acquire(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_sub_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_sub_release(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_sub_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_sub_relaxed(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_sub_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_inc(atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_inc_return(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_inc_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_inc_return_acquire(atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_inc_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_inc_return_release(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_inc_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_inc_return_relaxed(atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_inc_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_inc(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_inc_acquire(atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_inc_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_inc_release(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_inc_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_inc_relaxed(atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_inc_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_dec(atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_dec_return(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_dec_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_dec_return_acquire(atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_dec_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_dec_return_release(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_dec_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_dec_return_relaxed(atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_dec_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_dec(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_dec_acquire(atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_dec_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_dec_release(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_dec_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_dec_relaxed(atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_dec_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_and(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_and(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_and_acquire(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_and_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_and_release(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_and_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_and_relaxed(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_and_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_andnot(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_andnot(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_andnot_acquire(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_andnot_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_andnot_release(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_andnot_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_andnot_relaxed(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_andnot_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_or(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_or(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_or_acquire(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_or_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_or_release(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_or_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_or_relaxed(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_or_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_xor(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_xor(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_xor_acquire(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_xor_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_xor_release(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_xor_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_xor_relaxed(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_xor_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_xchg(atomic_t *v, int new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_xchg_acquire(atomic_t *v, int new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_xchg_acquire(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_xchg_release(atomic_t *v, int new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_xchg_release(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_xchg_relaxed(atomic_t *v, int new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_xchg_relaxed(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_cmpxchg(atomic_t *v, int old, int new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_cmpxchg_acquire(atomic_t *v, int old, int new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_cmpxchg_release(atomic_t *v, int old, int new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_cmpxchg_relaxed(atomic_t *v, int old, int new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg(atomic_t *v, int *old, int new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg_acquire(atomic_t *v, int *old, int new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic_try_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg_release(atomic_t *v, int *old, int new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic_try_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg_relaxed(atomic_t *v, int *old, int new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic_try_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_sub_and_test(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_sub_and_test(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_dec_and_test(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_dec_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_inc_and_test(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_inc_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_add_negative(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_add_negative_acquire(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_add_negative_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_add_negative_release(int i, atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_add_negative_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_add_negative_relaxed(int i, atomic_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_add_negative_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_fetch_add_unless(atomic_t *v, int a, int u)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_fetch_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_add_unless(atomic_t *v, int a, int u)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_inc_not_zero(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_inc_not_zero(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_inc_unless_negative(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_inc_unless_negative(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_dec_unless_positive(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_dec_unless_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
atomic_dec_if_positive(atomic_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_dec_if_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_read(const atomic64_t *v)
{
	instrument_atomic_read(v, sizeof(*v));
	return raw_atomic64_read(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_read_acquire(const atomic64_t *v)
{
	instrument_atomic_read(v, sizeof(*v));
	return raw_atomic64_read_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic64_set(atomic64_t *v, s64 i)
{
	instrument_atomic_write(v, sizeof(*v));
	raw_atomic64_set(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic64_set_release(atomic64_t *v, s64 i)
{
	do {
	} while (0);
	instrument_atomic_write(v, sizeof(*v));
	raw_atomic64_set_release(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic64_add(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic64_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_add_return(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_add_return_acquire(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_add_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_add_return_release(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_add_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_add_return_relaxed(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_add_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_add(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_add_acquire(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_add_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_add_release(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_add_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_add_relaxed(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_add_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic64_sub(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic64_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_sub_return(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_sub_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_sub_return_acquire(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_sub_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_sub_return_release(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_sub_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_sub_return_relaxed(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_sub_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_sub(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_sub_acquire(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_sub_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_sub_release(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_sub_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_sub_relaxed(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_sub_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic64_inc(atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic64_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_inc_return(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_inc_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_inc_return_acquire(atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_inc_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_inc_return_release(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_inc_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_inc_return_relaxed(atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_inc_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_inc(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_inc_acquire(atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_inc_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_inc_release(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_inc_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_inc_relaxed(atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_inc_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic64_dec(atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic64_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_dec_return(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_dec_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_dec_return_acquire(atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_dec_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_dec_return_release(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_dec_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_dec_return_relaxed(atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_dec_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_dec(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_dec_acquire(atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_dec_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_dec_release(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_dec_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_dec_relaxed(atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_dec_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic64_and(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic64_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_and(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_and_acquire(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_and_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_and_release(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_and_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_and_relaxed(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_and_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic64_andnot(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic64_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot_acquire(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_andnot_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot_release(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_andnot_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot_relaxed(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_andnot_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic64_or(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic64_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_or(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_or_acquire(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_or_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_or_release(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_or_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_or_relaxed(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_or_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic64_xor(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic64_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_xor(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_xor_acquire(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_xor_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_xor_release(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_xor_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_xor_relaxed(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_xor_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_xchg(atomic64_t *v, s64 new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_xchg_acquire(atomic64_t *v, s64 new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_xchg_acquire(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_xchg_release(atomic64_t *v, s64 new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_xchg_release(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_xchg_relaxed(atomic64_t *v, s64 new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_xchg_relaxed(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_cmpxchg(atomic64_t *v, s64 old, s64 new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_cmpxchg_acquire(atomic64_t *v, s64 old, s64 new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_cmpxchg_release(atomic64_t *v, s64 old, s64 new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_cmpxchg_relaxed(atomic64_t *v, s64 old, s64 new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg(atomic64_t *v, s64 *old, s64 new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic64_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg_acquire(atomic64_t *v, s64 *old, s64 new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic64_try_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg_release(atomic64_t *v, s64 *old, s64 new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic64_try_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg_relaxed(atomic64_t *v, s64 *old, s64 new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic64_try_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_sub_and_test(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_sub_and_test(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_dec_and_test(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_dec_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_inc_and_test(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_inc_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_add_negative(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_add_negative_acquire(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_add_negative_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_add_negative_release(s64 i, atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_add_negative_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_add_negative_relaxed(s64 i, atomic64_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_add_negative_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_fetch_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_add_unless(atomic64_t *v, s64 a, s64 u)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_inc_not_zero(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_inc_not_zero(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_inc_unless_negative(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_inc_unless_negative(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic64_dec_unless_positive(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_dec_unless_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) s64
atomic64_dec_if_positive(atomic64_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic64_dec_if_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_read(const atomic_long_t *v)
{
	instrument_atomic_read(v, sizeof(*v));
	return raw_atomic_long_read(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_read_acquire(const atomic_long_t *v)
{
	instrument_atomic_read(v, sizeof(*v));
	return raw_atomic_long_read_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_long_set(atomic_long_t *v, long i)
{
	instrument_atomic_write(v, sizeof(*v));
	raw_atomic_long_set(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_long_set_release(atomic_long_t *v, long i)
{
	do {
	} while (0);
	instrument_atomic_write(v, sizeof(*v));
	raw_atomic_long_set_release(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_long_add(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_long_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_add_return(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_add_return_acquire(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_add_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_add_return_release(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_add_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_add_return_relaxed(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_add_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_add(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_add_acquire(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_add_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_add_release(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_add_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_add_relaxed(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_add_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_long_sub(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_long_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_sub_return(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_sub_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_sub_return_acquire(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_sub_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_sub_return_release(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_sub_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_sub_return_relaxed(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_sub_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_sub(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_sub_acquire(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_sub_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_sub_release(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_sub_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_sub_relaxed(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_sub_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_long_inc(atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_long_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_inc_return(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_inc_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_inc_return_acquire(atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_inc_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_inc_return_release(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_inc_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_inc_return_relaxed(atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_inc_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_inc(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_inc_acquire(atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_inc_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_inc_release(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_inc_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_inc_relaxed(atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_inc_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_long_dec(atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_long_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_dec_return(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_dec_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_dec_return_acquire(atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_dec_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_dec_return_release(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_dec_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_dec_return_relaxed(atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_dec_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_dec(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_dec_acquire(atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_dec_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_dec_release(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_dec_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_dec_relaxed(atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_dec_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_long_and(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_long_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_and(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_and_acquire(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_and_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_and_release(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_and_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_and_relaxed(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_and_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_long_andnot(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_long_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot_acquire(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_andnot_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot_release(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_andnot_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot_relaxed(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_andnot_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_long_or(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_long_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_or(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_or_acquire(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_or_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_or_release(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_or_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_or_relaxed(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_or_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
atomic_long_xor(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	raw_atomic_long_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_xor(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_xor_acquire(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_xor_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_xor_release(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_xor_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_xor_relaxed(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_xor_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_xchg(atomic_long_t *v, long new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_xchg_acquire(atomic_long_t *v, long new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_xchg_acquire(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_xchg_release(atomic_long_t *v, long new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_xchg_release(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_xchg_relaxed(atomic_long_t *v, long new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_xchg_relaxed(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_cmpxchg(atomic_long_t *v, long old, long new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_cmpxchg_acquire(atomic_long_t *v, long old, long new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_cmpxchg_release(atomic_long_t *v, long old, long new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_cmpxchg_relaxed(atomic_long_t *v, long old, long new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg(atomic_long_t *v, long *old, long new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic_long_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg_acquire(atomic_long_t *v, long *old, long new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic_long_try_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg_release(atomic_long_t *v, long *old, long new)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic_long_try_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg_relaxed(atomic_long_t *v, long *old, long new)
{
	instrument_atomic_read_write(v, sizeof(*v));
	instrument_atomic_read_write(old, sizeof(*old));
	return raw_atomic_long_try_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_sub_and_test(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_sub_and_test(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_dec_and_test(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_dec_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_inc_and_test(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_inc_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_add_negative(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_add_negative_acquire(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_add_negative_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_add_negative_release(long i, atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_add_negative_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_add_negative_relaxed(long i, atomic_long_t *v)
{
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_add_negative_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_fetch_add_unless(atomic_long_t *v, long a, long u)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_fetch_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_add_unless(atomic_long_t *v, long a, long u)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_inc_not_zero(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_inc_not_zero(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_inc_unless_negative(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_inc_unless_negative(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
atomic_long_dec_unless_positive(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_dec_unless_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
atomic_long_dec_if_positive(atomic_long_t *v)
{
	do {
	} while (0);
	instrument_atomic_read_write(v, sizeof(*v));
	return raw_atomic_long_dec_if_positive(v);
}

struct thread_info {
	unsigned long flags;
	unsigned long syscall_work;
	u32 status;

	u32 cpu;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
arch_within_stack_frames(const void *const stack, const void *const stackend,
			 const void *obj, unsigned long len)
{
	return NOT_STACK;
}
extern void arch_setup_new_exec(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) long
set_restart_fn(struct restart_block *restart,
	       long (*fn)(struct restart_block *))
{
	restart->fn = fn;
	do {
		restart->arch_data =
			((struct thread_info *)get_current())->status;
	} while (0);
	return -516;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_ti_thread_flag(struct thread_info *ti, int flag)
{
	set_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_ti_thread_flag(struct thread_info *ti, int flag)
{
	clear_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
update_ti_thread_flag(struct thread_info *ti, int flag, bool value)
{
	if (value)
		set_ti_thread_flag(ti, flag);
	else
		clear_ti_thread_flag(ti, flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
test_and_set_ti_thread_flag(struct thread_info *ti, int flag)
{
	return test_and_set_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
test_and_clear_ti_thread_flag(struct thread_info *ti, int flag)
{
	return test_and_clear_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
test_ti_thread_flag(struct thread_info *ti, int flag)
{
	return ((__builtin_constant_p(flag) &&
		 __builtin_constant_p((uintptr_t)((unsigned long *)&ti->flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)((unsigned long *)&ti->flags) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)((unsigned long *)&ti->flags))) ?
			const_test_bit(flag, (unsigned long *)&ti->flags) :
			_test_bit(flag, (unsigned long *)&ti->flags));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
read_ti_thread_flags(struct thread_info *ti)
{
	return ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_42(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(ti->flags) == sizeof(char) ||
			       sizeof(ti->flags) == sizeof(short) ||
			       sizeof(ti->flags) == sizeof(int) ||
			       sizeof(ti->flags) == sizeof(long)) ||
			      sizeof(ti->flags) == sizeof(long long)))
				__compiletime_assert_42();
		} while (0);
		(*(const volatile typeof(_Generic((ti->flags),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (ti->flags)))
			   *)&(ti->flags));
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
tif_need_resched(void)
{
	return arch_test_bit(
		3, (unsigned long *)(&((struct thread_info *)get_current())
					      ->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
check_object_size(const void *ptr, unsigned long n, bool to_user)
{
}

extern void __attribute__((__error__("copy source size is too small")))
__bad_copy_from(void);
extern void __attribute__((__error__("copy destination size is too small")))
__bad_copy_to(void);

void __copy_overflow(int size, unsigned long count);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
copy_overflow(int size, unsigned long count)
{
	if (1)
		__copy_overflow(size, count);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) bool
check_copy_size(const void *addr, size_t bytes, bool is_source)
{
	int sz = __builtin_object_size(addr, 0);
	if (__builtin_expect(!!(sz >= 0 && sz < bytes), 0)) {
		if (!__builtin_constant_p(bytes))
			copy_overflow(sz, bytes);
		else if (is_source)
			__bad_copy_from();
		else
			__bad_copy_to();
		return false;
	}
	if (({
		    int __ret_warn_on = !!(bytes > ((int)(~0U >> 1)));
		    if (__builtin_expect(!!(__ret_warn_on), 0))
			    do {
				    __auto_type __flags =
					    (1 << 0) | ((1 << 1) | ((9) << 8));
				    ({
					    asm volatile(
						    "43"
						    ": nop\n\t"
						    ".pushsection .discard.instr_begin\n\t"
						    ".long "
						    "43"
						    "b - .\n\t"
						    ".popsection\n\t"
						    :
						    : "i"(43));
				    });
				    do {
					    asm __inline volatile(
						    "1:\t"
						    ".byte 0x0f, 0x0b"
						    "\n"
						    ".pushsection __bug_table,\"aw\"\n"
						    "2:\t"
						    ".long "
						    "1b"
						    " - ."
						    "\t# bug_entry::bug_addr\n"
						    "\t"
						    ".long "
						    "%c0"
						    " - ."
						    "\t# bug_entry::file\n"
						    "\t.word %c1"
						    "\t# bug_entry::line\n"
						    "\t.word %c2"
						    "\t# bug_entry::flags\n"
						    "\t.org 2b+%c3\n"
						    ".popsection\n"
						    "998:\n\t"
						    ".pushsection .discard.reachable\n\t"
						    ".long 998b\n\t"
						    ".popsection\n\t"
						    :
						    : "i"("include/linux/thread_info.h"),
						      "i"(249), "i"(__flags),
						      "i"(sizeof(
							      struct bug_entry)));
				    } while (0);
				    ({
					    asm volatile(
						    "44"
						    ": nop\n\t"
						    ".pushsection .discard.instr_end\n\t"
						    ".long "
						    "44"
						    "b - .\n\t"
						    ".popsection\n\t"
						    :
						    : "i"(44));
				    });
			    } while (0);
		    __builtin_expect(!!(__ret_warn_on), 0);
	    }))
		return false;
	check_object_size(addr, bytes, is_source);
	return true;
}

void arch_task_cache_init(void);
void arch_release_task_struct(struct task_struct *tsk);
int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__local_bh_disable_ip(unsigned long ip, unsigned int cnt)
{
	__preempt_count_add(cnt);
	__asm__ __volatile__("" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
local_bh_disable(void)
{
	__local_bh_disable_ip(({
				      unsigned long __here;
				      asm("lea 0(%%rip), %0" : "=r"(__here));
				      __here;
			      }),
			      (2 * (1UL << (0 + 8))));
}

extern void _local_bh_enable(void);
extern void __local_bh_enable_ip(unsigned long ip, unsigned int cnt);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
local_bh_enable_ip(unsigned long ip)
{
	__local_bh_enable_ip(ip, (2 * (1UL << (0 + 8))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
local_bh_enable(void)
{
	__local_bh_enable_ip(({
				     unsigned long __here;
				     asm("lea 0(%%rip), %0" : "=r"(__here));
				     __here;
			     }),
			     (2 * (1UL << (0 + 8))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
local_bh_blocked(void)
{
	return false;
}

extern const char hex_asc[];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) char *
hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc[((byte) & 0xf0) >> 4];
	*buf++ = hex_asc[((byte) & 0x0f)];
	return buf;
}

extern const char hex_asc_upper[];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) char *
hex_byte_pack_upper(char *buf, u8 byte)
{
	*buf++ = hex_asc_upper[((byte) & 0xf0) >> 4];
	*buf++ = hex_asc_upper[((byte) & 0x0f)];
	return buf;
}

extern int hex_to_bin(unsigned char ch);
extern int __attribute__((__warn_unused_result__))
hex2bin(u8 *dst, const char *src, size_t count);
extern char *bin2hex(char *dst, const void *src, size_t count);

bool mac_pton(const char *s, u8 *mac);

int __attribute__((__warn_unused_result__))
_kstrtoul(const char *s, unsigned int base, unsigned long *res);
int __attribute__((__warn_unused_result__))
_kstrtol(const char *s, unsigned int base, long *res);

int __attribute__((__warn_unused_result__))
kstrtoull(const char *s, unsigned int base, unsigned long long *res);
int __attribute__((__warn_unused_result__))
kstrtoll(const char *s, unsigned int base, long long *res);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	kstrtoul(const char *s, unsigned int base, unsigned long *res)
{
	if (sizeof(unsigned long) == sizeof(unsigned long long) &&
	    __alignof__(unsigned long) == __alignof__(unsigned long long))
		return kstrtoull(s, base, (unsigned long long *)res);
	else
		return _kstrtoul(s, base, res);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	kstrtol(const char *s, unsigned int base, long *res)
{
	if (sizeof(long) == sizeof(long long) &&
	    __alignof__(long) == __alignof__(long long))
		return kstrtoll(s, base, (long long *)res);
	else
		return _kstrtol(s, base, res);
}

int __attribute__((__warn_unused_result__))
kstrtouint(const char *s, unsigned int base, unsigned int *res);
int __attribute__((__warn_unused_result__))
kstrtoint(const char *s, unsigned int base, int *res);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	kstrtou64(const char *s, unsigned int base, u64 *res)
{
	return kstrtoull(s, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	kstrtos64(const char *s, unsigned int base, s64 *res)
{
	return kstrtoll(s, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	kstrtou32(const char *s, unsigned int base, u32 *res)
{
	return kstrtouint(s, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	kstrtos32(const char *s, unsigned int base, s32 *res)
{
	return kstrtoint(s, base, res);
}

int __attribute__((__warn_unused_result__))
kstrtou16(const char *s, unsigned int base, u16 *res);
int __attribute__((__warn_unused_result__))
kstrtos16(const char *s, unsigned int base, s16 *res);
int __attribute__((__warn_unused_result__))
kstrtou8(const char *s, unsigned int base, u8 *res);
int __attribute__((__warn_unused_result__))
kstrtos8(const char *s, unsigned int base, s8 *res);
int __attribute__((__warn_unused_result__)) kstrtobool(const char *s,
						       bool *res);

int __attribute__((__warn_unused_result__))
kstrtoull_from_user(const char *s, size_t count, unsigned int base,
		    unsigned long long *res);
int __attribute__((__warn_unused_result__))
kstrtoll_from_user(const char *s, size_t count, unsigned int base,
		   long long *res);
int __attribute__((__warn_unused_result__))
kstrtoul_from_user(const char *s, size_t count, unsigned int base,
		   unsigned long *res);
int __attribute__((__warn_unused_result__))
kstrtol_from_user(const char *s, size_t count, unsigned int base, long *res);
int __attribute__((__warn_unused_result__))
kstrtouint_from_user(const char *s, size_t count, unsigned int base,
		     unsigned int *res);
int __attribute__((__warn_unused_result__))
kstrtoint_from_user(const char *s, size_t count, unsigned int base, int *res);
int __attribute__((__warn_unused_result__))
kstrtou16_from_user(const char *s, size_t count, unsigned int base, u16 *res);
int __attribute__((__warn_unused_result__))
kstrtos16_from_user(const char *s, size_t count, unsigned int base, s16 *res);
int __attribute__((__warn_unused_result__))
kstrtou8_from_user(const char *s, size_t count, unsigned int base, u8 *res);
int __attribute__((__warn_unused_result__))
kstrtos8_from_user(const char *s, size_t count, unsigned int base, s8 *res);
int __attribute__((__warn_unused_result__))
kstrtobool_from_user(const char *s, size_t count, bool *res);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	kstrtou64_from_user(const char *s, size_t count, unsigned int base,
			    u64 *res)
{
	return kstrtoull_from_user(s, count, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	kstrtos64_from_user(const char *s, size_t count, unsigned int base,
			    s64 *res)
{
	return kstrtoll_from_user(s, count, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	kstrtou32_from_user(const char *s, size_t count, unsigned int base,
			    u32 *res)
{
	return kstrtouint_from_user(s, count, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	kstrtos32_from_user(const char *s, size_t count, unsigned int base,
			    s32 *res)
{
	return kstrtoint_from_user(s, count, base, res);
}
extern unsigned long simple_strtoul(const char *, char **, unsigned int);
extern long simple_strtol(const char *, char **, unsigned int);
extern unsigned long long simple_strtoull(const char *, char **, unsigned int);
extern long long simple_strtoll(const char *, char **, unsigned int);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
in_range64(u64 val, u64 start, u64 len)
{
	return (val - start) < len;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
in_range32(u32 val, u32 start, u32 len)
{
	return (val - start) < len;
}

int num_to_str(char *buf, int size, unsigned long long num, unsigned int width);

__attribute__((__format__(printf, 2, 3))) int sprintf(char *buf,
						      const char *fmt, ...);
__attribute__((__format__(printf, 2, 0))) int vsprintf(char *buf, const char *,
						       va_list);
__attribute__((__format__(printf, 3, 4))) int snprintf(char *buf, size_t size,
						       const char *fmt, ...);
__attribute__((__format__(printf, 3, 0))) int
vsnprintf(char *buf, size_t size, const char *fmt, va_list args);
__attribute__((__format__(printf, 3, 4))) int scnprintf(char *buf, size_t size,
							const char *fmt, ...);
__attribute__((__format__(printf, 3, 0))) int
vscnprintf(char *buf, size_t size, const char *fmt, va_list args);
__attribute__((__format__(printf, 2, 3))) __attribute__((__malloc__)) char *
kasprintf(gfp_t gfp, const char *fmt, ...);
__attribute__((__format__(printf, 2, 0))) __attribute__((__malloc__)) char *
kvasprintf(gfp_t gfp, const char *fmt, va_list args);
__attribute__((__format__(printf, 2, 0))) const char *
kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);

__attribute__((__format__(scanf, 2, 3))) int sscanf(const char *, const char *,
						    ...);
__attribute__((__format__(scanf, 2, 0))) int vsscanf(const char *, const char *,
						     va_list);

extern bool no_hash_pointers;
int no_hash_pointers_enable(char *str);

struct completion;
struct user;
extern int __cond_resched(void);

extern struct static_call_key __SCK__might_resched;
extern typeof(__cond_resched) __SCT__might_resched;
;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
might_resched(void)
{
	({
		static void *__attribute__((__used__))
		__attribute__((__section__(".discard.addressable")))
		__UNIQUE_ID___addressable___SCK__might_resched45 =
			(void *)(uintptr_t)&__SCK__might_resched;
		;
		(&__SCT__might_resched);
	})();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__might_resched(const char *file, int line, unsigned int offsets)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__might_sleep(const char *file, int line)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
might_fault(void)
{
}

void do_exit(long error_code) __attribute__((__noreturn__));

extern int core_kernel_text(unsigned long addr);
extern int __kernel_text_address(unsigned long addr);
extern int kernel_text_address(unsigned long addr);
extern int func_ptr_is_kernel_text(void *ptr);

extern void bust_spinlocks(int yes);

extern int root_mountflags;

extern bool early_boot_irqs_disabled;

extern enum system_states {
	SYSTEM_BOOTING,
	SYSTEM_SCHEDULING,
	SYSTEM_FREEING_INITMEM,
	SYSTEM_RUNNING,
	SYSTEM_HALT,
	SYSTEM_POWER_OFF,
	SYSTEM_RESTART,
	SYSTEM_SUSPEND,
} system_state;
enum ftrace_dump_mode {
	DUMP_NONE,
	DUMP_ALL,
	DUMP_ORIG,
	DUMP_PARAM,
};

void tracing_on(void);
void tracing_off(void);
int tracing_is_on(void);
void tracing_snapshot(void);
void tracing_snapshot_alloc(void);

extern void tracing_start(void);
extern void tracing_stop(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__format__(printf, 1, 2))) void
____trace_printk_check_format(const char *fmt, ...)
{
}
extern __attribute__((__format__(printf, 2, 3))) int
__trace_bprintk(unsigned long ip, const char *fmt, ...);

extern __attribute__((__format__(printf, 2, 3))) int
__trace_printk(unsigned long ip, const char *fmt, ...);
extern int __trace_bputs(unsigned long ip, const char *str);
extern int __trace_puts(unsigned long ip, const char *str, int size);

extern void trace_dump_stack(int skip);
extern __attribute__((__format__(printf, 2, 0))) int
__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);

extern __attribute__((__format__(printf, 2, 0))) int
__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);

extern void ftrace_dump(enum ftrace_dump_mode oops_dump_mode);
unsigned long _find_next_bit(const unsigned long *addr1, unsigned long nbits,
			     unsigned long start);
unsigned long _find_next_and_bit(const unsigned long *addr1,
				 const unsigned long *addr2,
				 unsigned long nbits, unsigned long start);
unsigned long _find_next_andnot_bit(const unsigned long *addr1,
				    const unsigned long *addr2,
				    unsigned long nbits, unsigned long start);
unsigned long _find_next_or_bit(const unsigned long *addr1,
				const unsigned long *addr2, unsigned long nbits,
				unsigned long start);
unsigned long _find_next_zero_bit(const unsigned long *addr,
				  unsigned long nbits, unsigned long start);
extern unsigned long _find_first_bit(const unsigned long *addr,
				     unsigned long size);
unsigned long __find_nth_bit(const unsigned long *addr, unsigned long size,
			     unsigned long n);
unsigned long __find_nth_and_bit(const unsigned long *addr1,
				 const unsigned long *addr2, unsigned long size,
				 unsigned long n);
unsigned long __find_nth_andnot_bit(const unsigned long *addr1,
				    const unsigned long *addr2,
				    unsigned long size, unsigned long n);
unsigned long __find_nth_and_andnot_bit(const unsigned long *addr1,
					const unsigned long *addr2,
					const unsigned long *addr3,
					unsigned long size, unsigned long n);
extern unsigned long _find_first_and_bit(const unsigned long *addr1,
					 const unsigned long *addr2,
					 unsigned long size);
unsigned long _find_first_and_and_bit(const unsigned long *addr1,
				      const unsigned long *addr2,
				      const unsigned long *addr3,
				      unsigned long size);
extern unsigned long _find_first_zero_bit(const unsigned long *addr,
					  unsigned long size);
extern unsigned long _find_last_bit(const unsigned long *addr,
				    unsigned long size);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_next_bit(const unsigned long *addr, unsigned long size,
	      unsigned long offset)
{
	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val;

		if (__builtin_expect(!!(offset >= size), 0))
			return size;

		val = *addr &
		      ((((int)(sizeof(struct {
			       int : (-!!(__builtin_choose_expr(
					     (sizeof(int) ==
					      sizeof(*(
						      8 ? ((void *)((long)((offset) >
									   (size -
									    1)) *
								    0l)) :
							  (int *)8))),
					     (offset) > (size - 1), 0)));
		       })))) +
		       (((~((0UL))) - (((1UL)) << (offset)) + 1) &
			(~((0UL)) >> (64 - 1 - (size - 1)))));
		return val ? (__builtin_constant_p(val) ?
				      (unsigned long)__builtin_ctzl(val) :
				      variable__ffs(val)) :
			     size;
	}

	return _find_next_bit(addr, size, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_next_and_bit(const unsigned long *addr1, const unsigned long *addr2,
		  unsigned long size, unsigned long offset)
{
	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val;

		if (__builtin_expect(!!(offset >= size), 0))
			return size;

		val = *addr1 & *addr2 &
		      ((((int)(sizeof(struct {
			       int : (-!!(__builtin_choose_expr(
					     (sizeof(int) ==
					      sizeof(*(
						      8 ? ((void *)((long)((offset) >
									   (size -
									    1)) *
								    0l)) :
							  (int *)8))),
					     (offset) > (size - 1), 0)));
		       })))) +
		       (((~((0UL))) - (((1UL)) << (offset)) + 1) &
			(~((0UL)) >> (64 - 1 - (size - 1)))));
		return val ? (__builtin_constant_p(val) ?
				      (unsigned long)__builtin_ctzl(val) :
				      variable__ffs(val)) :
			     size;
	}

	return _find_next_and_bit(addr1, addr2, size, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_next_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,
		     unsigned long size, unsigned long offset)
{
	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val;

		if (__builtin_expect(!!(offset >= size), 0))
			return size;

		val = *addr1 & ~*addr2 &
		      ((((int)(sizeof(struct {
			       int : (-!!(__builtin_choose_expr(
					     (sizeof(int) ==
					      sizeof(*(
						      8 ? ((void *)((long)((offset) >
									   (size -
									    1)) *
								    0l)) :
							  (int *)8))),
					     (offset) > (size - 1), 0)));
		       })))) +
		       (((~((0UL))) - (((1UL)) << (offset)) + 1) &
			(~((0UL)) >> (64 - 1 - (size - 1)))));
		return val ? (__builtin_constant_p(val) ?
				      (unsigned long)__builtin_ctzl(val) :
				      variable__ffs(val)) :
			     size;
	}

	return _find_next_andnot_bit(addr1, addr2, size, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_next_or_bit(const unsigned long *addr1, const unsigned long *addr2,
		 unsigned long size, unsigned long offset)
{
	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val;

		if (__builtin_expect(!!(offset >= size), 0))
			return size;

		val = (*addr1 | *addr2) &
		      ((((int)(sizeof(struct {
			       int : (-!!(__builtin_choose_expr(
					     (sizeof(int) ==
					      sizeof(*(
						      8 ? ((void *)((long)((offset) >
									   (size -
									    1)) *
								    0l)) :
							  (int *)8))),
					     (offset) > (size - 1), 0)));
		       })))) +
		       (((~((0UL))) - (((1UL)) << (offset)) + 1) &
			(~((0UL)) >> (64 - 1 - (size - 1)))));
		return val ? (__builtin_constant_p(val) ?
				      (unsigned long)__builtin_ctzl(val) :
				      variable__ffs(val)) :
			     size;
	}

	return _find_next_or_bit(addr1, addr2, size, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_next_zero_bit(const unsigned long *addr, unsigned long size,
		   unsigned long offset)
{
	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val;

		if (__builtin_expect(!!(offset >= size), 0))
			return size;

		val = *addr |
		      ~((((int)(sizeof(struct {
				int : (-!!(__builtin_choose_expr(
					      (sizeof(int) ==
					       sizeof(*(
						       8 ? ((void *)((long)((offset) >
									    (size -
									     1)) *
								     0l)) :
							   (int *)8))),
					      (offset) > (size - 1), 0)));
			})))) +
			(((~((0UL))) - (((1UL)) << (offset)) + 1) &
			 (~((0UL)) >> (64 - 1 - (size - 1)))));
		return val == ~0UL ?
			       size :
			       (__builtin_constant_p(val) ?
					(unsigned long)__builtin_ctzl(~val) :
					variable_ffz(val));
	}

	return _find_next_zero_bit(addr, size, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_first_bit(const unsigned long *addr, unsigned long size)
{
	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val =
			*addr &
			((((int)(sizeof(struct {
				 int : (-!!(__builtin_choose_expr(
					       (sizeof(int) ==
						sizeof(*(
							8 ? ((void *)((long)((0) >
									     (size -
									      1)) *
								      0l)) :
							    (int *)8))),
					       (0) > (size - 1), 0)));
			 })))) +
			 (((~((0UL))) - (((1UL)) << (0)) + 1) &
			  (~((0UL)) >> (64 - 1 - (size - 1)))));

		return val ? (__builtin_constant_p(val) ?
				      (unsigned long)__builtin_ctzl(val) :
				      variable__ffs(val)) :
			     size;
	}

	return _find_first_bit(addr, size);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_nth_bit(const unsigned long *addr, unsigned long size, unsigned long n)
{
	if (n >= size)
		return size;

	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val =
			*addr &
			((((int)(sizeof(struct {
				 int : (-!!(__builtin_choose_expr(
					       (sizeof(int) ==
						sizeof(*(
							8 ? ((void *)((long)((0) >
									     (size -
									      1)) *
								      0l)) :
							    (int *)8))),
					       (0) > (size - 1), 0)));
			 })))) +
			 (((~((0UL))) - (((1UL)) << (0)) + 1) &
			  (~((0UL)) >> (64 - 1 - (size - 1)))));

		return val ? fns(val, n) : size;
	}

	return __find_nth_bit(addr, size, n);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_nth_and_bit(const unsigned long *addr1, const unsigned long *addr2,
		 unsigned long size, unsigned long n)
{
	if (n >= size)
		return size;

	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val =
			*addr1 & *addr2 &
			((((int)(sizeof(struct {
				 int : (-!!(__builtin_choose_expr(
					       (sizeof(int) ==
						sizeof(*(
							8 ? ((void *)((long)((0) >
									     (size -
									      1)) *
								      0l)) :
							    (int *)8))),
					       (0) > (size - 1), 0)));
			 })))) +
			 (((~((0UL))) - (((1UL)) << (0)) + 1) &
			  (~((0UL)) >> (64 - 1 - (size - 1)))));

		return val ? fns(val, n) : size;
	}

	return __find_nth_and_bit(addr1, addr2, size, n);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_nth_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,
		    unsigned long size, unsigned long n)
{
	if (n >= size)
		return size;

	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val =
			*addr1 & (~*addr2) &
			((((int)(sizeof(struct {
				 int : (-!!(__builtin_choose_expr(
					       (sizeof(int) ==
						sizeof(*(
							8 ? ((void *)((long)((0) >
									     (size -
									      1)) *
								      0l)) :
							    (int *)8))),
					       (0) > (size - 1), 0)));
			 })))) +
			 (((~((0UL))) - (((1UL)) << (0)) + 1) &
			  (~((0UL)) >> (64 - 1 - (size - 1)))));

		return val ? fns(val, n) : size;
	}

	return __find_nth_andnot_bit(addr1, addr2, size, n);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_nth_and_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,
			const unsigned long *addr3, unsigned long size,
			unsigned long n)
{
	if (n >= size)
		return size;

	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val =
			*addr1 & *addr2 & (~*addr3) &
			((((int)(sizeof(struct {
				 int : (-!!(__builtin_choose_expr(
					       (sizeof(int) ==
						sizeof(*(
							8 ? ((void *)((long)((0) >
									     (size -
									      1)) *
								      0l)) :
							    (int *)8))),
					       (0) > (size - 1), 0)));
			 })))) +
			 (((~((0UL))) - (((1UL)) << (0)) + 1) &
			  (~((0UL)) >> (64 - 1 - (size - 1)))));

		return val ? fns(val, n) : size;
	}

	return __find_nth_and_andnot_bit(addr1, addr2, addr3, size, n);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_first_and_bit(const unsigned long *addr1, const unsigned long *addr2,
		   unsigned long size)
{
	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val =
			*addr1 & *addr2 &
			((((int)(sizeof(struct {
				 int : (-!!(__builtin_choose_expr(
					       (sizeof(int) ==
						sizeof(*(
							8 ? ((void *)((long)((0) >
									     (size -
									      1)) *
								      0l)) :
							    (int *)8))),
					       (0) > (size - 1), 0)));
			 })))) +
			 (((~((0UL))) - (((1UL)) << (0)) + 1) &
			  (~((0UL)) >> (64 - 1 - (size - 1)))));

		return val ? (__builtin_constant_p(val) ?
				      (unsigned long)__builtin_ctzl(val) :
				      variable__ffs(val)) :
			     size;
	}

	return _find_first_and_bit(addr1, addr2, size);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_first_and_and_bit(const unsigned long *addr1, const unsigned long *addr2,
		       const unsigned long *addr3, unsigned long size)
{
	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val =
			*addr1 & *addr2 & *addr3 &
			((((int)(sizeof(struct {
				 int : (-!!(__builtin_choose_expr(
					       (sizeof(int) ==
						sizeof(*(
							8 ? ((void *)((long)((0) >
									     (size -
									      1)) *
								      0l)) :
							    (int *)8))),
					       (0) > (size - 1), 0)));
			 })))) +
			 (((~((0UL))) - (((1UL)) << (0)) + 1) &
			  (~((0UL)) >> (64 - 1 - (size - 1)))));

		return val ? (__builtin_constant_p(val) ?
				      (unsigned long)__builtin_ctzl(val) :
				      variable__ffs(val)) :
			     size;
	}

	return _find_first_and_and_bit(addr1, addr2, addr3, size);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_first_zero_bit(const unsigned long *addr, unsigned long size)
{
	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val =
			*addr |
			~((((int)(sizeof(struct {
				  int : (-!!(__builtin_choose_expr(
						(sizeof(int) ==
						 sizeof(*(
							 8 ? ((void *)((long)((0) >
									      (size -
									       1)) *
								       0l)) :
							     (int *)8))),
						(0) > (size - 1), 0)));
			  })))) +
			  (((~((0UL))) - (((1UL)) << (0)) + 1) &
			   (~((0UL)) >> (64 - 1 - (size - 1)))));

		return val == ~0UL ?
			       size :
			       (__builtin_constant_p(val) ?
					(unsigned long)__builtin_ctzl(~val) :
					variable_ffz(val));
	}

	return _find_first_zero_bit(addr, size);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_last_bit(const unsigned long *addr, unsigned long size)
{
	if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
		unsigned long val =
			*addr &
			((((int)(sizeof(struct {
				 int : (-!!(__builtin_choose_expr(
					       (sizeof(int) ==
						sizeof(*(
							8 ? ((void *)((long)((0) >
									     (size -
									      1)) *
								      0l)) :
							    (int *)8))),
					       (0) > (size - 1), 0)));
			 })))) +
			 (((~((0UL))) - (((1UL)) << (0)) + 1) &
			  (~((0UL)) >> (64 - 1 - (size - 1)))));

		return val ? __fls(val) : size;
	}

	return _find_last_bit(addr, size);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_next_and_bit_wrap(const unsigned long *addr1, const unsigned long *addr2,
		       unsigned long size, unsigned long offset)
{
	unsigned long bit = find_next_and_bit(addr1, addr2, size, offset);

	if (bit < size || offset == 0)
		return bit;

	bit = find_first_and_bit(addr1, addr2, offset);
	return bit < offset ? bit : size;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_next_bit_wrap(const unsigned long *addr, unsigned long size,
		   unsigned long offset)
{
	unsigned long bit = find_next_bit(addr, size, offset);

	if (bit < size || offset == 0)
		return bit;

	bit = find_first_bit(addr, offset);
	return bit < offset ? bit : size;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
__for_each_wrap(const unsigned long *bitmap, unsigned long size,
		unsigned long start, unsigned long n)
{
	unsigned long bit;

	if (n > start) {
		bit = find_next_bit(bitmap, size, n);
		if (bit < size)
			return bit;

		n = 0;
	}

	bit = find_next_bit(bitmap, start, n);
	return bit < start ? bit : size;
}
extern unsigned long find_next_clump8(unsigned long *clump,
				      const unsigned long *addr,
				      unsigned long size, unsigned long offset);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_next_zero_bit_le(const void *addr, unsigned long size,
		      unsigned long offset)
{
	return find_next_zero_bit(addr, size, offset);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_next_bit_le(const void *addr, unsigned long size, unsigned long offset)
{
	return find_next_bit(addr, size, offset);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
find_first_zero_bit_le(const void *addr, unsigned long size)
{
	return find_first_zero_bit(addr, size);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
	__attribute__((__warn_unused_result__))
	__must_check_overflow(bool overflow)
{
	return __builtin_expect(!!(overflow), 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
	__attribute__((__warn_unused_result__))
	size_mul(size_t factor1, size_t factor2)
{
	size_t bytes;

	if (__must_check_overflow(
		    __builtin_mul_overflow(factor1, factor2, &bytes)))
		return (~(size_t)0);

	return bytes;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
	__attribute__((__warn_unused_result__))
	size_add(size_t addend1, size_t addend2)
{
	size_t bytes;

	if (__must_check_overflow(
		    __builtin_add_overflow(addend1, addend2, &bytes)))
		return (~(size_t)0);

	return bytes;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
	__attribute__((__warn_unused_result__))
	size_sub(size_t minuend, size_t subtrahend)
{
	size_t bytes;

	if (minuend == (~(size_t)0) || subtrahend == (~(size_t)0) ||
	    __must_check_overflow(
		    __builtin_sub_overflow(minuend, subtrahend, &bytes)))
		return (~(size_t)0);

	return bytes;
}

extern char *strndup_user(const char *, long);
extern void *memdup_user(const void *, size_t)
	__attribute__((__alloc_size__(2)));
extern void *vmemdup_user(const void *, size_t)
	__attribute__((__alloc_size__(2)));
extern void *memdup_user_nul(const void *, size_t);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__alloc_size__(2, 3))) void *
memdup_array_user(const void *src, size_t n, size_t size)
{
	size_t nbytes;

	if (__must_check_overflow(__builtin_mul_overflow(n, size, &nbytes)))
		return ERR_PTR(-75);

	return memdup_user(src, nbytes);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__alloc_size__(2, 3))) void *
vmemdup_array_user(const void *src, size_t n, size_t size)
{
	size_t nbytes;

	if (__must_check_overflow(__builtin_mul_overflow(n, size, &nbytes)))
		return ERR_PTR(-75);

	return vmemdup_user(src, nbytes);
}

extern char *strcpy(char *, const char *);

extern char *strncpy(char *, const char *, __kernel_size_t);

ssize_t sized_strscpy(char *, const char *, size_t);
extern char *strcat(char *, const char *);

extern char *strncat(char *, const char *, __kernel_size_t);

extern size_t strlcat(char *, const char *, __kernel_size_t);

extern int strcmp(const char *, const char *);

extern int strncmp(const char *, const char *, __kernel_size_t);

extern int strcasecmp(const char *s1, const char *s2);

extern int strncasecmp(const char *s1, const char *s2, size_t n);

extern char *strchr(const char *, int);

extern char *strchrnul(const char *, int);

extern char *strnchrnul(const char *, size_t, int);

extern char *strnchr(const char *, size_t, int);

extern char *strrchr(const char *, int);

extern char *__attribute__((__warn_unused_result__)) skip_spaces(const char *);

extern char *strim(char *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) char *
strstrip(char *str)
{
	return strim(str);
}

extern char *strstr(const char *, const char *);

extern char *strnstr(const char *, const char *, size_t);

extern __kernel_size_t strlen(const char *);

extern __kernel_size_t strnlen(const char *, __kernel_size_t);

extern char *strpbrk(const char *, const char *);

extern char *strsep(char **, const char *);

extern __kernel_size_t strspn(const char *, const char *);

extern __kernel_size_t strcspn(const char *, const char *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
memset_l(unsigned long *p, unsigned long v, __kernel_size_t n)
{
	if (64 == 32)
		return memset32((uint32_t *)p, v, n);
	else
		return memset64((uint64_t *)p, v, n);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
memset_p(void **p, void *v, __kernel_size_t n)
{
	if (64 == 32)
		return memset32((uint32_t *)p, (uintptr_t)v, n);
	else
		return memset64((uint64_t *)p, (uintptr_t)v, n);
}

extern void **__memcat_p(void **a, void **b);
extern void *memscan(void *, int, __kernel_size_t);

extern int memcmp(const void *, const void *, __kernel_size_t);

extern int bcmp(const void *, const void *, __kernel_size_t);

extern void *memchr(const void *, int, __kernel_size_t);
void *memchr_inv(const void *s, int c, size_t n);
char *strreplace(char *str, char old, char new);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mem_is_zero(const void *s, size_t n)
{
	return !memchr_inv(s, 0, n);
}

extern void kfree_const(const void *x);

extern char *kstrdup(const char *s, gfp_t gfp) __attribute__((__malloc__));
extern const char *kstrdup_const(const char *s, gfp_t gfp);
extern char *kstrndup(const char *s, size_t len, gfp_t gfp);
extern void *kmemdup_noprof(const void *src, size_t len, gfp_t gfp)
	__attribute__((__alloc_size__(2)));

extern void *kvmemdup(const void *src, size_t len, gfp_t gfp)
	__attribute__((__alloc_size__(2)));
extern char *kmemdup_nul(const char *s, size_t len, gfp_t gfp);
extern void *kmemdup_array(const void *src, size_t count, size_t element_size,
			   gfp_t gfp) __attribute__((__alloc_size__(2, 3)));

extern char **argv_split(gfp_t gfp, const char *str, int *argcp);
extern void argv_free(char **argv);

extern int get_option(char **str, int *pint);
extern char *get_options(const char *str, int nints, int *ints);
extern unsigned long long memparse(const char *ptr, char **retptr);
extern bool parse_option_str(const char *str, const char *option);
extern char *next_arg(char *args, char **param, char **val);

extern bool sysfs_streq(const char *s1, const char *s2);
int match_string(const char *const *array, size_t n, const char *string);
int __sysfs_match_string(const char *const *array, size_t n, const char *s);
int vbin_printf(u32 *bin_buf, size_t size, const char *fmt, va_list args);
int bstr_printf(char *buf, size_t size, const char *fmt, const u32 *bin_buf);
int bprintf(u32 *bin_buf, size_t size, const char *fmt, ...)
	__attribute__((__format__(printf, 3, 4)));

extern ssize_t memory_read_from_buffer(void *to, size_t count, loff_t *ppos,
				       const void *from, size_t available);

int ptr_to_hashval(const void *ptr, unsigned long *hashval_out);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
strstarts(const char *str, const char *prefix)
{
	return strncmp(str, prefix, strlen(prefix)) == 0;
}

size_t memweight(const void *ptr, size_t bytes);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
memzero_explicit(void *s, size_t count)
{
	memset(s, 0, count);
	__asm__ __volatile__("" : : "r"(s) : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const char *
kbasename(const char *path)
{
	const char *tail = strrchr(path, '/');
	return tail ? tail + 1 : path;
}

extern void __attribute__((__error__("value doesn't fit into mask")))
__field_overflow(void);
extern void __attribute__((__error__("bad bitfield mask"))) __bad_mask(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
field_multiplier(u64 field)
{
	if ((field | (field - 1)) & ((field | (field - 1)) + 1))
		__bad_mask();
	return field & -field;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
field_mask(u64 field)
{
	return field / field_multiplier(field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) __u8
u8_encode_bits(u8 v, u8 field)
{
	if (__builtin_constant_p(v) && (v & ~field_mask(field)))
		__field_overflow();
	return ((v & field_mask(field)) * field_multiplier(field));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) __u8
u8_replace_bits(__u8 old, u8 val, u8 field)
{
	return (old & ~(field)) | u8_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
u8p_replace_bits(__u8 *p, u8 val, u8 field)
{
	*p = (*p & ~(field)) | u8_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u8
u8_get_bits(__u8 v, u8 field)
{
	return ((v)&field) / field_multiplier(field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __le16
le16_encode_bits(u16 v, u16 field)
{
	if (__builtin_constant_p(v) && (v & ~field_mask(field)))
		__field_overflow();
	return ((__le16)(__u16)((v & field_mask(field)) *
				field_multiplier(field)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __le16
le16_replace_bits(__le16 old, u16 val, u16 field)
{
	return (old & ~((__le16)(__u16)(field))) | le16_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
le16p_replace_bits(__le16 *p, u16 val, u16 field)
{
	*p = (*p & ~((__le16)(__u16)(field))) | le16_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u16
le16_get_bits(__le16 v, u16 field)
{
	return (((__u16)(__le16)(v)) & field) / field_multiplier(field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __be16
be16_encode_bits(u16 v, u16 field)
{
	if (__builtin_constant_p(v) && (v & ~field_mask(field)))
		__field_overflow();
	return ((__be16)(__u16)__builtin_bswap16(
		(__u16)(((v & field_mask(field)) * field_multiplier(field)))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __be16
be16_replace_bits(__be16 old, u16 val, u16 field)
{
	return (old & ~((__be16)(__u16)__builtin_bswap16((__u16)((field))))) |
	       be16_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
be16p_replace_bits(__be16 *p, u16 val, u16 field)
{
	*p = (*p & ~((__be16)(__u16)__builtin_bswap16((__u16)((field))))) |
	     be16_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u16
be16_get_bits(__be16 v, u16 field)
{
	return ((__u16)__builtin_bswap16((__u16)((__u16)(__be16)(v))) & field) /
	       field_multiplier(field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) __u16
u16_encode_bits(u16 v, u16 field)
{
	if (__builtin_constant_p(v) && (v & ~field_mask(field)))
		__field_overflow();
	return ((v & field_mask(field)) * field_multiplier(field));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) __u16
u16_replace_bits(__u16 old, u16 val, u16 field)
{
	return (old & ~(field)) | u16_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
u16p_replace_bits(__u16 *p, u16 val, u16 field)
{
	*p = (*p & ~(field)) | u16_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u16
u16_get_bits(__u16 v, u16 field)
{
	return ((v)&field) / field_multiplier(field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __le32
le32_encode_bits(u32 v, u32 field)
{
	if (__builtin_constant_p(v) && (v & ~field_mask(field)))
		__field_overflow();
	return ((__le32)(__u32)((v & field_mask(field)) *
				field_multiplier(field)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __le32
le32_replace_bits(__le32 old, u32 val, u32 field)
{
	return (old & ~((__le32)(__u32)(field))) | le32_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
le32p_replace_bits(__le32 *p, u32 val, u32 field)
{
	*p = (*p & ~((__le32)(__u32)(field))) | le32_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u32
le32_get_bits(__le32 v, u32 field)
{
	return (((__u32)(__le32)(v)) & field) / field_multiplier(field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __be32
be32_encode_bits(u32 v, u32 field)
{
	if (__builtin_constant_p(v) && (v & ~field_mask(field)))
		__field_overflow();
	return ((__be32)(__u32)__builtin_bswap32(
		(__u32)(((v & field_mask(field)) * field_multiplier(field)))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __be32
be32_replace_bits(__be32 old, u32 val, u32 field)
{
	return (old & ~((__be32)(__u32)__builtin_bswap32((__u32)((field))))) |
	       be32_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
be32p_replace_bits(__be32 *p, u32 val, u32 field)
{
	*p = (*p & ~((__be32)(__u32)__builtin_bswap32((__u32)((field))))) |
	     be32_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u32
be32_get_bits(__be32 v, u32 field)
{
	return ((__u32)__builtin_bswap32((__u32)((__u32)(__be32)(v))) & field) /
	       field_multiplier(field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) __u32
u32_encode_bits(u32 v, u32 field)
{
	if (__builtin_constant_p(v) && (v & ~field_mask(field)))
		__field_overflow();
	return ((v & field_mask(field)) * field_multiplier(field));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) __u32
u32_replace_bits(__u32 old, u32 val, u32 field)
{
	return (old & ~(field)) | u32_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
u32p_replace_bits(__u32 *p, u32 val, u32 field)
{
	*p = (*p & ~(field)) | u32_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u32
u32_get_bits(__u32 v, u32 field)
{
	return ((v)&field) / field_multiplier(field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __le64
le64_encode_bits(u64 v, u64 field)
{
	if (__builtin_constant_p(v) && (v & ~field_mask(field)))
		__field_overflow();
	return ((__le64)(__u64)((v & field_mask(field)) *
				field_multiplier(field)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __le64
le64_replace_bits(__le64 old, u64 val, u64 field)
{
	return (old & ~((__le64)(__u64)(field))) | le64_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
le64p_replace_bits(__le64 *p, u64 val, u64 field)
{
	*p = (*p & ~((__le64)(__u64)(field))) | le64_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
le64_get_bits(__le64 v, u64 field)
{
	return (((__u64)(__le64)(v)) & field) / field_multiplier(field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __be64
be64_encode_bits(u64 v, u64 field)
{
	if (__builtin_constant_p(v) && (v & ~field_mask(field)))
		__field_overflow();
	return ((__be64)(__u64)__builtin_bswap64(
		(__u64)(((v & field_mask(field)) * field_multiplier(field)))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) __be64
be64_replace_bits(__be64 old, u64 val, u64 field)
{
	return (old & ~((__be64)(__u64)__builtin_bswap64((__u64)((field))))) |
	       be64_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
be64p_replace_bits(__be64 *p, u64 val, u64 field)
{
	*p = (*p & ~((__be64)(__u64)__builtin_bswap64((__u64)((field))))) |
	     be64_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
be64_get_bits(__be64 v, u64 field)
{
	return ((__u64)__builtin_bswap64((__u64)((__u64)(__be64)(v))) & field) /
	       field_multiplier(field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) __u64
u64_encode_bits(u64 v, u64 field)
{
	if (__builtin_constant_p(v) && (v & ~field_mask(field)))
		__field_overflow();
	return ((v & field_mask(field)) * field_multiplier(field));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) __u64
u64_replace_bits(__u64 old, u64 val, u64 field)
{
	return (old & ~(field)) | u64_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
u64p_replace_bits(__u64 *p, u64 val, u64 field)
{
	*p = (*p & ~(field)) | u64_encode_bits(val, field);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
u64_get_bits(__u64 v, u64 field)
{
	return ((v)&field) / field_multiplier(field);
}
enum fortify_func {
	FORTIFY_FUNC_strncpy,
	FORTIFY_FUNC_strnlen,
	FORTIFY_FUNC_strlen,
	FORTIFY_FUNC_strscpy,
	FORTIFY_FUNC_strlcat,
	FORTIFY_FUNC_strcat,
	FORTIFY_FUNC_strncat,
	FORTIFY_FUNC_memset,
	FORTIFY_FUNC_memcpy,
	FORTIFY_FUNC_memmove,
	FORTIFY_FUNC_memscan,
	FORTIFY_FUNC_memcmp,
	FORTIFY_FUNC_memchr,
	FORTIFY_FUNC_memchr_inv,
	FORTIFY_FUNC_kmemdup,
	FORTIFY_FUNC_strcpy,
	FORTIFY_FUNC_UNKNOWN,
};

void __fortify_report(const u8 reason, const size_t avail, const size_t size);
void __fortify_panic(const u8 reason, const size_t avail, const size_t size)
	__attribute__((__cold__)) __attribute__((__noreturn__));
void __read_overflow(void) __attribute__((
	__error__("detected read beyond size of object (1st parameter)")));
void __read_overflow2(void) __attribute__((
	__error__("detected read beyond size of object (2nd parameter)")));
void __read_overflow2_field(size_t avail, size_t wanted) __attribute__((__warning__(
	"detected read beyond size of field (2nd parameter); maybe use struct_group()?")));
void __write_overflow(void) __attribute__((
	__error__("detected write beyond size of object (1st parameter)")));
void __write_overflow_field(size_t avail, size_t wanted) __attribute__((__warning__(
	"detected write beyond size of field (1st parameter); maybe use struct_group()?")));
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__))
__attribute__((__diagnose_as_builtin__(__builtin_strncpy, 1, 2, 3))) char *
strncpy(char *const __attribute__((__pass_dynamic_object_size__(1))) p,
	const char *q, __kernel_size_t size)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 1);

	if ((__builtin_constant_p((p_size) < (size)) && (p_size) < (size)))
		__write_overflow();
	if (p_size < size)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_46(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_46();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_47(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_47();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_48(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 1) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (1)) :
								 0)))
							 __compiletime_assert_48();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_49(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_49();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_50(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_50();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(1)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_51(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_51();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_52(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_52();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_53(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_strncpy) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_strncpy)) :
								 0)))
							 __compiletime_assert_53();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_54(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_54();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_55(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_55();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_strncpy)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, size);
	return __builtin_strncpy(p, q, size);
}

extern __kernel_size_t __real_strnlen(const char *,
				      __kernel_size_t) __asm__("strnlen");
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__))
__attribute__((__overloadable__)) __kernel_size_t
strnlen(const char *const __attribute__((__pass_dynamic_object_size__(1))) p,
	__kernel_size_t maxlen)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 1);
	const size_t p_len = ({
		char *__p = (char *)(p);
		size_t __ret = (~(size_t)0);
		const size_t __p_size = __builtin_dynamic_object_size(p, 1);
		if (__p_size != (~(size_t)0) && __builtin_constant_p(*__p)) {
			size_t __p_len = __p_size - 1;
			if (__builtin_constant_p(__p[__p_len]) &&
			    __p[__p_len] == '\0')
				__ret = __builtin_strlen(__p);
		}
		__ret;
	});
	size_t ret;

	if (__builtin_constant_p(maxlen) && p_len != (~(size_t)0)) {
		if (maxlen >= p_size)
			return p_len;
	}

	ret = __real_strnlen(p, maxlen < p_size ? maxlen : p_size);
	if (p_size <= ret && maxlen != ret)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_56(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_56();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_57(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_57();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_58(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 0) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (0)) :
								 0)))
							 __compiletime_assert_58();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_59(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_59();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_60(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_60();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(0)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_61(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_61();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_62(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_62();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_63(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_strnlen) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_strnlen)) :
								 0)))
							 __compiletime_assert_63();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_64(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_64();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_65(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_65();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_strnlen)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, ret + 1);
	return ret;
}
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__))
__attribute__((__diagnose_as_builtin__(__builtin_strlen, 1))) __kernel_size_t
__fortify_strlen(const char *const
		 __attribute__((__pass_dynamic_object_size__(1))) p)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 1);
	__kernel_size_t ret;

	if (p_size == (~(size_t)0))
		return __builtin_strlen(p);
	ret = strnlen(p, p_size);
	if (p_size <= ret)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_66(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_66();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_67(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_67();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_68(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 0) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (0)) :
								 0)))
							 __compiletime_assert_68();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_69(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_69();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_70(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_70();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(0)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_71(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_71();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_72(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_72();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_73(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_strlen) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_strlen)) :
								 0)))
							 __compiletime_assert_73();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_74(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_74();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_75(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_75();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_strlen)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, ret + 1);
	return ret;
}

extern ssize_t __real_strscpy(char *, const char *,
			      size_t) __asm__("sized_strscpy");
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__)) ssize_t
sized_strscpy(char *const __attribute__((__pass_dynamic_object_size__(1))) p,
	      const char *const
	      __attribute__((__pass_dynamic_object_size__(1))) q,
	      size_t size)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 1);
	const size_t q_size = __builtin_dynamic_object_size(q, 1);
	size_t len;

	if (p_size == (~(size_t)0) && q_size == (~(size_t)0))
		return __real_strscpy(p, q, size);

	if ((__builtin_constant_p((p_size) < (size)) && (p_size) < (size)))
		__write_overflow();

	if ((__builtin_constant_p((p_size) < ((~(size_t)0))) &&
	     (p_size) < ((~(size_t)0)))) {
		len = ({
			char *__p = (char *)(q);
			size_t __ret = (~(size_t)0);
			const size_t __p_size =
				__builtin_dynamic_object_size(q, 1);
			if (__p_size != (~(size_t)0) &&
			    __builtin_constant_p(*__p)) {
				size_t __p_len = __p_size - 1;
				if (__builtin_constant_p(__p[__p_len]) &&
				    __p[__p_len] == '\0')
					__ret = __builtin_strlen(__p);
			}
			__ret;
		});

		if (len < (~(size_t)0) &&
		    (__builtin_constant_p((len) < (size)) && (len) < (size))) {
			__builtin_memcpy(p, q, len + 1);
			return len;
		}
	}

	len = strnlen(q, size);

	len = len == size ? size : len + 1;

	if (p_size < len)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_76(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_76();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_77(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_77();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_78(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 1) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (1)) :
								 0)))
							 __compiletime_assert_78();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_79(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_79();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_80(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_80();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(1)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_81(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_81();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_82(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_82();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_83(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_strscpy) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_strscpy)) :
								 0)))
							 __compiletime_assert_83();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_84(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_84();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_85(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_85();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_strscpy)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, len);

	return __real_strscpy(p, q, len);
}

extern size_t __real_strlcat(char *p, const char *q,
			     size_t avail) __asm__("strlcat");
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__)) size_t
strlcat(char *const __attribute__((__pass_dynamic_object_size__(1))) p,
	const char *const __attribute__((__pass_dynamic_object_size__(1))) q,
	size_t avail)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 1);
	const size_t q_size = __builtin_dynamic_object_size(q, 1);
	size_t p_len, copy_len;
	size_t actual, wanted;

	if (p_size == (~(size_t)0) && q_size == (~(size_t)0))
		return __real_strlcat(p, q, avail);

	p_len = strnlen(p, avail);
	copy_len = __builtin_choose_expr(
		(sizeof(int) ==
		 sizeof(*(8 ? ((void *)((long)(__builtin_strlen(q)) * 0l)) :
			      (int *)8))),
		__builtin_strlen(q), __fortify_strlen(q));
	wanted = actual = p_len + copy_len;

	if (avail <= p_len)
		return wanted;

	if (p_size <= p_len)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_86(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_86();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_87(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_87();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_88(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 0) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (0)) :
								 0)))
							 __compiletime_assert_88();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_89(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_89();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_90(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_90();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(0)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_91(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_91();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_92(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_92();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_93(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_strlcat) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_strlcat)) :
								 0)))
							 __compiletime_assert_93();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_94(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_94();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_95(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_95();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_strlcat)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, p_len + 1);

	if (actual >= avail) {
		copy_len = avail - p_len - 1;
		actual = p_len + copy_len;
	}

	if (p_size <= actual)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_96(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_96();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_97(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_97();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_98(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 1) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (1)) :
								 0)))
							 __compiletime_assert_98();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_99(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_99();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_100(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_100();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(1)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_101(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_101();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_102(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_102();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_103(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_strlcat) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_strlcat)) :
								 0)))
							 __compiletime_assert_103();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_104(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_104();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_105(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_105();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_strlcat)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, actual + 1);
	__builtin_memcpy(p + p_len, q, copy_len);
	p[actual] = '\0';

	return wanted;
}
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__))
__attribute__((__diagnose_as_builtin__(__builtin_strcat, 1, 2))) char *
strcat(char *const __attribute__((__pass_dynamic_object_size__(1))) p,
       const char *q)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 1);
	const size_t wanted = strlcat(p, q, p_size);

	if (p_size <= wanted)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_106(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_106();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_107(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_107();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_108(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 1) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (1)) :
								 0)))
							 __compiletime_assert_108();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_109(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_109();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_110(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_110();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(1)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_111(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_111();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_112(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_112();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_113(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_strcat) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_strcat)) :
								 0)))
							 __compiletime_assert_113();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_114(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_114();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_115(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_115();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_strcat)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, wanted + 1);
	return p;
}
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__))
__attribute__((__diagnose_as_builtin__(__builtin_strncat, 1, 2, 3))) char *
strncat(char *const __attribute__((__pass_dynamic_object_size__(1))) p,
	const char *const __attribute__((__pass_dynamic_object_size__(1))) q,
	__kernel_size_t count)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 1);
	const size_t q_size = __builtin_dynamic_object_size(q, 1);
	size_t p_len, copy_len, total;

	if (p_size == (~(size_t)0) && q_size == (~(size_t)0))
		return __builtin_strncat(p, q, count);
	p_len = __builtin_choose_expr(
		(sizeof(int) ==
		 sizeof(*(8 ? ((void *)((long)(__builtin_strlen(p)) * 0l)) :
			      (int *)8))),
		__builtin_strlen(p), __fortify_strlen(p));
	copy_len = strnlen(q, count);
	total = p_len + copy_len + 1;
	if (p_size < total)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_116(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_116();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_117(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_117();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_118(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 1) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (1)) :
								 0)))
							 __compiletime_assert_118();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_119(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_119();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_120(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_120();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(1)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_121(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_121();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_122(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_122();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_123(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_strncat) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_strncat)) :
								 0)))
							 __compiletime_assert_123();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_124(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_124();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_125(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_125();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_strncat)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, total);
	__builtin_memcpy(p + p_len, q, copy_len);
	p[p_len + copy_len] = '\0';
	return p;
}

extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__)) bool
fortify_memset_chk(__kernel_size_t size, const size_t p_size,
		   const size_t p_size_field)
{
	if (__builtin_constant_p(size)) {
		if ((__builtin_constant_p((p_size_field) < (p_size)) &&
		     (p_size_field) < (p_size)) &&
		    (__builtin_constant_p((p_size) < (size)) &&
		     (p_size) < (size)))
			__write_overflow();

		if ((__builtin_constant_p((p_size_field) < (size)) &&
		     (p_size_field) < (size)))
			__write_overflow_field(p_size_field, size);
	}
	if (p_size != (~(size_t)0) && p_size < size)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_126(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_126();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_127(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_127();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_128(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 1) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (1)) :
								 0)))
							 __compiletime_assert_128();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_129(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_129();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_130(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_130();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(1)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_131(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_131();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_132(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_132();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_133(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_memset) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_memset)) :
								 0)))
							 __compiletime_assert_133();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_134(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_134();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_135(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_135();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_memset)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, size);
	return false;
}
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__)) bool
fortify_memcpy_chk(__kernel_size_t size, const size_t p_size,
		   const size_t q_size, const size_t p_size_field,
		   const size_t q_size_field, const u8 func)
{
	if (__builtin_constant_p(size)) {
		if ((__builtin_constant_p((p_size_field) < (p_size)) &&
		     (p_size_field) < (p_size)) &&
		    (__builtin_constant_p((p_size) < (size)) &&
		     (p_size) < (size)))
			__write_overflow();
		if ((__builtin_constant_p((q_size_field) < (q_size)) &&
		     (q_size_field) < (q_size)) &&
		    (__builtin_constant_p((q_size) < (size)) &&
		     (q_size) < (size)))
			__read_overflow2();

		if ((__builtin_constant_p((p_size_field) < (size)) &&
		     (p_size_field) < (size)))
			__write_overflow_field(p_size_field, size);

		if ((0 || (__builtin_constant_p((p_size_field) < (size)) &&
			   (p_size_field) < (size))) &&
		    (__builtin_constant_p((q_size_field) < (size)) &&
		     (q_size_field) < (size)))
			__read_overflow2_field(q_size_field, size);
	}
	if (p_size != (~(size_t)0) && p_size < size)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_136(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_136();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_137(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_137();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_138(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 1) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (1)) :
								 0)))
							 __compiletime_assert_138();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_139(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_139();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_140(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_140();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(1)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_141(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_141();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_142(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_142();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_143(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 func) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (func)) :
								 0)))
							 __compiletime_assert_143();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_144(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_144();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_145(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_145();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >> (64 - 1 - (7)))))))(func)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, size);
	else if (q_size != (~(size_t)0) && q_size < size)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_146(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_146();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_147(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_147();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_148(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 0) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (0)) :
								 0)))
							 __compiletime_assert_148();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_149(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_149();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_150(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_150();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(0)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_151(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_151();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_152(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_152();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_153(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 func) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (func)) :
								 0)))
							 __compiletime_assert_153();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_154(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_154();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_155(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_155();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >> (64 - 1 - (7)))))))(func)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, size);
	if (p_size_field != (~(size_t)0) && p_size != p_size_field &&
	    p_size_field < size)
		return true;

	return false;
}
extern void *__real_memscan(void *, int, __kernel_size_t) __asm__("memscan");
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__)) void *
memscan(void *const __attribute__((__pass_dynamic_object_size__(0))) p, int c,
	__kernel_size_t size)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 0);

	if ((__builtin_constant_p((p_size) < (size)) && (p_size) < (size)))
		__read_overflow();
	if (p_size < size)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_156(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_156();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_157(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_157();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_158(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 0) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (0)) :
								 0)))
							 __compiletime_assert_158();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_159(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_159();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_160(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_160();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(0)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_161(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_161();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_162(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_162();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_163(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_memscan) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_memscan)) :
								 0)))
							 __compiletime_assert_163();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_164(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_164();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_165(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_165();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_memscan)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, size);
	return __real_memscan(p, c, size);
}

extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__))
__attribute__((__diagnose_as_builtin__(__builtin_memcmp, 1, 2, 3))) int
memcmp(const void *const __attribute__((__pass_dynamic_object_size__(0))) p,
       const void *const __attribute__((__pass_dynamic_object_size__(0))) q,
       __kernel_size_t size)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 0);
	const size_t q_size = __builtin_dynamic_object_size(q, 0);

	if (__builtin_constant_p(size)) {
		if ((__builtin_constant_p((p_size) < (size)) &&
		     (p_size) < (size)))
			__read_overflow();
		if ((__builtin_constant_p((q_size) < (size)) &&
		     (q_size) < (size)))
			__read_overflow2();
	}
	if (p_size < size)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_166(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_166();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_167(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_167();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_168(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 0) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (0)) :
								 0)))
							 __compiletime_assert_168();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_169(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_169();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_170(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_170();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(0)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_171(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_171();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_172(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_172();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_173(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_memcmp) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_memcmp)) :
								 0)))
							 __compiletime_assert_173();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_174(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_174();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_175(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_175();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_memcmp)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, size);
	else if (q_size < size)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_176(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_176();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_177(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_177();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_178(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 0) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (0)) :
								 0)))
							 __compiletime_assert_178();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_179(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_179();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_180(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_180();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(0)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_181(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_181();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_182(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_182();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_183(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_memcmp) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_memcmp)) :
								 0)))
							 __compiletime_assert_183();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_184(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_184();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_185(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_185();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_memcmp)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			q_size, size);
	return __builtin_memcmp(p, q, size);
}

extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__))
__attribute__((__diagnose_as_builtin__(__builtin_memchr, 1, 2, 3))) void *
memchr(const void *const __attribute__((__pass_dynamic_object_size__(0))) p,
       int c, __kernel_size_t size)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 0);

	if ((__builtin_constant_p((p_size) < (size)) && (p_size) < (size)))
		__read_overflow();
	if (p_size < size)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_186(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_186();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_187(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_187();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_188(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 0) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (0)) :
								 0)))
							 __compiletime_assert_188();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_189(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_189();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_190(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_190();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(0)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_191(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_191();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_192(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_192();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_193(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_memchr) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_memchr)) :
								 0)))
							 __compiletime_assert_193();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_194(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_194();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_195(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_195();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_memchr)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, size);
	return __builtin_memchr(p, c, size);
}

void *__real_memchr_inv(const void *s, int c, size_t n) __asm__("memchr_inv");
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__)) void *
memchr_inv(const void *const __attribute__((__pass_dynamic_object_size__(0))) p,
	   int c, size_t size)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 0);

	if ((__builtin_constant_p((p_size) < (size)) && (p_size) < (size)))
		__read_overflow();
	if (p_size < size)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_196(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_196();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_197(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_197();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_198(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 0) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (0)) :
								 0)))
							 __compiletime_assert_198();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_199(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_199();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_200(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_200();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(0)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_201(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_201();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_202(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_202();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_203(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_memchr_inv) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_memchr_inv)) :
								 0)))
							 __compiletime_assert_203();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_204(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_204();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_205(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_205();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_memchr_inv)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, size);
	return __real_memchr_inv(p, c, size);
}

extern void *__real_kmemdup(const void *src, size_t len,
			    gfp_t gfp) __asm__("kmemdup_noprof")
	__attribute__((__alloc_size__(2)));
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__)) void *
kmemdup_noprof(const void *const
	       __attribute__((__pass_dynamic_object_size__(0))) p,
	       size_t size, gfp_t gfp)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 0);

	if ((__builtin_constant_p((p_size) < (size)) && (p_size) < (size)))
		__read_overflow();
	if (p_size < size)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_206(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_206();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_207(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_207();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_208(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 0) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (0)) :
								 0)))
							 __compiletime_assert_208();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_209(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_209();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_210(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_210();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(0)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_211(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_211();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_212(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_212();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_213(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_kmemdup) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_kmemdup)) :
								 0)))
							 __compiletime_assert_213();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_214(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_214();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_215(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_215();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_kmemdup)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, size);

	return __real_kmemdup(p, size, gfp);
}
extern inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__gnu_inline__)) __attribute__((__overloadable__))
__attribute__((__diagnose_as_builtin__(__builtin_strcpy, 1, 2))) char *
strcpy(char *const __attribute__((__pass_dynamic_object_size__(1))) p,
       const char *const __attribute__((__pass_dynamic_object_size__(1))) q)
{
	const size_t p_size = __builtin_dynamic_object_size(p, 1);
	const size_t q_size = __builtin_dynamic_object_size(q, 1);
	size_t size;

	if (__builtin_constant_p(p_size) && __builtin_constant_p(q_size) &&
	    p_size == (~(size_t)0) && q_size == (~(size_t)0))
		return __builtin_strcpy(p, q);
	size = __builtin_choose_expr(
		       (sizeof(int) ==
			sizeof(*(8 ? ((void *)((long)(__builtin_strlen(q)) *
					       0l)) :
				     (int *)8))),
		       __builtin_strlen(q), __fortify_strlen(q)) +
	       1;

	if ((__builtin_constant_p((p_size) < (size)) && (p_size) < (size)))
		__write_overflow();

	if (p_size < size)
		__fortify_panic(
			(({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_216(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p(
							     ((((1UL)))
							      << (0))))))
							 __compiletime_assert_216();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_217(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((1UL))) << (0))) ==
							 0)))
							 __compiletime_assert_217();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_218(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 1) ?
								 ~((((((1UL)))
								     << (0))) >>
								   (__builtin_ffsll((
									    (((1UL)))
									    << (0))) -
								    1)) & (0 +
									   (1)) :
								 0)))
							 __compiletime_assert_218();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_219(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((1UL)))
								   << (0))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((1UL)))
										  << (0))))))((
								 (((1UL)))
								 << (0)))) >
							 ((typeof(_Generic(
								 (0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: (
										  0ULL))))(~0ull)))))
							 __compiletime_assert_219();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_220(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) & (((((((1UL))) << (0))) + (1ULL << (__builtin_ffsll(((((1UL))) << (0))) - 1))) - 1)) != 0")));
						 if (!(!((((((((1UL))) << (0))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((1UL)))
									<< (0))) -
								1))) &
							  (((((((1UL)))
							      << (0))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((1UL)))
									 << (0))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_220();
					 } while (0);
				 });
				 ((typeof(((((1UL))) << (0))))(1)
				  << (__builtin_ffsll(((((1UL))) << (0))) -
				      1)) &
					 (((((1UL))) << (0)));
			 }) |
			 ({
				 ({
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_221(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is not constant")));
						 if (!(!(!__builtin_constant_p((
							     (((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))))))
							 __compiletime_assert_221();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_222(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "mask is zero")));
						 if (!(!((((((int)(sizeof(struct {
								   int : (-!!(__builtin_choose_expr(
										 (sizeof(int) ==
										  sizeof(*(
											  8 ? ((void *)((long)((1) >
													       (7)) *
													0l)) :
											      (int *)8))),
										 (1) > (7),
										 0)));
							   })))) +
							   (((~((0UL))) -
							     (((1UL)) << (1)) +
							     1) &
							    (~((0UL)) >>
							     (64 - 1 -
							      (7)))))) == 0)))
							 __compiletime_assert_222();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_223(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "value too large for the field")));
						 if (!(!(__builtin_constant_p(
								 FORTIFY_FUNC_strcpy) ?
								 ~((((((int)(sizeof(struct {
									     int : (-!!(__builtin_choose_expr(
											   (sizeof(int) ==
											    sizeof(*(
												    8 ? ((void *)((long)((1) >
															 (7)) *
														  0l)) :
													(int *)8))),
											   (1) > (7),
											   0)));
								     })))) +
								     (((~((0UL))) -
								       (((1UL))
									<< (1)) +
								       1) &
								      (~((0UL)) >>
								       (64 - 1 -
									(7)))))) >>
								   (__builtin_ffsll((
									    (((int)(sizeof(struct {
										    int : (-!!(__builtin_choose_expr(
												  (sizeof(int) ==
												   sizeof(*(
													   8 ? ((void *)((long)((1) >
																(7)) *
															 0l)) :
													       (int *)8))),
												  (1) > (7),
												  0)));
									    })))) +
									    (((~((0UL))) -
									      (((1UL))
									       << (1)) +
									      1) &
									     (~((0UL)) >>
									      (64 -
									       1 -
									       (7)))))) -
								    1)) &
									 (0 +
									  (FORTIFY_FUNC_strcpy)) :
								 0)))
							 __compiletime_assert_223();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_224(void)
							 __attribute__((__error__(
								 "FIELD_PREP: "
								 "type of reg too small for mask")));
						 if (!(!(((typeof(_Generic(
								 (((((int)(sizeof(struct {
									   int : (-!!(__builtin_choose_expr(
											 (sizeof(int) ==
											  sizeof(*(
												  8 ? ((void *)((long)((1) >
														       (7)) *
														0l)) :
												      (int *)8))),
											 (1) > (7),
											 0)));
								   })))) +
								   (((~((0UL))) -
								     (((1UL))
								      << (1)) +
								     1) &
								    (~((0UL)) >>
								     (64 - 1 -
								      (7)))))),
									  char: (unsigned char)0,
									  unsigned char: (
										  unsigned char)0,
									  signed char: (
										  unsigned char)0,
									  unsigned short: (
										  unsigned short)0,
									  signed short: (
										  unsigned short)0,
									  unsigned int: (
										  unsigned int)0,
									  signed int: (
										  unsigned int)0,
									  unsigned long: (
										  unsigned long)0,
									  signed long: (
										  unsigned long)0,
									  unsigned long long: (
										  unsigned long long)0,
									  signed long long: (
										  unsigned long long)0,
									  default: ((
										  (((int)(sizeof(struct {
											  int : (-!!(__builtin_choose_expr(
													(sizeof(int) ==
													 sizeof(*(
														 8 ? ((void *)((long)((1) >
																      (7)) *
															       0l)) :
														     (int *)8))),
													(1) > (7),
													0)));
										  })))) +
										  (((~((0UL))) -
										    (((1UL))
										     << (1)) +
										    1) &
										   (~((0UL)) >>
										    (64 -
										     1 -
										     (7)))))))))((
								 (((int)(sizeof(struct {
									 int : (-!!(__builtin_choose_expr(
										       (sizeof(int) ==
											sizeof(*(
												8 ? ((void *)((long)((1) >
														     (7)) *
													      0l)) :
												    (int *)8))),
										       (1) > (7),
										       0)));
								 })))) +
								 (((~((0UL))) -
								   (((1UL))
								    << (1)) +
								   1) &
								  (~((0UL)) >>
								   (64 - 1 -
								    (7))))))) >
							 ((typeof(_Generic((0ULL),
									  char: (unsigned char)0,
									  unsigned char: (
										   unsigned char)0,
									  signed char: (
										   unsigned char)0,
									  unsigned short: (
										   unsigned short)0,
									  signed short: (
										   unsigned short)0,
									  unsigned int: (
										   unsigned int)0,
									  signed int: (
										   unsigned int)0,
									  unsigned long: (
										   unsigned long)0,
									  signed long: (
										   unsigned long)0,
									  unsigned long long: (
										   unsigned long long)0,
									  signed long long: (
										   unsigned long long)0,
									  default: (
										   0ULL))))(~0ull)))))
							 __compiletime_assert_224();
					 } while (0);
					 do {
						 __attribute__((
							 __noreturn__)) extern void
						 __compiletime_assert_225(void)
							 __attribute__((__error__(
								 "BUILD_BUG_ON failed: "
								 "(((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) & (((((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) + (1ULL << (__builtin_ffsll(((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((1) > (7)) * 0l)) : (int *)8))), (1) > (7), 0))); })))) + (((~((0UL))) - (((1UL)) << (1)) + 1) & (~((0UL)) >> (64 - 1 - (7)))))) - 1))) - 1)) != 0")));
						 if (!(!((((((((int)(sizeof(struct {
								     int : (-!!(__builtin_choose_expr(
										   (sizeof(int) ==
										    sizeof(*(
											    8 ? ((void *)((long)((1) >
														 (7)) *
													  0l)) :
												(int *)8))),
										   (1) > (7),
										   0)));
							     })))) +
							     (((~((0UL))) -
							       (((1UL)) << (1)) +
							       1) &
							      (~((0UL)) >>
							       (64 - 1 -
								(7)))))) +
							   (1ULL
							    << (__builtin_ffsll((
									(((int)(sizeof(struct {
										int : (-!!(__builtin_choose_expr(
											      (sizeof(int) ==
											       sizeof(*(
												       8 ? ((void *)((long)((1) >
															    (7)) *
														     0l)) :
													   (int *)8))),
											      (1) > (7),
											      0)));
									})))) +
									(((~((0UL))) -
									  (((1UL))
									   << (1)) +
									  1) &
									 (~((0UL)) >>
									  (64 -
									   1 -
									   (7)))))) -
								1))) &
							  (((((((int)(sizeof(struct {
								      int : (-!!(__builtin_choose_expr(
										    (sizeof(int) ==
										     sizeof(*(
											     8 ? ((void *)((long)((1) >
														  (7)) *
													   0l)) :
												 (int *)8))),
										    (1) > (7),
										    0)));
							      })))) +
							      (((~((0UL))) -
								(((1UL)) << (1)) +
								1) &
							       (~((0UL)) >>
								(64 - 1 -
								 (7)))))) +
							    (1ULL
							     << (__builtin_ffsll((
									 (((int)(sizeof(struct {
										 int : (-!!(__builtin_choose_expr(
											       (sizeof(int) ==
												sizeof(*(
													8 ? ((void *)((long)((1) >
															     (7)) *
														      0l)) :
													    (int *)8))),
											       (1) > (7),
											       0)));
									 })))) +
									 (((~((0UL))) -
									   (((1UL))
									    << (1)) +
									   1) &
									  (~((0UL)) >>
									   (64 -
									    1 -
									    (7)))))) -
								 1))) -
							   1)) != 0)))
							 __compiletime_assert_225();
					 } while (0);
				 });
				 ((typeof((
					  (((int)(sizeof(struct {
						  int : (-!!(__builtin_choose_expr(
								(sizeof(int) ==
								 sizeof(*(
									 8 ? ((void *)((long)((1) >
											      (7)) *
										       0l)) :
									     (int *)8))),
								(1) > (7), 0)));
					  })))) +
					  (((~((0UL))) - (((1UL)) << (1)) + 1) &
					   (~((0UL)) >>
					    (64 - 1 -
					     (7)))))))(FORTIFY_FUNC_strcpy)
				  << (__builtin_ffsll((
					      (((int)(sizeof(struct {
						      int : (-!!(__builtin_choose_expr(
								    (sizeof(int) ==
								     sizeof(*(
									     8 ? ((void *)((long)((1) >
												  (7)) *
											   0l)) :
										 (int *)8))),
								    (1) > (7),
								    0)));
					      })))) +
					      (((~((0UL))) - (((1UL)) << (1)) +
						1) &
					       (~((0UL)) >> (64 - 1 - (7)))))) -
				      1)) &
					 (((((int)(sizeof(struct {
						   int : (-!!(__builtin_choose_expr(
								 (sizeof(int) ==
								  sizeof(*(
									  8 ? ((void *)((long)((1) >
											       (7)) *
											0l)) :
									      (int *)8))),
								 (1) > (7),
								 0)));
					   })))) +
					   (((~((0UL))) - (((1UL)) << (1)) + 1) &
					    (~((0UL)) >> (64 - 1 - (7))))));
			 })),
			p_size, size);
	__builtin_memcpy(p, q, size);
	return p;
}

void memcpy_and_pad(void *dest, size_t dest_len, const void *src, size_t count,
		    int pad);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) size_t
str_has_prefix(const char *str, const char *prefix)
{
	size_t len = __builtin_choose_expr(
		(sizeof(int) ==
		 sizeof(*(8 ? ((void *)((long)(__builtin_strlen(prefix)) * 0l)) :
			      (int *)8))),
		__builtin_strlen(prefix), __fortify_strlen(prefix));
	return strncmp(str, prefix, len) == 0 ? len : 0;
}

int bitmap_parse_user(const char *ubuf, unsigned int ulen, unsigned long *dst,
		      int nbits);
int bitmap_print_to_pagebuf(bool list, char *buf, const unsigned long *maskp,
			    int nmaskbits);
extern int bitmap_print_bitmask_to_buf(char *buf, const unsigned long *maskp,
				       int nmaskbits, loff_t off, size_t count);
extern int bitmap_print_list_to_buf(char *buf, const unsigned long *maskp,
				    int nmaskbits, loff_t off, size_t count);
int bitmap_parse(const char *buf, unsigned int buflen, unsigned long *dst,
		 int nbits);
int bitmap_parselist(const char *buf, unsigned long *maskp, int nmaskbits);
int bitmap_parselist_user(const char *ubuf, unsigned int ulen,
			  unsigned long *dst, int nbits);

struct device;
unsigned long *bitmap_alloc(unsigned int nbits, gfp_t flags);
unsigned long *bitmap_zalloc(unsigned int nbits, gfp_t flags);
unsigned long *bitmap_alloc_node(unsigned int nbits, gfp_t flags, int node);
unsigned long *bitmap_zalloc_node(unsigned int nbits, gfp_t flags, int node);
void bitmap_free(const unsigned long *bitmap);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__free_bitmap(void *p)
{
	unsigned long *_T = *(unsigned long **)p;
	if (_T)
		bitmap_free(_T);
}

unsigned long *devm_bitmap_alloc(struct device *dev, unsigned int nbits,
				 gfp_t flags);
unsigned long *devm_bitmap_zalloc(struct device *dev, unsigned int nbits,
				  gfp_t flags);

bool __bitmap_equal(const unsigned long *bitmap1, const unsigned long *bitmap2,
		    unsigned int nbits);
bool __attribute__((__pure__)) __bitmap_or_equal(const unsigned long *src1,
						 const unsigned long *src2,
						 const unsigned long *src3,
						 unsigned int nbits);
void __bitmap_complement(unsigned long *dst, const unsigned long *src,
			 unsigned int nbits);
void __bitmap_shift_right(unsigned long *dst, const unsigned long *src,
			  unsigned int shift, unsigned int nbits);
void __bitmap_shift_left(unsigned long *dst, const unsigned long *src,
			 unsigned int shift, unsigned int nbits);
void bitmap_cut(unsigned long *dst, const unsigned long *src,
		unsigned int first, unsigned int cut, unsigned int nbits);
bool __bitmap_and(unsigned long *dst, const unsigned long *bitmap1,
		  const unsigned long *bitmap2, unsigned int nbits);
void __bitmap_or(unsigned long *dst, const unsigned long *bitmap1,
		 const unsigned long *bitmap2, unsigned int nbits);
void __bitmap_xor(unsigned long *dst, const unsigned long *bitmap1,
		  const unsigned long *bitmap2, unsigned int nbits);
bool __bitmap_andnot(unsigned long *dst, const unsigned long *bitmap1,
		     const unsigned long *bitmap2, unsigned int nbits);
void __bitmap_replace(unsigned long *dst, const unsigned long *old,
		      const unsigned long *new, const unsigned long *mask,
		      unsigned int nbits);
bool __bitmap_intersects(const unsigned long *bitmap1,
			 const unsigned long *bitmap2, unsigned int nbits);
bool __bitmap_subset(const unsigned long *bitmap1, const unsigned long *bitmap2,
		     unsigned int nbits);
unsigned int __bitmap_weight(const unsigned long *bitmap, unsigned int nbits);
unsigned int __bitmap_weight_and(const unsigned long *bitmap1,
				 const unsigned long *bitmap2,
				 unsigned int nbits);
unsigned int __bitmap_weight_andnot(const unsigned long *bitmap1,
				    const unsigned long *bitmap2,
				    unsigned int nbits);
void __bitmap_set(unsigned long *map, unsigned int start, int len);
void __bitmap_clear(unsigned long *map, unsigned int start, int len);

unsigned long bitmap_find_next_zero_area_off(
	unsigned long *map, unsigned long size, unsigned long start,
	unsigned int nr, unsigned long align_mask, unsigned long align_offset);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
bitmap_find_next_zero_area(unsigned long *map, unsigned long size,
			   unsigned long start, unsigned int nr,
			   unsigned long align_mask)
{
	return bitmap_find_next_zero_area_off(map, size, start, nr, align_mask,
					      0);
}

void bitmap_remap(unsigned long *dst, const unsigned long *src,
		  const unsigned long *old, const unsigned long *new,
		  unsigned int nbits);
int bitmap_bitremap(int oldbit, const unsigned long *old,
		    const unsigned long *new, int bits);
void bitmap_onto(unsigned long *dst, const unsigned long *orig,
		 const unsigned long *relmap, unsigned int bits);
void bitmap_fold(unsigned long *dst, const unsigned long *orig, unsigned int sz,
		 unsigned int nbits);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_zero(unsigned long *dst, unsigned int nbits)
{
	unsigned int len = (((((nbits)) + ((__typeof__((nbits)))((64)) - 1)) &
			     ~((__typeof__((nbits)))((64)) - 1)) /
			    8);

	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		*dst = 0;
	else
		({
			size_t __fortify_size = (size_t)(len);
			fortify_memset_chk(
				__fortify_size,
				__builtin_dynamic_object_size(dst, 0),
				__builtin_dynamic_object_size(dst, 1)),
				__builtin_memset(dst, 0, __fortify_size);
		});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_fill(unsigned long *dst, unsigned int nbits)
{
	unsigned int len = (((((nbits)) + ((__typeof__((nbits)))((64)) - 1)) &
			     ~((__typeof__((nbits)))((64)) - 1)) /
			    8);

	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		*dst = ~0UL;
	else
		({
			size_t __fortify_size = (size_t)(len);
			fortify_memset_chk(
				__fortify_size,
				__builtin_dynamic_object_size(dst, 0),
				__builtin_dynamic_object_size(dst, 1)),
				__builtin_memset(dst, 0xff, __fortify_size);
		});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_copy(unsigned long *dst, const unsigned long *src, unsigned int nbits)
{
	unsigned int len = (((((nbits)) + ((__typeof__((nbits)))((64)) - 1)) &
			     ~((__typeof__((nbits)))((64)) - 1)) /
			    8);

	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		*dst = *src;
	else
		({
			const size_t __fortify_size = (size_t)(len);
			const size_t __p_size =
				(__builtin_dynamic_object_size(dst, 0));
			const size_t __q_size =
				(__builtin_dynamic_object_size(src, 0));
			const size_t __p_size_field =
				(__builtin_dynamic_object_size(dst, 1));
			const size_t __q_size_field =
				(__builtin_dynamic_object_size(src, 1));
			({
				bool __ret_do_once = !!(fortify_memcpy_chk(
					__fortify_size, __p_size, __q_size,
					__p_size_field, __q_size_field,
					FORTIFY_FUNC_memcpy));
				if (({
					    static bool __attribute__((
						    __section__(".data.once")))
					    __already_done;
					    bool __ret_cond = !!(__ret_do_once);
					    bool __ret_once = false;
					    if (__builtin_expect(
							!!(__ret_cond &&
							   !__already_done),
							0)) {
						    __already_done = true;
						    __ret_once = true;
					    }
					    __builtin_expect(!!(__ret_once), 0);
				    }))
					({
						int __ret_warn_on = !!(1);
						if (__builtin_expect(
							    !!(__ret_warn_on),
							    0))
							do {
								({
									asm volatile(
										"226"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"226"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(226));
								});
								__warn_printk(
									"memcpy"
									": detected field-spanning write (size %zu) of single %s (size %zu)\n",
									__fortify_size,
									"field \""
									"dst"
									"\" at "
									"include/linux/bitmap.h"
									":"
									"259",
									__p_size_field);
								do {
									__auto_type __flags =
										(1
										 << 0) |
										((1
										  << 3) |
										 ((9)
										  << 8));
									({
										asm volatile(
											"227"
											": nop\n\t"
											".pushsection .discard.instr_begin\n\t"
											".long "
											"227"
											"b - .\n\t"
											".popsection\n\t"
											:
											: "i"(227));
									});
									do {
										asm __inline volatile(
											"1:\t"
											".byte 0x0f, 0x0b"
											"\n"
											".pushsection __bug_table,\"aw\"\n"
											"2:\t"
											".long "
											"1b"
											" - ."
											"\t# bug_entry::bug_addr\n"
											"\t"
											".long "
											"%c0"
											" - ."
											"\t# bug_entry::file\n"
											"\t.word %c1"
											"\t# bug_entry::line\n"
											"\t.word %c2"
											"\t# bug_entry::flags\n"
											"\t.org 2b+%c3\n"
											".popsection\n"
											"998:\n\t"
											".pushsection .discard.reachable\n\t"
											".long 998b\n\t"
											".popsection\n\t"
											:
											: "i"("include/linux/bitmap.h"),
											  "i"(259),
											  "i"(__flags),
											  "i"(sizeof(
												  struct bug_entry)));
									} while (
										0);
									({
										asm volatile(
											"228"
											": nop\n\t"
											".pushsection .discard.instr_end\n\t"
											".long "
											"228"
											"b - .\n\t"
											".popsection\n\t"
											:
											: "i"(228));
									});
								} while (0);
								({
									asm volatile(
										"229"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"229"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(229));
								});
							} while (0);
						__builtin_expect(
							!!(__ret_warn_on), 0);
					});
				__builtin_expect(!!(__ret_do_once), 0);
			});
			__builtin_memcpy(dst, src, __fortify_size);
		});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_copy_clear_tail(unsigned long *dst, const unsigned long *src,
		       unsigned int nbits)
{
	bitmap_copy(dst, src, nbits);
	if (nbits % 64)
		dst[nbits / 64] &= (~0UL >> (-(nbits) & (64 - 1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
bitmap_copy_and_extend(unsigned long *to, const unsigned long *from,
		       unsigned int count, unsigned int size)
{
	unsigned int copy =
		(((count) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)));

	({
		const size_t __fortify_size = (size_t)(copy * sizeof(long));
		const size_t __p_size = (__builtin_dynamic_object_size(to, 0));
		const size_t __q_size =
			(__builtin_dynamic_object_size(from, 0));
		const size_t __p_size_field =
			(__builtin_dynamic_object_size(to, 1));
		const size_t __q_size_field =
			(__builtin_dynamic_object_size(from, 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"230"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"230"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(230));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"to"
								"\" at "
								"include/linux/bitmap.h"
								":"
								"279",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"231"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"231"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(231));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("include/linux/bitmap.h"),
										  "i"(279),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"232"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"232"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(232));
								});
							} while (0);
							({
								asm volatile(
									"233"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"233"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(233));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(to, from, __fortify_size);
	});
	if (count % 64)
		to[copy - 1] &= (~0UL >> (-(count) & (64 - 1)));
	({
		size_t __fortify_size =
			(size_t)((((((size)) +
				    ((__typeof__((size)))((64)) - 1)) &
				   ~((__typeof__((size)))((64)) - 1)) /
				  8) -
				 copy * sizeof(long));
		fortify_memset_chk(__fortify_size,
				   __builtin_dynamic_object_size(to + copy, 0),
				   __builtin_dynamic_object_size(to + copy, 1)),
			__builtin_memset(to + copy, 0, __fortify_size);
	});
}
void bitmap_from_arr32(unsigned long *bitmap, const u32 *buf,
		       unsigned int nbits);
void bitmap_to_arr32(u32 *buf, const unsigned long *bitmap, unsigned int nbits);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
bitmap_and(unsigned long *dst, const unsigned long *src1,
	   const unsigned long *src2, unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		return (*dst = *src1 & *src2 &
			       (~0UL >> (-(nbits) & (64 - 1)))) != 0;
	return __bitmap_and(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_or(unsigned long *dst, const unsigned long *src1,
	  const unsigned long *src2, unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		*dst = *src1 | *src2;
	else
		__bitmap_or(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_xor(unsigned long *dst, const unsigned long *src1,
	   const unsigned long *src2, unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		*dst = *src1 ^ *src2;
	else
		__bitmap_xor(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
bitmap_andnot(unsigned long *dst, const unsigned long *src1,
	      const unsigned long *src2, unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		return (*dst = *src1 & ~(*src2) &
			       (~0UL >> (-(nbits) & (64 - 1)))) != 0;
	return __bitmap_andnot(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_complement(unsigned long *dst, const unsigned long *src,
		  unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		*dst = ~(*src);
	else
		__bitmap_complement(dst, src, nbits);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
bitmap_equal(const unsigned long *src1, const unsigned long *src2,
	     unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		return !((*src1 ^ *src2) & (~0UL >> (-(nbits) & (64 - 1))));
	if (__builtin_constant_p(nbits & (8 - 1)) &&
	    (((nbits) & ((typeof(nbits))(8) - 1)) == 0))
		return !memcmp(src1, src2, nbits / 8);
	return __bitmap_equal(src1, src2, nbits);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
bitmap_or_equal(const unsigned long *src1, const unsigned long *src2,
		const unsigned long *src3, unsigned int nbits)
{
	if (!(__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		return __bitmap_or_equal(src1, src2, src3, nbits);

	return !(((*src1 | *src2) ^ *src3) & (~0UL >> (-(nbits) & (64 - 1))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
bitmap_intersects(const unsigned long *src1, const unsigned long *src2,
		  unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		return ((*src1 & *src2) & (~0UL >> (-(nbits) & (64 - 1)))) != 0;
	else
		return __bitmap_intersects(src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
bitmap_subset(const unsigned long *src1, const unsigned long *src2,
	      unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		return !((*src1 & ~(*src2)) & (~0UL >> (-(nbits) & (64 - 1))));
	else
		return __bitmap_subset(src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
bitmap_empty(const unsigned long *src, unsigned nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		return !(*src & (~0UL >> (-(nbits) & (64 - 1))));

	return find_first_bit(src, nbits) == nbits;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
bitmap_full(const unsigned long *src, unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		return !(~(*src) & (~0UL >> (-(nbits) & (64 - 1))));

	return find_first_zero_bit(src, nbits) == nbits;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
bitmap_weight(const unsigned long *src, unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		return hweight_long(*src & (~0UL >> (-(nbits) & (64 - 1))));
	return __bitmap_weight(src, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
bitmap_weight_and(const unsigned long *src1, const unsigned long *src2,
		  unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		return hweight_long(*src1 & *src2 &
				    (~0UL >> (-(nbits) & (64 - 1))));
	return __bitmap_weight_and(src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
bitmap_weight_andnot(const unsigned long *src1, const unsigned long *src2,
		     unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		return hweight_long(*src1 & ~(*src2) &
				    (~0UL >> (-(nbits) & (64 - 1))));
	return __bitmap_weight_andnot(src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_set(unsigned long *map, unsigned int start, unsigned int nbits)
{
	if (__builtin_constant_p(nbits) && nbits == 1)
		((__builtin_constant_p(start) &&
		  __builtin_constant_p((uintptr_t)(map) !=
				       (uintptr_t)((void *)0)) &&
		  (uintptr_t)(map) != (uintptr_t)((void *)0) &&
		  __builtin_constant_p(*(const unsigned long *)(map))) ?
			 generic___set_bit(start, map) :
			 ___set_bit(start, map));
	else if ((__builtin_constant_p(start + nbits) &&
		  (start + nbits) <= 64 && (start + nbits) > 0))
		*map |= ((((int)(sizeof(struct {
				 int : (-!!(__builtin_choose_expr(
					       (sizeof(int) ==
						sizeof(*(
							8 ? ((void *)((long)((start) >
									     (start +
									      nbits -
									      1)) *
								      0l)) :
							    (int *)8))),
					       (start) > (start + nbits - 1),
					       0)));
			 })))) +
			 (((~((0UL))) - (((1UL)) << (start)) + 1) &
			  (~((0UL)) >> (64 - 1 - (start + nbits - 1)))));
	else if (__builtin_constant_p(start & (8 - 1)) &&
		 (((start) & ((typeof(start))(8) - 1)) == 0) &&
		 __builtin_constant_p(nbits & (8 - 1)) &&
		 (((nbits) & ((typeof(nbits))(8) - 1)) == 0))
		({
			size_t __fortify_size = (size_t)(nbits / 8);
			fortify_memset_chk(__fortify_size,
					   __builtin_dynamic_object_size(
						   (char *)map + start / 8, 0),
					   __builtin_dynamic_object_size(
						   (char *)map + start / 8, 1)),
				__builtin_memset((char *)map + start / 8, 0xff,
						 __fortify_size);
		});
	else
		__bitmap_set(map, start, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_clear(unsigned long *map, unsigned int start, unsigned int nbits)
{
	if (__builtin_constant_p(nbits) && nbits == 1)
		((__builtin_constant_p(start) &&
		  __builtin_constant_p((uintptr_t)(map) !=
				       (uintptr_t)((void *)0)) &&
		  (uintptr_t)(map) != (uintptr_t)((void *)0) &&
		  __builtin_constant_p(*(const unsigned long *)(map))) ?
			 generic___clear_bit(start, map) :
			 ___clear_bit(start, map));
	else if ((__builtin_constant_p(start + nbits) &&
		  (start + nbits) <= 64 && (start + nbits) > 0))
		*map &= ~(
			(((int)(sizeof(struct {
				int : (-!!(__builtin_choose_expr(
					      (sizeof(int) ==
					       sizeof(*(
						       8 ? ((void *)((long)((start) >
									    (start +
									     nbits -
									     1)) *
								     0l)) :
							   (int *)8))),
					      (start) > (start + nbits - 1),
					      0)));
			})))) +
			(((~((0UL))) - (((1UL)) << (start)) + 1) &
			 (~((0UL)) >> (64 - 1 - (start + nbits - 1)))));
	else if (__builtin_constant_p(start & (8 - 1)) &&
		 (((start) & ((typeof(start))(8) - 1)) == 0) &&
		 __builtin_constant_p(nbits & (8 - 1)) &&
		 (((nbits) & ((typeof(nbits))(8) - 1)) == 0))
		({
			size_t __fortify_size = (size_t)(nbits / 8);
			fortify_memset_chk(__fortify_size,
					   __builtin_dynamic_object_size(
						   (char *)map + start / 8, 0),
					   __builtin_dynamic_object_size(
						   (char *)map + start / 8, 1)),
				__builtin_memset((char *)map + start / 8, 0,
						 __fortify_size);
		});
	else
		__bitmap_clear(map, start, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_shift_right(unsigned long *dst, const unsigned long *src,
		   unsigned int shift, unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		*dst = (*src & (~0UL >> (-(nbits) & (64 - 1)))) >> shift;
	else
		__bitmap_shift_right(dst, src, shift, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_shift_left(unsigned long *dst, const unsigned long *src,
		  unsigned int shift, unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		*dst = (*src << shift) & (~0UL >> (-(nbits) & (64 - 1)));
	else
		__bitmap_shift_left(dst, src, shift, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_replace(unsigned long *dst, const unsigned long *old,
	       const unsigned long *new, const unsigned long *mask,
	       unsigned int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
		*dst = (*old & ~(*mask)) | (*new &*mask);
	else
		__bitmap_replace(dst, old, new, mask, nbits);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_scatter(unsigned long *dst, const unsigned long *src,
	       const unsigned long *mask, unsigned int nbits)
{
	unsigned int n = 0;
	unsigned int bit;

	bitmap_zero(dst, nbits);

	for ((bit) = 0;
	     (bit) = find_next_bit((mask), (nbits), (bit)), (bit) < (nbits);
	     (bit)++)
		((((__builtin_constant_p(n++) &&
		    __builtin_constant_p((uintptr_t)(src) !=
					 (uintptr_t)((void *)0)) &&
		    (uintptr_t)(src) != (uintptr_t)((void *)0) &&
		    __builtin_constant_p(*(const unsigned long *)(src))) ?
			   const_test_bit(n++, src) :
			   _test_bit(n++, src))) ?
			 ((__builtin_constant_p((bit)) &&
			   __builtin_constant_p((uintptr_t)((dst)) !=
						(uintptr_t)((void *)0)) &&
			   (uintptr_t)((dst)) != (uintptr_t)((void *)0) &&
			   __builtin_constant_p(
				   *(const unsigned long *)((dst)))) ?
				  generic___set_bit((bit), (dst)) :
				  ___set_bit((bit), (dst))) :
			 ((__builtin_constant_p((bit)) &&
			   __builtin_constant_p((uintptr_t)((dst)) !=
						(uintptr_t)((void *)0)) &&
			   (uintptr_t)((dst)) != (uintptr_t)((void *)0) &&
			   __builtin_constant_p(
				   *(const unsigned long *)((dst)))) ?
				  generic___clear_bit((bit), (dst)) :
				  ___clear_bit((bit), (dst))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_gather(unsigned long *dst, const unsigned long *src,
	      const unsigned long *mask, unsigned int nbits)
{
	unsigned int n = 0;
	unsigned int bit;

	bitmap_zero(dst, nbits);

	for ((bit) = 0;
	     (bit) = find_next_bit((mask), (nbits), (bit)), (bit) < (nbits);
	     (bit)++)
		((((__builtin_constant_p(bit) &&
		    __builtin_constant_p((uintptr_t)(src) !=
					 (uintptr_t)((void *)0)) &&
		    (uintptr_t)(src) != (uintptr_t)((void *)0) &&
		    __builtin_constant_p(*(const unsigned long *)(src))) ?
			   const_test_bit(bit, src) :
			   _test_bit(bit, src))) ?
			 ((__builtin_constant_p((n++)) &&
			   __builtin_constant_p((uintptr_t)((dst)) !=
						(uintptr_t)((void *)0)) &&
			   (uintptr_t)((dst)) != (uintptr_t)((void *)0) &&
			   __builtin_constant_p(
				   *(const unsigned long *)((dst)))) ?
				  generic___set_bit((n++), (dst)) :
				  ___set_bit((n++), (dst))) :
			 ((__builtin_constant_p((n++)) &&
			   __builtin_constant_p((uintptr_t)((dst)) !=
						(uintptr_t)((void *)0)) &&
			   (uintptr_t)((dst)) != (uintptr_t)((void *)0) &&
			   __builtin_constant_p(
				   *(const unsigned long *)((dst)))) ?
				  generic___clear_bit((n++), (dst)) :
				  ___clear_bit((n++), (dst))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_next_set_region(unsigned long *bitmap, unsigned int *rs,
		       unsigned int *re, unsigned int end)
{
	*rs = find_next_bit(bitmap, end, *rs);
	*re = find_next_zero_bit(bitmap, end, *rs + 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_release_region(unsigned long *bitmap, unsigned int pos, int order)
{
	bitmap_clear(bitmap, pos, ((((1UL))) << (order)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
bitmap_allocate_region(unsigned long *bitmap, unsigned int pos, int order)
{
	unsigned int len = ((((1UL))) << (order));

	if (find_next_bit(bitmap, pos + len, pos) < pos + len)
		return -16;
	bitmap_set(bitmap, pos, len);
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
bitmap_find_free_region(unsigned long *bitmap, unsigned int bits, int order)
{
	unsigned int pos, end;

	for (pos = 0; (end = pos + ((((1UL))) << (order))) <= bits; pos = end) {
		if (!bitmap_allocate_region(bitmap, pos, order))
			return pos;
	}
	return -12;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_from_u64(unsigned long *dst, u64 mask)
{
	bitmap_copy_clear_tail((unsigned long *)(dst),
			       (const unsigned long *)(&mask), (64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
bitmap_read(const unsigned long *map, unsigned long start, unsigned long nbits)
{
	size_t index = ((start) / 64);
	unsigned long offset = start % 64;
	unsigned long space = 64 - offset;
	unsigned long value_low, value_high;

	if (__builtin_expect(!!(!nbits || nbits > 64), 0))
		return 0;

	if (space >= nbits)
		return (map[index] >> offset) & (~0UL >> (-(nbits) & (64 - 1)));

	value_low = map[index] & (~0UL << ((start) & (64 - 1)));
	value_high = map[index + 1] & (~0UL >> (-(start + nbits) & (64 - 1)));
	return (value_low >> offset) | (value_high << space);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
bitmap_write(unsigned long *map, unsigned long value, unsigned long start,
	     unsigned long nbits)
{
	size_t index;
	unsigned long offset;
	unsigned long space;
	unsigned long mask;
	bool fit;

	if (__builtin_expect(!!(!nbits || nbits > 64), 0))
		return;

	mask = (~0UL >> (-(nbits) & (64 - 1)));
	value &= mask;
	offset = start % 64;
	space = 64 - offset;
	fit = space >= nbits;
	index = ((start) / 64);

	map[index] &=
		(fit ? (~(mask << offset)) : ~(~0UL << ((start) & (64 - 1))));
	map[index] |= value << offset;
	if (fit)
		return;

	map[index + 1] &= (~0UL << ((start + nbits) & (64 - 1)));
	map[index + 1] |= (value >> space);
}

typedef struct cpumask {
	unsigned long bits[(((64) + ((sizeof(long) * 8)) - 1) /
			    ((sizeof(long) * 8)))];
} cpumask_t;
typedef struct cpumask cpumask_var_t[1];

enum {
	___GFP_DMA_BIT,
	___GFP_HIGHMEM_BIT,
	___GFP_DMA32_BIT,
	___GFP_MOVABLE_BIT,
	___GFP_RECLAIMABLE_BIT,
	___GFP_HIGH_BIT,
	___GFP_IO_BIT,
	___GFP_FS_BIT,
	___GFP_ZERO_BIT,
	___GFP_UNUSED_BIT,
	___GFP_DIRECT_RECLAIM_BIT,
	___GFP_KSWAPD_RECLAIM_BIT,
	___GFP_WRITE_BIT,
	___GFP_NOWARN_BIT,
	___GFP_RETRY_MAYFAIL_BIT,
	___GFP_NOFAIL_BIT,
	___GFP_NORETRY_BIT,
	___GFP_MEMALLOC_BIT,
	___GFP_COMP_BIT,
	___GFP_NOMEMALLOC_BIT,
	___GFP_HARDWALL_BIT,
	___GFP_THISNODE_BIT,
	___GFP_ACCOUNT_BIT,
	___GFP_ZEROTAGS_BIT,
	___GFP_LAST_BIT
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
numa_valid_node(int nid)
{
	return nid >= 0 && nid < (1 << 6);
}
extern struct pglist_data *node_data[];

void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
alloc_node_data(int nid);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
alloc_offline_node_data(int nid);

int numa_nearest_node(int node, unsigned int state);

int memory_add_physaddr_to_nid(u64 start);

int phys_to_target_node(u64 start);

int numa_fill_memblks(u64 start, u64 end);
extern unsigned int nr_cpu_ids;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
set_nr_cpu_ids(unsigned int nr)
{
	nr_cpu_ids = nr;
}
extern struct cpumask __cpu_possible_mask;
extern struct cpumask __cpu_online_mask;
extern struct cpumask __cpu_enabled_mask;
extern struct cpumask __cpu_present_mask;
extern struct cpumask __cpu_active_mask;
extern struct cpumask __cpu_dying_mask;

extern atomic_t __num_online_cpus;

extern cpumask_t cpus_booted_once_mask;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpu_max_bits_warn(unsigned int cpu, unsigned int bits)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_check(unsigned int cpu)
{
	cpu_max_bits_warn(cpu, ((unsigned int)64));
	return cpu;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_first(const struct cpumask *srcp)
{
	return find_first_bit(((srcp)->bits), ((unsigned int)64));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_first_zero(const struct cpumask *srcp)
{
	return find_first_zero_bit(((srcp)->bits), ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_first_and(const struct cpumask *srcp1, const struct cpumask *srcp2)
{
	return find_first_and_bit(((srcp1)->bits), ((srcp2)->bits),
				  ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_first_and_and(const struct cpumask *srcp1, const struct cpumask *srcp2,
		      const struct cpumask *srcp3)
{
	return find_first_and_and_bit(((srcp1)->bits), ((srcp2)->bits),
				      ((srcp3)->bits), ((unsigned int)64));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_last(const struct cpumask *srcp)
{
	return find_last_bit(((srcp)->bits), ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_next(int n, const struct cpumask *srcp)
{
	if (n != -1)
		cpumask_check(n);
	return find_next_bit(((srcp)->bits), ((unsigned int)64), n + 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_next_zero(int n, const struct cpumask *srcp)
{
	if (n != -1)
		cpumask_check(n);
	return find_next_zero_bit(((srcp)->bits), ((unsigned int)64), n + 1);
}
unsigned int cpumask_local_spread(unsigned int i, int node);
unsigned int cpumask_any_and_distribute(const struct cpumask *src1p,
					const struct cpumask *src2p);
unsigned int cpumask_any_distribute(const struct cpumask *srcp);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_next_and(int n, const struct cpumask *src1p,
		 const struct cpumask *src2p)
{
	if (n != -1)
		cpumask_check(n);
	return find_next_and_bit(((src1p)->bits), ((src2p)->bits),
				 ((unsigned int)64), n + 1);
}
unsigned int __attribute__((__pure__))
cpumask_next_wrap(int n, const struct cpumask *mask, int start, bool wrap);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_any_but(const struct cpumask *mask, unsigned int cpu)
{
	unsigned int i;

	cpumask_check(cpu);
	for ((i) = 0;
	     (i) = find_next_bit((((mask)->bits)), (((unsigned int)64)), (i)),
	    (i) < (((unsigned int)64));
	     (i)++)
		if (i != cpu)
			break;
	return i;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_any_and_but(const struct cpumask *mask1, const struct cpumask *mask2,
		    unsigned int cpu)
{
	unsigned int i;

	cpumask_check(cpu);
	i = cpumask_first_and(mask1, mask2);
	if (i != cpu)
		return i;

	return cpumask_next_and(cpu, mask1, mask2);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_nth(unsigned int cpu, const struct cpumask *srcp)
{
	return find_nth_bit(((srcp)->bits), ((unsigned int)64),
			    cpumask_check(cpu));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_nth_and(unsigned int cpu, const struct cpumask *srcp1,
		const struct cpumask *srcp2)
{
	return find_nth_and_bit(((srcp1)->bits), ((srcp2)->bits),
				((unsigned int)64), cpumask_check(cpu));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_nth_andnot(unsigned int cpu, const struct cpumask *srcp1,
		   const struct cpumask *srcp2)
{
	return find_nth_andnot_bit(((srcp1)->bits), ((srcp2)->bits),
				   ((unsigned int)64), cpumask_check(cpu));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_nth_and_andnot(unsigned int cpu, const struct cpumask *srcp1,
		       const struct cpumask *srcp2, const struct cpumask *srcp3)
{
	return find_nth_and_andnot_bit(((srcp1)->bits), ((srcp2)->bits),
				       ((srcp3)->bits), ((unsigned int)64),
				       cpumask_check(cpu));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)
{
	set_bit(cpumask_check(cpu), ((dstp)->bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)
{
	((__builtin_constant_p(cpumask_check(cpu)) &&
	  __builtin_constant_p((uintptr_t)(((dstp)->bits)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(((dstp)->bits)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(*(const unsigned long *)(((dstp)->bits)))) ?
		 generic___set_bit(cpumask_check(cpu), ((dstp)->bits)) :
		 ___set_bit(cpumask_check(cpu), ((dstp)->bits)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpumask_clear_cpu(int cpu, struct cpumask *dstp)
{
	clear_bit(cpumask_check(cpu), ((dstp)->bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__cpumask_clear_cpu(int cpu, struct cpumask *dstp)
{
	((__builtin_constant_p(cpumask_check(cpu)) &&
	  __builtin_constant_p((uintptr_t)(((dstp)->bits)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(((dstp)->bits)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(*(const unsigned long *)(((dstp)->bits)))) ?
		 generic___clear_bit(cpumask_check(cpu), ((dstp)->bits)) :
		 ___clear_bit(cpumask_check(cpu), ((dstp)->bits)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpumask_assign_cpu(int cpu, struct cpumask *dstp, bool value)
{
	((value) ? set_bit((cpumask_check(cpu)), (((dstp)->bits))) :
		   clear_bit((cpumask_check(cpu)), (((dstp)->bits))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__cpumask_assign_cpu(int cpu, struct cpumask *dstp, bool value)
{
	((value) ?
		 ((__builtin_constant_p((cpumask_check(cpu))) &&
		   __builtin_constant_p((uintptr_t)((((dstp)->bits))) !=
					(uintptr_t)((void *)0)) &&
		   (uintptr_t)((((dstp)->bits))) != (uintptr_t)((void *)0) &&
		   __builtin_constant_p(
			   *(const unsigned long *)((((dstp)->bits))))) ?
			  generic___set_bit((cpumask_check(cpu)),
					    (((dstp)->bits))) :
			  ___set_bit((cpumask_check(cpu)), (((dstp)->bits)))) :
		 ((__builtin_constant_p((cpumask_check(cpu))) &&
		   __builtin_constant_p((uintptr_t)((((dstp)->bits))) !=
					(uintptr_t)((void *)0)) &&
		   (uintptr_t)((((dstp)->bits))) != (uintptr_t)((void *)0) &&
		   __builtin_constant_p(
			   *(const unsigned long *)((((dstp)->bits))))) ?
			  generic___clear_bit((cpumask_check(cpu)),
					      (((dstp)->bits))) :
			  ___clear_bit((cpumask_check(cpu)),
				       (((dstp)->bits)))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_test_cpu(int cpu, const struct cpumask *cpumask)
{
	return ((__builtin_constant_p(cpumask_check(cpu)) &&
		 __builtin_constant_p((uintptr_t)((((cpumask))->bits)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)((((cpumask))->bits)) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(
			 *(const unsigned long *)((((cpumask))->bits)))) ?
			const_test_bit(cpumask_check(cpu),
				       (((cpumask))->bits)) :
			_test_bit(cpumask_check(cpu), (((cpumask))->bits)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_test_and_set_cpu(int cpu, struct cpumask *cpumask)
{
	return test_and_set_bit(cpumask_check(cpu), ((cpumask)->bits));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_test_and_clear_cpu(int cpu, struct cpumask *cpumask)
{
	return test_and_clear_bit(cpumask_check(cpu), ((cpumask)->bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpumask_setall(struct cpumask *dstp)
{
	if ((__builtin_constant_p(((unsigned int)64)) &&
	     (((unsigned int)64)) <= 64 && (((unsigned int)64)) > 0)) {
		((dstp)->bits)[0] = (~0UL >> (-(nr_cpu_ids) & (64 - 1)));
		return;
	}
	bitmap_fill(((dstp)->bits), nr_cpu_ids);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpumask_clear(struct cpumask *dstp)
{
	bitmap_zero(((dstp)->bits), ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_and(struct cpumask *dstp, const struct cpumask *src1p,
	    const struct cpumask *src2p)
{
	return bitmap_and(((dstp)->bits), ((src1p)->bits), ((src2p)->bits),
			  ((unsigned int)64));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpumask_or(struct cpumask *dstp, const struct cpumask *src1p,
	   const struct cpumask *src2p)
{
	bitmap_or(((dstp)->bits), ((src1p)->bits), ((src2p)->bits),
		  ((unsigned int)64));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpumask_xor(struct cpumask *dstp, const struct cpumask *src1p,
	    const struct cpumask *src2p)
{
	bitmap_xor(((dstp)->bits), ((src1p)->bits), ((src2p)->bits),
		   ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_andnot(struct cpumask *dstp, const struct cpumask *src1p,
	       const struct cpumask *src2p)
{
	return bitmap_andnot(((dstp)->bits), ((src1p)->bits), ((src2p)->bits),
			     ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_equal(const struct cpumask *src1p, const struct cpumask *src2p)
{
	return bitmap_equal(((src1p)->bits), ((src2p)->bits),
			    ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_or_equal(const struct cpumask *src1p, const struct cpumask *src2p,
		 const struct cpumask *src3p)
{
	return bitmap_or_equal(((src1p)->bits), ((src2p)->bits),
			       ((src3p)->bits), ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_intersects(const struct cpumask *src1p, const struct cpumask *src2p)
{
	return bitmap_intersects(((src1p)->bits), ((src2p)->bits),
				 ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_subset(const struct cpumask *src1p, const struct cpumask *src2p)
{
	return bitmap_subset(((src1p)->bits), ((src2p)->bits),
			     ((unsigned int)64));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_empty(const struct cpumask *srcp)
{
	return bitmap_empty(((srcp)->bits), ((unsigned int)64));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_full(const struct cpumask *srcp)
{
	return bitmap_full(((srcp)->bits), nr_cpu_ids);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_weight(const struct cpumask *srcp)
{
	return bitmap_weight(((srcp)->bits), ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_weight_and(const struct cpumask *srcp1, const struct cpumask *srcp2)
{
	return bitmap_weight_and(((srcp1)->bits), ((srcp2)->bits),
				 ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_weight_andnot(const struct cpumask *srcp1, const struct cpumask *srcp2)
{
	return bitmap_weight_andnot(((srcp1)->bits), ((srcp2)->bits),
				    ((unsigned int)64));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpumask_shift_right(struct cpumask *dstp, const struct cpumask *srcp, int n)
{
	bitmap_shift_right(((dstp)->bits), ((srcp)->bits), n,
			   ((unsigned int)64));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpumask_shift_left(struct cpumask *dstp, const struct cpumask *srcp, int n)
{
	bitmap_shift_left(((dstp)->bits), ((srcp)->bits), n, nr_cpu_ids);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
cpumask_copy(struct cpumask *dstp, const struct cpumask *srcp)
{
	bitmap_copy(((dstp)->bits), ((srcp)->bits), ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
cpumask_parse_user(const char *buf, int len, struct cpumask *dstp)
{
	return bitmap_parse_user(buf, len, ((dstp)->bits), nr_cpu_ids);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
cpumask_parselist_user(const char *buf, int len, struct cpumask *dstp)
{
	return bitmap_parselist_user(buf, len, ((dstp)->bits), nr_cpu_ids);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
cpumask_parse(const char *buf, struct cpumask *dstp)
{
	return bitmap_parse(buf, (~0U), ((dstp)->bits), nr_cpu_ids);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
cpulist_parse(const char *buf, struct cpumask *dstp)
{
	return bitmap_parselist(buf, ((dstp)->bits), nr_cpu_ids);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
cpumask_size(void)
{
	return (((((((unsigned int)64))) +
		  ((__typeof__((((unsigned int)64))))((64)) - 1)) &
		 ~((__typeof__((((unsigned int)64))))((64)) - 1)) /
		8);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)
{
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node)
{
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)
{
	cpumask_clear(*mask);
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node)
{
	cpumask_clear(*mask);
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
alloc_bootmem_cpumask_var(cpumask_var_t *mask)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
free_cpumask_var(cpumask_var_t mask)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
free_bootmem_cpumask_var(cpumask_var_t mask)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpumask_available(cpumask_var_t mask)
{
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__free_free_cpumask_var(void *p)
{
	struct cpumask *_T = *(struct cpumask **)p;
	if (_T)
		free_cpumask_var(_T);
};

extern const unsigned long cpu_all_bits[(((64) + ((sizeof(long) * 8)) - 1) /
					 ((sizeof(long) * 8)))];
void init_cpu_present(const struct cpumask *src);
void init_cpu_possible(const struct cpumask *src);
void init_cpu_online(const struct cpumask *src);
void set_cpu_online(unsigned int cpu, bool online);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
__check_is_bitmap(const unsigned long *bitmap)
{
	return 1;
}
extern const unsigned long cpu_bit_bitmap[64 + 1][(
	((64) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
const struct cpumask *
get_cpu_mask(unsigned int cpu)
{
	const unsigned long *p = cpu_bit_bitmap[1 + cpu % 64];
	p -= cpu / 64;
	return ((struct cpumask *)(1 ? (p) :
				       (void *)sizeof(__check_is_bitmap(p))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
num_online_cpus(void)
{
	return raw_atomic_read(&__num_online_cpus);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpu_online(unsigned int cpu)
{
	return cpumask_test_cpu(cpu,
				((const struct cpumask *)&__cpu_online_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpu_enabled(unsigned int cpu)
{
	return cpumask_test_cpu(cpu,
				((const struct cpumask *)&__cpu_enabled_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpu_possible(unsigned int cpu)
{
	return cpumask_test_cpu(cpu,
				((const struct cpumask *)&__cpu_possible_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpu_present(unsigned int cpu)
{
	return cpumask_test_cpu(cpu,
				((const struct cpumask *)&__cpu_present_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpu_active(unsigned int cpu)
{
	return cpumask_test_cpu(cpu,
				((const struct cpumask *)&__cpu_active_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
cpu_dying(unsigned int cpu)
{
	return cpumask_test_cpu(cpu,
				((const struct cpumask *)&__cpu_dying_mask));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) ssize_t
cpumap_print_to_pagebuf(bool list, char *buf, const struct cpumask *mask)
{
	return bitmap_print_to_pagebuf(list, buf, ((mask)->bits), nr_cpu_ids);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) ssize_t
cpumap_print_bitmask_to_buf(char *buf, const struct cpumask *mask, loff_t off,
			    size_t count)
{
	return bitmap_print_bitmask_to_buf(buf, ((mask)->bits), nr_cpu_ids, off,
					   count) -
	       1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) ssize_t
cpumap_print_list_to_buf(char *buf, const struct cpumask *mask, loff_t off,
			 size_t count)
{
	return bitmap_print_list_to_buf(buf, ((mask)->bits), nr_cpu_ids, off,
					count) -
	       1;
}

struct llist_head {
	struct llist_node *first;
};

struct llist_node {
	struct llist_node *next;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
init_llist_head(struct llist_head *list)
{
	list->first = ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
init_llist_node(struct llist_node *node)
{
	node->next = node;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
llist_on_list(const struct llist_node *node)
{
	return node->next != node;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
llist_empty(const struct llist_head *head)
{
	return ({
		       do {
			       __attribute__((__noreturn__)) extern void
			       __compiletime_assert_234(void) __attribute__((__error__(
				       "Unsupported access size for {READ,WRITE}_ONCE().")));
			       if (!((sizeof(head->first) == sizeof(char) ||
				      sizeof(head->first) == sizeof(short) ||
				      sizeof(head->first) == sizeof(int) ||
				      sizeof(head->first) == sizeof(long)) ||
				     sizeof(head->first) == sizeof(long long)))
				       __compiletime_assert_234();
		       } while (0);
		       (*(const volatile typeof(_Generic(
			       (head->first),
							char: (char)0,
							unsigned char: (
								unsigned char)0,
							signed char: (
								signed char)0,
							unsigned short: (
								unsigned short)0,
							signed short: (
								signed short)0,
							unsigned int: (
								unsigned int)0,
							signed int: (
								signed int)0,
							unsigned long: (
								unsigned long)0,
							signed long: (
								signed long)0,
							unsigned long long: (
								unsigned long long)0,
							signed long long: (
								signed long long)0,
							default: (head->first)))
				  *)&(head->first));
	       }) == ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct llist_node *
llist_next(struct llist_node *node)
{
	return node->next;
}

extern bool llist_add_batch(struct llist_node *new_first,
			    struct llist_node *new_last,
			    struct llist_head *head);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
		  struct llist_head *head)
{
	new_last->next = head->first;
	head->first = new_first;
	return new_last->next == ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
llist_add(struct llist_node *new, struct llist_head *head)
{
	return llist_add_batch(new, new, head);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__llist_add(struct llist_node *new, struct llist_head *head)
{
	return __llist_add_batch(new, new, head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct llist_node *
llist_del_all(struct llist_head *head)
{
	return ({
		typeof(&head->first) __ai_ptr = (&head->first);
		do {
		} while (0);
		instrument_atomic_read_write(__ai_ptr, sizeof(*__ai_ptr));
		({
			__typeof__(*((__ai_ptr))) __ret = ((((void *)0)));
			switch (sizeof(*((__ai_ptr)))) {
			case 1:
				asm volatile(""
					     "xchg"
					     "b %b0, %1\n"
					     : "+q"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 2:
				asm volatile(""
					     "xchg"
					     "w %w0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 4:
				asm volatile(""
					     "xchg"
					     "l %0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 8:
				asm volatile(""
					     "xchg"
					     "q %q0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			default:
				__xchg_wrong_size();
			}
			__ret;
		});
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct llist_node *
__llist_del_all(struct llist_head *head)
{
	struct llist_node *first = head->first;

	head->first = ((void *)0);
	return first;
}

extern struct llist_node *llist_del_first(struct llist_head *head);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct llist_node *
llist_del_first_init(struct llist_head *head)
{
	struct llist_node *n = llist_del_first(head);

	if (n)
		init_llist_node(n);
	return n;
}

extern bool llist_del_first_this(struct llist_head *head,
				 struct llist_node *this);

struct llist_node *llist_reverse_order(struct llist_node *head);

enum {
	CSD_FLAG_LOCK = 0x01,

	IRQ_WORK_PENDING = 0x01,
	IRQ_WORK_BUSY = 0x02,
	IRQ_WORK_LAZY = 0x04,
	IRQ_WORK_HARD_IRQ = 0x08,

	IRQ_WORK_CLAIMED = (IRQ_WORK_PENDING | IRQ_WORK_BUSY),

	CSD_TYPE_ASYNC = 0x00,
	CSD_TYPE_SYNC = 0x10,
	CSD_TYPE_IRQ_WORK = 0x20,
	CSD_TYPE_TTWU = 0x30,

	CSD_FLAG_TYPE_MASK = 0xF0,
};
struct __call_single_node {
	struct llist_node llist;
	union {
		unsigned int u_flags;
		atomic_t a_flags;
	};

	u16 src, dst;
};

typedef void (*smp_call_func_t)(void *info);
typedef bool (*smp_cond_func_t)(int cpu, void *info);

struct __call_single_data {
	struct __call_single_node node;
	smp_call_func_t func;
	void *info;
};

typedef struct __call_single_data call_single_data_t
	__attribute__((__aligned__(sizeof(struct __call_single_data))));
extern void __smp_call_single_queue(int cpu, struct llist_node *node);

extern unsigned int total_cpus;

int smp_call_function_single(int cpuid, smp_call_func_t func, void *info,
			     int wait);

void on_each_cpu_cond_mask(smp_cond_func_t cond_func, smp_call_func_t func,
			   void *info, bool wait, const struct cpumask *mask);

int smp_call_function_single_async(int cpu, call_single_data_t *csd);

void __attribute__((__noreturn__)) panic_smp_self_stop(void);
void __attribute__((__noreturn__)) nmi_panic_self_stop(struct pt_regs *regs);
void crash_smp_send_stop(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
on_each_cpu(smp_call_func_t func, void *info, int wait)
{
	on_each_cpu_cond_mask(((void *)0), func, info, wait,
			      ((const struct cpumask *)&__cpu_online_mask));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
on_each_cpu_mask(const struct cpumask *mask, smp_call_func_t func, void *info,
		 bool wait)
{
	on_each_cpu_cond_mask(((void *)0), func, info, wait, mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
on_each_cpu_cond(smp_cond_func_t cond_func, smp_call_func_t func, void *info,
		 bool wait)
{
	on_each_cpu_cond_mask(cond_func, func, info, wait,
			      ((const struct cpumask *)&__cpu_online_mask));
}

void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
smp_prepare_boot_cpu(void);

extern void setup_cpu_local_masks(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
arch_cpu_online(int cpu)
{
	return arch_test_bit(
		cpu, ((((const struct cpumask *)&__cpu_online_mask))->bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
arch_cpumask_clear_cpu(int cpu, struct cpumask *dstp)
{
	arch_clear_bit(cpumask_check(cpu), ((dstp)->bits));
}

extern __attribute__((
	section(".data..percpu"
		"..read_mostly"))) __typeof__(cpumask_var_t) cpu_sibling_map;
extern __attribute__((
	section(".data..percpu"
		"..read_mostly"))) __typeof__(cpumask_var_t) cpu_core_map;
extern __attribute__((
	section(".data..percpu"
		"..read_mostly"))) __typeof__(cpumask_var_t) cpu_die_map;

extern __attribute__((
	section(".data..percpu"
		"..read_mostly"))) __typeof__(cpumask_var_t) cpu_llc_shared_map;
extern __attribute__((
	section(".data..percpu"
		"..read_mostly"))) __typeof__(cpumask_var_t) cpu_l2c_shared_map;

extern __attribute__((
	section(".data..percpu"
		"..read_mostly"))) __typeof__(u32) x86_cpu_to_apicid;
extern __typeof__(u32) *x86_cpu_to_apicid_early_ptr;
extern __typeof__(u32) x86_cpu_to_apicid_early_map[];
extern __attribute__((
	section(".data..percpu"
		"..read_mostly"))) __typeof__(u32) x86_cpu_to_acpiid;
extern __typeof__(u32) *x86_cpu_to_acpiid_early_ptr;
extern __typeof__(u32) x86_cpu_to_acpiid_early_map[];

struct task_struct;

struct smp_ops {
	void (*smp_prepare_boot_cpu)(void);
	void (*smp_prepare_cpus)(unsigned max_cpus);
	void (*smp_cpus_done)(unsigned max_cpus);

	void (*stop_other_cpus)(int wait);
	void (*crash_stop_other_cpus)(void);
	void (*smp_send_reschedule)(int cpu);

	void (*cleanup_dead_cpu)(unsigned cpu);
	void (*poll_sync_state)(void);
	int (*kick_ap_alive)(unsigned cpu, struct task_struct *tidle);
	int (*cpu_disable)(void);
	void (*cpu_die)(unsigned int cpu);
	void (*play_dead)(void);
	void (*stop_this_cpu)(void);

	void (*send_call_func_ipi)(const struct cpumask *mask);
	void (*send_call_func_single_ipi)(int cpu);
};

extern void set_cpu_sibling_map(int cpu);

extern struct smp_ops smp_ops;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
smp_send_stop(void)
{
	smp_ops.stop_other_cpus(0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
stop_other_cpus(void)
{
	smp_ops.stop_other_cpus(1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
smp_prepare_cpus(unsigned int max_cpus)
{
	smp_ops.smp_prepare_cpus(max_cpus);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
smp_cpus_done(unsigned int max_cpus)
{
	smp_ops.smp_cpus_done(max_cpus);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__cpu_disable(void)
{
	return smp_ops.cpu_disable();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__cpu_die(unsigned int cpu)
{
	if (smp_ops.cpu_die)
		smp_ops.cpu_die(cpu);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void __attribute__((__noreturn__))
play_dead(void)
{
	smp_ops.play_dead();
	do {
		({
			asm volatile("235"
				     ": nop\n\t"
				     ".pushsection .discard.instr_begin\n\t"
				     ".long "
				     "235"
				     "b - .\n\t"
				     ".popsection\n\t"
				     :
				     : "i"(235));
		});
		do {
			asm __inline volatile(
				"1:\t"
				".byte 0x0f, 0x0b"
				"\n"
				".pushsection __bug_table,\"aw\"\n"
				"2:\t"
				".long "
				"1b"
				" - ."
				"\t# bug_entry::bug_addr\n"
				"\t"
				".long "
				"%c0"
				" - ."
				"\t# bug_entry::file\n"
				"\t.word %c1"
				"\t# bug_entry::line\n"
				"\t.word %c2"
				"\t# bug_entry::flags\n"
				"\t.org 2b+%c3\n"
				".popsection\n"
				""
				:
				: "i"("arch/x86/include/asm/smp.h"), "i"(84),
				  "i"(0), "i"(sizeof(struct bug_entry)));
		} while (0);
		__builtin_unreachable();
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
arch_smp_send_reschedule(int cpu)
{
	smp_ops.smp_send_reschedule(cpu);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
arch_send_call_function_single_ipi(int cpu)
{
	smp_ops.send_call_func_single_ipi(cpu);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
arch_send_call_function_ipi_mask(const struct cpumask *mask)
{
	smp_ops.send_call_func_ipi(mask);
}

void cpu_disable_common(void);
void native_smp_prepare_boot_cpu(void);
void smp_prepare_cpus_common(void);
void native_smp_prepare_cpus(unsigned int max_cpus);
void native_smp_cpus_done(unsigned int max_cpus);
int common_cpu_up(unsigned int cpunum, struct task_struct *tidle);
int native_kick_ap(unsigned int cpu, struct task_struct *tidle);
int native_cpu_disable(void);
void __attribute__((__noreturn__)) hlt_play_dead(void);
void native_play_dead(void);
void play_dead_common(void);
void wbinvd_on_cpu(int cpu);
int wbinvd_on_all_cpus(void);

void smp_kick_mwait_play_dead(void);

void native_smp_send_reschedule(int cpu);
void native_send_call_func_ipi(const struct cpumask *mask);
void native_send_call_func_single_ipi(int cpu);

void smp_store_cpu_info(int id);

void smp_reboot_interrupt(void);
void smp_reschedule_interrupt(struct pt_regs *regs);
void smp_call_function_interrupt(struct pt_regs *regs);
void smp_call_function_single_interrupt(struct pt_regs *r);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct cpumask *
cpu_llc_shared_mask(int cpu)
{
	return (*({
		do {
			const void *__vpp_verify =
				(typeof((&(cpu_llc_shared_map)) +
					0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((
				typeof(*((&(cpu_llc_shared_map)))) *)((
				&(cpu_llc_shared_map))));
			(typeof((typeof(*((&(cpu_llc_shared_map)))) *)((
				&(cpu_llc_shared_map)))))(__ptr +
							  (((__per_cpu_offset[(
								  cpu)]))));
		});
	}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct cpumask *
cpu_l2c_shared_mask(int cpu)
{
	return (*({
		do {
			const void *__vpp_verify =
				(typeof((&(cpu_l2c_shared_map)) +
					0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((
				typeof(*((&(cpu_l2c_shared_map)))) *)((
				&(cpu_l2c_shared_map))));
			(typeof((typeof(*((&(cpu_l2c_shared_map)))) *)((
				&(cpu_l2c_shared_map)))))(__ptr +
							  (((__per_cpu_offset[(
								  cpu)]))));
		});
	}));
}
extern unsigned int smpboot_control;
extern unsigned long apic_mmio_base;
extern void smp_send_stop(void);

extern void arch_smp_send_reschedule(int cpu);
extern void smp_prepare_cpus(unsigned int max_cpus);

extern int __cpu_up(unsigned int cpunum, struct task_struct *tidle);

extern void smp_cpus_done(unsigned int max_cpus);

void smp_call_function(smp_call_func_t func, void *info, int wait);
void smp_call_function_many(const struct cpumask *mask, smp_call_func_t func,
			    void *info, bool wait);

int smp_call_function_any(const struct cpumask *mask, smp_call_func_t func,
			  void *info, int wait);

void kick_all_cpus_sync(void);
void wake_up_all_idle_cpus(void);

void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
call_function_init(void);
void generic_smp_call_function_single_interrupt(void);

extern unsigned int setup_max_cpus;
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
setup_nr_cpu_ids(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
smp_init(void);

extern int __boot_cpu_id;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_boot_cpu_id(void)
{
	return __boot_cpu_id;
}
extern void arch_disable_smp_support(void);

extern void arch_thaw_secondary_cpus_begin(void);
extern void arch_thaw_secondary_cpus_end(void);

void smp_setup_processor_id(void);

int smp_call_on_cpu(unsigned int cpu, int (*func)(void *), void *par,
		    bool phys);

int smpcfd_prepare_cpu(unsigned int cpu);
int smpcfd_dead_cpu(unsigned int cpu);
int smpcfd_dying_cpu(unsigned int cpu);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
csd_lock_is_stuck(void)
{
	return false;
}

struct task_struct;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_init_task(struct task_struct *task)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_off(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_on(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_set_selftest_task(struct task_struct *task)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_register_key(struct lock_class_key *key)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_unregister_key(struct lock_class_key *key)
{
}

extern int lock_is_held(const void *);
extern int lockdep_is_held(const void *);
enum xhlock_context_t {
	XHLOCK_HARD,
	XHLOCK_SOFT,
	XHLOCK_CTX_NR,
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_invariant_state(bool force)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_free_task(struct task_struct *task)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
print_irqtrace_events(struct task_struct *curr)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_rcu_suspicious(const char *file, const int line, const char *s)
{
}

typedef struct spinlock {
	union {
		struct raw_spinlock rlock;
	};
} spinlock_t;
typedef struct {
	arch_rwlock_t raw_lock;

} rwlock_t;

struct mm_struct;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
encode_frame_pointer(struct pt_regs *regs)
{
	return 0;
}

u64 dummy_steal_clock(int cpu);
u64 dummy_sched_clock(void);

extern struct static_call_key __SCK__pv_steal_clock;
extern typeof(dummy_steal_clock) __SCT__pv_steal_clock;
;
extern struct static_call_key __SCK__pv_sched_clock;
extern typeof(dummy_sched_clock) __SCT__pv_sched_clock;
;

void paravirt_set_sched_clock(u64 (*func)(void));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
paravirt_sched_clock(void)
{
	return ({
		static void *__attribute__((__used__))
		__attribute__((__section__(".discard.addressable")))
		__UNIQUE_ID___addressable___SCK__pv_sched_clock236 =
			(void *)(uintptr_t)&__SCK__pv_sched_clock;
		;
		(&__SCT__pv_sched_clock);
	})();
}

struct static_key;
extern struct static_key paravirt_steal_enabled;
extern struct static_key paravirt_steal_rq_enabled;

void __native_queued_spin_unlock(struct qspinlock *lock);
bool pv_is_native_spin_unlock(void);
bool __native_vcpu_is_preempted(long cpu);
bool pv_is_native_vcpu_is_preempted(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
paravirt_steal_clock(int cpu)
{
	return ({
		static void *__attribute__((__used__))
		__attribute__((__section__(".discard.addressable")))
		__UNIQUE_ID___addressable___SCK__pv_steal_clock237 =
			(void *)(uintptr_t)&__SCK__pv_steal_clock;
		;
		(&__SCT__pv_steal_clock);
	})(cpu);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
slow_down_io(void)
{
	(void)({
		unsigned long __edi = __edi, __esi = __esi, __edx = __edx,
			      __ecx = __ecx, __eax = __eax;
		;
		((void)pv_ops.cpu.io_delay);
		asm volatile("# ALT: oldinstr\n"
			     "771:\n\t"
			     "999:\n\t"
			     ".pushsection .discard.retpoline_safe\n\t"
			     ".long 999b\n\t"
			     ".popsection\n\t"
			     "call *%[paravirt_opptr];"
			     "\n772:\n"
			     "# ALT: padding\n"
			     ".skip -((("
			     "775f-774f"
			     ")-("
			     "772b-771b"
			     ")) > 0) * "
			     "(("
			     "775f-774f"
			     ")-("
			     "772b-771b"
			     ")),0x90\n"
			     "773:\n"
			     ".pushsection .altinstructions,\"a\"\n"
			     " .long 771b - .\n"
			     " .long 774f - .\n"
			     " .4byte "
			     "(((1 << 1) << 16) | (( 3*32+21)))"
			     "\n"
			     " .byte "
			     "773b-771b"
			     "\n"
			     " .byte "
			     "775f-774f"
			     "\n"
			     ".popsection\n"
			     ".pushsection .altinstr_replacement, \"ax\"\n"
			     "# ALT: replacement\n"
			     "774:\n\t"
			     "call BUG_func"
			     "\n775:\n"
			     ".popsection\n"
			     : "=D"(__edi), "=S"(__esi), "=d"(__edx),
			       "=c"(__ecx), "+r"(current_stack_pointer)
			     : [paravirt_opptr] "m"(pv_ops.cpu.io_delay)
			     : "memory", "cc", "rax", "r8", "r9", "r10", "r11");
		;
	});
}

void native_flush_tlb_local(void);
void native_flush_tlb_global(void);
void native_flush_tlb_one_user(unsigned long addr);
void native_flush_tlb_multi(const struct cpumask *cpumask,
			    const struct flush_tlb_info *info);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__flush_tlb_local(void)
{
	(void)({
		unsigned long __edi = __edi, __esi = __esi, __edx = __edx,
			      __ecx = __ecx, __eax = __eax;
		;
		((void)pv_ops.mmu.flush_tlb_user);
		asm volatile("# ALT: oldinstr\n"
			     "771:\n\t"
			     "999:\n\t"
			     ".pushsection .discard.retpoline_safe\n\t"
			     ".long 999b\n\t"
			     ".popsection\n\t"
			     "call *%[paravirt_opptr];"
			     "\n772:\n"
			     "# ALT: padding\n"
			     ".skip -((("
			     "775f-774f"
			     ")-("
			     "772b-771b"
			     ")) > 0) * "
			     "(("
			     "775f-774f"
			     ")-("
			     "772b-771b"
			     ")),0x90\n"
			     "773:\n"
			     ".pushsection .altinstructions,\"a\"\n"
			     " .long 771b - .\n"
			     " .long 774f - .\n"
			     " .4byte "
			     "(((1 << 1) << 16) | (( 3*32+21)))"
			     "\n"
			     " .byte "
			     "773b-771b"
			     "\n"
			     " .byte "
			     "775f-774f"
			     "\n"
			     ".popsection\n"
			     ".pushsection .altinstr_replacement, \"ax\"\n"
			     "# ALT: replacement\n"
			     "774:\n\t"
			     "call BUG_func"
			     "\n775:\n"
			     ".popsection\n"
			     : "=D"(__edi), "=S"(__esi), "=d"(__edx),
			       "=c"(__ecx), "+r"(current_stack_pointer)
			     : [paravirt_opptr] "m"(pv_ops.mmu.flush_tlb_user)
			     : "memory", "cc", "rax", "r8", "r9", "r10", "r11");
		;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__flush_tlb_global(void)
{
	(void)({
		unsigned long __edi = __edi, __esi = __esi, __edx = __edx,
			      __ecx = __ecx, __eax = __eax;
		;
		((void)pv_ops.mmu.flush_tlb_kernel);
		asm volatile("# ALT: oldinstr\n"
			     "771:\n\t"
			     "999:\n\t"
			     ".pushsection .discard.retpoline_safe\n\t"
			     ".long 999b\n\t"
			     ".popsection\n\t"
			     "call *%[paravirt_opptr];"
			     "\n772:\n"
			     "# ALT: padding\n"
			     ".skip -((("
			     "775f-774f"
			     ")-("
			     "772b-771b"
			     ")) > 0) * "
			     "(("
			     "775f-774f"
			     ")-("
			     "772b-771b"
			     ")),0x90\n"
			     "773:\n"
			     ".pushsection .altinstructions,\"a\"\n"
			     " .long 771b - .\n"
			     " .long 774f - .\n"
			     " .4byte "
			     "(((1 << 1) << 16) | (( 3*32+21)))"
			     "\n"
			     " .byte "
			     "773b-771b"
			     "\n"
			     " .byte "
			     "775f-774f"
			     "\n"
			     ".popsection\n"
			     ".pushsection .altinstr_replacement, \"ax\"\n"
			     "# ALT: replacement\n"
			     "774:\n\t"
			     "call BUG_func"
			     "\n775:\n"
			     ".popsection\n"
			     : "=D"(__edi), "=S"(__esi), "=d"(__edx),
			       "=c"(__ecx), "+r"(current_stack_pointer)
			     : [paravirt_opptr] "m"(pv_ops.mmu.flush_tlb_kernel)
			     : "memory", "cc", "rax", "r8", "r9", "r10", "r11");
		;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__flush_tlb_one_user(unsigned long addr)
{
	(void)({
		unsigned long __edi = __edi, __esi = __esi, __edx = __edx,
			      __ecx = __ecx, __eax = __eax;
		;
		((void)pv_ops.mmu.flush_tlb_one_user);
		asm volatile(
			"# ALT: oldinstr\n"
			"771:\n\t"
			"999:\n\t"
			".pushsection .discard.retpoline_safe\n\t"
			".long 999b\n\t"
			".popsection\n\t"
			"call *%[paravirt_opptr];"
			"\n772:\n"
			"# ALT: padding\n"
			".skip -((("
			"775f-774f"
			")-("
			"772b-771b"
			")) > 0) * "
			"(("
			"775f-774f"
			")-("
			"772b-771b"
			")),0x90\n"
			"773:\n"
			".pushsection .altinstructions,\"a\"\n"
			" .long 771b - .\n"
			" .long 774f - .\n"
			" .4byte "
			"(((1 << 1) << 16) | (( 3*32+21)))"
			"\n"
			" .byte "
			"773b-771b"
			"\n"
			" .byte "
			"775f-774f"
			"\n"
			".popsection\n"
			".pushsection .altinstr_replacement, \"ax\"\n"
			"# ALT: replacement\n"
			"774:\n\t"
			"call BUG_func"
			"\n775:\n"
			".popsection\n"
			: "=D"(__edi), "=S"(__esi), "=d"(__edx), "=c"(__ecx),
			  "+r"(current_stack_pointer)
			: [paravirt_opptr] "m"(pv_ops.mmu.flush_tlb_one_user),
			  "D"((unsigned long)(addr))
			: "memory", "cc", "rax", "r8", "r9", "r10", "r11");
		;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__flush_tlb_multi(const struct cpumask *cpumask,
		  const struct flush_tlb_info *info)
{
	(void)({
		unsigned long __edi = __edi, __esi = __esi, __edx = __edx,
			      __ecx = __ecx, __eax = __eax;
		;
		((void)pv_ops.mmu.flush_tlb_multi);
		asm volatile("# ALT: oldinstr\n"
			     "771:\n\t"
			     "999:\n\t"
			     ".pushsection .discard.retpoline_safe\n\t"
			     ".long 999b\n\t"
			     ".popsection\n\t"
			     "call *%[paravirt_opptr];"
			     "\n772:\n"
			     "# ALT: padding\n"
			     ".skip -((("
			     "775f-774f"
			     ")-("
			     "772b-771b"
			     ")) > 0) * "
			     "(("
			     "775f-774f"
			     ")-("
			     "772b-771b"
			     ")),0x90\n"
			     "773:\n"
			     ".pushsection .altinstructions,\"a\"\n"
			     " .long 771b - .\n"
			     " .long 774f - .\n"
			     " .4byte "
			     "(((1 << 1) << 16) | (( 3*32+21)))"
			     "\n"
			     " .byte "
			     "773b-771b"
			     "\n"
			     " .byte "
			     "775f-774f"
			     "\n"
			     ".popsection\n"
			     ".pushsection .altinstr_replacement, \"ax\"\n"
			     "# ALT: replacement\n"
			     "774:\n\t"
			     "call BUG_func"
			     "\n775:\n"
			     ".popsection\n"
			     : "=D"(__edi), "=S"(__esi), "=d"(__edx),
			       "=c"(__ecx), "+r"(current_stack_pointer)
			     : [paravirt_opptr] "m"(pv_ops.mmu.flush_tlb_multi),
			       "D"((unsigned long)(cpumask)),
			       "S"((unsigned long)(info))
			     : "memory", "cc", "rax", "r8", "r9", "r10", "r11");
		;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
paravirt_tlb_remove_table(struct mmu_gather *tlb, void *table)
{
	(void)({
		unsigned long __edi = __edi, __esi = __esi, __edx = __edx,
			      __ecx = __ecx, __eax = __eax;
		;
		((void)pv_ops.mmu.tlb_remove_table);
		asm volatile(
			"# ALT: oldinstr\n"
			"771:\n\t"
			"999:\n\t"
			".pushsection .discard.retpoline_safe\n\t"
			".long 999b\n\t"
			".popsection\n\t"
			"call *%[paravirt_opptr];"
			"\n772:\n"
			"# ALT: padding\n"
			".skip -((("
			"775f-774f"
			")-("
			"772b-771b"
			")) > 0) * "
			"(("
			"775f-774f"
			")-("
			"772b-771b"
			")),0x90\n"
			"773:\n"
			".pushsection .altinstructions,\"a\"\n"
			" .long 771b - .\n"
			" .long 774f - .\n"
			" .4byte "
			"(((1 << 1) << 16) | (( 3*32+21)))"
			"\n"
			" .byte "
			"773b-771b"
			"\n"
			" .byte "
			"775f-774f"
			"\n"
			".popsection\n"
			".pushsection .altinstr_replacement, \"ax\"\n"
			"# ALT: replacement\n"
			"774:\n\t"
			"call BUG_func"
			"\n775:\n"
			".popsection\n"
			: "=D"(__edi), "=S"(__esi), "=d"(__edx), "=c"(__ecx),
			  "+r"(current_stack_pointer)
			: [paravirt_opptr] "m"(pv_ops.mmu.tlb_remove_table),
			  "D"((unsigned long)(tlb)), "S"((unsigned long)(table))
			: "memory", "cc", "rax", "r8", "r9", "r10", "r11");
		;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
paravirt_arch_exit_mmap(struct mm_struct *mm)
{
	(void)({
		unsigned long __edi = __edi, __esi = __esi, __edx = __edx,
			      __ecx = __ecx, __eax = __eax;
		;
		((void)pv_ops.mmu.exit_mmap);
		asm volatile("# ALT: oldinstr\n"
			     "771:\n\t"
			     "999:\n\t"
			     ".pushsection .discard.retpoline_safe\n\t"
			     ".long 999b\n\t"
			     ".popsection\n\t"
			     "call *%[paravirt_opptr];"
			     "\n772:\n"
			     "# ALT: padding\n"
			     ".skip -((("
			     "775f-774f"
			     ")-("
			     "772b-771b"
			     ")) > 0) * "
			     "(("
			     "775f-774f"
			     ")-("
			     "772b-771b"
			     ")),0x90\n"
			     "773:\n"
			     ".pushsection .altinstructions,\"a\"\n"
			     " .long 771b - .\n"
			     " .long 774f - .\n"
			     " .4byte "
			     "(((1 << 1) << 16) | (( 3*32+21)))"
			     "\n"
			     " .byte "
			     "773b-771b"
			     "\n"
			     " .byte "
			     "775f-774f"
			     "\n"
			     ".popsection\n"
			     ".pushsection .altinstr_replacement, \"ax\"\n"
			     "# ALT: replacement\n"
			     "774:\n\t"
			     "call BUG_func"
			     "\n775:\n"
			     ".popsection\n"
			     : "=D"(__edi), "=S"(__esi), "=d"(__edx),
			       "=c"(__ecx), "+r"(current_stack_pointer)
			     : [paravirt_opptr] "m"(pv_ops.mmu.exit_mmap),
			       "D"((unsigned long)(mm))
			     : "memory", "cc", "rax", "r8", "r9", "r10", "r11");
		;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
notify_page_enc_status_changed(unsigned long pfn, int npages, bool enc)
{
	(void)({
		unsigned long __edi = __edi, __esi = __esi, __edx = __edx,
			      __ecx = __ecx, __eax = __eax;
		;
		((void)pv_ops.mmu.notify_page_enc_status_changed);
		asm volatile(
			"# ALT: oldinstr\n"
			"771:\n\t"
			"999:\n\t"
			".pushsection .discard.retpoline_safe\n\t"
			".long 999b\n\t"
			".popsection\n\t"
			"call *%[paravirt_opptr];"
			"\n772:\n"
			"# ALT: padding\n"
			".skip -((("
			"775f-774f"
			")-("
			"772b-771b"
			")) > 0) * "
			"(("
			"775f-774f"
			")-("
			"772b-771b"
			")),0x90\n"
			"773:\n"
			".pushsection .altinstructions,\"a\"\n"
			" .long 771b - .\n"
			" .long 774f - .\n"
			" .4byte "
			"(((1 << 1) << 16) | (( 3*32+21)))"
			"\n"
			" .byte "
			"773b-771b"
			"\n"
			" .byte "
			"775f-774f"
			"\n"
			".popsection\n"
			".pushsection .altinstr_replacement, \"ax\"\n"
			"# ALT: replacement\n"
			"774:\n\t"
			"call BUG_func"
			"\n775:\n"
			".popsection\n"
			: "=D"(__edi), "=S"(__esi), "=d"(__edx), "=c"(__ecx),
			  "+r"(current_stack_pointer)
			: [paravirt_opptr] "m"(
				  pv_ops.mmu.notify_page_enc_status_changed),
			  "D"((unsigned long)(pfn)),
			  "S"((unsigned long)(npages)),
			  "d"((unsigned long)(enc))
			: "memory", "cc", "rax", "r8", "r9", "r10", "r11");
		;
	});
}
extern void default_banner(void);
void native_pv_lock_init(void) __attribute__((__section__(".init.text")))
__attribute__((__cold__));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
paravirt_enter_mmap(struct mm_struct *mm)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
paravirt_set_cap(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u32
queued_fetch_set_pending_acquire(struct qspinlock *lock)
{
	u32 val;

	val = ({
		      bool c;
		      asm volatile(".pushsection .smp_locks,\"a\"\n"
				   ".balign 4\n"
				   ".long 671f - .\n"
				   ".popsection\n"
				   "671:"
				   "\n\tlock; "
				   "btsl"
				   " %[val], "
				   "%[var]"
				   "\n\t/* output condition code "
				   "c"
				   "*/\n"
				   : [var] "+m"(lock->val.counter), "=@cc"
								    "c"(c)
				   : [val] "I"((0 + 8))
				   : "memory");
		      c;
	      }) *
	      (1U << (0 + 8));

	val |= atomic_read(&lock->val) & ~(((1U << 8) - 1) << (0 + 8));

	return val;
}
extern struct static_key_false virt_spin_lock_key;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
virt_spin_lock(struct qspinlock *lock)
{
	int val;

	if (!({
		    bool branch;
		    if (__builtin_types_compatible_p(
				typeof(*&virt_spin_lock_key),
				struct static_key_true))
			    branch = !arch_static_branch(
				    &(&virt_spin_lock_key)->key, true);
		    else if (__builtin_types_compatible_p(
				     typeof(*&virt_spin_lock_key),
				     struct static_key_false))
			    branch = !arch_static_branch_jump(
				    &(&virt_spin_lock_key)->key, true);
		    else
			    branch = ____wrong_branch_error();
		    __builtin_expect(!!(branch), 1);
	    }))
		return false;

__retry:
	val = atomic_read(&lock->val);

	if (val || !atomic_try_cmpxchg(&lock->val, &val, (1U << 0))) {
		cpu_relax();
		goto __retry;
	}

	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
queued_spin_is_locked(struct qspinlock *lock)
{
	return atomic_read(&lock->val);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
queued_spin_value_unlocked(struct qspinlock lock)
{
	return !lock.val.counter;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
queued_spin_is_contended(struct qspinlock *lock)
{
	return atomic_read(&lock->val) & ~(((1U << 8) - 1) << 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
queued_spin_trylock(struct qspinlock *lock)
{
	int val = atomic_read(&lock->val);

	if (__builtin_expect(!!(val), 0))
		return 0;

	return __builtin_expect(
		!!(atomic_try_cmpxchg_acquire(&lock->val, &val, (1U << 0))), 1);
}

extern void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
queued_spin_lock(struct qspinlock *lock)
{
	int val = 0;

	if (__builtin_expect(
		    !!(atomic_try_cmpxchg_acquire(&lock->val, &val, (1U << 0))),
		    1))
		return;

	queued_spin_lock_slowpath(lock, val);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
queued_spin_unlock(struct qspinlock *lock)
{
	do {
		do {
		} while (0);
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_238(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*&lock->locked) == sizeof(char) ||
				       sizeof(*&lock->locked) == sizeof(short) ||
				       sizeof(*&lock->locked) == sizeof(int) ||
				       sizeof(*&lock->locked) == sizeof(long))))
					__compiletime_assert_238();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_239(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*&lock->locked) ==
						       sizeof(char) ||
					       sizeof(*&lock->locked) ==
						       sizeof(short) ||
					       sizeof(*&lock->locked) ==
						       sizeof(int) ||
					       sizeof(*&lock->locked) ==
						       sizeof(long)) ||
					      sizeof(*&lock->locked) ==
						      sizeof(long long)))
						__compiletime_assert_239();
				} while (0);
				do {
					*(volatile typeof(*&lock->locked) *)&(
						*&lock->locked) = (0);
				} while (0);
			} while (0);
		} while (0);
	} while (0);
}

extern void queued_read_lock_slowpath(struct qrwlock *lock);
extern void queued_write_lock_slowpath(struct qrwlock *lock);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
queued_read_trylock(struct qrwlock *lock)
{
	int cnts;

	cnts = atomic_read(&lock->cnts);
	if (__builtin_expect(!!(!(cnts & 0x1ff)), 1)) {
		cnts = (u32)atomic_add_return_acquire((1U << 9), &lock->cnts);
		if (__builtin_expect(!!(!(cnts & 0x1ff)), 1))
			return 1;
		atomic_sub((1U << 9), &lock->cnts);
	}
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
queued_write_trylock(struct qrwlock *lock)
{
	int cnts;

	cnts = atomic_read(&lock->cnts);
	if (__builtin_expect(!!(cnts), 0))
		return 0;

	return __builtin_expect(
		!!(atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, 0x0ff)), 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
queued_read_lock(struct qrwlock *lock)
{
	int cnts;

	cnts = atomic_add_return_acquire((1U << 9), &lock->cnts);
	if (__builtin_expect(!!(!(cnts & 0x1ff)), 1))
		return;

	queued_read_lock_slowpath(lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
queued_write_lock(struct qrwlock *lock)
{
	int cnts = 0;

	if (__builtin_expect(
		    !!(atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, 0x0ff)),
		    1))
		return;

	queued_write_lock_slowpath(lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
queued_read_unlock(struct qrwlock *lock)
{
	(void)atomic_sub_return_release((1U << 9), &lock->cnts);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
queued_write_unlock(struct qrwlock *lock)
{
	do {
		do {
		} while (0);
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_240(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*&lock->wlocked) == sizeof(char) ||
				       sizeof(*&lock->wlocked) ==
					       sizeof(short) ||
				       sizeof(*&lock->wlocked) == sizeof(int) ||
				       sizeof(*&lock->wlocked) == sizeof(long))))
					__compiletime_assert_240();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_241(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*&lock->wlocked) ==
						       sizeof(char) ||
					       sizeof(*&lock->wlocked) ==
						       sizeof(short) ||
					       sizeof(*&lock->wlocked) ==
						       sizeof(int) ||
					       sizeof(*&lock->wlocked) ==
						       sizeof(long)) ||
					      sizeof(*&lock->wlocked) ==
						      sizeof(long long)))
						__compiletime_assert_241();
				} while (0);
				do {
					*(volatile typeof(*&lock->wlocked) *)&(
						*&lock->wlocked) = (0);
				} while (0);
			} while (0);
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
queued_rwlock_is_contended(struct qrwlock *lock)
{
	return queued_spin_is_locked(&lock->wait_lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
do_raw_spin_lock(raw_spinlock_t *lock)
{
	(void)0;
	queued_spin_lock(&lock->raw_lock);
	do {
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
do_raw_spin_trylock(raw_spinlock_t *lock)
{
	int ret = queued_spin_trylock(&(lock)->raw_lock);

	if (ret)
		do {
		} while (0);

	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
do_raw_spin_unlock(raw_spinlock_t *lock)
{
	do {
	} while (0);
	queued_spin_unlock(&lock->raw_lock);
	(void)0;
}

int in_lock_functions(unsigned long addr);

void __attribute__((__section__(".spinlock.text")))
_raw_spin_lock(raw_spinlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_spin_lock_nested(raw_spinlock_t *lock, int subclass);
void __attribute__((__section__(".spinlock.text")))
_raw_spin_lock_nest_lock(raw_spinlock_t *lock, struct lockdep_map *map);
void __attribute__((__section__(".spinlock.text")))
_raw_spin_lock_bh(raw_spinlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_spin_lock_irq(raw_spinlock_t *lock);

unsigned long __attribute__((__section__(".spinlock.text")))
_raw_spin_lock_irqsave(raw_spinlock_t *lock);
unsigned long __attribute__((__section__(".spinlock.text")))
_raw_spin_lock_irqsave_nested(raw_spinlock_t *lock, int subclass);
int __attribute__((__section__(".spinlock.text")))
_raw_spin_trylock(raw_spinlock_t *lock);
int __attribute__((__section__(".spinlock.text")))
_raw_spin_trylock_bh(raw_spinlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_spin_unlock(raw_spinlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_spin_unlock_bh(raw_spinlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_spin_unlock_irq(raw_spinlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__raw_spin_trylock(raw_spinlock_t *lock)
{
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	if (do_raw_spin_trylock(lock)) {
		do {
		} while (0);
		return 1;
	}
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule242 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
__raw_spin_lock_irqsave(raw_spinlock_t *lock)
{
	unsigned long flags;

	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			flags = arch_local_irq_save();
		} while (0);
	} while (0);
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	do {
	} while (0);
	do_raw_spin_lock(lock);
	return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_spin_lock_irq(raw_spinlock_t *lock)
{
	do {
		arch_local_irq_disable();
	} while (0);
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	do {
	} while (0);
	do_raw_spin_lock(lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_spin_lock_bh(raw_spinlock_t *lock)
{
	__local_bh_disable_ip((unsigned long)__builtin_return_address(0),
			      ((2 * (1UL << (0 + 8))) + (1UL << 0)));
	do {
	} while (0);
	do_raw_spin_lock(lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_spin_lock(raw_spinlock_t *lock)
{
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	do {
	} while (0);
	do_raw_spin_lock(lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_spin_unlock(raw_spinlock_t *lock)
{
	do {
	} while (0);
	do_raw_spin_unlock(lock);
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule243 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)
{
	do {
	} while (0);
	do_raw_spin_unlock(lock);
	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			do {
			} while (0);
			arch_local_irq_restore(flags);
		} while (0);
	} while (0);
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule244 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_spin_unlock_irq(raw_spinlock_t *lock)
{
	do {
	} while (0);
	do_raw_spin_unlock(lock);
	do {
		arch_local_irq_enable();
	} while (0);
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule245 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_spin_unlock_bh(raw_spinlock_t *lock)
{
	do {
	} while (0);
	do_raw_spin_unlock(lock);
	__local_bh_enable_ip((unsigned long)__builtin_return_address(0),
			     ((2 * (1UL << (0 + 8))) + (1UL << 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__raw_spin_trylock_bh(raw_spinlock_t *lock)
{
	__local_bh_disable_ip((unsigned long)__builtin_return_address(0),
			      ((2 * (1UL << (0 + 8))) + (1UL << 0)));
	if (do_raw_spin_trylock(lock)) {
		do {
		} while (0);
		return 1;
	}
	__local_bh_enable_ip((unsigned long)__builtin_return_address(0),
			     ((2 * (1UL << (0 + 8))) + (1UL << 0)));
	return 0;
}

void __attribute__((__section__(".spinlock.text")))
_raw_read_lock(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_write_lock(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_write_lock_nested(rwlock_t *lock, int subclass);
void __attribute__((__section__(".spinlock.text")))
_raw_read_lock_bh(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_write_lock_bh(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_read_lock_irq(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_write_lock_irq(rwlock_t *lock);
unsigned long __attribute__((__section__(".spinlock.text")))
_raw_read_lock_irqsave(rwlock_t *lock);
unsigned long __attribute__((__section__(".spinlock.text")))
_raw_write_lock_irqsave(rwlock_t *lock);
int __attribute__((__section__(".spinlock.text")))
_raw_read_trylock(rwlock_t *lock);
int __attribute__((__section__(".spinlock.text")))
_raw_write_trylock(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_read_unlock(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_write_unlock(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_read_unlock_bh(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_write_unlock_bh(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_read_unlock_irq(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_write_unlock_irq(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text")))
_raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags);
void __attribute__((__section__(".spinlock.text")))
_raw_write_unlock_irqrestore(rwlock_t *lock, unsigned long flags);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__raw_read_trylock(rwlock_t *lock)
{
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	if (queued_read_trylock(&(lock)->raw_lock)) {
		do {
			if (0)
				do {
				} while (0);
			else
				do {
				} while (0);
		} while (0);
		return 1;
	}
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule246 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__raw_write_trylock(rwlock_t *lock)
{
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	if (queued_write_trylock(&(lock)->raw_lock)) {
		do {
		} while (0);
		return 1;
	}
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule247 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_read_lock(rwlock_t *lock)
{
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	do {
		if (0)
			do {
			} while (0);
		else
			do {
			} while (0);
	} while (0);
	do {
		(void)0;
		queued_read_lock(&(lock)->raw_lock);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
__raw_read_lock_irqsave(rwlock_t *lock)
{
	unsigned long flags;

	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			flags = arch_local_irq_save();
		} while (0);
	} while (0);
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	do {
		if (0)
			do {
			} while (0);
		else
			do {
			} while (0);
	} while (0);
	do {
		(void)0;
		queued_read_lock(&(lock)->raw_lock);
	} while (0);
	return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_read_lock_irq(rwlock_t *lock)
{
	do {
		arch_local_irq_disable();
	} while (0);
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	do {
		if (0)
			do {
			} while (0);
		else
			do {
			} while (0);
	} while (0);
	do {
		(void)0;
		queued_read_lock(&(lock)->raw_lock);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_read_lock_bh(rwlock_t *lock)
{
	__local_bh_disable_ip((unsigned long)__builtin_return_address(0),
			      ((2 * (1UL << (0 + 8))) + (1UL << 0)));
	do {
		if (0)
			do {
			} while (0);
		else
			do {
			} while (0);
	} while (0);
	do {
		(void)0;
		queued_read_lock(&(lock)->raw_lock);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
__raw_write_lock_irqsave(rwlock_t *lock)
{
	unsigned long flags;

	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			flags = arch_local_irq_save();
		} while (0);
	} while (0);
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	do {
	} while (0);
	do {
		(void)0;
		queued_write_lock(&(lock)->raw_lock);
	} while (0);
	return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_write_lock_irq(rwlock_t *lock)
{
	do {
		arch_local_irq_disable();
	} while (0);
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	do {
	} while (0);
	do {
		(void)0;
		queued_write_lock(&(lock)->raw_lock);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_write_lock_bh(rwlock_t *lock)
{
	__local_bh_disable_ip((unsigned long)__builtin_return_address(0),
			      ((2 * (1UL << (0 + 8))) + (1UL << 0)));
	do {
	} while (0);
	do {
		(void)0;
		queued_write_lock(&(lock)->raw_lock);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_write_lock(rwlock_t *lock)
{
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	do {
	} while (0);
	do {
		(void)0;
		queued_write_lock(&(lock)->raw_lock);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_write_lock_nested(rwlock_t *lock, int subclass)
{
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	do {
	} while (0);
	do {
		(void)0;
		queued_write_lock(&(lock)->raw_lock);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_write_unlock(rwlock_t *lock)
{
	do {
	} while (0);
	do {
		queued_write_unlock(&(lock)->raw_lock);
		(void)0;
	} while (0);
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule248 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_read_unlock(rwlock_t *lock)
{
	do {
	} while (0);
	do {
		queued_read_unlock(&(lock)->raw_lock);
		(void)0;
	} while (0);
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule249 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
{
	do {
	} while (0);
	do {
		queued_read_unlock(&(lock)->raw_lock);
		(void)0;
	} while (0);
	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			do {
			} while (0);
			arch_local_irq_restore(flags);
		} while (0);
	} while (0);
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule250 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_read_unlock_irq(rwlock_t *lock)
{
	do {
	} while (0);
	do {
		queued_read_unlock(&(lock)->raw_lock);
		(void)0;
	} while (0);
	do {
		arch_local_irq_enable();
	} while (0);
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule251 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_read_unlock_bh(rwlock_t *lock)
{
	do {
	} while (0);
	do {
		queued_read_unlock(&(lock)->raw_lock);
		(void)0;
	} while (0);
	__local_bh_enable_ip((unsigned long)__builtin_return_address(0),
			     ((2 * (1UL << (0 + 8))) + (1UL << 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_write_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
{
	do {
	} while (0);
	do {
		queued_write_unlock(&(lock)->raw_lock);
		(void)0;
	} while (0);
	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			do {
			} while (0);
			arch_local_irq_restore(flags);
		} while (0);
	} while (0);
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule252 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_write_unlock_irq(rwlock_t *lock)
{
	do {
	} while (0);
	do {
		queued_write_unlock(&(lock)->raw_lock);
		(void)0;
	} while (0);
	do {
		arch_local_irq_enable();
	} while (0);
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule253 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__raw_write_unlock_bh(rwlock_t *lock)
{
	do {
	} while (0);
	do {
		queued_write_unlock(&(lock)->raw_lock);
		(void)0;
	} while (0);
	__local_bh_enable_ip((unsigned long)__builtin_return_address(0),
			     ((2 * (1UL << (0 + 8))) + (1UL << 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
raw_spinlock_t *
spinlock_check(spinlock_t *lock)
{
	return &lock->rlock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
spin_lock(spinlock_t *lock)
{
	_raw_spin_lock(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
spin_lock_bh(spinlock_t *lock)
{
	_raw_spin_lock_bh(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
spin_trylock(spinlock_t *lock)
{
	return (_raw_spin_trylock(&lock->rlock));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
spin_lock_irq(spinlock_t *lock)
{
	_raw_spin_lock_irq(&lock->rlock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
spin_unlock(spinlock_t *lock)
{
	_raw_spin_unlock(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
spin_unlock_bh(spinlock_t *lock)
{
	_raw_spin_unlock_bh(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
spin_unlock_irq(spinlock_t *lock)
{
	_raw_spin_unlock_irq(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
	do {
		({
			unsigned long __dummy;
			typeof(flags) __dummy2;
			(void)(&__dummy == &__dummy2);
			1;
		});
		_raw_spin_unlock_irqrestore(&lock->rlock, flags);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
spin_trylock_bh(spinlock_t *lock)
{
	return (_raw_spin_trylock_bh(&lock->rlock));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
spin_trylock_irq(spinlock_t *lock)
{
	return ({
		do {
			arch_local_irq_disable();
		} while (0);
		(_raw_spin_trylock(&lock->rlock)) ? 1 : ({
			do {
				arch_local_irq_enable();
			} while (0);
			0;
		});
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
spin_is_locked(spinlock_t *lock)
{
	return queued_spin_is_locked(&(&lock->rlock)->raw_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
spin_is_contended(spinlock_t *lock)
{
	return queued_spin_is_contended(&(&lock->rlock)->raw_lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
spin_needbreak(spinlock_t *lock)
{
	if (!preempt_model_preemptible())
		return 0;

	return spin_is_contended(lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
rwlock_needbreak(rwlock_t *lock)
{
	if (!preempt_model_preemptible())
		return 0;

	return queued_rwlock_is_contended(&(lock)->raw_lock);
}
extern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);

extern int _atomic_dec_and_lock_irqsave(atomic_t *atomic, spinlock_t *lock,
					unsigned long *flags);

extern int _atomic_dec_and_raw_lock(atomic_t *atomic, raw_spinlock_t *lock);

extern int _atomic_dec_and_raw_lock_irqsave(atomic_t *atomic,
					    raw_spinlock_t *lock,
					    unsigned long *flags);

int __alloc_bucket_spinlocks(spinlock_t **locks, unsigned int *lock_mask,
			     size_t max_size, unsigned int cpu_mult, gfp_t gfp,
			     const char *name, struct lock_class_key *key);
void free_bucket_spinlocks(spinlock_t *locks);

typedef struct {
	raw_spinlock_t *lock;
	;
} class_raw_spinlock_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_raw_spinlock_destructor(class_raw_spinlock_t *_T)
{
	if (_T->lock) {
		_raw_spin_unlock(_T->lock);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_raw_spinlock_lock_ptr(class_raw_spinlock_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_raw_spinlock_t
class_raw_spinlock_constructor(raw_spinlock_t *l)
{
	class_raw_spinlock_t _t = { .lock = l }, *_T = &_t;
	_raw_spin_lock(_T->lock);
	return _t;
}

typedef class_raw_spinlock_t class_raw_spinlock_try_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_raw_spinlock_try_destructor(class_raw_spinlock_t *p)
{
	class_raw_spinlock_destructor(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_raw_spinlock_t
class_raw_spinlock_try_constructor(typeof(((class_raw_spinlock_t *)0)->lock) l)
{
	class_raw_spinlock_t t = ({
		class_raw_spinlock_t _t = { .lock = l }, *_T = &_t;
		if (_T->lock && !((_raw_spin_trylock(_T->lock))))
			_T->lock = ((void *)0);
		_t;
	});
	return t;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_raw_spinlock_try_lock_ptr(class_raw_spinlock_t *_T)
{
	return class_raw_spinlock_lock_ptr(_T);
}

typedef struct {
	raw_spinlock_t *lock;
	;
} class_raw_spinlock_nested_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_raw_spinlock_nested_destructor(class_raw_spinlock_nested_t *_T)
{
	if (_T->lock) {
		_raw_spin_unlock(_T->lock);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_raw_spinlock_nested_lock_ptr(class_raw_spinlock_nested_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_raw_spinlock_nested_t
class_raw_spinlock_nested_constructor(raw_spinlock_t *l)
{
	class_raw_spinlock_nested_t _t = { .lock = l }, *_T = &_t;
	_raw_spin_lock(((void)(1), (_T->lock)));
	return _t;
}

typedef struct {
	raw_spinlock_t *lock;
	;
} class_raw_spinlock_irq_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_raw_spinlock_irq_destructor(class_raw_spinlock_irq_t *_T)
{
	if (_T->lock) {
		_raw_spin_unlock_irq(_T->lock);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_raw_spinlock_irq_lock_ptr(class_raw_spinlock_irq_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_raw_spinlock_irq_t
class_raw_spinlock_irq_constructor(raw_spinlock_t *l)
{
	class_raw_spinlock_irq_t _t = { .lock = l }, *_T = &_t;
	_raw_spin_lock_irq(_T->lock);
	return _t;
}

typedef class_raw_spinlock_irq_t class_raw_spinlock_irq_try_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_raw_spinlock_irq_try_destructor(class_raw_spinlock_irq_t *p)
{
	class_raw_spinlock_irq_destructor(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_raw_spinlock_irq_t
class_raw_spinlock_irq_try_constructor(
	typeof(((class_raw_spinlock_irq_t *)0)->lock) l)
{
	class_raw_spinlock_irq_t t = ({
		class_raw_spinlock_irq_t _t = { .lock = l }, *_T = &_t;
		if (_T->lock && !(({
			    do {
				    arch_local_irq_disable();
			    } while (0);
			    (_raw_spin_trylock(_T->lock)) ? 1 : ({
				    do {
					    arch_local_irq_enable();
				    } while (0);
				    0;
			    });
		    })))
			_T->lock = ((void *)0);
		_t;
	});
	return t;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_raw_spinlock_irq_try_lock_ptr(class_raw_spinlock_irq_t *_T)
{
	return class_raw_spinlock_irq_lock_ptr(_T);
}

typedef struct {
	raw_spinlock_t *lock;
	unsigned long flags;
} class_raw_spinlock_irqsave_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_raw_spinlock_irqsave_destructor(class_raw_spinlock_irqsave_t *_T)
{
	if (_T->lock) {
		do {
			({
				unsigned long __dummy;
				typeof(_T->flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			_raw_spin_unlock_irqrestore(_T->lock, _T->flags);
		} while (0);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_raw_spinlock_irqsave_lock_ptr(class_raw_spinlock_irqsave_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_raw_spinlock_irqsave_t
class_raw_spinlock_irqsave_constructor(raw_spinlock_t *l)
{
	class_raw_spinlock_irqsave_t _t = { .lock = l }, *_T = &_t;
	do {
		({
			unsigned long __dummy;
			typeof(_T->flags) __dummy2;
			(void)(&__dummy == &__dummy2);
			1;
		});
		_T->flags = _raw_spin_lock_irqsave(_T->lock);
	} while (0);
	return _t;
}

typedef class_raw_spinlock_irqsave_t class_raw_spinlock_irqsave_try_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_raw_spinlock_irqsave_try_destructor(class_raw_spinlock_irqsave_t *p)
{
	class_raw_spinlock_irqsave_destructor(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_raw_spinlock_irqsave_t
class_raw_spinlock_irqsave_try_constructor(
	typeof(((class_raw_spinlock_irqsave_t *)0)->lock) l)
{
	class_raw_spinlock_irqsave_t t = ({
		class_raw_spinlock_irqsave_t _t = { .lock = l }, *_T = &_t;
		if (_T->lock && !(({
			    do {
				    do {
					    ({
						    unsigned long __dummy;
						    typeof(_T->flags) __dummy2;
						    (void)(&__dummy ==
							   &__dummy2);
						    1;
					    });
					    _T->flags = arch_local_irq_save();
				    } while (0);
			    } while (0);
			    (_raw_spin_trylock(_T->lock)) ? 1 : ({
				    do {
					    do {
						    ({
							    unsigned long
								    __dummy;
							    typeof(_T->flags)
								    __dummy2;
							    (void)(&__dummy ==
								   &__dummy2);
							    1;
						    });
						    do {
						    } while (0);
						    arch_local_irq_restore(
							    _T->flags);
					    } while (0);
				    } while (0);
				    0;
			    });
		    })))
			_T->lock = ((void *)0);
		_t;
	});
	return t;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_raw_spinlock_irqsave_try_lock_ptr(class_raw_spinlock_irqsave_t *_T)
{
	return class_raw_spinlock_irqsave_lock_ptr(_T);
}

typedef struct {
	spinlock_t *lock;
	;
} class_spinlock_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_spinlock_destructor(class_spinlock_t *_T)
{
	if (_T->lock) {
		spin_unlock(_T->lock);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_spinlock_lock_ptr(class_spinlock_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_spinlock_t
class_spinlock_constructor(spinlock_t *l)
{
	class_spinlock_t _t = { .lock = l }, *_T = &_t;
	spin_lock(_T->lock);
	return _t;
}

typedef class_spinlock_t class_spinlock_try_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_spinlock_try_destructor(class_spinlock_t *p)
{
	class_spinlock_destructor(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_spinlock_t
class_spinlock_try_constructor(typeof(((class_spinlock_t *)0)->lock) l)
{
	class_spinlock_t t = ({
		class_spinlock_t _t = { .lock = l }, *_T = &_t;
		if (_T->lock && !(spin_trylock(_T->lock)))
			_T->lock = ((void *)0);
		_t;
	});
	return t;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_spinlock_try_lock_ptr(class_spinlock_t *_T)
{
	return class_spinlock_lock_ptr(_T);
}

typedef struct {
	spinlock_t *lock;
	;
} class_spinlock_irq_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_spinlock_irq_destructor(class_spinlock_irq_t *_T)
{
	if (_T->lock) {
		spin_unlock_irq(_T->lock);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_spinlock_irq_lock_ptr(class_spinlock_irq_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_spinlock_irq_t
class_spinlock_irq_constructor(spinlock_t *l)
{
	class_spinlock_irq_t _t = { .lock = l }, *_T = &_t;
	spin_lock_irq(_T->lock);
	return _t;
}

typedef class_spinlock_irq_t class_spinlock_irq_try_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_spinlock_irq_try_destructor(class_spinlock_irq_t *p)
{
	class_spinlock_irq_destructor(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_spinlock_irq_t
class_spinlock_irq_try_constructor(typeof(((class_spinlock_irq_t *)0)->lock) l)
{
	class_spinlock_irq_t t = ({
		class_spinlock_irq_t _t = { .lock = l }, *_T = &_t;
		if (_T->lock && !(spin_trylock_irq(_T->lock)))
			_T->lock = ((void *)0);
		_t;
	});
	return t;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_spinlock_irq_try_lock_ptr(class_spinlock_irq_t *_T)
{
	return class_spinlock_irq_lock_ptr(_T);
}

typedef struct {
	spinlock_t *lock;
	unsigned long flags;
} class_spinlock_irqsave_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_spinlock_irqsave_destructor(class_spinlock_irqsave_t *_T)
{
	if (_T->lock) {
		spin_unlock_irqrestore(_T->lock, _T->flags);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_spinlock_irqsave_lock_ptr(class_spinlock_irqsave_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_spinlock_irqsave_t
class_spinlock_irqsave_constructor(spinlock_t *l)
{
	class_spinlock_irqsave_t _t = { .lock = l }, *_T = &_t;
	do {
		do {
			({
				unsigned long __dummy;
				typeof(_T->flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			_T->flags = _raw_spin_lock_irqsave(
				spinlock_check(_T->lock));
		} while (0);
	} while (0);
	return _t;
}

typedef class_spinlock_irqsave_t class_spinlock_irqsave_try_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_spinlock_irqsave_try_destructor(class_spinlock_irqsave_t *p)
{
	class_spinlock_irqsave_destructor(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_spinlock_irqsave_t
class_spinlock_irqsave_try_constructor(
	typeof(((class_spinlock_irqsave_t *)0)->lock) l)
{
	class_spinlock_irqsave_t t = ({
		class_spinlock_irqsave_t _t = { .lock = l }, *_T = &_t;
		if (_T->lock && !(({
			    ({
				    do {
					    do {
						    ({
							    unsigned long
								    __dummy;
							    typeof(_T->flags)
								    __dummy2;
							    (void)(&__dummy ==
								   &__dummy2);
							    1;
						    });
						    _T->flags =
							    arch_local_irq_save();
					    } while (0);
				    } while (0);
				    (_raw_spin_trylock(spinlock_check(_T->lock))) ? 1 : ({
					    do {
						    do {
							    ({
								    unsigned long
									    __dummy;
								    typeof(_T->flags)
									    __dummy2;
								    (void)(&__dummy ==
									   &__dummy2);
								    1;
							    });
							    do {
							    } while (0);
							    arch_local_irq_restore(
								    _T->flags);
						    } while (0);
					    } while (0);
					    0;
				    });
			    });
		    })))
			_T->lock = ((void *)0);
		_t;
	});
	return t;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_spinlock_irqsave_try_lock_ptr(class_spinlock_irqsave_t *_T)
{
	return class_spinlock_irqsave_lock_ptr(_T);
}

typedef struct {
	rwlock_t *lock;
	;
} class_read_lock_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_read_lock_destructor(class_read_lock_t *_T)
{
	if (_T->lock) {
		_raw_read_unlock(_T->lock);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_read_lock_lock_ptr(class_read_lock_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_read_lock_t
class_read_lock_constructor(rwlock_t *l)
{
	class_read_lock_t _t = { .lock = l }, *_T = &_t;
	_raw_read_lock(_T->lock);
	return _t;
}

typedef struct {
	rwlock_t *lock;
	;
} class_read_lock_irq_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_read_lock_irq_destructor(class_read_lock_irq_t *_T)
{
	if (_T->lock) {
		_raw_read_unlock_irq(_T->lock);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_read_lock_irq_lock_ptr(class_read_lock_irq_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_read_lock_irq_t
class_read_lock_irq_constructor(rwlock_t *l)
{
	class_read_lock_irq_t _t = { .lock = l }, *_T = &_t;
	_raw_read_lock_irq(_T->lock);
	return _t;
}

typedef struct {
	rwlock_t *lock;
	unsigned long flags;
} class_read_lock_irqsave_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_read_lock_irqsave_destructor(class_read_lock_irqsave_t *_T)
{
	if (_T->lock) {
		do {
			({
				unsigned long __dummy;
				typeof(_T->flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			_raw_read_unlock_irqrestore(_T->lock, _T->flags);
		} while (0);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_read_lock_irqsave_lock_ptr(class_read_lock_irqsave_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_read_lock_irqsave_t
class_read_lock_irqsave_constructor(rwlock_t *l)
{
	class_read_lock_irqsave_t _t = { .lock = l }, *_T = &_t;
	do {
		({
			unsigned long __dummy;
			typeof(_T->flags) __dummy2;
			(void)(&__dummy == &__dummy2);
			1;
		});
		_T->flags = _raw_read_lock_irqsave(_T->lock);
	} while (0);
	return _t;
}

typedef struct {
	rwlock_t *lock;
	;
} class_write_lock_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_write_lock_destructor(class_write_lock_t *_T)
{
	if (_T->lock) {
		_raw_write_unlock(_T->lock);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_write_lock_lock_ptr(class_write_lock_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_write_lock_t
class_write_lock_constructor(rwlock_t *l)
{
	class_write_lock_t _t = { .lock = l }, *_T = &_t;
	_raw_write_lock(_T->lock);
	return _t;
}

typedef struct {
	rwlock_t *lock;
	;
} class_write_lock_irq_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_write_lock_irq_destructor(class_write_lock_irq_t *_T)
{
	if (_T->lock) {
		_raw_write_unlock_irq(_T->lock);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_write_lock_irq_lock_ptr(class_write_lock_irq_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_write_lock_irq_t
class_write_lock_irq_constructor(rwlock_t *l)
{
	class_write_lock_irq_t _t = { .lock = l }, *_T = &_t;
	_raw_write_lock_irq(_T->lock);
	return _t;
}

typedef struct {
	rwlock_t *lock;
	unsigned long flags;
} class_write_lock_irqsave_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_write_lock_irqsave_destructor(class_write_lock_irqsave_t *_T)
{
	if (_T->lock) {
		do {
			({
				unsigned long __dummy;
				typeof(_T->flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			_raw_write_unlock_irqrestore(_T->lock, _T->flags);
		} while (0);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_write_lock_irqsave_lock_ptr(class_write_lock_irqsave_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_write_lock_irqsave_t
class_write_lock_irqsave_constructor(rwlock_t *l)
{
	class_write_lock_irqsave_t _t = { .lock = l }, *_T = &_t;
	do {
		({
			unsigned long __dummy;
			typeof(_T->flags) __dummy2;
			(void)(&__dummy == &__dummy2);
			1;
		});
		_T->flags = _raw_write_lock_irqsave(_T->lock);
	} while (0);
	return _t;
}
typedef struct wait_queue_entry wait_queue_entry_t;

typedef int (*wait_queue_func_t)(struct wait_queue_entry *wq_entry,
				 unsigned mode, int flags, void *key);
int default_wake_function(struct wait_queue_entry *wq_entry, unsigned mode,
			  int flags, void *key);
struct wait_queue_entry {
	unsigned int flags;
	void *private;
	wait_queue_func_t func;
	struct list_head entry;
};

struct wait_queue_head {
	spinlock_t lock;
	struct list_head head;
};
typedef struct wait_queue_head wait_queue_head_t;

struct task_struct;
extern void __init_waitqueue_head(struct wait_queue_head *wq_head,
				  const char *name, struct lock_class_key *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
init_waitqueue_entry(struct wait_queue_entry *wq_entry, struct task_struct *p)
{
	wq_entry->flags = 0;
	wq_entry->private = p;
	wq_entry->func = default_wake_function;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
init_waitqueue_func_entry(struct wait_queue_entry *wq_entry,
			  wait_queue_func_t func)
{
	wq_entry->flags = 0;
	wq_entry->private = ((void *)0);
	wq_entry->func = func;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
waitqueue_active(struct wait_queue_head *wq_head)
{
	return !list_empty(&wq_head->head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
wq_has_single_sleeper(struct wait_queue_head *wq_head)
{
	return list_is_singular(&wq_head->head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
wq_has_sleeper(struct wait_queue_head *wq_head)
{
	do {
		do {
		} while (0);
		asm volatile("lock; addl $0,-4(%%"
			     "rsp"
			     ")" ::
				     : "memory", "cc");
	} while (0);
	return waitqueue_active(wq_head);
}

extern void add_wait_queue(struct wait_queue_head *wq_head,
			   struct wait_queue_entry *wq_entry);
extern void add_wait_queue_exclusive(struct wait_queue_head *wq_head,
				     struct wait_queue_entry *wq_entry);
extern void add_wait_queue_priority(struct wait_queue_head *wq_head,
				    struct wait_queue_entry *wq_entry);
extern void remove_wait_queue(struct wait_queue_head *wq_head,
			      struct wait_queue_entry *wq_entry);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__add_wait_queue(struct wait_queue_head *wq_head,
		 struct wait_queue_entry *wq_entry)
{
	struct list_head *head = &wq_head->head;
	struct wait_queue_entry *wq;

	for (wq = ({
		     void *__mptr = (void *)((&wq_head->head)->next);
		     _Static_assert(
			     __builtin_types_compatible_p(
				     typeof(*((&wq_head->head)->next)),
				     typeof(((typeof(*wq) *)0)->entry)) ||
				     __builtin_types_compatible_p(
					     typeof(*((&wq_head->head)->next)),
					     typeof(void)),
			     "pointer type mismatch in container_of()");
		     ((typeof(*wq) *)(__mptr -
				      __builtin_offsetof(typeof(*wq), entry)));
	     });
	     !list_is_head(&wq->entry, (&wq_head->head));
	     wq = ({
		     void *__mptr = (void *)((wq)->entry.next);
		     _Static_assert(
			     __builtin_types_compatible_p(
				     typeof(*((wq)->entry.next)),
				     typeof(((typeof(*(wq)) *)0)->entry)) ||
				     __builtin_types_compatible_p(
					     typeof(*((wq)->entry.next)),
					     typeof(void)),
			     "pointer type mismatch in container_of()");
		     ((typeof(*(wq)) *)(__mptr -
					__builtin_offsetof(typeof(*(wq)),
							   entry)));
	     })) {
		if (!(wq->flags & 0x10))
			break;
		head = &wq->entry;
	}
	list_add(&wq_entry->entry, head);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__add_wait_queue_exclusive(struct wait_queue_head *wq_head,
			   struct wait_queue_entry *wq_entry)
{
	wq_entry->flags |= 0x01;
	__add_wait_queue(wq_head, wq_entry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__add_wait_queue_entry_tail(struct wait_queue_head *wq_head,
			    struct wait_queue_entry *wq_entry)
{
	list_add_tail(&wq_entry->entry, &wq_head->head);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__add_wait_queue_entry_tail_exclusive(struct wait_queue_head *wq_head,
				      struct wait_queue_entry *wq_entry)
{
	wq_entry->flags |= 0x01;
	__add_wait_queue_entry_tail(wq_head, wq_entry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__remove_wait_queue(struct wait_queue_head *wq_head,
		    struct wait_queue_entry *wq_entry)
{
	list_del(&wq_entry->entry);
}

int __wake_up(struct wait_queue_head *wq_head, unsigned int mode, int nr,
	      void *key);
void __wake_up_on_current_cpu(struct wait_queue_head *wq_head,
			      unsigned int mode, void *key);
void __wake_up_locked_key(struct wait_queue_head *wq_head, unsigned int mode,
			  void *key);
void __wake_up_sync_key(struct wait_queue_head *wq_head, unsigned int mode,
			void *key);
void __wake_up_locked_sync_key(struct wait_queue_head *wq_head,
			       unsigned int mode, void *key);
void __wake_up_locked(struct wait_queue_head *wq_head, unsigned int mode,
		      int nr);
void __wake_up_sync(struct wait_queue_head *wq_head, unsigned int mode);
void __wake_up_pollfree(struct wait_queue_head *wq_head);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
wake_up_pollfree(struct wait_queue_head *wq_head)
{
	if (waitqueue_active(wq_head))
		__wake_up_pollfree(wq_head);
}
extern void init_wait_entry(struct wait_queue_entry *wq_entry, int flags);
extern int do_wait_intr(wait_queue_head_t *, wait_queue_entry_t *);
extern int do_wait_intr_irq(wait_queue_head_t *, wait_queue_entry_t *);
void prepare_to_wait(struct wait_queue_head *wq_head,
		     struct wait_queue_entry *wq_entry, int state);
bool prepare_to_wait_exclusive(struct wait_queue_head *wq_head,
			       struct wait_queue_entry *wq_entry, int state);
long prepare_to_wait_event(struct wait_queue_head *wq_head,
			   struct wait_queue_entry *wq_entry, int state);
void finish_wait(struct wait_queue_head *wq_head,
		 struct wait_queue_entry *wq_entry);
long wait_woken(struct wait_queue_entry *wq_entry, unsigned mode, long timeout);
int woken_wake_function(struct wait_queue_entry *wq_entry, unsigned mode,
			int sync, void *key);
int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode,
			     int sync, void *key);
typedef int (*task_call_f)(struct task_struct *p, void *arg);
extern int task_call_func(struct task_struct *p, task_call_f func, void *arg);
struct task_struct;

struct swait_queue_head {
	raw_spinlock_t lock;
	struct list_head task_list;
};

struct swait_queue {
	struct task_struct *task;
	struct list_head task_list;
};
extern void __init_swait_queue_head(struct swait_queue_head *q,
				    const char *name,
				    struct lock_class_key *key);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
swait_active(struct swait_queue_head *wq)
{
	return !list_empty(&wq->task_list);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
swq_has_sleeper(struct swait_queue_head *wq)
{
	do {
		do {
		} while (0);
		asm volatile("lock; addl $0,-4(%%"
			     "rsp"
			     ")" ::
				     : "memory", "cc");
	} while (0);
	return swait_active(wq);
}

extern void swake_up_one(struct swait_queue_head *q);
extern void swake_up_all(struct swait_queue_head *q);
extern void swake_up_locked(struct swait_queue_head *q, int wake_flags);

extern void prepare_to_swait_exclusive(struct swait_queue_head *q,
				       struct swait_queue *wait, int state);
extern long prepare_to_swait_event(struct swait_queue_head *q,
				   struct swait_queue *wait, int state);

extern void __finish_swait(struct swait_queue_head *q,
			   struct swait_queue *wait);
extern void finish_swait(struct swait_queue_head *q, struct swait_queue *wait);
struct completion {
	unsigned int done;
	struct swait_queue_head wait;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
complete_acquire(struct completion *x)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
complete_release(struct completion *x)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
init_completion(struct completion *x)
{
	x->done = 0;
	do {
		static struct lock_class_key __key;
		__init_swait_queue_head((&x->wait), "&x->wait", &__key);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
reinit_completion(struct completion *x)
{
	x->done = 0;
}

extern void wait_for_completion(struct completion *);
extern void wait_for_completion_io(struct completion *);
extern int wait_for_completion_interruptible(struct completion *x);
extern int wait_for_completion_killable(struct completion *x);
extern int wait_for_completion_state(struct completion *x, unsigned int state);
extern unsigned long wait_for_completion_timeout(struct completion *x,
						 unsigned long timeout);
extern unsigned long wait_for_completion_io_timeout(struct completion *x,
						    unsigned long timeout);
extern long wait_for_completion_interruptible_timeout(struct completion *x,
						      unsigned long timeout);
extern long wait_for_completion_killable_timeout(struct completion *x,
						 unsigned long timeout);
extern bool try_wait_for_completion(struct completion *x);
extern bool completion_done(struct completion *x);

extern void complete(struct completion *);
extern void complete_on_current_cpu(struct completion *x);
extern void complete_all(struct completion *);
typedef struct refcount_struct {
	atomic_t refs;
} refcount_t;

struct mutex;

enum refcount_saturation_type {
	REFCOUNT_ADD_NOT_ZERO_OVF,
	REFCOUNT_ADD_OVF,
	REFCOUNT_ADD_UAF,
	REFCOUNT_SUB_UAF,
	REFCOUNT_DEC_LEAK,
};

void refcount_warn_saturate(refcount_t *r, enum refcount_saturation_type t);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
refcount_set(refcount_t *r, int n)
{
	atomic_set(&r->refs, n);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
refcount_read(const refcount_t *r)
{
	return atomic_read(&r->refs);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) bool
__refcount_add_not_zero(int i, refcount_t *r, int *oldp)
{
	int old = refcount_read(r);

	do {
		if (!old)
			break;
	} while (!atomic_try_cmpxchg_relaxed(&r->refs, &old, old + i));

	if (oldp)
		*oldp = old;

	if (__builtin_expect(!!(old < 0 || old + i < 0), 0))
		refcount_warn_saturate(r, REFCOUNT_ADD_NOT_ZERO_OVF);

	return old;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) bool
refcount_add_not_zero(int i, refcount_t *r)
{
	return __refcount_add_not_zero(i, r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__refcount_add(int i, refcount_t *r, int *oldp)
{
	int old = atomic_fetch_add_relaxed(i, &r->refs);

	if (oldp)
		*oldp = old;

	if (__builtin_expect(!!(!old), 0))
		refcount_warn_saturate(r, REFCOUNT_ADD_UAF);
	else if (__builtin_expect(!!(old < 0 || old + i < 0), 0))
		refcount_warn_saturate(r, REFCOUNT_ADD_OVF);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
refcount_add(int i, refcount_t *r)
{
	__refcount_add(i, r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) bool
__refcount_inc_not_zero(refcount_t *r, int *oldp)
{
	return __refcount_add_not_zero(1, r, oldp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) bool
refcount_inc_not_zero(refcount_t *r)
{
	return __refcount_inc_not_zero(r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__refcount_inc(refcount_t *r, int *oldp)
{
	__refcount_add(1, r, oldp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
refcount_inc(refcount_t *r)
{
	__refcount_inc(r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) bool
__refcount_sub_and_test(int i, refcount_t *r, int *oldp)
{
	int old = atomic_fetch_sub_release(i, &r->refs);

	if (oldp)
		*oldp = old;

	if (old > 0 && old == i) {
		do {
			do {
			} while (0);
			do {
				do {
				} while (0);
				__asm__ __volatile__("" : : : "memory");
			} while (0);
		} while (0);
		return true;
	}

	if (__builtin_expect(!!(old <= 0 || old - i < 0), 0))
		refcount_warn_saturate(r, REFCOUNT_SUB_UAF);

	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) bool
refcount_sub_and_test(int i, refcount_t *r)
{
	return __refcount_sub_and_test(i, r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) bool
__refcount_dec_and_test(refcount_t *r, int *oldp)
{
	return __refcount_sub_and_test(1, r, oldp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) bool
refcount_dec_and_test(refcount_t *r)
{
	return __refcount_dec_and_test(r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__refcount_dec(refcount_t *r, int *oldp)
{
	int old = atomic_fetch_sub_release(1, &r->refs);

	if (oldp)
		*oldp = old;

	if (__builtin_expect(!!(old <= 1), 0))
		refcount_warn_saturate(r, REFCOUNT_DEC_LEAK);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
refcount_dec(refcount_t *r)
{
	__refcount_dec(r, ((void *)0));
}

extern __attribute__((__warn_unused_result__)) bool
refcount_dec_if_one(refcount_t *r);
extern __attribute__((__warn_unused_result__)) bool
refcount_dec_not_one(refcount_t *r);
extern __attribute__((__warn_unused_result__)) bool
refcount_dec_and_mutex_lock(refcount_t *r, struct mutex *lock);
extern __attribute__((__warn_unused_result__)) bool
refcount_dec_and_lock(refcount_t *r, spinlock_t *lock);
extern __attribute__((__warn_unused_result__)) bool
refcount_dec_and_lock_irqsave(refcount_t *r, spinlock_t *lock,
			      unsigned long *flags);

struct hlist_nulls_head {
	struct hlist_nulls_node *first;
};

struct hlist_nulls_node {
	struct hlist_nulls_node *next, **pprev;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
is_a_nulls(const struct hlist_nulls_node *ptr)
{
	return ((unsigned long)ptr & 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
get_nulls_value(const struct hlist_nulls_node *ptr)
{
	return ((unsigned long)ptr) >> 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
hlist_nulls_unhashed(const struct hlist_nulls_node *h)
{
	return !h->pprev;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
hlist_nulls_unhashed_lockless(const struct hlist_nulls_node *h)
{
	return !({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_254(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(h->pprev) == sizeof(char) ||
			       sizeof(h->pprev) == sizeof(short) ||
			       sizeof(h->pprev) == sizeof(int) ||
			       sizeof(h->pprev) == sizeof(long)) ||
			      sizeof(h->pprev) == sizeof(long long)))
				__compiletime_assert_254();
		} while (0);
		(*(const volatile typeof(_Generic((h->pprev),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (h->pprev)))
			   *)&(h->pprev));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
hlist_nulls_empty(const struct hlist_nulls_head *h)
{
	return is_a_nulls(({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_255(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(h->first) == sizeof(char) ||
			       sizeof(h->first) == sizeof(short) ||
			       sizeof(h->first) == sizeof(int) ||
			       sizeof(h->first) == sizeof(long)) ||
			      sizeof(h->first) == sizeof(long long)))
				__compiletime_assert_255();
		} while (0);
		(*(const volatile typeof(_Generic((h->first),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (h->first)))
			   *)&(h->first));
	}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_nulls_add_head(struct hlist_nulls_node *n, struct hlist_nulls_head *h)
{
	struct hlist_nulls_node *first = h->first;

	n->next = first;
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_256(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->pprev) == sizeof(char) ||
			       sizeof(n->pprev) == sizeof(short) ||
			       sizeof(n->pprev) == sizeof(int) ||
			       sizeof(n->pprev) == sizeof(long)) ||
			      sizeof(n->pprev) == sizeof(long long)))
				__compiletime_assert_256();
		} while (0);
		do {
			*(volatile typeof(n->pprev) *)&(n->pprev) = (&h->first);
		} while (0);
	} while (0);
	h->first = n;
	if (!is_a_nulls(first))
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_257(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(first->pprev) == sizeof(char) ||
				       sizeof(first->pprev) == sizeof(short) ||
				       sizeof(first->pprev) == sizeof(int) ||
				       sizeof(first->pprev) == sizeof(long)) ||
				      sizeof(first->pprev) ==
					      sizeof(long long)))
					__compiletime_assert_257();
			} while (0);
			do {
				*(volatile typeof(first->pprev) *)&(
					first->pprev) = (&n->next);
			} while (0);
		} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__hlist_nulls_del(struct hlist_nulls_node *n)
{
	struct hlist_nulls_node *next = n->next;
	struct hlist_nulls_node **pprev = n->pprev;

	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_258(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*pprev) == sizeof(char) ||
			       sizeof(*pprev) == sizeof(short) ||
			       sizeof(*pprev) == sizeof(int) ||
			       sizeof(*pprev) == sizeof(long)) ||
			      sizeof(*pprev) == sizeof(long long)))
				__compiletime_assert_258();
		} while (0);
		do {
			*(volatile typeof(*pprev) *)&(*pprev) = (next);
		} while (0);
	} while (0);
	if (!is_a_nulls(next))
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_259(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(next->pprev) == sizeof(char) ||
				       sizeof(next->pprev) == sizeof(short) ||
				       sizeof(next->pprev) == sizeof(int) ||
				       sizeof(next->pprev) == sizeof(long)) ||
				      sizeof(next->pprev) == sizeof(long long)))
					__compiletime_assert_259();
			} while (0);
			do {
				*(volatile typeof(next->pprev) *)&(
					next->pprev) = (pprev);
			} while (0);
		} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_nulls_del(struct hlist_nulls_node *n)
{
	__hlist_nulls_del(n);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_260(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->pprev) == sizeof(char) ||
			       sizeof(n->pprev) == sizeof(short) ||
			       sizeof(n->pprev) == sizeof(int) ||
			       sizeof(n->pprev) == sizeof(long)) ||
			      sizeof(n->pprev) == sizeof(long long)))
				__compiletime_assert_260();
		} while (0);
		do {
			*(volatile typeof(n->pprev) *)&(n->pprev) =
				(((void *)0x122 + (0xdead000000000000UL)));
		} while (0);
	} while (0);
}

struct optimistic_spin_queue {
	atomic_t tail;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
osq_lock_init(struct optimistic_spin_queue *lock)
{
	atomic_set(&lock->tail, (0));
}

extern bool osq_lock(struct optimistic_spin_queue *lock);
extern void osq_unlock(struct optimistic_spin_queue *lock);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
osq_is_locked(struct optimistic_spin_queue *lock)
{
	return atomic_read(&lock->tail) != (0);
}

struct task_struct;

extern int debug_locks __attribute__((__section__(".data..read_mostly")));
extern int debug_locks_silent
	__attribute__((__section__(".data..read_mostly")));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
__debug_locks_off(void)
{
	return ({
		typeof(&debug_locks) __ai_ptr = (&debug_locks);
		do {
		} while (0);
		instrument_atomic_read_write(__ai_ptr, sizeof(*__ai_ptr));
		({
			__typeof__(*((__ai_ptr))) __ret = ((0));
			switch (sizeof(*((__ai_ptr)))) {
			case 1:
				asm volatile(""
					     "xchg"
					     "b %b0, %1\n"
					     : "+q"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 2:
				asm volatile(""
					     "xchg"
					     "w %w0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 4:
				asm volatile(""
					     "xchg"
					     "l %0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 8:
				asm volatile(""
					     "xchg"
					     "q %q0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			default:
				__xchg_wrong_size();
			}
			__ret;
		});
	});
}

extern int debug_locks_off(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_show_all_locks(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_show_held_locks(struct task_struct *task)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_check_no_locks_freed(const void *from, unsigned long len)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_check_no_locks_held(void)
{
}

struct mutex {
	atomic_long_t owner;
	raw_spinlock_t wait_lock;

	struct optimistic_spin_queue osq;

	struct list_head wait_list;
};

struct device;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mutex_destroy(struct mutex *lock)
{
}
extern void __mutex_init(struct mutex *lock, const char *name,
			 struct lock_class_key *key);

extern bool mutex_is_locked(struct mutex *lock);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__devm_mutex_init(struct device *dev, struct mutex *lock)
{
	return 0;
}
extern void mutex_lock(struct mutex *lock);
extern int __attribute__((__warn_unused_result__))
mutex_lock_interruptible(struct mutex *lock);
extern int __attribute__((__warn_unused_result__))
mutex_lock_killable(struct mutex *lock);
extern void mutex_lock_io(struct mutex *lock);
extern int mutex_trylock(struct mutex *lock);
extern void mutex_unlock(struct mutex *lock);

extern int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);

typedef struct mutex *class_mutex_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_mutex_destructor(struct mutex **p)
{
	struct mutex *_T = *p;
	if (_T) {
		mutex_unlock(_T);
	};
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct mutex *
class_mutex_constructor(struct mutex *_T)
{
	struct mutex *t = ({
		mutex_lock(_T);
		_T;
	});
	return t;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_mutex_lock_ptr(class_mutex_t *_T)
{
	return *_T;
}
typedef class_mutex_t class_mutex_try_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_mutex_try_destructor(class_mutex_t *p)
{
	class_mutex_destructor(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_mutex_t
class_mutex_try_constructor(class_mutex_t _T)
{
	class_mutex_t t = ({
		void *_t = _T;
		if (_T && !(mutex_trylock(_T)))
			_t = ((void *)0);
		_t;
	});
	return t;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_mutex_try_lock_ptr(class_mutex_t *_T)
{
	return class_mutex_lock_ptr(_T);
}
typedef class_mutex_t class_mutex_intr_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_mutex_intr_destructor(class_mutex_t *p)
{
	class_mutex_destructor(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_mutex_t
class_mutex_intr_constructor(class_mutex_t _T)
{
	class_mutex_t t = ({
		void *_t = _T;
		if (_T && !(mutex_lock_interruptible(_T) == 0))
			_t = ((void *)0);
		_t;
	});
	return t;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_mutex_intr_lock_ptr(class_mutex_t *_T)
{
	return class_mutex_lock_ptr(_T);
}

typedef struct seqcount {
	unsigned sequence;

} seqcount_t;
typedef struct seqcount_raw_spinlock {
	seqcount_t seqcount;
	;
} seqcount_raw_spinlock_t;
typedef struct seqcount_spinlock {
	seqcount_t seqcount;
	;
} seqcount_spinlock_t;
typedef struct seqcount_rwlock {
	seqcount_t seqcount;
	;
} seqcount_rwlock_t;
typedef struct seqcount_mutex {
	seqcount_t seqcount;
	;
} seqcount_mutex_t;
typedef struct {
	seqcount_spinlock_t seqcount;
	spinlock_t lock;
} seqlock_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__seqcount_init(seqcount_t *s, const char *name, struct lock_class_key *key)
{
	do {
		(void)(name);
		(void)(key);
	} while (0);
	s->sequence = 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) seqcount_t *
__seqprop_ptr(seqcount_t *s)
{
	return s;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const seqcount_t *
__seqprop_const_ptr(const seqcount_t *s)
{
	return s;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned
__seqprop_sequence(const seqcount_t *s)
{
	return ({
		typeof(*&s->sequence) ___p1 = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_261(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*&s->sequence) == sizeof(char) ||
				       sizeof(*&s->sequence) == sizeof(short) ||
				       sizeof(*&s->sequence) == sizeof(int) ||
				       sizeof(*&s->sequence) == sizeof(long)) ||
				      sizeof(*&s->sequence) ==
					      sizeof(long long)))
					__compiletime_assert_261();
			} while (0);
			(*(const volatile typeof(_Generic(
				(*&s->sequence),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 *&s->sequence)))
				   *)&(*&s->sequence));
		});
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_262(void) __attribute__((__error__(
				"Need native word sized stores/loads for atomicity.")));
			if (!((sizeof(*&s->sequence) == sizeof(char) ||
			       sizeof(*&s->sequence) == sizeof(short) ||
			       sizeof(*&s->sequence) == sizeof(int) ||
			       sizeof(*&s->sequence) == sizeof(long))))
				__compiletime_assert_262();
		} while (0);
		__asm__ __volatile__("" : : : "memory");
		___p1;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__seqprop_preemptible(const seqcount_t *s)
{
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__seqprop_assert(const seqcount_t *s)
{
	do {
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
seqcount_t *
__seqprop_raw_spinlock_ptr(seqcount_raw_spinlock_t *s)
{
	return &s->seqcount;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
const seqcount_t *
__seqprop_raw_spinlock_const_ptr(const seqcount_raw_spinlock_t *s)
{
	return &s->seqcount;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned
__seqprop_raw_spinlock_sequence(const seqcount_raw_spinlock_t *s)
{
	unsigned seq = ({
		typeof(*&s->seqcount.sequence) ___p1 = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_263(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*&s->seqcount.sequence) ==
					       sizeof(char) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(short) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(int) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(long)) ||
				      sizeof(*&s->seqcount.sequence) ==
					      sizeof(long long)))
					__compiletime_assert_263();
			} while (0);
			(*(const volatile typeof(_Generic(
				(*&s->seqcount.sequence),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 *&s->seqcount
									   .sequence)))
				   *)&(*&s->seqcount.sequence));
		});
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_264(void) __attribute__((__error__(
				"Need native word sized stores/loads for atomicity.")));
			if (!((sizeof(*&s->seqcount.sequence) == sizeof(char) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(short) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(int) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(long))))
				__compiletime_assert_264();
		} while (0);
		__asm__ __volatile__("" : : : "memory");
		___p1;
	});
	if (!0)
		return seq;
	if (false && __builtin_expect(!!(seq & 1), 0)) {
		;
		;
		seq = ({
			typeof(*&s->seqcount.sequence) ___p1 = ({
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_265(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*&s->seqcount.sequence) ==
						       sizeof(char) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(short) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(int) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(long)) ||
					      sizeof(*&s->seqcount.sequence) ==
						      sizeof(long long)))
						__compiletime_assert_265();
				} while (0);
				(*(const volatile typeof(_Generic(
					(*&s->seqcount.sequence),
								 char: (char)0,
								 unsigned char: (
									 unsigned char)0,
								 signed char: (
									 signed char)0,
								 unsigned short: (
									 unsigned short)0,
								 signed short: (
									 signed short)0,
								 unsigned int: (
									 unsigned int)0,
								 signed int: (
									 signed int)0,
								 unsigned long: (
									 unsigned long)0,
								 signed long: (
									 signed long)0,
								 unsigned long long: (
									 unsigned long long)0,
								 signed long long: (
									 signed long long)0,
								 default: (
									 *&s->seqcount
										   .sequence)))
					   *)&(*&s->seqcount.sequence));
			});
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_266(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*&s->seqcount.sequence) ==
					       sizeof(char) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(short) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(int) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(long))))
					__compiletime_assert_266();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			___p1;
		});
	}
	return seq;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__seqprop_raw_spinlock_preemptible(const seqcount_raw_spinlock_t *s)
{
	if (!0)
		return false;
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__seqprop_raw_spinlock_assert(const seqcount_raw_spinlock_t *s)
{
	;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
seqcount_t *
__seqprop_spinlock_ptr(seqcount_spinlock_t *s)
{
	return &s->seqcount;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
const seqcount_t *
__seqprop_spinlock_const_ptr(const seqcount_spinlock_t *s)
{
	return &s->seqcount;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned
__seqprop_spinlock_sequence(const seqcount_spinlock_t *s)
{
	unsigned seq = ({
		typeof(*&s->seqcount.sequence) ___p1 = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_267(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*&s->seqcount.sequence) ==
					       sizeof(char) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(short) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(int) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(long)) ||
				      sizeof(*&s->seqcount.sequence) ==
					      sizeof(long long)))
					__compiletime_assert_267();
			} while (0);
			(*(const volatile typeof(_Generic(
				(*&s->seqcount.sequence),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 *&s->seqcount
									   .sequence)))
				   *)&(*&s->seqcount.sequence));
		});
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_268(void) __attribute__((__error__(
				"Need native word sized stores/loads for atomicity.")));
			if (!((sizeof(*&s->seqcount.sequence) == sizeof(char) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(short) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(int) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(long))))
				__compiletime_assert_268();
		} while (0);
		__asm__ __volatile__("" : : : "memory");
		___p1;
	});
	if (!0)
		return seq;
	if (0 && __builtin_expect(!!(seq & 1), 0)) {
		;
		;
		seq = ({
			typeof(*&s->seqcount.sequence) ___p1 = ({
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_269(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*&s->seqcount.sequence) ==
						       sizeof(char) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(short) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(int) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(long)) ||
					      sizeof(*&s->seqcount.sequence) ==
						      sizeof(long long)))
						__compiletime_assert_269();
				} while (0);
				(*(const volatile typeof(_Generic(
					(*&s->seqcount.sequence),
								 char: (char)0,
								 unsigned char: (
									 unsigned char)0,
								 signed char: (
									 signed char)0,
								 unsigned short: (
									 unsigned short)0,
								 signed short: (
									 signed short)0,
								 unsigned int: (
									 unsigned int)0,
								 signed int: (
									 signed int)0,
								 unsigned long: (
									 unsigned long)0,
								 signed long: (
									 signed long)0,
								 unsigned long long: (
									 unsigned long long)0,
								 signed long long: (
									 signed long long)0,
								 default: (
									 *&s->seqcount
										   .sequence)))
					   *)&(*&s->seqcount.sequence));
			});
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_270(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*&s->seqcount.sequence) ==
					       sizeof(char) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(short) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(int) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(long))))
					__compiletime_assert_270();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			___p1;
		});
	}
	return seq;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__seqprop_spinlock_preemptible(const seqcount_spinlock_t *s)
{
	if (!0)
		return 0;
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__seqprop_spinlock_assert(const seqcount_spinlock_t *s)
{
	;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
seqcount_t *
__seqprop_rwlock_ptr(seqcount_rwlock_t *s)
{
	return &s->seqcount;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
const seqcount_t *
__seqprop_rwlock_const_ptr(const seqcount_rwlock_t *s)
{
	return &s->seqcount;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned
__seqprop_rwlock_sequence(const seqcount_rwlock_t *s)
{
	unsigned seq = ({
		typeof(*&s->seqcount.sequence) ___p1 = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_271(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*&s->seqcount.sequence) ==
					       sizeof(char) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(short) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(int) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(long)) ||
				      sizeof(*&s->seqcount.sequence) ==
					      sizeof(long long)))
					__compiletime_assert_271();
			} while (0);
			(*(const volatile typeof(_Generic(
				(*&s->seqcount.sequence),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 *&s->seqcount
									   .sequence)))
				   *)&(*&s->seqcount.sequence));
		});
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_272(void) __attribute__((__error__(
				"Need native word sized stores/loads for atomicity.")));
			if (!((sizeof(*&s->seqcount.sequence) == sizeof(char) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(short) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(int) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(long))))
				__compiletime_assert_272();
		} while (0);
		__asm__ __volatile__("" : : : "memory");
		___p1;
	});
	if (!0)
		return seq;
	if (0 && __builtin_expect(!!(seq & 1), 0)) {
		;
		;
		seq = ({
			typeof(*&s->seqcount.sequence) ___p1 = ({
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_273(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*&s->seqcount.sequence) ==
						       sizeof(char) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(short) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(int) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(long)) ||
					      sizeof(*&s->seqcount.sequence) ==
						      sizeof(long long)))
						__compiletime_assert_273();
				} while (0);
				(*(const volatile typeof(_Generic(
					(*&s->seqcount.sequence),
								 char: (char)0,
								 unsigned char: (
									 unsigned char)0,
								 signed char: (
									 signed char)0,
								 unsigned short: (
									 unsigned short)0,
								 signed short: (
									 signed short)0,
								 unsigned int: (
									 unsigned int)0,
								 signed int: (
									 signed int)0,
								 unsigned long: (
									 unsigned long)0,
								 signed long: (
									 signed long)0,
								 unsigned long long: (
									 unsigned long long)0,
								 signed long long: (
									 signed long long)0,
								 default: (
									 *&s->seqcount
										   .sequence)))
					   *)&(*&s->seqcount.sequence));
			});
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_274(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*&s->seqcount.sequence) ==
					       sizeof(char) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(short) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(int) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(long))))
					__compiletime_assert_274();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			___p1;
		});
	}
	return seq;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__seqprop_rwlock_preemptible(const seqcount_rwlock_t *s)
{
	if (!0)
		return 0;
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__seqprop_rwlock_assert(const seqcount_rwlock_t *s)
{
	;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
seqcount_t *
__seqprop_mutex_ptr(seqcount_mutex_t *s)
{
	return &s->seqcount;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
const seqcount_t *
__seqprop_mutex_const_ptr(const seqcount_mutex_t *s)
{
	return &s->seqcount;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned
__seqprop_mutex_sequence(const seqcount_mutex_t *s)
{
	unsigned seq = ({
		typeof(*&s->seqcount.sequence) ___p1 = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_275(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*&s->seqcount.sequence) ==
					       sizeof(char) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(short) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(int) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(long)) ||
				      sizeof(*&s->seqcount.sequence) ==
					      sizeof(long long)))
					__compiletime_assert_275();
			} while (0);
			(*(const volatile typeof(_Generic(
				(*&s->seqcount.sequence),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 *&s->seqcount
									   .sequence)))
				   *)&(*&s->seqcount.sequence));
		});
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_276(void) __attribute__((__error__(
				"Need native word sized stores/loads for atomicity.")));
			if (!((sizeof(*&s->seqcount.sequence) == sizeof(char) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(short) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(int) ||
			       sizeof(*&s->seqcount.sequence) == sizeof(long))))
				__compiletime_assert_276();
		} while (0);
		__asm__ __volatile__("" : : : "memory");
		___p1;
	});
	if (!0)
		return seq;
	if (true && __builtin_expect(!!(seq & 1), 0)) {
		;
		;
		seq = ({
			typeof(*&s->seqcount.sequence) ___p1 = ({
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_277(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*&s->seqcount.sequence) ==
						       sizeof(char) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(short) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(int) ||
					       sizeof(*&s->seqcount.sequence) ==
						       sizeof(long)) ||
					      sizeof(*&s->seqcount.sequence) ==
						      sizeof(long long)))
						__compiletime_assert_277();
				} while (0);
				(*(const volatile typeof(_Generic(
					(*&s->seqcount.sequence),
								 char: (char)0,
								 unsigned char: (
									 unsigned char)0,
								 signed char: (
									 signed char)0,
								 unsigned short: (
									 unsigned short)0,
								 signed short: (
									 signed short)0,
								 unsigned int: (
									 unsigned int)0,
								 signed int: (
									 signed int)0,
								 unsigned long: (
									 unsigned long)0,
								 signed long: (
									 signed long)0,
								 unsigned long long: (
									 unsigned long long)0,
								 signed long long: (
									 signed long long)0,
								 default: (
									 *&s->seqcount
										   .sequence)))
					   *)&(*&s->seqcount.sequence));
			});
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_278(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*&s->seqcount.sequence) ==
					       sizeof(char) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(short) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(int) ||
				       sizeof(*&s->seqcount.sequence) ==
					       sizeof(long))))
					__compiletime_assert_278();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			___p1;
		});
	}
	return seq;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__seqprop_mutex_preemptible(const seqcount_mutex_t *s)
{
	if (!0)
		return true;
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__seqprop_mutex_assert(const seqcount_mutex_t *s)
{
	;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
do___read_seqcount_retry(const seqcount_t *s, unsigned start)
{
	kcsan_atomic_next(0);
	return __builtin_expect(
		!!(({
			   do {
				   __attribute__((__noreturn__)) extern void
				   __compiletime_assert_279(void) __attribute__((__error__(
					   "Unsupported access size for {READ,WRITE}_ONCE().")));
				   if (!((sizeof(s->sequence) == sizeof(char) ||
					  sizeof(s->sequence) == sizeof(short) ||
					  sizeof(s->sequence) == sizeof(int) ||
					  sizeof(s->sequence) == sizeof(long)) ||
					 sizeof(s->sequence) ==
						 sizeof(long long)))
					   __compiletime_assert_279();
			   } while (0);
			   (*(const volatile typeof(_Generic(
				   (s->sequence),
							    char: (char)0,
							    unsigned char: (
								    unsigned char)0,
							    signed char: (
								    signed char)0,
							    unsigned short: (
								    unsigned short)0,
							    signed short: (
								    signed short)0,
							    unsigned int: (
								    unsigned int)0,
							    signed int: (
								    signed int)0,
							    unsigned long: (
								    unsigned long)0,
							    signed long: (
								    signed long)0,
							    unsigned long long: (
								    unsigned long long)0,
							    signed long long: (
								    signed long long)0,
							    default: (
								    s->sequence)))
				      *)&(s->sequence));
		   }) != start),
		0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
do_read_seqcount_retry(const seqcount_t *s, unsigned start)
{
	do {
		do {
		} while (0);
		do {
			do {
			} while (0);
			__asm__ __volatile__("" : : : "memory");
		} while (0);
	} while (0);
	return do___read_seqcount_retry(s, start);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
do_raw_write_seqcount_begin(seqcount_t *s)
{
	kcsan_nestable_atomic_begin();
	s->sequence++;
	do {
		do {
		} while (0);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
do_raw_write_seqcount_end(seqcount_t *s)
{
	do {
		do {
		} while (0);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	s->sequence++;
	kcsan_nestable_atomic_end();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
do_write_seqcount_begin_nested(seqcount_t *s, int subclass)
{
	do {
	} while (0);
	do_raw_write_seqcount_begin(s);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
do_write_seqcount_begin(seqcount_t *s)
{
	do_write_seqcount_begin_nested(s, 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
do_write_seqcount_end(seqcount_t *s)
{
	do {
	} while (0);
	do_raw_write_seqcount_end(s);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
do_raw_write_seqcount_barrier(seqcount_t *s)
{
	kcsan_nestable_atomic_begin();
	s->sequence++;
	do {
		do {
		} while (0);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	s->sequence++;
	kcsan_nestable_atomic_end();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
do_write_seqcount_invalidate(seqcount_t *s)
{
	do {
		do {
		} while (0);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	kcsan_nestable_atomic_begin();
	s->sequence += 2;
	kcsan_nestable_atomic_end();
}
typedef struct {
	seqcount_t seqcount;
} seqcount_latch_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned
raw_read_seqcount_latch(const seqcount_latch_t *s)
{
	return ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_280(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(s->seqcount.sequence) == sizeof(char) ||
			       sizeof(s->seqcount.sequence) == sizeof(short) ||
			       sizeof(s->seqcount.sequence) == sizeof(int) ||
			       sizeof(s->seqcount.sequence) == sizeof(long)) ||
			      sizeof(s->seqcount.sequence) ==
				      sizeof(long long)))
				__compiletime_assert_280();
		} while (0);
		(*(const volatile typeof(_Generic((s->seqcount.sequence),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (
							  s->seqcount.sequence)))
			   *)&(s->seqcount.sequence));
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
raw_read_seqcount_latch_retry(const seqcount_latch_t *s, unsigned start)
{
	do {
		do {
		} while (0);
		do {
			do {
			} while (0);
			__asm__ __volatile__("" : : : "memory");
		} while (0);
	} while (0);
	return __builtin_expect(
		!!(({
			   do {
				   __attribute__((__noreturn__)) extern void
				   __compiletime_assert_281(void) __attribute__((__error__(
					   "Unsupported access size for {READ,WRITE}_ONCE().")));
				   if (!((sizeof(s->seqcount.sequence) ==
						  sizeof(char) ||
					  sizeof(s->seqcount.sequence) ==
						  sizeof(short) ||
					  sizeof(s->seqcount.sequence) ==
						  sizeof(int) ||
					  sizeof(s->seqcount.sequence) ==
						  sizeof(long)) ||
					 sizeof(s->seqcount.sequence) ==
						 sizeof(long long)))
					   __compiletime_assert_281();
			   } while (0);
			   (*(const volatile typeof(_Generic(
				   (s->seqcount.sequence),
							    char: (char)0,
							    unsigned char: (
								    unsigned char)0,
							    signed char: (
								    signed char)0,
							    unsigned short: (
								    unsigned short)0,
							    signed short: (
								    signed short)0,
							    unsigned int: (
								    unsigned int)0,
							    signed int: (
								    signed int)0,
							    unsigned long: (
								    unsigned long)0,
							    signed long: (
								    signed long)0,
							    unsigned long long: (
								    unsigned long long)0,
							    signed long long: (
								    signed long long)0,
							    default: (
								    s->seqcount
									    .sequence)))
				      *)&(s->seqcount.sequence));
		   }) != start),
		0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
raw_write_seqcount_latch(seqcount_latch_t *s)
{
	do {
		do {
		} while (0);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	s->seqcount.sequence++;
	do {
		do {
		} while (0);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned
read_seqbegin(const seqlock_t *sl)
{
	unsigned ret = ({
		;
		({
			unsigned __seq;
			while ((__seq = _Generic(*(&sl->seqcount),
					seqcount_t: __seqprop_sequence,
					seqcount_raw_spinlock_t: __seqprop_raw_spinlock_sequence,
					seqcount_spinlock_t: __seqprop_spinlock_sequence,
					seqcount_rwlock_t: __seqprop_rwlock_sequence,
					seqcount_mutex_t: __seqprop_mutex_sequence)(
					&sl->seqcount)) &
			       1)
				cpu_relax();
			kcsan_atomic_next(1000);
			__seq;
		});
	});

	kcsan_atomic_next(0);
	kcsan_flat_atomic_begin();
	return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned
read_seqretry(const seqlock_t *sl, unsigned start)
{
	kcsan_flat_atomic_end();

	return do_read_seqcount_retry(
		_Generic(*(&sl->seqcount),
			seqcount_t: __seqprop_const_ptr,
			seqcount_raw_spinlock_t: __seqprop_raw_spinlock_const_ptr,
			seqcount_spinlock_t: __seqprop_spinlock_const_ptr,
			seqcount_rwlock_t: __seqprop_rwlock_const_ptr,
			seqcount_mutex_t: __seqprop_mutex_const_ptr)(
			&sl->seqcount),
		start);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
write_seqlock(seqlock_t *sl)
{
	spin_lock(&sl->lock);
	do_write_seqcount_begin(&sl->seqcount.seqcount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
write_sequnlock(seqlock_t *sl)
{
	do_write_seqcount_end(&sl->seqcount.seqcount);
	spin_unlock(&sl->lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
write_seqlock_bh(seqlock_t *sl)
{
	spin_lock_bh(&sl->lock);
	do_write_seqcount_begin(&sl->seqcount.seqcount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
write_sequnlock_bh(seqlock_t *sl)
{
	do_write_seqcount_end(&sl->seqcount.seqcount);
	spin_unlock_bh(&sl->lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
write_seqlock_irq(seqlock_t *sl)
{
	spin_lock_irq(&sl->lock);
	do_write_seqcount_begin(&sl->seqcount.seqcount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
write_sequnlock_irq(seqlock_t *sl)
{
	do_write_seqcount_end(&sl->seqcount.seqcount);
	spin_unlock_irq(&sl->lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
__write_seqlock_irqsave(seqlock_t *sl)
{
	unsigned long flags;

	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			flags = _raw_spin_lock_irqsave(
				spinlock_check(&sl->lock));
		} while (0);
	} while (0);
	do_write_seqcount_begin(&sl->seqcount.seqcount);
	return flags;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
write_sequnlock_irqrestore(seqlock_t *sl, unsigned long flags)
{
	do_write_seqcount_end(&sl->seqcount.seqcount);
	spin_unlock_irqrestore(&sl->lock, flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
read_seqlock_excl(seqlock_t *sl)
{
	spin_lock(&sl->lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
read_sequnlock_excl(seqlock_t *sl)
{
	spin_unlock(&sl->lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
read_seqlock_excl_bh(seqlock_t *sl)
{
	spin_lock_bh(&sl->lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
read_sequnlock_excl_bh(seqlock_t *sl)
{
	spin_unlock_bh(&sl->lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
read_seqlock_excl_irq(seqlock_t *sl)
{
	spin_lock_irq(&sl->lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
read_sequnlock_excl_irq(seqlock_t *sl)
{
	spin_unlock_irq(&sl->lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
__read_seqlock_excl_irqsave(seqlock_t *sl)
{
	unsigned long flags;

	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			flags = _raw_spin_lock_irqsave(
				spinlock_check(&sl->lock));
		} while (0);
	} while (0);
	return flags;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
read_sequnlock_excl_irqrestore(seqlock_t *sl, unsigned long flags)
{
	spin_unlock_irqrestore(&sl->lock, flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
read_seqbegin_or_lock(seqlock_t *lock, int *seq)
{
	if (!(*seq & 1))
		*seq = read_seqbegin(lock);
	else
		read_seqlock_excl(lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
need_seqretry(seqlock_t *lock, int seq)
{
	return !(seq & 1) && read_seqretry(lock, seq);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
done_seqretry(seqlock_t *lock, int seq)
{
	if (seq & 1)
		read_sequnlock_excl(lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
read_seqbegin_or_lock_irqsave(seqlock_t *lock, int *seq)
{
	unsigned long flags = 0;

	if (!(*seq & 1))
		*seq = read_seqbegin(lock);
	else
		do {
			flags = __read_seqlock_excl_irqsave(lock);
		} while (0);

	return flags;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
done_seqretry_irqrestore(seqlock_t *lock, int seq, unsigned long flags)
{
	if (seq & 1)
		read_sequnlock_excl_irqrestore(lock, flags);
}

typedef struct {
	unsigned long bits[((((1 << 6)) + ((sizeof(long) * 8)) - 1) /
			    ((sizeof(long) * 8)))];
} nodemask_t;

extern unsigned int __invalid_size_argument_for_IOC;

extern int nr_irqs;
extern struct irq_desc *irq_to_desc(unsigned int irq);
unsigned int irq_get_next_irq(unsigned int offset);
struct rand_pool_info {
	int entropy_count;
	int buf_size;
	__u32 buf[];
};
struct vgetrandom_opaque_params {
	__u32 size_of_opaque_state;
	__u32 mmap_prot;
	__u32 mmap_flags;
	__u32 reserved[13];
};

struct notifier_block;

void add_device_randomness(const void *buf, size_t len);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
add_bootloader_randomness(const void *buf, size_t len);
void add_input_randomness(unsigned int type, unsigned int code,
			  unsigned int value);
void add_interrupt_randomness(int irq);
void add_hwgenerator_randomness(const void *buf, size_t len, size_t entropy,
				bool sleep_after);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
add_latent_entropy(void)
{
	add_device_randomness(((void *)0), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
register_random_vmfork_notifier(struct notifier_block *nb)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
unregister_random_vmfork_notifier(struct notifier_block *nb)
{
	return 0;
}

void get_random_bytes(void *buf, size_t len);
u8 get_random_u8(void);
u16 get_random_u16(void);
u32 get_random_u32(void);
u64 get_random_u64(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
get_random_long(void)
{
	return get_random_u64();
}

u32 __get_random_u32_below(u32 ceil);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
get_random_u32_below(u32 ceil)
{
	if (!__builtin_constant_p(ceil))
		return __get_random_u32_below(ceil);
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_282(void) __attribute__((__error__(
			"get_random_u32_below() must take ceil > 0")));
		if (!(!(!ceil)))
			__compiletime_assert_282();
	} while (0);
	if (ceil <= 1)
		return 0;
	for (;;) {
		if (ceil <= 1U << 8) {
			u32 mult = ceil * get_random_u8();
			if (__builtin_expect(!!(is_power_of_2(ceil) ||
						(u8)mult >= (1U << 8) % ceil),
					     1))
				return mult >> 8;
		} else if (ceil <= 1U << 16) {
			u32 mult = ceil * get_random_u16();
			if (__builtin_expect(!!(is_power_of_2(ceil) ||
						(u16)mult >= (1U << 16) % ceil),
					     1))
				return mult >> 16;
		} else {
			u64 mult = (u64)ceil * get_random_u32();
			if (__builtin_expect(!!(is_power_of_2(ceil) ||
						(u32)mult >= -ceil % ceil),
					     1))
				return mult >> 32;
		}
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
get_random_u32_above(u32 floor)
{
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_283(void) __attribute__((__error__(
			"get_random_u32_above() must take floor < U32_MAX")));
		if (!(!(__builtin_constant_p(floor) && floor == ((u32)~0U))))
			__compiletime_assert_283();
	} while (0);

	return floor + 1 + get_random_u32_below(((u32)~0U) - floor);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
get_random_u32_inclusive(u32 floor, u32 ceil)
{
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_284(void) __attribute__((__error__(
			"get_random_u32_inclusive() must take floor <= ceil")));
		if (!(!(__builtin_constant_p(floor) &&
			__builtin_constant_p(ceil) &&
			(floor > ceil || ceil - floor == ((u32)~0U)))))
			__compiletime_assert_284();
	} while (0);

	return floor + get_random_u32_below(ceil - floor + 1);
}

void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
random_init_early(const char *command_line);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
random_init(void);
bool rng_is_initialized(void);
int wait_for_random_bytes(void);
int execute_with_initialized_rng(struct notifier_block *nb);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_random_bytes_wait(void *buf, size_t nbytes)
{
	int ret = wait_for_random_bytes();
	get_random_bytes(buf, nbytes);
	return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_random_u8_wait(u8 *out)
{
	int ret = wait_for_random_bytes();
	if (__builtin_expect(!!(ret), 0))
		return ret;
	*out = get_random_u8();
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_random_u16_wait(u16 *out)
{
	int ret = wait_for_random_bytes();
	if (__builtin_expect(!!(ret), 0))
		return ret;
	*out = get_random_u16();
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_random_u32_wait(u32 *out)
{
	int ret = wait_for_random_bytes();
	if (__builtin_expect(!!(ret), 0))
		return ret;
	*out = get_random_u32();
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_random_u64_wait(u32 *out)
{
	int ret = wait_for_random_bytes();
	if (__builtin_expect(!!(ret), 0))
		return ret;
	*out = get_random_u64();
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_random_long_wait(unsigned long *out)
{
	int ret = wait_for_random_bytes();
	if (__builtin_expect(!!(ret), 0))
		return ret;
	*out = get_random_long();
	return 0;
}

bool __do_once_start(bool *done, unsigned long *flags);
void __do_once_done(bool *done, struct static_key_true *once_key,
		    unsigned long *flags, struct module *mod);

bool __do_once_sleepable_start(bool *done);
void __do_once_sleepable_done(bool *done, struct static_key_true *once_key,
			      struct module *mod);

struct rnd_state {
	__u32 s1, s2, s3, s4;
};

u32 prandom_u32_state(struct rnd_state *state);
void prandom_bytes_state(struct rnd_state *state, void *buf, size_t nbytes);
void prandom_seed_full_state(struct rnd_state *pcpu_state);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
__seed(u32 x, u32 m)
{
	return (x < m) ? x + m : x;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
prandom_seed_state(struct rnd_state *state, u64 seed)
{
	u32 i = ((seed >> 32) ^ (seed << 10) ^ seed) & 0xffffffffUL;

	state->s1 = __seed(i, 2U);
	state->s2 = __seed(i, 8U);
	state->s3 = __seed(i, 16U);
	state->s4 = __seed(i, 128U);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
next_pseudo_random32(u32 seed)
{
	return seed * 1664525 + 1013904223;
}

int random_prepare_cpu(unsigned int cpu);
int random_online_cpu(unsigned int cpu);

extern const struct file_operations random_fops, urandom_fops;

extern nodemask_t _unused_nodemask_arg_;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
__nodemask_pr_numnodes(const nodemask_t *m)
{
	return m ? (1 << 6) : 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
const unsigned long *
__nodemask_pr_bits(const nodemask_t *m)
{
	return m ? m->bits : ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__node_set(int node, volatile nodemask_t *dstp)
{
	set_bit(node, dstp->bits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__node_clear(int node, volatile nodemask_t *dstp)
{
	clear_bit(node, dstp->bits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_setall(nodemask_t *dstp, unsigned int nbits)
{
	bitmap_fill(dstp->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_clear(nodemask_t *dstp, unsigned int nbits)
{
	bitmap_zero(dstp->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__node_test_and_set(int node, nodemask_t *addr)
{
	return test_and_set_bit(node, addr->bits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_and(nodemask_t *dstp, const nodemask_t *src1p, const nodemask_t *src2p,
	    unsigned int nbits)
{
	bitmap_and(dstp->bits, src1p->bits, src2p->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_or(nodemask_t *dstp, const nodemask_t *src1p, const nodemask_t *src2p,
	   unsigned int nbits)
{
	bitmap_or(dstp->bits, src1p->bits, src2p->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_xor(nodemask_t *dstp, const nodemask_t *src1p, const nodemask_t *src2p,
	    unsigned int nbits)
{
	bitmap_xor(dstp->bits, src1p->bits, src2p->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_andnot(nodemask_t *dstp, const nodemask_t *src1p,
	       const nodemask_t *src2p, unsigned int nbits)
{
	bitmap_andnot(dstp->bits, src1p->bits, src2p->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_complement(nodemask_t *dstp, const nodemask_t *srcp, unsigned int nbits)
{
	bitmap_complement(dstp->bits, srcp->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__nodes_equal(const nodemask_t *src1p, const nodemask_t *src2p,
	      unsigned int nbits)
{
	return bitmap_equal(src1p->bits, src2p->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__nodes_intersects(const nodemask_t *src1p, const nodemask_t *src2p,
		   unsigned int nbits)
{
	return bitmap_intersects(src1p->bits, src2p->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__nodes_subset(const nodemask_t *src1p, const nodemask_t *src2p,
	       unsigned int nbits)
{
	return bitmap_subset(src1p->bits, src2p->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__nodes_empty(const nodemask_t *srcp, unsigned int nbits)
{
	return bitmap_empty(srcp->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__nodes_full(const nodemask_t *srcp, unsigned int nbits)
{
	return bitmap_full(srcp->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
__nodes_weight(const nodemask_t *srcp, unsigned int nbits)
{
	return bitmap_weight(srcp->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_shift_right(nodemask_t *dstp, const nodemask_t *srcp, int n, int nbits)
{
	bitmap_shift_right(dstp->bits, srcp->bits, n, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_shift_left(nodemask_t *dstp, const nodemask_t *srcp, int n, int nbits)
{
	bitmap_shift_left(dstp->bits, srcp->bits, n, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
__first_node(const nodemask_t *srcp)
{
	return ({
		unsigned int __UNIQUE_ID_x_285 = ((1 << 6));
		unsigned int __UNIQUE_ID_y_286 =
			(find_first_bit(srcp->bits, (1 << 6)));
		((__UNIQUE_ID_x_285) < (__UNIQUE_ID_y_286) ?
			 (__UNIQUE_ID_x_285) :
			 (__UNIQUE_ID_y_286));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
__next_node(int n, const nodemask_t *srcp)
{
	return ({
		unsigned int __UNIQUE_ID_x_287 = ((1 << 6));
		unsigned int __UNIQUE_ID_y_288 =
			(find_next_bit(srcp->bits, (1 << 6), n + 1));
		((__UNIQUE_ID_x_287) < (__UNIQUE_ID_y_288) ?
			 (__UNIQUE_ID_x_287) :
			 (__UNIQUE_ID_y_288));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
__next_node_in(int node, const nodemask_t *srcp)
{
	unsigned int ret = __next_node(node, srcp);

	if (ret == (1 << 6))
		ret = __first_node(srcp);
	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
init_nodemask_of_node(nodemask_t *mask, int node)
{
	__nodes_clear(&(*mask), (1 << 6));
	__node_set((node), &(*mask));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
__first_unset_node(const nodemask_t *maskp)
{
	return ({
		unsigned int __UNIQUE_ID_x_289 = ((1 << 6));
		unsigned int __UNIQUE_ID_y_290 =
			(find_first_zero_bit(maskp->bits, (1 << 6)));
		((__UNIQUE_ID_x_289) < (__UNIQUE_ID_y_290) ?
			 (__UNIQUE_ID_x_289) :
			 (__UNIQUE_ID_y_290));
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
__nodemask_parse_user(const char *buf, int len, nodemask_t *dstp, int nbits)
{
	return bitmap_parse_user(buf, len, dstp->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
__nodelist_parse(const char *buf, nodemask_t *dstp, int nbits)
{
	return bitmap_parselist(buf, dstp->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
__node_remap(int oldbit, const nodemask_t *oldp, const nodemask_t *newp,
	     int nbits)
{
	return bitmap_bitremap(oldbit, oldp->bits, newp->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_remap(nodemask_t *dstp, const nodemask_t *srcp, const nodemask_t *oldp,
	      const nodemask_t *newp, int nbits)
{
	bitmap_remap(dstp->bits, srcp->bits, oldp->bits, newp->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_onto(nodemask_t *dstp, const nodemask_t *origp,
	     const nodemask_t *relmapp, int nbits)
{
	bitmap_onto(dstp->bits, origp->bits, relmapp->bits, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__nodes_fold(nodemask_t *dstp, const nodemask_t *origp, int sz, int nbits)
{
	bitmap_fold(dstp->bits, origp->bits, sz, nbits);
}
enum node_states {
	N_POSSIBLE,
	N_ONLINE,
	N_NORMAL_MEMORY,

	N_HIGH_MEMORY = N_NORMAL_MEMORY,

	N_MEMORY,
	N_CPU,
	N_GENERIC_INITIATOR,
	NR_NODE_STATES
};

extern nodemask_t node_states[NR_NODE_STATES];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
node_state(int node, enum node_states state)
{
	return ((__builtin_constant_p((node)) &&
		 __builtin_constant_p((uintptr_t)((node_states[state]).bits) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)((node_states[state]).bits) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(
			 *(const unsigned long *)((node_states[state]).bits))) ?
			const_test_bit((node), (node_states[state]).bits) :
			_test_bit((node), (node_states[state]).bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
node_set_state(int node, enum node_states state)
{
	__node_set(node, &node_states[state]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
node_clear_state(int node, enum node_states state)
{
	__node_clear(node, &node_states[state]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
num_node_state(enum node_states state)
{
	return __nodes_weight(&(node_states[state]), (1 << 6));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
next_online_node(int nid)
{
	return __next_node((nid), &(node_states[N_ONLINE]));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
next_memory_node(int nid)
{
	return __next_node((nid), &(node_states[N_MEMORY]));
}

extern unsigned int nr_node_ids;
extern unsigned int nr_online_nodes;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
node_set_online(int nid)
{
	node_set_state(nid, N_ONLINE);
	nr_online_nodes = num_node_state(N_ONLINE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
node_set_offline(int nid)
{
	node_clear_state(nid, N_ONLINE);
	nr_online_nodes = num_node_state(N_ONLINE);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
node_random(const nodemask_t *maskp)
{
	int w, bit;

	w = __nodes_weight(&(*maskp), (1 << 6));
	switch (w) {
	case 0:
		bit = (-1);
		break;
	case 1:
		bit = __first_node(&(*maskp));
		break;
	default:
		bit = find_nth_bit(maskp->bits, (1 << 6),
				   get_random_u32_below(w));
		break;
	}
	return bit;
}
struct nodemask_scratch {
	nodemask_t mask1;
	nodemask_t mask2;
};
enum pageblock_bits {
	PB_migrate,
	PB_migrate_end = PB_migrate + 3 - 1,

	PB_migrate_skip,

	NR_PAGEBLOCK_BITS
};
struct page;

unsigned long get_pfnblock_flags_mask(const struct page *page,
				      unsigned long pfn, unsigned long mask);

void set_pfnblock_flags_mask(struct page *page, unsigned long flags,
			     unsigned long pfn, unsigned long mask);

struct arch_tlbflush_unmap_batch {
	struct cpumask cpumask;
};

enum { MM_FILEPAGES, MM_ANONPAGES, MM_SWAPENTS, MM_SHMEMPAGES, NR_MM_COUNTERS };

struct page;

struct page_frag {
	struct page *page;

	__u32 offset;
	__u32 size;
};

struct tlbflush_unmap_batch {
	struct arch_tlbflush_unmap_batch arch;

	bool flush_required;

	bool writable;
};

struct kref {
	refcount_t refcount;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kref_init(struct kref *kref)
{
	refcount_set(&kref->refcount, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
kref_read(const struct kref *kref)
{
	return refcount_read(&kref->refcount);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kref_get(struct kref *kref)
{
	refcount_inc(&kref->refcount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kref_put(struct kref *kref, void (*release)(struct kref *kref))
{
	if (refcount_dec_and_test(&kref->refcount)) {
		release(kref);
		return 1;
	}
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kref_put_mutex(struct kref *kref, void (*release)(struct kref *kref),
	       struct mutex *lock)
{
	if (refcount_dec_and_mutex_lock(&kref->refcount, lock)) {
		release(kref);
		return 1;
	}
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kref_put_lock(struct kref *kref, void (*release)(struct kref *kref),
	      spinlock_t *lock)
{
	if (refcount_dec_and_lock(&kref->refcount, lock)) {
		release(kref);
		return 1;
	}
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	kref_get_unless_zero(struct kref *kref)
{
	return refcount_inc_not_zero(&kref->refcount);
}

struct rb_node {
	unsigned long __rb_parent_color;
	struct rb_node *rb_right;
	struct rb_node *rb_left;
} __attribute__((aligned(sizeof(long))));

struct rb_root {
	struct rb_node *rb_node;
};
struct rb_root_cached {
	struct rb_root rb_root;
	struct rb_node *rb_leftmost;
};

void ct_irq_enter(void);
void ct_irq_exit(void);
void ct_irq_enter_irqson(void);
void ct_irq_exit_irqson(void);
void ct_nmi_enter(void);
void ct_nmi_exit(void);

void call_rcu(struct callback_head *head, rcu_callback_t func);
void rcu_barrier_tasks(void);
void synchronize_rcu(void);

struct rcu_gp_oldstate;
unsigned long get_completed_synchronize_rcu(void);
void get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
same_state_synchronize_rcu(unsigned long oldstate1, unsigned long oldstate2)
{
	return oldstate1 == oldstate2;
}

void __rcu_read_lock(void);
void __rcu_read_unlock(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
call_rcu_hurry(struct callback_head *head, rcu_callback_t func)
{
	call_rcu(head, func);
}

void rcu_init(void);
extern int rcu_scheduler_active;
void rcu_sched_clock_irq(int user);

void rcu_init_tasks_generic(void);

void rcu_sysrq_start(void);
void rcu_sysrq_end(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcu_irq_work_resched(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcu_init_nohz(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
rcu_nocb_cpu_offload(int cpu)
{
	return -22;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
rcu_nocb_cpu_deoffload(int cpu)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcu_nocb_flush_deferred_wakeup(void)
{
}
void call_rcu_tasks(struct callback_head *head, rcu_callback_t func);
void synchronize_rcu_tasks(void);
void rcu_tasks_torture_stats_print(char *tt, char *tf);
void exit_tasks_rcu_start(void);
void exit_tasks_rcu_finish(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
rcu_trace_implies_rcu_gp(void)
{
	return true;
}
void rcu_softirq_qs(void);
void rcu_note_context_switch(bool preempt);
int rcu_needs_cpu(void);
void rcu_cpu_stall_reset(void);
void rcu_request_urgent_qs_task(struct task_struct *t);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcu_virt_note_context_switch(void)
{
	rcu_note_context_switch(false);
}

void synchronize_rcu_expedited(void);
void kvfree_call_rcu(struct callback_head *head, void *ptr);
void kvfree_rcu_barrier(void);

void rcu_barrier(void);
void rcu_momentary_eqs(void);
void kfree_rcu_scheduler_running(void);
bool rcu_gp_might_be_stalled(void);

struct rcu_gp_oldstate {
	unsigned long rgos_norm;
	unsigned long rgos_exp;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
same_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp1,
				struct rcu_gp_oldstate *rgosp2)
{
	return rgosp1->rgos_norm == rgosp2->rgos_norm &&
	       rgosp1->rgos_exp == rgosp2->rgos_exp;
}

unsigned long start_poll_synchronize_rcu_expedited(void);
void start_poll_synchronize_rcu_expedited_full(struct rcu_gp_oldstate *rgosp);
void cond_synchronize_rcu_expedited(unsigned long oldstate);
void cond_synchronize_rcu_expedited_full(struct rcu_gp_oldstate *rgosp);
unsigned long get_state_synchronize_rcu(void);
void get_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);
unsigned long start_poll_synchronize_rcu(void);
void start_poll_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);
bool poll_state_synchronize_rcu(unsigned long oldstate);
bool poll_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);
void cond_synchronize_rcu(unsigned long oldstate);
void cond_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcu_irq_exit_check_preempt(void)
{
}

struct task_struct;
void rcu_preempt_deferred_qs(struct task_struct *t);

void exit_rcu(void);

void rcu_scheduler_starting(void);
extern int rcu_scheduler_active;
void rcu_end_inkernel_boot(void);
bool rcu_inkernel_boot_has_ended(void);
bool rcu_is_watching(void);

int rcutree_prepare_cpu(unsigned int cpu);
int rcutree_online_cpu(unsigned int cpu);
void rcutree_report_cpu_starting(unsigned int cpu);

int rcutree_dead_cpu(unsigned int cpu);
int rcutree_dying_cpu(unsigned int cpu);
int rcutree_offline_cpu(unsigned int cpu);

void rcutree_migrate_callbacks(int cpu);

void rcutree_report_cpu_dead(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
init_rcu_head(struct callback_head *head)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
destroy_rcu_head(struct callback_head *head)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
init_rcu_head_on_stack(struct callback_head *head)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
destroy_rcu_head_on_stack(struct callback_head *head)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
rcu_lockdep_current_cpu_online(void)
{
	return true;
}

extern struct lockdep_map rcu_lock_map;
extern struct lockdep_map rcu_bh_lock_map;
extern struct lockdep_map rcu_sched_lock_map;
extern struct lockdep_map rcu_callback_map;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
rcu_read_lock_held(void)
{
	return 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
rcu_read_lock_bh_held(void)
{
	return 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
rcu_read_lock_sched_held(void)
{
	return !(preempt_count() == 0 && !({
			 unsigned long _flags;
			 do {
				 ({
					 unsigned long __dummy;
					 typeof(_flags) __dummy2;
					 (void)(&__dummy == &__dummy2);
					 1;
				 });
				 _flags = arch_local_save_flags();
			 } while (0);
			 ({
				 ({
					 unsigned long __dummy;
					 typeof(_flags) __dummy2;
					 (void)(&__dummy == &__dummy2);
					 1;
				 });
				 arch_irqs_disabled_flags(_flags);
			 });
		 }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
rcu_read_lock_any_held(void)
{
	return !(preempt_count() == 0 && !({
			 unsigned long _flags;
			 do {
				 ({
					 unsigned long __dummy;
					 typeof(_flags) __dummy2;
					 (void)(&__dummy == &__dummy2);
					 1;
				 });
				 _flags = arch_local_save_flags();
			 } while (0);
			 ({
				 ({
					 unsigned long __dummy;
					 typeof(_flags) __dummy2;
					 (void)(&__dummy == &__dummy2);
					 1;
				 });
				 arch_irqs_disabled_flags(_flags);
			 });
		 }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
debug_lockdep_rcu_enabled(void)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
rcu_read_lock(void)
{
	__rcu_read_lock();
	(void)0;
	do {
	} while (0);
	do {
	} while (0 && (!rcu_is_watching()));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcu_read_unlock(void)
{
	do {
	} while (0 && (!rcu_is_watching()));

	do {
	} while (0);
	(void)0;
	__rcu_read_unlock();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcu_read_lock_bh(void)
{
	local_bh_disable();
	(void)0;
	do {
	} while (0);
	do {
	} while (0 && (!rcu_is_watching()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcu_read_unlock_bh(void)
{
	do {
	} while (0 && (!rcu_is_watching()));

	do {
	} while (0);
	(void)0;
	local_bh_enable();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcu_read_lock_sched(void)
{
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	(void)0;
	do {
	} while (0);
	do {
	} while (0 && (!rcu_is_watching()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((no_instrument_function)) void
rcu_read_lock_sched_notrace(void)
{
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	(void)0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcu_read_unlock_sched(void)
{
	do {
	} while (0 && (!rcu_is_watching()));

	do {
	} while (0);
	(void)0;
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule291 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((no_instrument_function)) void
rcu_read_unlock_sched_notrace(void)
{
	(void)0;
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule_notrace292 =
					(void *)(uintptr_t)&__SCK__preempt_schedule_notrace;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule_notrace"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcu_head_init(struct callback_head *rhp)
{
	rhp->func = (rcu_callback_t)~0L;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
rcu_head_after_call_rcu(struct callback_head *rhp, rcu_callback_t f)
{
	rcu_callback_t func = ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_293(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(rhp->func) == sizeof(char) ||
			       sizeof(rhp->func) == sizeof(short) ||
			       sizeof(rhp->func) == sizeof(int) ||
			       sizeof(rhp->func) == sizeof(long)) ||
			      sizeof(rhp->func) == sizeof(long long)))
				__compiletime_assert_293();
		} while (0);
		(*(const volatile typeof(_Generic((rhp->func),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (rhp->func)))
			   *)&(rhp->func));
	});

	if (func == f)
		return true;
	({
		int __ret_warn_on = !!(func != (rcu_callback_t)~0L);
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) |
						      ((1 << 1) | ((9) << 8));
				({
					asm volatile(
						"294"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"294"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(294));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/rcupdate.h"),
						  "i"(1153), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"295"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"295"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(295));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	return false;
}

extern int rcu_expedited;
extern int rcu_normal;

typedef struct {
	void *lock;
	;
} class_rcu_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_rcu_destructor(class_rcu_t *_T)
{
	if (_T->lock) {
		rcu_read_unlock();
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_rcu_lock_ptr(class_rcu_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_rcu_t
class_rcu_constructor(void)
{
	class_rcu_t _t = { .lock = (void *)1 },
		    *_T __attribute__((__unused__)) = &_t;
	do {
		rcu_read_lock();
		(void)0;
	} while (0);
	return _t;
}
extern void rb_insert_color(struct rb_node *, struct rb_root *);
extern void rb_erase(struct rb_node *, struct rb_root *);

extern struct rb_node *rb_next(const struct rb_node *);
extern struct rb_node *rb_prev(const struct rb_node *);
extern struct rb_node *rb_first(const struct rb_root *);
extern struct rb_node *rb_last(const struct rb_root *);

extern struct rb_node *rb_first_postorder(const struct rb_root *);
extern struct rb_node *rb_next_postorder(const struct rb_node *);

extern void rb_replace_node(struct rb_node *victim, struct rb_node *new,
			    struct rb_root *root);
extern void rb_replace_node_rcu(struct rb_node *victim, struct rb_node *new,
				struct rb_root *root);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rb_link_node(struct rb_node *node, struct rb_node *parent,
	     struct rb_node **rb_link)
{
	node->__rb_parent_color = (unsigned long)parent;
	node->rb_left = node->rb_right = ((void *)0);

	*rb_link = node;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rb_link_node_rcu(struct rb_node *node, struct rb_node *parent,
		 struct rb_node **rb_link)
{
	node->__rb_parent_color = (unsigned long)parent;
	node->rb_left = node->rb_right = ((void *)0);

	do {
		uintptr_t _r_a_p__v = (uintptr_t)(node);
		;
		if (__builtin_constant_p(node) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_296(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof((*rb_link)) ==
						       sizeof(char) ||
					       sizeof((*rb_link)) ==
						       sizeof(short) ||
					       sizeof((*rb_link)) ==
						       sizeof(int) ||
					       sizeof((*rb_link)) ==
						       sizeof(long)) ||
					      sizeof((*rb_link)) ==
						      sizeof(long long)))
						__compiletime_assert_296();
				} while (0);
				do {
					*(volatile typeof((*rb_link)) *)&(
						(*rb_link)) =
						((typeof(*rb_link))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_297(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&*rb_link) ==
							       sizeof(char) ||
						       sizeof(*&*rb_link) ==
							       sizeof(short) ||
						       sizeof(*&*rb_link) ==
							       sizeof(int) ||
						       sizeof(*&*rb_link) ==
							       sizeof(long))))
							__compiletime_assert_297();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_298(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&*rb_link) ==
								       sizeof(char) ||
							       sizeof(*&*rb_link) ==
								       sizeof(short) ||
							       sizeof(*&*rb_link) ==
								       sizeof(int) ||
							       sizeof(*&*rb_link) ==
								       sizeof(long)) ||
							      sizeof(*&*rb_link) ==
								      sizeof(long long)))
								__compiletime_assert_298();
						} while (0);
						do {
							*(volatile typeof(*&*rb_link)
								  *)&(*&*rb_link) =
								((typeof(*(
									(typeof(*rb_link))
										_r_a_p__v))
									  *)((
									typeof(*rb_link))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rb_insert_color_cached(struct rb_node *node, struct rb_root_cached *root,
		       bool leftmost)
{
	if (leftmost)
		root->rb_leftmost = node;
	rb_insert_color(node, &root->rb_root);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct rb_node *
rb_erase_cached(struct rb_node *node, struct rb_root_cached *root)
{
	struct rb_node *leftmost = ((void *)0);

	if (root->rb_leftmost == node)
		leftmost = root->rb_leftmost = rb_next(node);

	rb_erase(node, &root->rb_root);

	return leftmost;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rb_replace_node_cached(struct rb_node *victim, struct rb_node *new,
		       struct rb_root_cached *root)
{
	if (root->rb_leftmost == victim)
		root->rb_leftmost = new;
	rb_replace_node(victim, new, &root->rb_root);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct rb_node *
rb_add_cached(struct rb_node *node, struct rb_root_cached *tree,
	      bool (*less)(struct rb_node *, const struct rb_node *))
{
	struct rb_node **link = &tree->rb_root.rb_node;
	struct rb_node *parent = ((void *)0);
	bool leftmost = true;

	while (*link) {
		parent = *link;
		if (less(node, parent)) {
			link = &parent->rb_left;
		} else {
			link = &parent->rb_right;
			leftmost = false;
		}
	}

	rb_link_node(node, parent, link);
	rb_insert_color_cached(node, tree, leftmost);

	return leftmost ? node : ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
rb_add(struct rb_node *node, struct rb_root *tree,
       bool (*less)(struct rb_node *, const struct rb_node *))
{
	struct rb_node **link = &tree->rb_node;
	struct rb_node *parent = ((void *)0);

	while (*link) {
		parent = *link;
		if (less(node, parent))
			link = &parent->rb_left;
		else
			link = &parent->rb_right;
	}

	rb_link_node(node, parent, link);
	rb_insert_color(node, tree);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct rb_node *
rb_find_add(struct rb_node *node, struct rb_root *tree,
	    int (*cmp)(struct rb_node *, const struct rb_node *))
{
	struct rb_node **link = &tree->rb_node;
	struct rb_node *parent = ((void *)0);
	int c;

	while (*link) {
		parent = *link;
		c = cmp(node, parent);

		if (c < 0)
			link = &parent->rb_left;
		else if (c > 0)
			link = &parent->rb_right;
		else
			return parent;
	}

	rb_link_node(node, parent, link);
	rb_insert_color(node, tree);
	return ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct rb_node *
rb_find_add_rcu(struct rb_node *node, struct rb_root *tree,
		int (*cmp)(struct rb_node *, const struct rb_node *))
{
	struct rb_node **link = &tree->rb_node;
	struct rb_node *parent = ((void *)0);
	int c;

	while (*link) {
		parent = *link;
		c = cmp(node, parent);

		if (c < 0)
			link = &parent->rb_left;
		else if (c > 0)
			link = &parent->rb_right;
		else
			return parent;
	}

	rb_link_node_rcu(node, parent, link);
	rb_insert_color(node, tree);
	return ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct rb_node *
rb_find(const void *key, const struct rb_root *tree,
	int (*cmp)(const void *key, const struct rb_node *))
{
	struct rb_node *node = tree->rb_node;

	while (node) {
		int c = cmp(key, node);

		if (c < 0)
			node = node->rb_left;
		else if (c > 0)
			node = node->rb_right;
		else
			return node;
	}

	return ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct rb_node *
rb_find_rcu(const void *key, const struct rb_root *tree,
	    int (*cmp)(const void *key, const struct rb_node *))
{
	struct rb_node *node = tree->rb_node;

	while (node) {
		int c = cmp(key, node);

		if (c < 0)
			node = ({
				typeof(node->rb_left) __UNIQUE_ID_rcu299 = ({
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_300(void)
							__attribute__((__error__(
								"Unsupported access size for {READ,WRITE}_ONCE().")));
						if (!((sizeof(node->rb_left) ==
							       sizeof(char) ||
						       sizeof(node->rb_left) ==
							       sizeof(short) ||
						       sizeof(node->rb_left) ==
							       sizeof(int) ||
						       sizeof(node->rb_left) ==
							       sizeof(long)) ||
						      sizeof(node->rb_left) ==
							      sizeof(long long)))
							__compiletime_assert_300();
					} while (0);
					(*(const volatile typeof(_Generic(
						(node->rb_left),
									 char: (char)0,
									 unsigned char: (
										 unsigned char)0,
									 signed char: (
										 signed char)0,
									 unsigned short: (
										 unsigned short)0,
									 signed short: (
										 signed short)0,
									 unsigned int: (
										 unsigned int)0,
									 signed int: (
										 signed int)0,
									 unsigned long: (
										 unsigned long)0,
									 signed long: (
										 signed long)0,
									 unsigned long long: (
										 unsigned long long)0,
									 signed long long: (
										 signed long long)0,
									 default: (
										 node->rb_left)))
						   *)&(node->rb_left));
				});
				((typeof(*node->rb_left) *)(__UNIQUE_ID_rcu299));
			});
		else if (c > 0)
			node = ({
				typeof(node->rb_right) __UNIQUE_ID_rcu301 = ({
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_302(void)
							__attribute__((__error__(
								"Unsupported access size for {READ,WRITE}_ONCE().")));
						if (!((sizeof(node->rb_right) ==
							       sizeof(char) ||
						       sizeof(node->rb_right) ==
							       sizeof(short) ||
						       sizeof(node->rb_right) ==
							       sizeof(int) ||
						       sizeof(node->rb_right) ==
							       sizeof(long)) ||
						      sizeof(node->rb_right) ==
							      sizeof(long long)))
							__compiletime_assert_302();
					} while (0);
					(*(const volatile typeof(_Generic(
						(node->rb_right),
									 char: (char)0,
									 unsigned char: (
										 unsigned char)0,
									 signed char: (
										 signed char)0,
									 unsigned short: (
										 unsigned short)0,
									 signed short: (
										 signed short)0,
									 unsigned int: (
										 unsigned int)0,
									 signed int: (
										 signed int)0,
									 unsigned long: (
										 unsigned long)0,
									 signed long: (
										 signed long)0,
									 unsigned long long: (
										 unsigned long long)0,
									 signed long long: (
										 signed long long)0,
									 default: (
										 node->rb_right)))
						   *)&(node->rb_right));
				});
				((typeof(*node->rb_right)
					  *)(__UNIQUE_ID_rcu301));
			});
		else
			return node;
	}

	return ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct rb_node *
rb_find_first(const void *key, const struct rb_root *tree,
	      int (*cmp)(const void *key, const struct rb_node *))
{
	struct rb_node *node = tree->rb_node;
	struct rb_node *match = ((void *)0);

	while (node) {
		int c = cmp(key, node);

		if (c <= 0) {
			if (!c)
				match = node;
			node = node->rb_left;
		} else if (c > 0) {
			node = node->rb_right;
		}
	}

	return match;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct rb_node *
rb_next_match(const void *key, struct rb_node *node,
	      int (*cmp)(const void *key, const struct rb_node *))
{
	node = rb_next(node);
	if (node && cmp(key, node))
		node = ((void *)0);
	return node;
}
struct maple_metadata {
	unsigned char end;
	unsigned char gap;
};
struct maple_range_64 {
	struct maple_pnode *parent;
	unsigned long pivot[16 - 1];
	union {
		void *slot[16];
		struct {
			void *pad[16 - 1];
			struct maple_metadata meta;
		};
	};
};
struct maple_arange_64 {
	struct maple_pnode *parent;
	unsigned long pivot[10 - 1];
	void *slot[10];
	unsigned long gap[10];
	struct maple_metadata meta;
};

struct maple_alloc {
	unsigned long total;
	unsigned char node_count;
	unsigned int request_count;
	struct maple_alloc *slot[(31 - 1)];
};

struct maple_topiary {
	struct maple_pnode *parent;
	struct maple_enode *next;
};

enum maple_type {
	maple_dense,
	maple_leaf_64,
	maple_range_64,
	maple_arange_64,
};

enum store_type {
	wr_invalid,
	wr_new_root,
	wr_store_root,
	wr_exact_fit,
	wr_spanning_store,
	wr_split_store,
	wr_rebalance,
	wr_append,
	wr_node_store,
	wr_slot_store,
};
typedef struct {
} lockdep_map_p;
struct maple_tree {
	union {
		spinlock_t ma_lock;
		lockdep_map_p ma_external_lock;
	};
	unsigned int ma_flags;
	void *ma_root;
};
struct maple_node {
	union {
		struct {
			struct maple_pnode *parent;
			void *slot[31];
		};
		struct {
			void *pad;
			struct callback_head rcu;
			struct maple_enode *piv_parent;
			unsigned char parent_slot;
			enum maple_type type;
			unsigned char slot_len;
			unsigned int ma_flags;
		};
		struct maple_range_64 mr64;
		struct maple_arange_64 ma64;
		struct maple_alloc alloc;
	};
};
struct ma_topiary {
	struct maple_enode *head;
	struct maple_enode *tail;
	struct maple_tree *mtree;
};

void *mtree_load(struct maple_tree *mt, unsigned long index);

int mtree_insert(struct maple_tree *mt, unsigned long index, void *entry,
		 gfp_t gfp);
int mtree_insert_range(struct maple_tree *mt, unsigned long first,
		       unsigned long last, void *entry, gfp_t gfp);
int mtree_alloc_range(struct maple_tree *mt, unsigned long *startp, void *entry,
		      unsigned long size, unsigned long min, unsigned long max,
		      gfp_t gfp);
int mtree_alloc_cyclic(struct maple_tree *mt, unsigned long *startp,
		       void *entry, unsigned long range_lo,
		       unsigned long range_hi, unsigned long *next, gfp_t gfp);
int mtree_alloc_rrange(struct maple_tree *mt, unsigned long *startp,
		       void *entry, unsigned long size, unsigned long min,
		       unsigned long max, gfp_t gfp);

int mtree_store_range(struct maple_tree *mt, unsigned long first,
		      unsigned long last, void *entry, gfp_t gfp);
int mtree_store(struct maple_tree *mt, unsigned long index, void *entry,
		gfp_t gfp);
void *mtree_erase(struct maple_tree *mt, unsigned long index);

int mtree_dup(struct maple_tree *mt, struct maple_tree *new, gfp_t gfp);
int __mt_dup(struct maple_tree *mt, struct maple_tree *new, gfp_t gfp);

void mtree_destroy(struct maple_tree *mt);
void __mt_destroy(struct maple_tree *mt);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mtree_empty(const struct maple_tree *mt)
{
	return mt->ma_root == ((void *)0);
}
enum maple_status {
	ma_active,
	ma_start,
	ma_root,
	ma_none,
	ma_pause,
	ma_overflow,
	ma_underflow,
	ma_error,
};
struct ma_state {
	struct maple_tree *tree;
	unsigned long index;
	unsigned long last;
	struct maple_enode *node;
	unsigned long min;
	unsigned long max;
	struct maple_alloc *alloc;
	enum maple_status status;
	unsigned char depth;
	unsigned char offset;
	unsigned char mas_flags;
	unsigned char end;
	enum store_type store_type;
};

struct ma_wr_state {
	struct ma_state *mas;
	struct maple_node *node;
	unsigned long r_min;
	unsigned long r_max;
	enum maple_type type;
	unsigned char offset_end;
	unsigned long *pivots;
	unsigned long end_piv;
	void **slots;
	void *entry;
	void *content;
};
void *mas_walk(struct ma_state *mas);
void *mas_store(struct ma_state *mas, void *entry);
void *mas_erase(struct ma_state *mas);
int mas_store_gfp(struct ma_state *mas, void *entry, gfp_t gfp);
void mas_store_prealloc(struct ma_state *mas, void *entry);
void *mas_find(struct ma_state *mas, unsigned long max);
void *mas_find_range(struct ma_state *mas, unsigned long max);
void *mas_find_rev(struct ma_state *mas, unsigned long min);
void *mas_find_range_rev(struct ma_state *mas, unsigned long max);
int mas_preallocate(struct ma_state *mas, void *entry, gfp_t gfp);
int mas_alloc_cyclic(struct ma_state *mas, unsigned long *startp, void *entry,
		     unsigned long range_lo, unsigned long range_hi,
		     unsigned long *next, gfp_t gfp);

bool mas_nomem(struct ma_state *mas, gfp_t gfp);
void mas_pause(struct ma_state *mas);
void maple_tree_init(void);
void mas_destroy(struct ma_state *mas);
int mas_expected_entries(struct ma_state *mas, unsigned long nr_entries);

void *mas_prev(struct ma_state *mas, unsigned long min);
void *mas_prev_range(struct ma_state *mas, unsigned long max);
void *mas_next(struct ma_state *mas, unsigned long max);
void *mas_next_range(struct ma_state *mas, unsigned long max);

int mas_empty_area(struct ma_state *mas, unsigned long min, unsigned long max,
		   unsigned long size);

int mas_empty_area_rev(struct ma_state *mas, unsigned long min,
		       unsigned long max, unsigned long size);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mas_init(struct ma_state *mas, struct maple_tree *tree, unsigned long addr)
{
	({
		size_t __fortify_size = (size_t)(sizeof(struct ma_state));
		fortify_memset_chk(__fortify_size,
				   __builtin_dynamic_object_size(mas, 0),
				   __builtin_dynamic_object_size(mas, 1)),
			__builtin_memset(mas, 0, __fortify_size);
	});
	mas->tree = tree;
	mas->index = mas->last = addr;
	mas->max = (~0UL);
	mas->status = ma_start;
	mas->node = ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mas_is_active(struct ma_state *mas)
{
	return mas->status == ma_active;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mas_is_err(struct ma_state *mas)
{
	return mas->status == ma_error;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
mas_reset(struct ma_state *mas)
{
	mas->status = ma_start;
	mas->node = ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__mas_set_range(struct ma_state *mas, unsigned long start, unsigned long last)
{
	({
		int __ret_warn_on =
			!!(mas_is_active(mas) &&
			   (mas->index > start || mas->last < start));
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) | (((9) << 8));
				({
					asm volatile(
						"303"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"303"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(303));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/maple_tree.h"),
						  "i"(734), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"304"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"304"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(304));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});

	mas->index = start;
	mas->last = last;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mas_set_range(struct ma_state *mas, unsigned long start, unsigned long last)
{
	mas_reset(mas);
	__mas_set_range(mas, start, last);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mas_set(struct ma_state *mas, unsigned long index)
{
	mas_set_range(mas, index, index);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mt_external_lock(const struct maple_tree *mt)
{
	return (mt->ma_flags & 0x300) == 0x300;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mt_init_flags(struct maple_tree *mt, unsigned int flags)
{
	mt->ma_flags = flags;
	if (!mt_external_lock(mt))
		do {
			spinlock_check(&mt->ma_lock);
			*(&mt->ma_lock) =
				(spinlock_t){ { .rlock = {
							.raw_lock = { { .val = { (
										0) } } },
						} } };
		} while (0);
	do {
		uintptr_t _r_a_p__v = (uintptr_t)(((void *)0));
		;
		if (__builtin_constant_p(((void *)0)) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_305(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof((mt->ma_root)) ==
						       sizeof(char) ||
					       sizeof((mt->ma_root)) ==
						       sizeof(short) ||
					       sizeof((mt->ma_root)) ==
						       sizeof(int) ||
					       sizeof((mt->ma_root)) ==
						       sizeof(long)) ||
					      sizeof((mt->ma_root)) ==
						      sizeof(long long)))
						__compiletime_assert_305();
				} while (0);
				do {
					*(volatile typeof((mt->ma_root)) *)&(
						(mt->ma_root)) =
						((typeof(mt->ma_root))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_306(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&mt->ma_root) ==
							       sizeof(char) ||
						       sizeof(*&mt->ma_root) ==
							       sizeof(short) ||
						       sizeof(*&mt->ma_root) ==
							       sizeof(int) ||
						       sizeof(*&mt->ma_root) ==
							       sizeof(long))))
							__compiletime_assert_306();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_307(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&mt->ma_root) ==
								       sizeof(char) ||
							       sizeof(*&mt->ma_root) ==
								       sizeof(short) ||
							       sizeof(*&mt->ma_root) ==
								       sizeof(int) ||
							       sizeof(*&mt->ma_root) ==
								       sizeof(long)) ||
							      sizeof(*&mt->ma_root) ==
								      sizeof(long long)))
								__compiletime_assert_307();
						} while (0);
						do {
							*(volatile typeof(*&mt->ma_root)
								  *)&(*&mt->ma_root) =
								((typeof(*(
									(typeof(mt->ma_root))
										_r_a_p__v))
									  *)((
									typeof(mt->ma_root))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mt_init(struct maple_tree *mt)
{
	mt_init_flags(mt, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mt_in_rcu(struct maple_tree *mt)
{
	return mt->ma_flags & 0x02;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mt_clear_in_rcu(struct maple_tree *mt)
{
	if (!mt_in_rcu(mt))
		return;

	if (mt_external_lock(mt)) {
		({
			int __ret_warn_on = !!(!1);
			if (__builtin_expect(!!(__ret_warn_on), 0))
				do {
					__auto_type __flags = (1 << 0) |
							      (((9) << 8));
					({
						asm volatile(
							"308"
							": nop\n\t"
							".pushsection .discard.instr_begin\n\t"
							".long "
							"308"
							"b - .\n\t"
							".popsection\n\t"
							:
							: "i"(308));
					});
					do {
						asm __inline volatile(
							"1:\t"
							".byte 0x0f, 0x0b"
							"\n"
							".pushsection __bug_table,\"aw\"\n"
							"2:\t"
							".long "
							"1b"
							" - ."
							"\t# bug_entry::bug_addr\n"
							"\t"
							".long "
							"%c0"
							" - ."
							"\t# bug_entry::file\n"
							"\t.word %c1"
							"\t# bug_entry::line\n"
							"\t.word %c2"
							"\t# bug_entry::flags\n"
							"\t.org 2b+%c3\n"
							".popsection\n"
							"998:\n\t"
							".pushsection .discard.reachable\n\t"
							".long 998b\n\t"
							".popsection\n\t"
							:
							: "i"("include/linux/maple_tree.h"),
							  "i"(825),
							  "i"(__flags),
							  "i"(sizeof(
								  struct bug_entry)));
					} while (0);
					({
						asm volatile(
							"309"
							": nop\n\t"
							".pushsection .discard.instr_end\n\t"
							".long "
							"309"
							"b - .\n\t"
							".popsection\n\t"
							:
							: "i"(309));
					});
				} while (0);
			__builtin_expect(!!(__ret_warn_on), 0);
		});
		mt->ma_flags &= ~0x02;
	} else {
		spin_lock((&(mt)->ma_lock));
		mt->ma_flags &= ~0x02;
		spin_unlock((&(mt)->ma_lock));
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mt_set_in_rcu(struct maple_tree *mt)
{
	if (mt_in_rcu(mt))
		return;

	if (mt_external_lock(mt)) {
		({
			int __ret_warn_on = !!(!1);
			if (__builtin_expect(!!(__ret_warn_on), 0))
				do {
					__auto_type __flags = (1 << 0) |
							      (((9) << 8));
					({
						asm volatile(
							"310"
							": nop\n\t"
							".pushsection .discard.instr_begin\n\t"
							".long "
							"310"
							"b - .\n\t"
							".popsection\n\t"
							:
							: "i"(310));
					});
					do {
						asm __inline volatile(
							"1:\t"
							".byte 0x0f, 0x0b"
							"\n"
							".pushsection __bug_table,\"aw\"\n"
							"2:\t"
							".long "
							"1b"
							" - ."
							"\t# bug_entry::bug_addr\n"
							"\t"
							".long "
							"%c0"
							" - ."
							"\t# bug_entry::file\n"
							"\t.word %c1"
							"\t# bug_entry::line\n"
							"\t.word %c2"
							"\t# bug_entry::flags\n"
							"\t.org 2b+%c3\n"
							".popsection\n"
							"998:\n\t"
							".pushsection .discard.reachable\n\t"
							".long 998b\n\t"
							".popsection\n\t"
							:
							: "i"("include/linux/maple_tree.h"),
							  "i"(844),
							  "i"(__flags),
							  "i"(sizeof(
								  struct bug_entry)));
					} while (0);
					({
						asm volatile(
							"311"
							": nop\n\t"
							".pushsection .discard.instr_end\n\t"
							".long "
							"311"
							"b - .\n\t"
							".popsection\n\t"
							:
							: "i"(311));
					});
				} while (0);
			__builtin_expect(!!(__ret_warn_on), 0);
		});
		mt->ma_flags |= 0x02;
	} else {
		spin_lock((&(mt)->ma_lock));
		mt->ma_flags |= 0x02;
		spin_unlock((&(mt)->ma_lock));
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
mt_height(const struct maple_tree *mt)
{
	return (mt->ma_flags & 0x7C) >> 0x02;
}

void *mt_find(struct maple_tree *mt, unsigned long *index, unsigned long max);
void *mt_find_after(struct maple_tree *mt, unsigned long *index,
		    unsigned long max);
void *mt_prev(struct maple_tree *mt, unsigned long index, unsigned long min);
void *mt_next(struct maple_tree *mt, unsigned long index, unsigned long max);
struct rw_semaphore {
	atomic_long_t count;

	atomic_long_t owner;

	struct optimistic_spin_queue osq;

	raw_spinlock_t wait_lock;
	struct list_head wait_list;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
rwsem_is_locked(struct rw_semaphore *sem)
{
	return atomic_long_read(&sem->count) != 0UL;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rwsem_assert_held_nolockdep(const struct rw_semaphore *sem)
{
	({
		int __ret_warn_on = !!(atomic_long_read(&sem->count) == 0UL);
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) | (((9) << 8));
				({
					asm volatile(
						"312"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"312"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(312));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/rwsem.h"),
						  "i"(80), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"313"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"313"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(313));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rwsem_assert_held_write_nolockdep(const struct rw_semaphore *sem)
{
	({
		int __ret_warn_on =
			!!(!(atomic_long_read(&sem->count) & (1UL << 0)));
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) | (((9) << 8));
				({
					asm volatile(
						"314"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"314"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(314));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/rwsem.h"),
						  "i"(85), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"315"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"315"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(315));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
}
extern void __init_rwsem(struct rw_semaphore *sem, const char *name,
			 struct lock_class_key *key);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
rwsem_is_contended(struct rw_semaphore *sem)
{
	return !list_empty(&sem->wait_list);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rwsem_assert_held(const struct rw_semaphore *sem)
{
	if (0)
		do {
			(void)(sem);
		} while (0);
	else
		rwsem_assert_held_nolockdep(sem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rwsem_assert_held_write(const struct rw_semaphore *sem)
{
	if (0)
		do {
			(void)(sem);
		} while (0);
	else
		rwsem_assert_held_write_nolockdep(sem);
}

extern void down_read(struct rw_semaphore *sem);
extern int __attribute__((__warn_unused_result__))
down_read_interruptible(struct rw_semaphore *sem);
extern int __attribute__((__warn_unused_result__))
down_read_killable(struct rw_semaphore *sem);

extern int down_read_trylock(struct rw_semaphore *sem);

extern void down_write(struct rw_semaphore *sem);
extern int __attribute__((__warn_unused_result__))
down_write_killable(struct rw_semaphore *sem);

extern int down_write_trylock(struct rw_semaphore *sem);

extern void up_read(struct rw_semaphore *sem);

extern void up_write(struct rw_semaphore *sem);

typedef struct rw_semaphore *class_rwsem_read_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_rwsem_read_destructor(struct rw_semaphore **p)
{
	struct rw_semaphore *_T = *p;
	if (_T) {
		up_read(_T);
	};
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct rw_semaphore *
class_rwsem_read_constructor(struct rw_semaphore *_T)
{
	struct rw_semaphore *t = ({
		down_read(_T);
		_T;
	});
	return t;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_rwsem_read_lock_ptr(class_rwsem_read_t *_T)
{
	return *_T;
}
typedef class_rwsem_read_t class_rwsem_read_try_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_rwsem_read_try_destructor(class_rwsem_read_t *p)
{
	class_rwsem_read_destructor(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_rwsem_read_t
class_rwsem_read_try_constructor(class_rwsem_read_t _T)
{
	class_rwsem_read_t t = ({
		void *_t = _T;
		if (_T && !(down_read_trylock(_T)))
			_t = ((void *)0);
		_t;
	});
	return t;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_rwsem_read_try_lock_ptr(class_rwsem_read_t *_T)
{
	return class_rwsem_read_lock_ptr(_T);
}
typedef class_rwsem_read_t class_rwsem_read_intr_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_rwsem_read_intr_destructor(class_rwsem_read_t *p)
{
	class_rwsem_read_destructor(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_rwsem_read_t
class_rwsem_read_intr_constructor(class_rwsem_read_t _T)
{
	class_rwsem_read_t t = ({
		void *_t = _T;
		if (_T && !(down_read_interruptible(_T) == 0))
			_t = ((void *)0);
		_t;
	});
	return t;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_rwsem_read_intr_lock_ptr(class_rwsem_read_t *_T)
{
	return class_rwsem_read_lock_ptr(_T);
}

typedef struct rw_semaphore *class_rwsem_write_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_rwsem_write_destructor(struct rw_semaphore **p)
{
	struct rw_semaphore *_T = *p;
	if (_T) {
		up_write(_T);
	};
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct rw_semaphore *
class_rwsem_write_constructor(struct rw_semaphore *_T)
{
	struct rw_semaphore *t = ({
		down_write(_T);
		_T;
	});
	return t;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_rwsem_write_lock_ptr(class_rwsem_write_t *_T)
{
	return *_T;
}
typedef class_rwsem_write_t class_rwsem_write_try_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_rwsem_write_try_destructor(class_rwsem_write_t *p)
{
	class_rwsem_write_destructor(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_rwsem_write_t
class_rwsem_write_try_constructor(class_rwsem_write_t _T)
{
	class_rwsem_write_t t = ({
		void *_t = _T;
		if (_T && !(down_write_trylock(_T)))
			_t = ((void *)0);
		_t;
	});
	return t;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_rwsem_write_try_lock_ptr(class_rwsem_write_t *_T)
{
	return class_rwsem_write_lock_ptr(_T);
}

extern void downgrade_write(struct rw_semaphore *sem);

struct uprobe;
struct vm_area_struct;
struct mm_struct;
struct inode;
struct notifier_block;
struct page;

struct uprobe_consumer {
	int (*handler)(struct uprobe_consumer *self, struct pt_regs *regs);
	int (*ret_handler)(struct uprobe_consumer *self, unsigned long func,
			   struct pt_regs *regs);
	bool (*filter)(struct uprobe_consumer *self, struct mm_struct *mm);

	struct list_head cons_node;
};

typedef __s64 time64_t;
typedef __u64 timeu64_t;

struct __kernel_timespec {
	__kernel_time64_t tv_sec;
	long long tv_nsec;
};

struct __kernel_itimerspec {
	struct __kernel_timespec it_interval;
	struct __kernel_timespec it_value;
};
struct __kernel_old_timeval {
	__kernel_long_t tv_sec;
	__kernel_long_t tv_usec;
};

struct __kernel_old_timespec {
	__kernel_old_time_t tv_sec;
	long tv_nsec;
};

struct __kernel_old_itimerval {
	struct __kernel_old_timeval it_interval;
	struct __kernel_old_timeval it_value;
};

struct __kernel_sock_timeval {
	__s64 tv_sec;
	__s64 tv_usec;
};
struct timezone {
	int tz_minuteswest;
	int tz_dsttime;
};

struct timespec64 {
	time64_t tv_sec;
	long tv_nsec;
};

struct itimerspec64 {
	struct timespec64 it_interval;
	struct timespec64 it_value;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
timespec64_equal(const struct timespec64 *a, const struct timespec64 *b)
{
	return (a->tv_sec == b->tv_sec) && (a->tv_nsec == b->tv_nsec);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
timespec64_compare(const struct timespec64 *lhs, const struct timespec64 *rhs)
{
	if (lhs->tv_sec < rhs->tv_sec)
		return -1;
	if (lhs->tv_sec > rhs->tv_sec)
		return 1;
	return lhs->tv_nsec - rhs->tv_nsec;
}

extern void set_normalized_timespec64(struct timespec64 *ts, time64_t sec,
				      s64 nsec);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timespec64
timespec64_add(struct timespec64 lhs, struct timespec64 rhs)
{
	struct timespec64 ts_delta;
	set_normalized_timespec64(&ts_delta, lhs.tv_sec + rhs.tv_sec,
				  lhs.tv_nsec + rhs.tv_nsec);
	return ts_delta;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timespec64
timespec64_sub(struct timespec64 lhs, struct timespec64 rhs)
{
	struct timespec64 ts_delta;
	set_normalized_timespec64(&ts_delta, lhs.tv_sec - rhs.tv_sec,
				  lhs.tv_nsec - rhs.tv_nsec);
	return ts_delta;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
timespec64_valid(const struct timespec64 *ts)
{
	if (ts->tv_sec < 0)
		return false;

	if ((unsigned long)ts->tv_nsec >= 1000000000L)
		return false;
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
timespec64_valid_strict(const struct timespec64 *ts)
{
	if (!timespec64_valid(ts))
		return false;

	if ((unsigned long long)ts->tv_sec >=
	    (((s64) ~((u64)1 << 63)) / 1000000000L))
		return false;
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
timespec64_valid_settod(const struct timespec64 *ts)
{
	if (!timespec64_valid(ts))
		return false;

	if ((unsigned long long)ts->tv_sec >=
	    ((((s64) ~((u64)1 << 63)) / 1000000000L) -
	     (30LL * 365 * 24 * 3600)))
		return false;
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
timespec64_to_ns(const struct timespec64 *ts)
{
	if (ts->tv_sec >= (((s64) ~((u64)1 << 63)) / 1000000000L))
		return ((s64) ~((u64)1 << 63));

	if (ts->tv_sec <= ((-((s64) ~((u64)1 << 63)) - 1) / 1000000000L))
		return (-((s64) ~((u64)1 << 63)) - 1);

	return ((s64)ts->tv_sec * 1000000000L) + ts->tv_nsec;
}

extern struct timespec64 ns_to_timespec64(s64 nsec);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
timespec64_add_ns(struct timespec64 *a, u64 ns)
{
	a->tv_sec += __iter_div_u64_rem(a->tv_nsec + ns, 1000000000L, &ns);
	a->tv_nsec = ns;
}

extern struct timespec64 timespec64_add_safe(const struct timespec64 lhs,
					     const struct timespec64 rhs);

extern struct timezone sys_tz;

int get_timespec64(struct timespec64 *ts, const struct __kernel_timespec *uts);
int put_timespec64(const struct timespec64 *ts, struct __kernel_timespec *uts);
int get_itimerspec64(struct itimerspec64 *it,
		     const struct __kernel_itimerspec *uit);
int put_itimerspec64(const struct itimerspec64 *it,
		     struct __kernel_itimerspec *uit);

extern time64_t mktime64(const unsigned int year, const unsigned int mon,
			 const unsigned int day, const unsigned int hour,
			 const unsigned int min, const unsigned int sec);

extern void clear_itimer(void);

extern long do_utimes(int dfd, const char *filename, struct timespec64 *times,
		      int flags);

struct tm {
	int tm_sec;

	int tm_min;

	int tm_hour;

	int tm_mday;

	int tm_mon;

	long tm_year;

	int tm_wday;

	int tm_yday;
};

void time64_to_tm(time64_t totalsecs, int offset, struct tm *result);

struct __kernel_timex_timeval {
	__kernel_time64_t tv_sec;
	long long tv_usec;
};

struct __kernel_timex {
	unsigned int modes;
	int : 32;
	long long offset;
	long long freq;
	long long maxerror;
	long long esterror;
	int status;
	int : 32;
	long long constant;
	long long precision;
	long long tolerance;

	struct __kernel_timex_timeval time;
	long long tick;

	long long ppsfreq;
	long long jitter;
	int shift;
	int : 32;
	long long stabil;
	long long jitcnt;
	long long calcnt;
	long long errcnt;
	long long stbcnt;

	int tai;

	int : 32;
	int : 32;
	int : 32;
	int : 32;
	int : 32;
	int : 32;
	int : 32;
	int : 32;
	int : 32;
	int : 32;
	int : 32;
};

unsigned long random_get_entropy_fallback(void);

struct msr {
	union {
		struct {
			u32 l;
			u32 h;
		};
		u64 q;
	};
};

struct codetag_iterator;
struct codetag_type;
struct codetag_module;
struct seq_buf;
struct module;

struct codetag {
	unsigned int flags;
	unsigned int lineno;
	const char *modname;
	const char *function;
	const char *filename;
} __attribute__((__aligned__(8)));

union codetag_ref {
	struct codetag *ct;
};

struct codetag_type_desc {
	const char *section;
	size_t tag_size;
	void (*module_load)(struct codetag_type *cttype,
			    struct codetag_module *cmod);
	bool (*module_unload)(struct codetag_type *cttype,
			      struct codetag_module *cmod);
};

struct codetag_iterator {
	struct codetag_type *cttype;
	struct codetag_module *cmod;
	unsigned long mod_id;
	struct codetag *ct;
};
void codetag_lock_module_list(struct codetag_type *cttype, bool lock);
bool codetag_trylock_module_list(struct codetag_type *cttype);
struct codetag_iterator codetag_get_ct_iter(struct codetag_type *cttype);
struct codetag *codetag_next_ct(struct codetag_iterator *iter);

void codetag_to_text(struct seq_buf *out, struct codetag *ct);

struct codetag_type *
codetag_register_type(const struct codetag_type_desc *desc);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
codetag_load_module(struct module *mod)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
codetag_unload_module(struct module *mod)
{
	return true;
}

struct alloc_tag_counters {
	u64 bytes;
	u64 calls;
};

struct alloc_tag {
	struct codetag ct;
	struct alloc_tag_counters *counters;
} __attribute__((__aligned__(8)));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_codetag_empty(union codetag_ref *ref)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_codetag_empty(union codetag_ref *ref)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mem_alloc_profiling_enabled(void)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
alloc_tag_add(union codetag_ref *ref, struct alloc_tag *tag, size_t bytes)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
alloc_tag_sub(union codetag_ref *ref, size_t bytes)
{
}

struct page;
struct vm_area_struct;
struct mm_struct;
struct vma_iterator;

void dump_page(const struct page *page, const char *reason);
void dump_vma(const struct vm_area_struct *vma);
void dump_mm(const struct mm_struct *mm);
void vma_iter_dump_tree(const struct vma_iterator *vmi);

struct clone_args {
	__u64 __attribute__((aligned(8))) flags;
	__u64 __attribute__((aligned(8))) pidfd;
	__u64 __attribute__((aligned(8))) child_tid;
	__u64 __attribute__((aligned(8))) parent_tid;
	__u64 __attribute__((aligned(8))) exit_signal;
	__u64 __attribute__((aligned(8))) stack;
	__u64 __attribute__((aligned(8))) stack_size;
	__u64 __attribute__((aligned(8))) tls;
	__u64 __attribute__((aligned(8))) set_tid;
	__u64 __attribute__((aligned(8))) set_tid_size;
	__u64 __attribute__((aligned(8))) cgroup;
};

enum pid_type {
	PIDTYPE_PID,
	PIDTYPE_TGID,
	PIDTYPE_PGID,
	PIDTYPE_SID,
	PIDTYPE_MAX,
};

struct pid_namespace;
extern struct pid_namespace init_pid_ns;

struct sem_undo_list;

struct sysv_sem {
	struct sem_undo_list *undo_list;
};

struct file;
struct task_struct;

struct sysv_shm {
	struct list_head shm_clist;
};

long do_shmat(int shmid, char *shmaddr, int shmflg, unsigned long *addr,
	      unsigned long shmlba);
void exit_shm(struct task_struct *task);
struct kmsan_context_state {
	char param_tls[800];
	char retval_tls[800];
	char va_arg_tls[800];
	char va_arg_origin_tls[800];
	u64 va_arg_overflow_size_tls;
	char param_origin_tls[800];
	u32 retval_origin_tls;
};

struct kmsan_ctx {
	struct kmsan_context_state cstate;
	int kmsan_in_runtime;
	unsigned int depth;
};

struct plist_head {
	struct list_head node_list;
};

struct plist_node {
	int prio;
	struct list_head prio_list;
	struct list_head node_list;
};

struct timerqueue_node {
	struct rb_node node;
	ktime_t expires;
};

struct timerqueue_head {
	struct rb_root_cached rb_root;
};

struct hrtimer_clock_base;

enum hrtimer_restart {
	HRTIMER_NORESTART,
	HRTIMER_RESTART,
};
struct hrtimer {
	struct timerqueue_node node;
	ktime_t _softexpires;
	enum hrtimer_restart (*function)(struct hrtimer *);
	struct hrtimer_clock_base *base;
	u8 state;
	u8 is_rel;
	u8 is_soft;
	u8 is_hard;
};

struct timer_list {
	struct hlist_node entry;
	unsigned long expires;
	void (*function)(struct timer_list *);
	u32 flags;
};

struct seccomp_filter;
struct seccomp {
	int mode;
	atomic_t filter_count;
	struct seccomp_filter *filter;
};

struct rusage {
	struct __kernel_old_timeval ru_utime;
	struct __kernel_old_timeval ru_stime;
	__kernel_long_t ru_maxrss;
	__kernel_long_t ru_ixrss;
	__kernel_long_t ru_idrss;
	__kernel_long_t ru_isrss;
	__kernel_long_t ru_minflt;
	__kernel_long_t ru_majflt;
	__kernel_long_t ru_nswap;
	__kernel_long_t ru_inblock;
	__kernel_long_t ru_oublock;
	__kernel_long_t ru_msgsnd;
	__kernel_long_t ru_msgrcv;
	__kernel_long_t ru_nsignals;
	__kernel_long_t ru_nvcsw;
	__kernel_long_t ru_nivcsw;
};

struct rlimit {
	__kernel_ulong_t rlim_cur;
	__kernel_ulong_t rlim_max;
};

struct rlimit64 {
	__u64 rlim_cur;
	__u64 rlim_max;
};

struct task_struct;

void getrusage(struct task_struct *p, int who, struct rusage *ru);
struct task_struct;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
account_scheduler_latency(struct task_struct *task, int usecs, int inter)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_tsk_latency_tracing(struct task_struct *p)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) long
nice_to_rlimit(long nice)
{
	return (19 - nice + 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) long
rlimit_to_nice(long prio)
{
	return (19 - prio + 1);
}
struct task_cputime {
	u64 stime;
	u64 utime;
	unsigned long long sum_exec_runtime;
};

typedef unsigned long old_sigset_t;

typedef struct {
	unsigned long sig[(64 / 64)];
} sigset_t;

struct siginfo;
typedef void __signalfn_t(int);
typedef __signalfn_t *__sighandler_t;

typedef void __restorefn_t(void);
typedef __restorefn_t *__sigrestore_t;
typedef struct sigaltstack {
	void *ss_sp;
	int ss_flags;
	__kernel_size_t ss_size;
} stack_t;

typedef union sigval {
	int sival_int;
	void *sival_ptr;
} sigval_t;
union __sifields {
	struct {
		__kernel_pid_t _pid;
		__kernel_uid32_t _uid;
	} _kill;

	struct {
		__kernel_timer_t _tid;
		int _overrun;
		sigval_t _sigval;
		int _sys_private;
	} _timer;

	struct {
		__kernel_pid_t _pid;
		__kernel_uid32_t _uid;
		sigval_t _sigval;
	} _rt;

	struct {
		__kernel_pid_t _pid;
		__kernel_uid32_t _uid;
		int _status;
		__kernel_clock_t _utime;
		__kernel_clock_t _stime;
	} _sigchld;

	struct {
		void *_addr;

		union {
			int _trapno;

			short _addr_lsb;

			struct {
				char _dummy_bnd[(__alignof__(void *) <
								 sizeof(short) ?
							 sizeof(short) :
							 __alignof__(void *))];
				void *_lower;
				void *_upper;
			} _addr_bnd;

			struct {
				char _dummy_pkey[(
					__alignof__(void *) < sizeof(short) ?
						sizeof(short) :
						__alignof__(void *))];
				__u32 _pkey;
			} _addr_pkey;

			struct {
				unsigned long _data;
				__u32 _type;
				__u32 _flags;
			} _perf;
		};
	} _sigfault;

	struct {
		long _band;
		int _fd;
	} _sigpoll;

	struct {
		void *_call_addr;
		int _syscall;
		unsigned int _arch;
	} _sigsys;
};
typedef struct siginfo {
	union {
		struct {
			int si_signo;
			int si_errno;
			int si_code;
			union __sifields _sifields;
		};
		int _si_pad[128 / sizeof(int)];
	};
} siginfo_t;
typedef struct sigevent {
	sigval_t sigev_value;
	int sigev_signo;
	int sigev_notify;
	union {
		int _pad[((64 - (sizeof(int) * 2 + sizeof(sigval_t))) /
			  sizeof(int))];
		int _tid;

		struct {
			void (*_function)(sigval_t);
			void *_attribute;
		} _sigev_thread;
	} _sigev_un;
} sigevent_t;

typedef struct kernel_siginfo {
	struct {
		int si_signo;
		int si_errno;
		int si_code;
		union __sifields _sifields;
	};
} kernel_siginfo_t;

struct ucounts;

struct sigqueue {
	struct list_head list;
	int flags;
	kernel_siginfo_t info;
	struct ucounts *ucounts;
};

struct sigpending {
	struct list_head list;
	sigset_t signal;
};

struct sigaction {
	__sighandler_t sa_handler;
	unsigned long sa_flags;

	__sigrestore_t sa_restorer;

	sigset_t sa_mask;
};

struct k_sigaction {
	struct sigaction sa;
};
struct ksignal {
	struct k_sigaction ka;
	kernel_siginfo_t info;
	int sig;
};

struct syscall_user_dispatch {
	char *selector;
	unsigned long offset;
	unsigned long len;
	bool on_dispatch;
};

struct netdev_xmit {
	u16 recursion;
	u8 more;

	u8 skip_txqueue;
};
struct task_io_accounting {
	u64 rchar;

	u64 wchar;

	u64 syscr;

	u64 syscw;

	u64 read_bytes;

	u64 write_bytes;
	u64 cancelled_write_bytes;
};
struct posix_cputimer_base {
	u64 nextevt;
	struct timerqueue_head tqhead;
};
struct posix_cputimers {
	struct posix_cputimer_base bases[3];
	unsigned int timers_active;
	unsigned int expiry_active;
};

struct posix_cputimers_work {
	struct callback_head work;
	struct mutex mutex;
	unsigned int scheduled;
};

enum rseq_cpu_id_state {
	RSEQ_CPU_ID_UNINITIALIZED = -1,
	RSEQ_CPU_ID_REGISTRATION_FAILED = -2,
};

enum rseq_flags {
	RSEQ_FLAG_UNREGISTER = (1 << 0),
};

enum rseq_cs_flags_bit {
	RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT_BIT = 0,
	RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL_BIT = 1,
	RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE_BIT = 2,
};

enum rseq_cs_flags {
	RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT =
		(1U << RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT_BIT),
	RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL =
		(1U << RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL_BIT),
	RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE =
		(1U << RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE_BIT),
};

struct rseq_cs {
	__u32 version;

	__u32 flags;
	__u64 start_ip;

	__u64 post_commit_offset;
	__u64 abort_ip;
} __attribute__((aligned(4 * sizeof(__u64))));

struct rseq {
	__u32 cpu_id_start;
	__u32 cpu_id;
	__u64 rseq_cs;
	__u32 flags;

	__u32 node_id;
	__u32 mm_cid;

	char end[];
} __attribute__((aligned(4 * sizeof(__u64))));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kcsan_init(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
klp_sched_try_switch(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__klp_sched_try_switch(void)
{
}

typedef struct {
	uid_t val;
} kuid_t;

typedef struct {
	gid_t val;
} kgid_t;

struct audit_context;
struct bio_list;
struct blk_plug;
struct bpf_local_storage;
struct bpf_run_ctx;
struct bpf_net_context;
struct capture_control;
struct cfs_rq;
struct fs_struct;
struct futex_pi_state;
struct io_context;
struct io_uring_task;
struct mempolicy;
struct nameidata;
struct nsproxy;
struct perf_event_context;
struct pid_namespace;
struct pipe_inode_info;
struct rcu_node;
struct reclaim_state;
struct robust_list_head;
struct root_domain;
struct rq;
struct sched_attr;
struct sched_dl_entity;
struct seq_file;
struct sighand_struct;
struct signal_struct;
struct task_delay_info;
struct task_group;
struct task_struct;
struct user_event_mm;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sched_ext_free(struct task_struct *p)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
print_scx_info(const char *log_lvl, struct task_struct *p)
{
}
enum {
	TASK_COMM_LEN = 16,
};

extern void sched_tick(void);

extern long schedule_timeout(long timeout);
extern long schedule_timeout_interruptible(long timeout);
extern long schedule_timeout_killable(long timeout);
extern long schedule_timeout_uninterruptible(long timeout);
extern long schedule_timeout_idle(long timeout);
void schedule(void);
extern void schedule_preempt_disabled(void);
void preempt_schedule_irq(void);

extern int __attribute__((__warn_unused_result__)) io_schedule_prepare(void);
extern void io_schedule_finish(int token);
extern long io_schedule_timeout(long timeout);
extern void io_schedule(void);
struct prev_cputime {
	u64 utime;
	u64 stime;
	raw_spinlock_t lock;
};

enum vtime_state {

	VTIME_INACTIVE = 0,

	VTIME_IDLE,

	VTIME_SYS,

	VTIME_USER,

	VTIME_GUEST,
};

struct vtime {
	seqcount_t seqcount;
	unsigned long long starttime;
	enum vtime_state state;
	unsigned int cpu;
	u64 utime;
	u64 stime;
	u64 gtime;
};

enum uclamp_id { UCLAMP_MIN = 0, UCLAMP_MAX, UCLAMP_CNT };

extern struct root_domain def_root_domain;
extern struct mutex sched_domains_mutex;

struct sched_param {
	int sched_priority;
};

struct sched_info {
	unsigned long pcount;

	unsigned long long run_delay;

	unsigned long long last_arrival;

	unsigned long long last_queued;
};
struct load_weight {
	unsigned long weight;
	u32 inv_weight;
};
struct sched_avg {
	u64 last_update_time;
	u64 load_sum;
	u64 runnable_sum;
	u32 util_sum;
	u32 period_contrib;
	unsigned long load_avg;
	unsigned long runnable_avg;
	unsigned long util_avg;
	unsigned int util_est;
} __attribute__((__aligned__((1 << (6)))));
struct sched_statistics {
	u64 wait_start;
	u64 wait_max;
	u64 wait_count;
	u64 wait_sum;
	u64 iowait_count;
	u64 iowait_sum;

	u64 sleep_start;
	u64 sleep_max;
	s64 sum_sleep_runtime;

	u64 block_start;
	u64 block_max;
	s64 sum_block_runtime;

	s64 exec_max;
	u64 slice_max;

	u64 nr_migrations_cold;
	u64 nr_failed_migrations_affine;
	u64 nr_failed_migrations_running;
	u64 nr_failed_migrations_hot;
	u64 nr_forced_migrations;

	u64 nr_wakeups;
	u64 nr_wakeups_sync;
	u64 nr_wakeups_migrate;
	u64 nr_wakeups_local;
	u64 nr_wakeups_remote;
	u64 nr_wakeups_affine;
	u64 nr_wakeups_affine_attempts;
	u64 nr_wakeups_passive;
	u64 nr_wakeups_idle;

} __attribute__((__aligned__((1 << (6)))));

struct sched_entity {
	struct load_weight load;
	struct rb_node run_node;
	u64 deadline;
	u64 min_vruntime;
	u64 min_slice;

	struct list_head group_node;
	unsigned char on_rq;
	unsigned char sched_delayed;
	unsigned char rel_deadline;
	unsigned char custom_slice;

	u64 exec_start;
	u64 sum_exec_runtime;
	u64 prev_sum_exec_runtime;
	u64 vruntime;
	s64 vlag;
	u64 slice;

	u64 nr_migrations;

	int depth;
	struct sched_entity *parent;

	struct cfs_rq *cfs_rq;

	struct cfs_rq *my_q;

	unsigned long runnable_weight;
	struct sched_avg avg;
};

struct sched_rt_entity {
	struct list_head run_list;
	unsigned long timeout;
	unsigned long watchdog_stamp;
	unsigned int time_slice;
	unsigned short on_rq;
	unsigned short on_list;

	struct sched_rt_entity *back;
};

typedef bool (*dl_server_has_tasks_f)(struct sched_dl_entity *);
typedef struct task_struct *(*dl_server_pick_f)(struct sched_dl_entity *);

struct sched_dl_entity {
	struct rb_node rb_node;

	u64 dl_runtime;
	u64 dl_deadline;
	u64 dl_period;
	u64 dl_bw;
	u64 dl_density;

	s64 runtime;
	u64 deadline;
	unsigned int flags;
	unsigned int dl_throttled : 1;
	unsigned int dl_yielded : 1;
	unsigned int dl_non_contending : 1;
	unsigned int dl_overrun : 1;
	unsigned int dl_server : 1;
	unsigned int dl_defer : 1;
	unsigned int dl_defer_armed : 1;
	unsigned int dl_defer_running : 1;

	struct hrtimer dl_timer;
	struct hrtimer inactive_timer;
	struct rq *rq;
	dl_server_has_tasks_f server_has_tasks;
	dl_server_pick_f server_pick_task;

	struct sched_dl_entity *pi_se;
};
union rcu_special {
	struct {
		u8 blocked;
		u8 need_qs;
		u8 exp_hint;
		u8 need_mb;
	} b;
	u32 s;
};

enum perf_event_task_context {
	perf_invalid_context = -1,
	perf_hw_context = 0,
	perf_sw_context,
	perf_nr_task_contexts,
};

struct wake_q_node {
	struct wake_q_node *next;
};

struct kmap_ctrl {};

struct task_struct {
	struct thread_info thread_info;

	unsigned int __state;

	unsigned int saved_state;

	void *stack;
	refcount_t usage;

	unsigned int flags;
	unsigned int ptrace;

	int on_cpu;
	struct __call_single_node wake_entry;
	unsigned int wakee_flips;
	unsigned long wakee_flip_decay_ts;
	struct task_struct *last_wakee;
	int recent_used_cpu;
	int wake_cpu;

	int on_rq;

	int prio;
	int static_prio;
	int normal_prio;
	unsigned int rt_priority;

	struct sched_entity se;
	struct sched_rt_entity rt;
	struct sched_dl_entity dl;
	struct sched_dl_entity *dl_server;

	const struct sched_class *sched_class;
	struct task_group *sched_task_group;
	struct sched_statistics stats;

	unsigned int btrace_seq;

	unsigned int policy;
	unsigned long max_allowed_capacity;
	int nr_cpus_allowed;
	const cpumask_t *cpus_ptr;
	cpumask_t *user_cpus_ptr;
	cpumask_t cpus_mask;
	void *migration_pending;

	unsigned short migration_disabled;

	unsigned short migration_flags;

	int rcu_read_lock_nesting;
	union rcu_special rcu_read_unlock_special;
	struct list_head rcu_node_entry;
	struct rcu_node *rcu_blocked_node;

	unsigned long rcu_tasks_nvcsw;
	u8 rcu_tasks_holdout;
	u8 rcu_tasks_idx;
	int rcu_tasks_idle_cpu;
	struct list_head rcu_tasks_holdout_list;
	int rcu_tasks_exit_cpu;
	struct list_head rcu_tasks_exit_list;
	struct sched_info sched_info;

	struct list_head tasks;

	struct plist_node pushable_tasks;
	struct rb_node pushable_dl_tasks;

	struct mm_struct *mm;
	struct mm_struct *active_mm;
	struct address_space *faults_disabled_mapping;

	int exit_state;
	int exit_code;
	int exit_signal;

	int pdeath_signal;

	unsigned long jobctl;

	unsigned int personality;

	unsigned sched_reset_on_fork : 1;
	unsigned sched_contributes_to_load : 1;
	unsigned sched_migrated : 1;

	unsigned : 0;
	unsigned sched_remote_wakeup : 1;

	unsigned sched_rt_mutex : 1;

	unsigned in_execve : 1;
	unsigned in_iowait : 1;

	unsigned restore_sigmask : 1;
	unsigned no_cgroup_migration : 1;

	unsigned frozen : 1;

	unsigned use_memdelay : 1;
	unsigned in_eventfd : 1;

	unsigned pasid_activated : 1;

	unsigned reported_split_lock : 1;

	unsigned in_thrashing : 1;

	unsigned long atomic_flags;

	struct restart_block restart_block;

	pid_t pid;
	pid_t tgid;

	unsigned long stack_canary;
	struct task_struct *real_parent;

	struct task_struct *parent;

	struct list_head children;
	struct list_head sibling;
	struct task_struct *group_leader;

	struct list_head ptraced;
	struct list_head ptrace_entry;

	struct pid *thread_pid;
	struct hlist_node pid_links[PIDTYPE_MAX];
	struct list_head thread_node;

	struct completion *vfork_done;

	int *set_child_tid;

	int *clear_child_tid;

	void *worker_private;

	u64 utime;
	u64 stime;

	u64 gtime;
	struct prev_cputime prev_cputime;
	unsigned long nvcsw;
	unsigned long nivcsw;

	u64 start_time;

	u64 start_boottime;

	unsigned long min_flt;
	unsigned long maj_flt;

	struct posix_cputimers posix_cputimers;

	struct posix_cputimers_work posix_cputimers_work;

	const struct cred *ptracer_cred;

	const struct cred *real_cred;

	const struct cred *cred;

	struct key *cached_requested_key;
	char comm[TASK_COMM_LEN];

	struct nameidata *nameidata;

	struct sysv_sem sysvsem;
	struct sysv_shm sysvshm;

	struct fs_struct *fs;

	struct files_struct *files;

	struct io_uring_task *io_uring;

	struct nsproxy *nsproxy;

	struct signal_struct *signal;
	struct sighand_struct *sighand;
	sigset_t blocked;
	sigset_t real_blocked;

	sigset_t saved_sigmask;
	struct sigpending pending;
	unsigned long sas_ss_sp;
	size_t sas_ss_size;
	unsigned int sas_ss_flags;

	struct callback_head *task_works;

	struct audit_context *audit_context;

	kuid_t loginuid;
	unsigned int sessionid;

	struct seccomp seccomp;
	struct syscall_user_dispatch syscall_dispatch;

	u64 parent_exec_id;
	u64 self_exec_id;

	spinlock_t alloc_lock;

	raw_spinlock_t pi_lock;

	struct wake_q_node wake_q;

	struct rb_root_cached pi_waiters;

	struct task_struct *pi_top_task;

	struct rt_mutex_waiter *pi_blocked_on;
	void *journal_info;

	struct bio_list *bio_list;

	struct blk_plug *plug;

	struct reclaim_state *reclaim_state;

	struct io_context *io_context;

	struct capture_control *capture_control;

	unsigned long ptrace_message;
	kernel_siginfo_t *last_siginfo;

	struct task_io_accounting ioac;

	u64 acct_rss_mem1;

	u64 acct_vm_mem1;

	u64 acct_timexpd;

	nodemask_t mems_allowed;

	seqcount_spinlock_t mems_allowed_seq;
	int cpuset_mem_spread_rotor;

	struct css_set *cgroups;

	struct list_head cg_list;

	struct robust_list_head *robust_list;

	struct compat_robust_list_head *compat_robust_list;

	struct list_head pi_state_list;
	struct futex_pi_state *pi_state_cache;
	struct mutex futex_exit_mutex;
	unsigned int futex_state;

	u8 perf_recursion[4];
	struct perf_event_context *perf_event_ctxp;
	struct mutex perf_event_mutex;
	struct list_head perf_event_list;

	struct mempolicy *mempolicy;
	short il_prev;
	u8 il_weight;
	short pref_node_fork;
	struct rseq *rseq;
	u32 rseq_len;
	u32 rseq_sig;

	unsigned long rseq_event_mask;

	int mm_cid;
	int last_mm_cid;
	int migrate_from_cpu;
	int mm_cid_active;
	struct callback_head cid_work;

	struct tlbflush_unmap_batch tlb_ubc;

	struct pipe_inode_info *splice_pipe;

	struct page_frag task_frag;

	struct task_delay_info *delays;
	int nr_dirtied;
	int nr_dirtied_pause;

	unsigned long dirty_paused_when;
	u64 timer_slack_ns;
	u64 default_timer_slack_ns;
	unsigned long trace_recursion;
	struct gendisk *throttle_disk;

	struct uprobe_task *utask;

	struct kmap_ctrl kmap_ctrl;

	struct callback_head rcu;
	refcount_t rcu_users;
	int pagefault_disabled;

	struct task_struct *oom_reaper_list;
	struct timer_list oom_reaper_timer;

	struct vm_struct *stack_vm_area;

	refcount_t stack_refcount;

	void *security;
	struct bpf_net_context *bpf_net_context;

	void *mce_vaddr;
	__u64 mce_kflags;
	u64 mce_addr;
	__u64 mce_ripv : 1, mce_whole_page : 1, __mce_reserved : 62;
	struct callback_head mce_kill_me;
	int mce_count;

	struct llist_head kretprobe_instances;

	struct llist_head rethooks;
	struct callback_head l1d_flush_kill;
	struct thread_struct thread;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
__task_state_index(unsigned int tsk_state, unsigned int tsk_exit_state)
{
	unsigned int state =
		(tsk_state | tsk_exit_state) &
		(0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 |
		 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040);

	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_316(void) __attribute__((__error__(
			"BUILD_BUG_ON failed: "
			"((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) == 0 || ((((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) & (((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 | 0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) + 1) << 1)) - 1)) != 0)")));
		if (!(!(((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 |
			    0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) +
			   1)
			  << 1)) == 0 ||
			((((((0x00000000 | 0x00000001 | 0x00000002 |
			      0x00000004 | 0x00000008 | 0x00000010 |
			      0x00000020 | 0x00000040) +
			     1)
			    << 1)) &
			  (((((0x00000000 | 0x00000001 | 0x00000002 |
			       0x00000004 | 0x00000008 | 0x00000010 |
			       0x00000020 | 0x00000040) +
			      1)
			     << 1)) -
			   1)) != 0))))
			__compiletime_assert_316();
	} while (0);

	if ((tsk_state & (0x00000002 | 0x00000400)) ==
	    (0x00000002 | 0x00000400))
		state = ((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 |
			  0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) +
			 1);

	if (tsk_state & 0x00001000)
		state = 0x00000002;

	return fls(state);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
task_state_index(struct task_struct *tsk)
{
	return __task_state_index(
		({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_317(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(tsk->__state) == sizeof(char) ||
				       sizeof(tsk->__state) == sizeof(short) ||
				       sizeof(tsk->__state) == sizeof(int) ||
				       sizeof(tsk->__state) == sizeof(long)) ||
				      sizeof(tsk->__state) ==
					      sizeof(long long)))
					__compiletime_assert_317();
			} while (0);
			(*(const volatile typeof(_Generic(
				(tsk->__state),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 tsk->__state)))
				   *)&(tsk->__state));
		}),
		tsk->exit_state);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) char
task_index_to_char(unsigned int state)
{
	static const char state_char[] = "RSDTtXZPI";

	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_318(void) __attribute__((__error__(
			"BUILD_BUG_ON failed: "
			"TASK_REPORT_MAX * 2 != 1 << (sizeof(state_char) - 1)")));
		if (!(!((((0x00000000 | 0x00000001 | 0x00000002 | 0x00000004 |
			   0x00000008 | 0x00000010 | 0x00000020 | 0x00000040) +
			  1)
			 << 1) * 2 !=
			1 << (sizeof(state_char) - 1))))
			__compiletime_assert_318();
	} while (0);

	return state_char[state];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) char
task_state_to_char(struct task_struct *tsk)
{
	return task_index_to_char(task_state_index(tsk));
}

extern struct pid *cad_pid;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
is_percpu_thread(void)
{
	return (get_current()->flags & 0x04000000) &&
	       (get_current()->nr_cpus_allowed == 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
task_no_new_privs(struct task_struct *p)
{
	return ((__builtin_constant_p(0) &&
		 __builtin_constant_p((uintptr_t)(&p->atomic_flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(
			 *(const unsigned long *)(&p->atomic_flags))) ?
			const_test_bit(0, &p->atomic_flags) :
			_test_bit(0, &p->atomic_flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_set_no_new_privs(struct task_struct *p)
{
	set_bit(0, &p->atomic_flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
task_spread_page(struct task_struct *p)
{
	return ((__builtin_constant_p(1) &&
		 __builtin_constant_p((uintptr_t)(&p->atomic_flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(
			 *(const unsigned long *)(&p->atomic_flags))) ?
			const_test_bit(1, &p->atomic_flags) :
			_test_bit(1, &p->atomic_flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_set_spread_page(struct task_struct *p)
{
	set_bit(1, &p->atomic_flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_clear_spread_page(struct task_struct *p)
{
	clear_bit(1, &p->atomic_flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
task_spread_slab(struct task_struct *p)
{
	return ((__builtin_constant_p(2) &&
		 __builtin_constant_p((uintptr_t)(&p->atomic_flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(
			 *(const unsigned long *)(&p->atomic_flags))) ?
			const_test_bit(2, &p->atomic_flags) :
			_test_bit(2, &p->atomic_flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_set_spread_slab(struct task_struct *p)
{
	set_bit(2, &p->atomic_flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_clear_spread_slab(struct task_struct *p)
{
	clear_bit(2, &p->atomic_flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
task_spec_ssb_disable(struct task_struct *p)
{
	return ((__builtin_constant_p(3) &&
		 __builtin_constant_p((uintptr_t)(&p->atomic_flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(
			 *(const unsigned long *)(&p->atomic_flags))) ?
			const_test_bit(3, &p->atomic_flags) :
			_test_bit(3, &p->atomic_flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_set_spec_ssb_disable(struct task_struct *p)
{
	set_bit(3, &p->atomic_flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_clear_spec_ssb_disable(struct task_struct *p)
{
	clear_bit(3, &p->atomic_flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
task_spec_ssb_noexec(struct task_struct *p)
{
	return ((__builtin_constant_p(7) &&
		 __builtin_constant_p((uintptr_t)(&p->atomic_flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(
			 *(const unsigned long *)(&p->atomic_flags))) ?
			const_test_bit(7, &p->atomic_flags) :
			_test_bit(7, &p->atomic_flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_set_spec_ssb_noexec(struct task_struct *p)
{
	set_bit(7, &p->atomic_flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_clear_spec_ssb_noexec(struct task_struct *p)
{
	clear_bit(7, &p->atomic_flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
task_spec_ssb_force_disable(struct task_struct *p)
{
	return ((__builtin_constant_p(4) &&
		 __builtin_constant_p((uintptr_t)(&p->atomic_flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(
			 *(const unsigned long *)(&p->atomic_flags))) ?
			const_test_bit(4, &p->atomic_flags) :
			_test_bit(4, &p->atomic_flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_set_spec_ssb_force_disable(struct task_struct *p)
{
	set_bit(4, &p->atomic_flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
task_spec_ib_disable(struct task_struct *p)
{
	return ((__builtin_constant_p(5) &&
		 __builtin_constant_p((uintptr_t)(&p->atomic_flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(
			 *(const unsigned long *)(&p->atomic_flags))) ?
			const_test_bit(5, &p->atomic_flags) :
			_test_bit(5, &p->atomic_flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_set_spec_ib_disable(struct task_struct *p)
{
	set_bit(5, &p->atomic_flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_clear_spec_ib_disable(struct task_struct *p)
{
	clear_bit(5, &p->atomic_flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
task_spec_ib_force_disable(struct task_struct *p)
{
	return ((__builtin_constant_p(6) &&
		 __builtin_constant_p((uintptr_t)(&p->atomic_flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&p->atomic_flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(
			 *(const unsigned long *)(&p->atomic_flags))) ?
			const_test_bit(6, &p->atomic_flags) :
			_test_bit(6, &p->atomic_flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_set_spec_ib_force_disable(struct task_struct *p)
{
	set_bit(6, &p->atomic_flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
current_restore_flags(unsigned long orig_flags, unsigned long flags)
{
	get_current()->flags &= ~flags;
	get_current()->flags |= orig_flags & flags;
}

extern int cpuset_cpumask_can_shrink(const struct cpumask *cur,
				     const struct cpumask *trial);
extern int task_can_attach(struct task_struct *p);
extern int dl_bw_alloc(int cpu, u64 dl_bw);
extern void dl_bw_free(int cpu, u64 dl_bw);

extern void do_set_cpus_allowed(struct task_struct *p,
				const struct cpumask *new_mask);
extern int set_cpus_allowed_ptr(struct task_struct *p,
				const struct cpumask *new_mask);
extern int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src,
			     int node);
extern void release_user_cpus_ptr(struct task_struct *p);
extern int dl_task_check_affinity(struct task_struct *p,
				  const struct cpumask *mask);
extern void force_compatible_cpus_allowed_ptr(struct task_struct *p);
extern void relax_compatible_cpus_allowed_ptr(struct task_struct *p);
extern int yield_to(struct task_struct *p, bool preempt);
extern void set_user_nice(struct task_struct *p, long nice);
extern int task_prio(const struct task_struct *p);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
task_nice(const struct task_struct *p)
{
	return (((p)->static_prio) - (100 + (19 - -20 + 1) / 2));
}

extern int can_nice(const struct task_struct *p, const int nice);
extern int task_curr(const struct task_struct *p);
extern int idle_cpu(int cpu);
extern int available_idle_cpu(int cpu);
extern int sched_setscheduler(struct task_struct *, int,
			      const struct sched_param *);
extern int sched_setscheduler_nocheck(struct task_struct *, int,
				      const struct sched_param *);
extern void sched_set_fifo(struct task_struct *p);
extern void sched_set_fifo_low(struct task_struct *p);
extern void sched_set_normal(struct task_struct *p, int nice);
extern int sched_setattr(struct task_struct *, const struct sched_attr *);
extern int sched_setattr_nocheck(struct task_struct *,
				 const struct sched_attr *);
extern struct task_struct *idle_task(int cpu);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
is_idle_task(const struct task_struct *p)
{
	return !!(p->flags & 0x00000002);
}

extern struct task_struct *curr_task(int cpu);
extern void ia64_set_curr_task(int cpu, struct task_struct *p);

void yield(void);

union thread_union {
	struct task_struct task;

	unsigned long stack[(((1UL) << 12) << (2 + 0)) / sizeof(long)];
};

extern unsigned long
	init_stack[(((1UL) << 12) << (2 + 0)) / sizeof(unsigned long)];
extern struct task_struct *find_task_by_vpid(pid_t nr);
extern struct task_struct *find_task_by_pid_ns(pid_t nr,
					       struct pid_namespace *ns);

extern struct task_struct *find_get_task_by_vpid(pid_t nr);

extern int wake_up_state(struct task_struct *tsk, unsigned int state);
extern int wake_up_process(struct task_struct *tsk);
extern void wake_up_new_task(struct task_struct *tsk);

extern void kick_process(struct task_struct *tsk);

extern void __set_task_comm(struct task_struct *tsk, const char *from,
			    bool exec);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_task_comm(struct task_struct *tsk, const char *from)
{
	__set_task_comm(tsk, from, false);
}

extern char *__get_task_comm(char *to, size_t len, struct task_struct *tsk);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
scheduler_ipi(void)
{
	do {
		if (tif_need_resched())
			set_preempt_need_resched();
	} while (0);
}

extern unsigned long wait_task_inactive(struct task_struct *,
					unsigned int match_state);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_tsk_thread_flag(struct task_struct *tsk, int flag)
{
	set_ti_thread_flag((&(tsk)->thread_info), flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_tsk_thread_flag(struct task_struct *tsk, int flag)
{
	clear_ti_thread_flag((&(tsk)->thread_info), flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
update_tsk_thread_flag(struct task_struct *tsk, int flag, bool value)
{
	update_ti_thread_flag((&(tsk)->thread_info), flag, value);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)
{
	return test_and_set_ti_thread_flag((&(tsk)->thread_info), flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
test_and_clear_tsk_thread_flag(struct task_struct *tsk, int flag)
{
	return test_and_clear_ti_thread_flag((&(tsk)->thread_info), flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
test_tsk_thread_flag(struct task_struct *tsk, int flag)
{
	return test_ti_thread_flag((&(tsk)->thread_info), flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_tsk_need_resched(struct task_struct *tsk)
{
	set_tsk_thread_flag(tsk, 3);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_tsk_need_resched(struct task_struct *tsk)
{
	clear_tsk_thread_flag(tsk, 3);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
test_tsk_need_resched(struct task_struct *tsk)
{
	return __builtin_expect(!!(test_tsk_thread_flag(tsk, 3)), 0);
}
extern int __cond_resched(void);

void sched_dynamic_klp_enable(void);
void sched_dynamic_klp_disable(void);

extern struct static_call_key __SCK__cond_resched;
extern typeof(__cond_resched) __SCT__cond_resched;
;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
_cond_resched(void)
{
	return ({
		static void *__attribute__((__used__))
		__attribute__((__section__(".discard.addressable")))
		__UNIQUE_ID___addressable___SCK__cond_resched319 =
			(void *)(uintptr_t)&__SCK__cond_resched;
		;
		(&__SCT__cond_resched);
	})();
}
extern int __cond_resched_lock(spinlock_t *lock);
extern int __cond_resched_rwlock_read(rwlock_t *lock);
extern int __cond_resched_rwlock_write(rwlock_t *lock);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
need_resched(void)
{
	return __builtin_expect(!!(tif_need_resched()), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
task_cpu(const struct task_struct *p)
{
	return ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_320(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof((&(p)->thread_info)->cpu) ==
				       sizeof(char) ||
			       sizeof((&(p)->thread_info)->cpu) ==
				       sizeof(short) ||
			       sizeof((&(p)->thread_info)->cpu) == sizeof(int) ||
			       sizeof((&(p)->thread_info)->cpu) ==
				       sizeof(long)) ||
			      sizeof((&(p)->thread_info)->cpu) ==
				      sizeof(long long)))
				__compiletime_assert_320();
		} while (0);
		(*(const volatile typeof(_Generic(((&(p)->thread_info)->cpu),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: ((&(p)->thread_info)
								   ->cpu)))
			   *)&((&(p)->thread_info)->cpu));
	});
}

extern void set_task_cpu(struct task_struct *p, unsigned int cpu);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
task_is_runnable(struct task_struct *p)
{
	return p->on_rq && !p->se.sched_delayed;
}

extern bool sched_task_on_rq(struct task_struct *p);
extern unsigned long get_wchan(struct task_struct *p);
extern struct task_struct *cpu_curr_snapshot(int cpu);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vcpu_is_preempted(int cpu)
{
	return false;
}

extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
extern long sched_getaffinity(pid_t pid, struct cpumask *mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
owner_on_cpu(struct task_struct *owner)
{
	return ({
		       do {
			       __attribute__((__noreturn__)) extern void
			       __compiletime_assert_321(void) __attribute__((__error__(
				       "Unsupported access size for {READ,WRITE}_ONCE().")));
			       if (!((sizeof(owner->on_cpu) == sizeof(char) ||
				      sizeof(owner->on_cpu) == sizeof(short) ||
				      sizeof(owner->on_cpu) == sizeof(int) ||
				      sizeof(owner->on_cpu) == sizeof(long)) ||
				     sizeof(owner->on_cpu) ==
					     sizeof(long long)))
				       __compiletime_assert_321();
		       } while (0);
		       (*(const volatile typeof(_Generic(
			       (owner->on_cpu),
							char: (char)0,
							unsigned char: (
								unsigned char)0,
							signed char: (
								signed char)0,
							unsigned short: (
								unsigned short)0,
							signed short: (
								signed short)0,
							unsigned int: (
								unsigned int)0,
							signed int: (
								signed int)0,
							unsigned long: (
								unsigned long)0,
							signed long: (
								signed long)0,
							unsigned long long: (
								unsigned long long)0,
							signed long long: (
								signed long long)0,
							default: (
								owner->on_cpu)))
				  *)&(owner->on_cpu));
	       }) &&
	       !vcpu_is_preempted(task_cpu(owner));
}

unsigned long sched_cpu_util(int cpu);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sched_core_free(struct task_struct *tsk)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sched_core_fork(struct task_struct *p)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sched_core_idle_cpu(int cpu)
{
	return idle_cpu(cpu);
}

extern void sched_set_stop_task(int cpu, struct task_struct *stop);
extern void *pcpu_base_addr;
extern const unsigned long *pcpu_unit_offsets;

struct pcpu_group_info {
	int nr_units;
	unsigned long base_offset;
	unsigned int *cpu_map;
};

struct pcpu_alloc_info {
	size_t static_size;
	size_t reserved_size;
	size_t dyn_size;
	size_t unit_size;
	size_t atom_size;
	size_t alloc_size;
	size_t __ai_size;
	int nr_groups;
	struct pcpu_group_info groups[];
};

enum pcpu_fc {
	PCPU_FC_AUTO,
	PCPU_FC_EMBED,
	PCPU_FC_PAGE,

	PCPU_FC_NR,
};
extern const char *const pcpu_fc_names[PCPU_FC_NR];

extern enum pcpu_fc pcpu_chosen_fc;

typedef int(pcpu_fc_cpu_to_node_fn_t)(int cpu);
typedef int(pcpu_fc_cpu_distance_fn_t)(unsigned int from, unsigned int to);

extern struct pcpu_alloc_info *__attribute__((__section__(".init.text")))
__attribute__((__cold__))
pcpu_alloc_alloc_info(int nr_groups, int nr_units);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
pcpu_free_alloc_info(struct pcpu_alloc_info *ai);

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
pcpu_setup_first_chunk(const struct pcpu_alloc_info *ai, void *base_addr);

extern int __attribute__((__section__(".init.text"))) __attribute__((__cold__))
pcpu_embed_first_chunk(size_t reserved_size, size_t dyn_size, size_t atom_size,
		       pcpu_fc_cpu_distance_fn_t cpu_distance_fn,
		       pcpu_fc_cpu_to_node_fn_t cpu_to_nd_fn);

void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
pcpu_populate_pte(unsigned long addr);
extern int __attribute__((__section__(".init.text"))) __attribute__((__cold__))
pcpu_page_first_chunk(size_t reserved_size,
		      pcpu_fc_cpu_to_node_fn_t cpu_to_nd_fn);

extern bool __is_kernel_percpu_address(unsigned long addr,
				       unsigned long *can_addr);
extern bool is_kernel_percpu_address(unsigned long addr);

extern void *pcpu_alloc_noprof(size_t size, size_t align, bool reserved,
			       gfp_t gfp) __attribute__((__alloc_size__(1)))
__attribute__((__malloc__));
extern void free_percpu(void *__pdata);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__free_free_percpu(void *p)
{
	void *_T = *(void **)p;
	free_percpu(_T);
}

extern phys_addr_t per_cpu_ptr_to_phys(void *addr);

extern unsigned long pcpu_nr_pages(void);

struct msr_info {
	u32 msr_no;
	struct msr reg;
	struct msr *msrs;
	int err;
};

struct msr_regs_info {
	u32 *regs;
	int err;
};

struct saved_msr {
	bool valid;
	struct msr_info info;
};

struct saved_msrs {
	unsigned int num;
	struct saved_msr *array;
};

struct static_call_key;

struct trace_print_flags {
	unsigned long mask;
	const char *name;
};

struct trace_print_flags_u64 {
	unsigned long long mask;
	const char *name;
};

struct tracepoint_func {
	void *func;
	void *data;
	int prio;
};

struct tracepoint {
	const char *name;
	struct static_key key;
	struct static_call_key *static_call_key;
	void *static_call_tramp;
	void *iterator;
	void *probestub;
	int (*regfunc)(void);
	void (*unregfunc)(void);
	struct tracepoint_func *funcs;
};

typedef const int tracepoint_ptr_t;

struct bpf_raw_event_map {
	struct tracepoint *tp;
	void *bpf_func;
	u32 num_args;
	u32 writable_size;
} __attribute__((__aligned__(32)));

extern struct tracepoint __tracepoint_read_msr;
extern struct tracepoint __tracepoint_write_msr;
extern struct tracepoint __tracepoint_rdpmc;
extern void do_trace_write_msr(unsigned int msr, u64 val, int failed);
extern void do_trace_read_msr(unsigned int msr, u64 val, int failed);
extern void do_trace_rdpmc(unsigned int msr, u64 val, int failed);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long long
__rdmsr(unsigned int msr)
{
	unsigned long low, high;

	asm volatile("1: rdmsr\n"
		     "2:\n"
		     " .pushsection \"__ex_table\",\"a\"\n"
		     " .balign 4\n"
		     " .long ("
		     "1b"
		     ") - .\n"
		     " .long ("
		     "2b"
		     ") - .\n"
		     " .long "
		     "9"
		     " \n"
		     " .popsection\n"
		     : "=a"(low), "=d"(high)
		     : "c"(msr));

	return ((low) | (high) << 32);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__wrmsr(unsigned int msr, u32 low, u32 high)
{
	asm volatile("1: wrmsr\n"
		     "2:\n"
		     " .pushsection \"__ex_table\",\"a\"\n"
		     " .balign 4\n"
		     " .long ("
		     "1b"
		     ") - .\n"
		     " .long ("
		     "2b"
		     ") - .\n"
		     " .long "
		     "8"
		     " \n"
		     " .popsection\n"
		     :
		     : "c"(msr), "a"(low), "d"(high)
		     : "memory");
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long long
native_read_msr(unsigned int msr)
{
	unsigned long long val;

	val = __rdmsr(msr);

	if (static_key_false(&(__tracepoint_read_msr).key))
		do_trace_read_msr(msr, val, 0);

	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long long
native_read_msr_safe(unsigned int msr, int *err)
{
	unsigned long low, high;

	asm volatile(
		"1: rdmsr ; xor %[err],%[err]\n"
		"2:\n\t"
		" .pushsection \"__ex_table\",\"a\"\n"
		" .balign 4\n"
		" .long ("
		"1b"
		") - .\n"
		" .long ("
		"2b"
		") - .\n"
		".macro extable_type_reg type:req reg:req\n"
		".set .Lfound, 0\n"
		".set .Lregnr, 0\n"
		".irp rs,rax,rcx,rdx,rbx,rsp,rbp,rsi,rdi,r8,r9,r10,r11,r12,r13,r14,r15\n"
		".ifc \\reg, %%\\rs\n"
		".set .Lfound, .Lfound+1\n"
		".long \\type + (.Lregnr << 8)\n"
		".endif\n"
		".set .Lregnr, .Lregnr+1\n"
		".endr\n"
		".set .Lregnr, 0\n"
		".irp rs,eax,ecx,edx,ebx,esp,ebp,esi,edi,r8d,r9d,r10d,r11d,r12d,r13d,r14d,r15d\n"
		".ifc \\reg, %%\\rs\n"
		".set .Lfound, .Lfound+1\n"
		".long \\type + (.Lregnr << 8)\n"
		".endif\n"
		".set .Lregnr, .Lregnr+1\n"
		".endr\n"
		".if (.Lfound != 1)\n"
		".error \"extable_type_reg: bad register argument\"\n"
		".endif\n"
		".endm\n"
		"extable_type_reg reg="
		"%[err]"
		", type="
		"11"
		" \n"
		".purgem extable_type_reg\n"
		" .popsection\n"
		: [err] "=r"(*err), "=a"(low), "=d"(high)
		: "c"(msr));
	if (static_key_false(&(__tracepoint_read_msr).key))
		do_trace_read_msr(msr, ((low) | (high) << 32), *err);
	return ((low) | (high) << 32);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
	__attribute__((no_instrument_function))
	native_write_msr(unsigned int msr, u32 low, u32 high)
{
	__wrmsr(msr, low, high);

	if (static_key_false(&(__tracepoint_write_msr).key))
		do_trace_write_msr(msr, ((u64)high << 32 | low), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((no_instrument_function))
	native_write_msr_safe(unsigned int msr, u32 low, u32 high)
{
	int err;

	asm volatile(
		"1: wrmsr ; xor %[err],%[err]\n"
		"2:\n\t"
		" .pushsection \"__ex_table\",\"a\"\n"
		" .balign 4\n"
		" .long ("
		"1b"
		") - .\n"
		" .long ("
		"2b"
		") - .\n"
		".macro extable_type_reg type:req reg:req\n"
		".set .Lfound, 0\n"
		".set .Lregnr, 0\n"
		".irp rs,rax,rcx,rdx,rbx,rsp,rbp,rsi,rdi,r8,r9,r10,r11,r12,r13,r14,r15\n"
		".ifc \\reg, %%\\rs\n"
		".set .Lfound, .Lfound+1\n"
		".long \\type + (.Lregnr << 8)\n"
		".endif\n"
		".set .Lregnr, .Lregnr+1\n"
		".endr\n"
		".set .Lregnr, 0\n"
		".irp rs,eax,ecx,edx,ebx,esp,ebp,esi,edi,r8d,r9d,r10d,r11d,r12d,r13d,r14d,r15d\n"
		".ifc \\reg, %%\\rs\n"
		".set .Lfound, .Lfound+1\n"
		".long \\type + (.Lregnr << 8)\n"
		".endif\n"
		".set .Lregnr, .Lregnr+1\n"
		".endr\n"
		".if (.Lfound != 1)\n"
		".error \"extable_type_reg: bad register argument\"\n"
		".endif\n"
		".endm\n"
		"extable_type_reg reg="
		"%[err]"
		", type="
		"10"
		" \n"
		".purgem extable_type_reg\n"
		" .popsection\n"
		: [err] "=a"(err)
		: "c"(msr), "0"(low), "d"(high)
		: "memory");
	if (static_key_false(&(__tracepoint_write_msr).key))
		do_trace_write_msr(msr, ((u64)high << 32 | low), err);
	return err;
}

extern int rdmsr_safe_regs(u32 regs[8]);
extern int wrmsr_safe_regs(u32 regs[8]);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long long
rdtsc(void)
{
	unsigned long low, high;

	asm volatile("rdtsc" : "=a"(low), "=d"(high));

	return ((low) | (high) << 32);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long long
rdtsc_ordered(void)
{
	unsigned long low, high;
	asm volatile("# ALT: oldinstr\n"
		     "771:\n\t"
		     "# ALT: oldinstr\n"
		     "771:\n\t"
		     "rdtsc"
		     "\n772:\n"
		     "# ALT: padding\n"
		     ".skip -((("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")) > 0) * "
		     "(("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")),0x90\n"
		     "773:\n"
		     ".pushsection .altinstructions,\"a\"\n"
		     " .long 771b - .\n"
		     " .long 774f - .\n"
		     " .4byte "
		     "(20*32+ 2)"
		     "\n"
		     " .byte "
		     "773b-771b"
		     "\n"
		     " .byte "
		     "775f-774f"
		     "\n"
		     ".popsection\n"
		     ".pushsection .altinstr_replacement, \"ax\"\n"
		     "# ALT: replacement\n"
		     "774:\n\t"
		     "lfence; rdtsc"
		     "\n775:\n"
		     ".popsection\n"
		     "\n772:\n"
		     "# ALT: padding\n"
		     ".skip -((("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")) > 0) * "
		     "(("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")),0x90\n"
		     "773:\n"
		     ".pushsection .altinstructions,\"a\"\n"
		     " .long 771b - .\n"
		     " .long 774f - .\n"
		     " .4byte "
		     "( 1*32+27)"
		     "\n"
		     " .byte "
		     "773b-771b"
		     "\n"
		     " .byte "
		     "775f-774f"
		     "\n"
		     ".popsection\n"
		     ".pushsection .altinstr_replacement, \"ax\"\n"
		     "# ALT: replacement\n"
		     "774:\n\t"
		     "rdtscp"
		     "\n775:\n"
		     ".popsection\n"

		     : "=a"(low), "=d"(high)

					  ::"ecx");

	return ((low) | (high) << 32);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long long
native_read_pmc(int counter)
{
	unsigned long low, high;

	asm volatile("rdpmc" : "=a"(low), "=d"(high) : "c"(counter));
	if (static_key_false(&(__tracepoint_rdpmc).key))
		do_trace_rdpmc(counter, ((low) | (high) << 32), 0);
	return ((low) | (high) << 32);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
wrmsr(unsigned int msr, u32 low, u32 high)
{
	native_write_msr(msr, low, high);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
wrmsrl(unsigned int msr, u64 val)
{
	native_write_msr(msr, (u32)(val & 0xffffffffULL), (u32)(val >> 32));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
wrmsr_safe(unsigned int msr, u32 low, u32 high)
{
	return native_write_msr_safe(msr, low, high);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
rdmsrl_safe(unsigned int msr, unsigned long long *p)
{
	int err;

	*p = native_read_msr_safe(msr, &err);
	return err;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
wrmsrns(u32 msr, u64 val)
{
	asm volatile("1: "
		     "# ALT: oldinstr\n"
		     "771:\n\t"
		     "ds wrmsr"
		     "\n772:\n"
		     "# ALT: padding\n"
		     ".skip -((("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")) > 0) * "
		     "(("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")),0x90\n"
		     "773:\n"
		     ".pushsection .altinstructions,\"a\"\n"
		     " .long 771b - .\n"
		     " .long 774f - .\n"
		     " .4byte "
		     "(12*32+19)"
		     "\n"
		     " .byte "
		     "773b-771b"
		     "\n"
		     " .byte "
		     "775f-774f"
		     "\n"
		     ".popsection\n"
		     ".pushsection .altinstr_replacement, \"ax\"\n"
		     "# ALT: replacement\n"
		     "774:\n\t"
		     " "
		     ".byte 0x0f,0x01,0xc6 ;"
		     " "
		     "\n775:\n"
		     ".popsection\n"
		     "2: "
		     " .pushsection \"__ex_table\",\"a\"\n"
		     " .balign 4\n"
		     " .long ("
		     "1b"
		     ") - .\n"
		     " .long ("
		     "2b"
		     ") - .\n"
		     " .long "
		     "8"
		     " \n"
		     " .popsection\n"
		     :
		     : "c"(msr), "a"((u32)val), "d"((u32)(val >> 32)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
wrmsrl_safe(u32 msr, u64 val)
{
	return wrmsr_safe(msr, (u32)val, (u32)(val >> 32));
}

struct msr *msrs_alloc(void);
void msrs_free(struct msr *msrs);
int msr_set_bit(u32 msr, u8 bit);
int msr_clear_bit(u32 msr, u8 bit);

int rdmsr_on_cpu(unsigned int cpu, u32 msr_no, u32 *l, u32 *h);
int wrmsr_on_cpu(unsigned int cpu, u32 msr_no, u32 l, u32 h);
int rdmsrl_on_cpu(unsigned int cpu, u32 msr_no, u64 *q);
int wrmsrl_on_cpu(unsigned int cpu, u32 msr_no, u64 q);
void rdmsr_on_cpus(const struct cpumask *mask, u32 msr_no, struct msr *msrs);
void wrmsr_on_cpus(const struct cpumask *mask, u32 msr_no, struct msr *msrs);
int rdmsr_safe_on_cpu(unsigned int cpu, u32 msr_no, u32 *l, u32 *h);
int wrmsr_safe_on_cpu(unsigned int cpu, u32 msr_no, u32 l, u32 h);
int rdmsrl_safe_on_cpu(unsigned int cpu, u32 msr_no, u64 *q);
int wrmsrl_safe_on_cpu(unsigned int cpu, u32 msr_no, u64 q);
int rdmsr_safe_regs_on_cpu(unsigned int cpu, u32 regs[8]);
int wrmsr_safe_regs_on_cpu(unsigned int cpu, u32 regs[8]);

typedef unsigned long long cycles_t;

extern unsigned int cpu_khz;
extern unsigned int tsc_khz;

extern void disable_TSC(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) cycles_t
get_cycles(void)
{
	if (!1 &&
	    !(__builtin_constant_p((0 * 32 + 4)) &&
			      (((((0 * 32 + 4)) >> 5) == (0) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((0 * 32 + 1) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (1) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (2) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (3) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((3 * 32 + 2) & 31)) |
				  (1 << ((3 * 32 + 3) & 31)) |
				  (1 << ((3 * 32 + 1) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (4) &&
				(1UL << (((0 * 32 + 4)) & 31) & (0))) ||
			       ((((0 * 32 + 4)) >> 5) == (5) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (6) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (7) &&
				(1UL << (((0 * 32 + 4)) & 31) & (0))) ||
			       ((((0 * 32 + 4)) >> 5) == (8) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((8 * 32 + 16) & 31)) |
				  (1 << ((8 * 32 + 22) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (9) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((9 * 32 + 2) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (10) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (11) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((11 * 32 + 23) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (12) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((12 * 32 + 17) & 31)) |
				  (1 << ((12 * 32 + 26) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (13) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (14) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (15) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (16) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((16 * 32 + 29) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (17) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (18) &&
				(1UL << (((0 * 32 + 4)) & 31) & (0))) ||
			       ((((0 * 32 + 4)) >> 5) == (19) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((19 * 32 + 4) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (20) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (21) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); }))) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); })))) ?
		      0 :
		      (__builtin_constant_p((
			       __builtin_constant_p((
				       0 * 32 +
				       4)) && (((((0 * 32 + 4)) >> 5) == (0) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 ((1 << ((0 * 32 + 0) & 31)) |
						  (1 << ((0 * 32 + 3)) & 31) |
						  (1 << ((0 * 32 + 5) & 31)) |
						  (1 << ((0 * 32 + 6) & 31)) |
						  (1 << ((0 * 32 + 8) & 31)) |
						  (1 << ((0 * 32 + 13)) & 31) |
						  (1 << ((0 * 32 + 24) & 31)) |
						  (1 << ((0 * 32 + 15) & 31)) |
						  (1 << ((0 * 32 + 25) & 31)) |
						  (1 << ((0 * 32 + 26) &
							 31))))) ||
					       ((((0 * 32 + 4)) >> 5) == (1) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 ((1 << ((1 * 32 + 29) & 31)) |
						  0))) ||
					       ((((0 * 32 + 4)) >> 5) == (2) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (3) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 ((1 << ((3 * 32 + 20) &
							 31))))) ||
					       ((((0 * 32 + 4)) >> 5) == (4) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 (0))) ||
					       ((((0 * 32 + 4)) >> 5) == (5) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (6) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (7) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (8) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (9) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (10) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (11) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (12) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (13) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (14) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (15) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (16) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (17) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (18) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (19) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (20) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (21) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       }))) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       })))) ?
				       1 :
				       arch_test_bit(
					       (0 * 32 + 4),
					       (unsigned long
							*)((&boot_cpu_data)
								   ->x86_capability)))) ?
			       (__builtin_constant_p((
					0 * 32 +
					4)) && (((((0 * 32 + 4)) >> 5) == (0) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  ((1 << ((0 * 32 + 0) & 31)) |
						   (1 << ((0 * 32 + 3)) & 31) |
						   (1 << ((0 * 32 + 5) & 31)) |
						   (1 << ((0 * 32 + 6) & 31)) |
						   (1 << ((0 * 32 + 8) & 31)) |
						   (1 << ((0 * 32 + 13)) & 31) |
						   (1 << ((0 * 32 + 24) & 31)) |
						   (1 << ((0 * 32 + 15) & 31)) |
						   (1 << ((0 * 32 + 25) & 31)) |
						   (1 << ((0 * 32 + 26) &
							  31))))) ||
						((((0 * 32 + 4)) >> 5) == (1) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  ((1 << ((1 * 32 + 29) & 31)) |
						   0))) ||
						((((0 * 32 + 4)) >> 5) == (2) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (3) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  ((1 << ((3 * 32 + 20) &
							  31))))) ||
						((((0 * 32 + 4)) >> 5) == (4) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  (0))) ||
						((((0 * 32 + 4)) >> 5) == (5) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (6) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (7) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (8) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (9) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (10) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (11) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (12) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (13) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (14) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (15) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (16) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (17) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (18) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (19) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (20) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (21) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						}))) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						})))) ?
					1 :
					arch_test_bit(
						(0 * 32 + 4),
						(unsigned long
							 *)((&boot_cpu_data)
								    ->x86_capability))) :
			       _static_cpu_has((0 * 32 + 4)))))
		return 0;
	return rdtsc();
}

extern void tsc_early_init(void);
extern void tsc_init(void);
extern void mark_tsc_unstable(char *reason);
extern int unsynchronized_tsc(void);
extern int check_tsc_unstable(void);
extern void mark_tsc_async_resets(char *reason);
extern unsigned long native_calibrate_cpu_early(void);
extern unsigned long native_calibrate_tsc(void);
extern unsigned long long native_sched_clock_from_tsc(u64 tsc);

extern int tsc_clocksource_reliable;

extern bool tsc_async_resets;
extern bool tsc_store_and_check_tsc_adjust(bool bootcpu);
extern void tsc_verify_tsc_adjust(bool resume);
extern void check_tsc_sync_target(void);

extern int notsc_setup(char *);
extern void tsc_save_sched_clock_state(void);
extern void tsc_restore_sched_clock_state(void);

unsigned long cpu_khz_from_msr(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
random_get_entropy(void)
{
	if (!1 &&
	    !(__builtin_constant_p((0 * 32 + 4)) &&
			      (((((0 * 32 + 4)) >> 5) == (0) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((0 * 32 + 1) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (1) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (2) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (3) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((3 * 32 + 2) & 31)) |
				  (1 << ((3 * 32 + 3) & 31)) |
				  (1 << ((3 * 32 + 1) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (4) &&
				(1UL << (((0 * 32 + 4)) & 31) & (0))) ||
			       ((((0 * 32 + 4)) >> 5) == (5) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (6) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (7) &&
				(1UL << (((0 * 32 + 4)) & 31) & (0))) ||
			       ((((0 * 32 + 4)) >> 5) == (8) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((8 * 32 + 16) & 31)) |
				  (1 << ((8 * 32 + 22) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (9) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((9 * 32 + 2) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (10) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (11) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((11 * 32 + 23) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (12) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((12 * 32 + 17) & 31)) |
				  (1 << ((12 * 32 + 26) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (13) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (14) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (15) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (16) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((16 * 32 + 29) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (17) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (18) &&
				(1UL << (((0 * 32 + 4)) & 31) & (0))) ||
			       ((((0 * 32 + 4)) >> 5) == (19) &&
				(1UL << (((0 * 32 + 4)) & 31) &
				 ((1 << ((19 * 32 + 4) & 31))))) ||
			       ((((0 * 32 + 4)) >> 5) == (20) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((((0 * 32 + 4)) >> 5) == (21) &&
				(1UL << (((0 * 32 + 4)) & 31) & 0)) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); }))) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); })))) ?
		      0 :
		      (__builtin_constant_p((
			       __builtin_constant_p((
				       0 * 32 +
				       4)) && (((((0 * 32 + 4)) >> 5) == (0) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 ((1 << ((0 * 32 + 0) & 31)) |
						  (1 << ((0 * 32 + 3)) & 31) |
						  (1 << ((0 * 32 + 5) & 31)) |
						  (1 << ((0 * 32 + 6) & 31)) |
						  (1 << ((0 * 32 + 8) & 31)) |
						  (1 << ((0 * 32 + 13)) & 31) |
						  (1 << ((0 * 32 + 24) & 31)) |
						  (1 << ((0 * 32 + 15) & 31)) |
						  (1 << ((0 * 32 + 25) & 31)) |
						  (1 << ((0 * 32 + 26) &
							 31))))) ||
					       ((((0 * 32 + 4)) >> 5) == (1) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 ((1 << ((1 * 32 + 29) & 31)) |
						  0))) ||
					       ((((0 * 32 + 4)) >> 5) == (2) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (3) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 ((1 << ((3 * 32 + 20) &
							 31))))) ||
					       ((((0 * 32 + 4)) >> 5) == (4) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 (0))) ||
					       ((((0 * 32 + 4)) >> 5) == (5) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (6) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (7) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (8) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (9) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (10) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (11) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (12) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (13) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (14) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (15) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (16) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (17) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (18) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (19) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (20) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((((0 * 32 + 4)) >> 5) == (21) &&
						(1UL << (((0 * 32 + 4)) & 31) &
						 0)) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       }))) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       })))) ?
				       1 :
				       arch_test_bit(
					       (0 * 32 + 4),
					       (unsigned long
							*)((&boot_cpu_data)
								   ->x86_capability)))) ?
			       (__builtin_constant_p((
					0 * 32 +
					4)) && (((((0 * 32 + 4)) >> 5) == (0) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  ((1 << ((0 * 32 + 0) & 31)) |
						   (1 << ((0 * 32 + 3)) & 31) |
						   (1 << ((0 * 32 + 5) & 31)) |
						   (1 << ((0 * 32 + 6) & 31)) |
						   (1 << ((0 * 32 + 8) & 31)) |
						   (1 << ((0 * 32 + 13)) & 31) |
						   (1 << ((0 * 32 + 24) & 31)) |
						   (1 << ((0 * 32 + 15) & 31)) |
						   (1 << ((0 * 32 + 25) & 31)) |
						   (1 << ((0 * 32 + 26) &
							  31))))) ||
						((((0 * 32 + 4)) >> 5) == (1) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  ((1 << ((1 * 32 + 29) & 31)) |
						   0))) ||
						((((0 * 32 + 4)) >> 5) == (2) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (3) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  ((1 << ((3 * 32 + 20) &
							  31))))) ||
						((((0 * 32 + 4)) >> 5) == (4) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  (0))) ||
						((((0 * 32 + 4)) >> 5) == (5) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (6) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (7) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (8) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (9) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (10) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (11) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (12) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (13) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (14) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (15) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (16) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (17) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (18) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (19) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (20) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((((0 * 32 + 4)) >> 5) == (21) &&
						 (1UL << (((0 * 32 + 4)) & 31) &
						  0)) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						}))) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						})))) ?
					1 :
					arch_test_bit(
						(0 * 32 + 4),
						(unsigned long
							 *)((&boot_cpu_data)
								    ->x86_capability))) :
			       _static_cpu_has((0 * 32 + 4)))))
		return random_get_entropy_fallback();
	return rdtsc();
}
extern unsigned long tick_usec;
extern unsigned long tick_nsec;
extern int do_adjtimex(struct __kernel_timex *);
extern int do_clock_adjtime(const clockid_t which_clock,
			    struct __kernel_timex *ktx);

extern void hardpps(const struct timespec64 *, const struct timespec64 *);

int read_current_timer(unsigned long *timer_val);

typedef s32 old_time32_t;

struct old_timespec32 {
	old_time32_t tv_sec;
	s32 tv_nsec;
};

struct old_timeval32 {
	old_time32_t tv_sec;
	s32 tv_usec;
};

struct old_itimerspec32 {
	struct old_timespec32 it_interval;
	struct old_timespec32 it_value;
};

struct old_utimbuf32 {
	old_time32_t actime;
	old_time32_t modtime;
};

struct old_timex32 {
	u32 modes;
	s32 offset;
	s32 freq;
	s32 maxerror;
	s32 esterror;
	s32 status;
	s32 constant;
	s32 precision;
	s32 tolerance;
	struct old_timeval32 time;
	s32 tick;
	s32 ppsfreq;
	s32 jitter;
	s32 shift;
	s32 stabil;
	s32 jitcnt;
	s32 calcnt;
	s32 errcnt;
	s32 stbcnt;
	s32 tai;

	s32 : 32;
	s32 : 32;
	s32 : 32;
	s32 : 32;
	s32 : 32;
	s32 : 32;
	s32 : 32;
	s32 : 32;
	s32 : 32;
	s32 : 32;
	s32 : 32;
};

extern int get_old_timespec32(struct timespec64 *, const void *);
extern int put_old_timespec32(const struct timespec64 *, void *);
extern int get_old_itimerspec32(struct itimerspec64 *its,
				const struct old_itimerspec32 *uits);
extern int put_old_itimerspec32(const struct itimerspec64 *its,
				struct old_itimerspec32 *uits);
struct __kernel_timex;
int get_old_timex32(struct __kernel_timex *, const struct old_timex32 *);
int put_old_timex32(struct old_timex32 *, const struct __kernel_timex *);

extern struct __kernel_old_timeval ns_to_kernel_old_timeval(s64 nsec);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
itimerspec64_valid(const struct itimerspec64 *its)
{
	if (!timespec64_valid(&(its->it_interval)) ||
	    !timespec64_valid(&(its->it_value)))
		return false;

	return true;
}

struct timens_offset {
	s64 sec;
	u64 nsec;
};

extern int register_refined_jiffies(long clock_tick_rate);
extern u64 __attribute__((__aligned__((1 << (6))),
			  __section__(".data..cacheline_aligned"))) jiffies_64;
extern unsigned long volatile
	__attribute__((__aligned__((1 << (6))),
		       __section__(".data..cacheline_aligned"))) jiffies;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
get_jiffies_64(void)
{
	return (u64)jiffies;
}
extern unsigned long preset_lpj;
extern unsigned int jiffies_to_msecs(const unsigned long j);
extern unsigned int jiffies_to_usecs(const unsigned long j);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
jiffies_to_nsecs(const unsigned long j)
{
	return (u64)jiffies_to_usecs(j) * 1000L;
}

extern u64 jiffies64_to_nsecs(u64 j);
extern u64 jiffies64_to_msecs(u64 j);

extern unsigned long __msecs_to_jiffies(const unsigned int m);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
_msecs_to_jiffies(const unsigned int m)
{
	return (m + (1000L / 1000) - 1) / (1000L / 1000);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
msecs_to_jiffies(const unsigned int m)
{
	if (__builtin_constant_p(m)) {
		if ((int)m < 0)
			return ((((long)(~0UL >> 1)) >> 1) - 1);
		return _msecs_to_jiffies(m);
	} else {
		return __msecs_to_jiffies(m);
	}
}

extern unsigned long __usecs_to_jiffies(const unsigned int u);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
_usecs_to_jiffies(const unsigned int u)
{
	return (u + (1000000L / 1000) - 1) / (1000000L / 1000);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
usecs_to_jiffies(const unsigned int u)
{
	if (__builtin_constant_p(u)) {
		if (u > jiffies_to_usecs(((((long)(~0UL >> 1)) >> 1) - 1)))
			return ((((long)(~0UL >> 1)) >> 1) - 1);
		return _usecs_to_jiffies(u);
	} else {
		return __usecs_to_jiffies(u);
	}
}

extern unsigned long timespec64_to_jiffies(const struct timespec64 *value);
extern void jiffies_to_timespec64(const unsigned long jiffies,
				  struct timespec64 *value);
extern clock_t jiffies_to_clock_t(unsigned long x);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) clock_t
jiffies_delta_to_clock_t(long delta)
{
	return jiffies_to_clock_t(({
		__auto_type __UNIQUE_ID_x_322 = (0L);
		__auto_type __UNIQUE_ID_y_323 = (delta);
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_324(void) __attribute__((
				__error__("max"
					  "("
					  "0L"
					  ", "
					  "delta"
					  ") signedness error")));
			if (!(!(!(((((typeof(__UNIQUE_ID_x_322))(-1)) <
				    (typeof(__UNIQUE_ID_x_322))1) ?
					   (2 + (__builtin_constant_p(
							 (long)(0L) >= 0) &&
						 ((long)(0L) >= 0))) :
					   (1 + 2 * (sizeof(__UNIQUE_ID_x_322) <
						     4))) &
				  ((((typeof(__UNIQUE_ID_y_323))(-1)) <
				    (typeof(__UNIQUE_ID_y_323))1) ?
					   (2 + (__builtin_constant_p(
							 (long)(delta) >= 0) &&
						 ((long)(delta) >= 0))) :
					   (1 + 2 * (sizeof(__UNIQUE_ID_y_323) <
						     4)))))))
				__compiletime_assert_324();
		} while (0);
		((__UNIQUE_ID_x_322) > (__UNIQUE_ID_y_323) ?
			 (__UNIQUE_ID_x_322) :
			 (__UNIQUE_ID_y_323));
	}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
jiffies_delta_to_msecs(long delta)
{
	return jiffies_to_msecs(({
		__auto_type __UNIQUE_ID_x_325 = (0L);
		__auto_type __UNIQUE_ID_y_326 = (delta);
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_327(void) __attribute__((
				__error__("max"
					  "("
					  "0L"
					  ", "
					  "delta"
					  ") signedness error")));
			if (!(!(!(((((typeof(__UNIQUE_ID_x_325))(-1)) <
				    (typeof(__UNIQUE_ID_x_325))1) ?
					   (2 + (__builtin_constant_p(
							 (long)(0L) >= 0) &&
						 ((long)(0L) >= 0))) :
					   (1 + 2 * (sizeof(__UNIQUE_ID_x_325) <
						     4))) &
				  ((((typeof(__UNIQUE_ID_y_326))(-1)) <
				    (typeof(__UNIQUE_ID_y_326))1) ?
					   (2 + (__builtin_constant_p(
							 (long)(delta) >= 0) &&
						 ((long)(delta) >= 0))) :
					   (1 + 2 * (sizeof(__UNIQUE_ID_y_326) <
						     4)))))))
				__compiletime_assert_327();
		} while (0);
		((__UNIQUE_ID_x_325) > (__UNIQUE_ID_y_326) ?
			 (__UNIQUE_ID_x_325) :
			 (__UNIQUE_ID_y_326));
	}));
}

extern unsigned long clock_t_to_jiffies(unsigned long x);
extern u64 jiffies_64_to_clock_t(u64 x);
extern u64 nsec_to_clock_t(u64 x);
extern u64 nsecs_to_jiffies64(u64 n);
extern unsigned long nsecs_to_jiffies(u64 n);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_set(const s64 secs, const unsigned long nsecs)
{
	if (__builtin_expect(
		    !!(secs >= (((s64) ~((u64)1 << 63)) / 1000000000L)), 0))
		return ((s64) ~((u64)1 << 63));

	return secs * 1000000000L + (s64)nsecs;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
timespec64_to_ktime(struct timespec64 ts)
{
	return ktime_set(ts.tv_sec, ts.tv_nsec);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
ktime_to_ns(const ktime_t kt)
{
	return kt;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
ktime_compare(const ktime_t cmp1, const ktime_t cmp2)
{
	if (cmp1 < cmp2)
		return -1;
	if (cmp1 > cmp2)
		return 1;
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
ktime_after(const ktime_t cmp1, const ktime_t cmp2)
{
	return ktime_compare(cmp1, cmp2) > 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
ktime_before(const ktime_t cmp1, const ktime_t cmp2)
{
	return ktime_compare(cmp1, cmp2) < 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
ktime_divns(const ktime_t kt, s64 div)
{
	({
		int __ret_warn_on = !!(div < 0);
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) | (((9) << 8));
				({
					asm volatile(
						"328"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"328"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(328));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/ktime.h"),
						  "i"(152), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"329"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"329"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(329));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	return kt / div;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
ktime_to_us(const ktime_t kt)
{
	return ktime_divns(kt, 1000L);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
ktime_to_ms(const ktime_t kt)
{
	return ktime_divns(kt, 1000000L);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
ktime_us_delta(const ktime_t later, const ktime_t earlier)
{
	return ktime_to_us(((later) - (earlier)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
ktime_ms_delta(const ktime_t later, const ktime_t earlier)
{
	return ktime_to_ms(((later) - (earlier)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_add_us(const ktime_t kt, const u64 usec)
{
	return ((kt) + (usec * 1000L));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_add_ms(const ktime_t kt, const u64 msec)
{
	return ((kt) + (msec * 1000000L));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_sub_us(const ktime_t kt, const u64 usec)
{
	return ((kt) - (usec * 1000L));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_sub_ms(const ktime_t kt, const u64 msec)
{
	return ((kt) - (msec * 1000000L));
}

extern ktime_t ktime_add_safe(const ktime_t lhs, const ktime_t rhs);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) bool
ktime_to_timespec64_cond(const ktime_t kt, struct timespec64 *ts)
{
	if (kt) {
		*ts = ns_to_timespec64((kt));
		return true;
	} else {
		return false;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ns_to_ktime(u64 ns)
{
	return ns;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ms_to_ktime(u64 ms)
{
	return ms * 1000000L;
}

enum clocksource_ids {
	CSID_GENERIC = 0,
	CSID_ARM_ARCH_COUNTER,
	CSID_X86_TSC_EARLY,
	CSID_X86_TSC,
	CSID_X86_KVM_CLK,
	CSID_X86_ART,
	CSID_MAX,
};

void timekeeping_init(void);
extern int timekeeping_suspended;

extern void legacy_timer_tick(unsigned long ticks);

extern int do_settimeofday64(const struct timespec64 *ts);
extern int do_sys_settimeofday64(const struct timespec64 *tv,
				 const struct timezone *tz);
extern void ktime_get_raw_ts64(struct timespec64 *ts);
extern void ktime_get_ts64(struct timespec64 *ts);
extern void ktime_get_real_ts64(struct timespec64 *tv);
extern void ktime_get_coarse_ts64(struct timespec64 *ts);
extern void ktime_get_coarse_real_ts64(struct timespec64 *ts);

void getboottime64(struct timespec64 *ts);

extern time64_t ktime_get_seconds(void);
extern time64_t __ktime_get_real_seconds(void);
extern time64_t ktime_get_real_seconds(void);

enum tk_offsets {
	TK_OFFS_REAL,
	TK_OFFS_BOOT,
	TK_OFFS_TAI,
	TK_OFFS_MAX,
};

extern ktime_t ktime_get(void);
extern ktime_t ktime_get_with_offset(enum tk_offsets offs);
extern ktime_t ktime_get_coarse_with_offset(enum tk_offsets offs);
extern ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs);
extern ktime_t ktime_get_raw(void);
extern u32 ktime_get_resolution_ns(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_get_real(void)
{
	return ktime_get_with_offset(TK_OFFS_REAL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_get_coarse_real(void)
{
	return ktime_get_coarse_with_offset(TK_OFFS_REAL);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_get_boottime(void)
{
	return ktime_get_with_offset(TK_OFFS_BOOT);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_get_coarse_boottime(void)
{
	return ktime_get_coarse_with_offset(TK_OFFS_BOOT);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_get_clocktai(void)
{
	return ktime_get_with_offset(TK_OFFS_TAI);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_get_coarse_clocktai(void)
{
	return ktime_get_coarse_with_offset(TK_OFFS_TAI);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_get_coarse(void)
{
	struct timespec64 ts;

	ktime_get_coarse_ts64(&ts);
	return timespec64_to_ktime(ts);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
ktime_get_coarse_ns(void)
{
	return ktime_to_ns(ktime_get_coarse());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
ktime_get_coarse_real_ns(void)
{
	return ktime_to_ns(ktime_get_coarse_real());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
ktime_get_coarse_boottime_ns(void)
{
	return ktime_to_ns(ktime_get_coarse_boottime());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
ktime_get_coarse_clocktai_ns(void)
{
	return ktime_to_ns(ktime_get_coarse_clocktai());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
ktime_mono_to_real(ktime_t mono)
{
	return ktime_mono_to_any(mono, TK_OFFS_REAL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
ktime_get_ns(void)
{
	return ktime_to_ns(ktime_get());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
ktime_get_real_ns(void)
{
	return ktime_to_ns(ktime_get_real());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
ktime_get_boottime_ns(void)
{
	return ktime_to_ns(ktime_get_boottime());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
ktime_get_clocktai_ns(void)
{
	return ktime_to_ns(ktime_get_clocktai());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
ktime_get_raw_ns(void)
{
	return ktime_to_ns(ktime_get_raw());
}

extern u64 ktime_get_mono_fast_ns(void);
extern u64 ktime_get_raw_fast_ns(void);
extern u64 ktime_get_boot_fast_ns(void);
extern u64 ktime_get_tai_fast_ns(void);
extern u64 ktime_get_real_fast_ns(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ktime_get_boottime_ts64(struct timespec64 *ts)
{
	*ts = ns_to_timespec64((ktime_get_boottime()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ktime_get_coarse_boottime_ts64(struct timespec64 *ts)
{
	*ts = ns_to_timespec64((ktime_get_coarse_boottime()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) time64_t
ktime_get_boottime_seconds(void)
{
	return ktime_divns(ktime_get_coarse_boottime(), 1000000000L);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ktime_get_clocktai_ts64(struct timespec64 *ts)
{
	*ts = ns_to_timespec64((ktime_get_clocktai()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ktime_get_coarse_clocktai_ts64(struct timespec64 *ts)
{
	*ts = ns_to_timespec64((ktime_get_coarse_clocktai()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) time64_t
ktime_get_clocktai_seconds(void)
{
	return ktime_divns(ktime_get_coarse_clocktai(), 1000000000L);
}

extern bool timekeeping_rtc_skipsuspend(void);
extern bool timekeeping_rtc_skipresume(void);

extern void timekeeping_inject_sleeptime64(const struct timespec64 *delta);

struct ktime_timestamps {
	u64 mono;
	u64 boot;
	u64 real;
};
struct system_time_snapshot {
	u64 cycles;
	ktime_t real;
	ktime_t raw;
	enum clocksource_ids cs_id;
	unsigned int clock_was_set_seq;
	u8 cs_was_changed_seq;
};
struct system_device_crosststamp {
	ktime_t device;
	ktime_t sys_realtime;
	ktime_t sys_monoraw;
};
struct system_counterval_t {
	u64 cycles;
	enum clocksource_ids cs_id;
	bool use_nsecs;
};

extern bool ktime_real_to_base_clock(ktime_t treal,
				     enum clocksource_ids base_id, u64 *cycles);
extern bool timekeeping_clocksource_has_base(enum clocksource_ids id);

extern int get_device_system_crosststamp(
	int (*get_time_fn)(ktime_t *device_time,
			   struct system_counterval_t *system_counterval,
			   void *ctx),
	void *ctx, struct system_time_snapshot *history,
	struct system_device_crosststamp *xtstamp);

extern void ktime_get_snapshot(struct system_time_snapshot *systime_snapshot);

extern void ktime_get_fast_timestamps(struct ktime_timestamps *snap);

extern int persistent_clock_is_local;

extern void read_persistent_clock64(struct timespec64 *ts);
void read_persistent_wall_and_boot_offset(struct timespec64 *wall_clock,
					  struct timespec64 *boot_offset);

extern int update_persistent_clock64(struct timespec64 now);

enum debug_obj_state {
	ODEBUG_STATE_NONE,
	ODEBUG_STATE_INIT,
	ODEBUG_STATE_INACTIVE,
	ODEBUG_STATE_ACTIVE,
	ODEBUG_STATE_DESTROYED,
	ODEBUG_STATE_NOTAVAILABLE,
	ODEBUG_STATE_MAX,
};

struct debug_obj_descr;
struct debug_obj {
	struct hlist_node node;
	enum debug_obj_state state;
	unsigned int astate;
	void *object;
	const struct debug_obj_descr *descr;
};
struct debug_obj_descr {
	const char *name;
	void *(*debug_hint)(void *addr);
	bool (*is_static_object)(void *addr);
	bool (*fixup_init)(void *addr, enum debug_obj_state state);
	bool (*fixup_activate)(void *addr, enum debug_obj_state state);
	bool (*fixup_destroy)(void *addr, enum debug_obj_state state);
	bool (*fixup_free)(void *addr, enum debug_obj_state state);
	bool (*fixup_assert_init)(void *addr, enum debug_obj_state state);
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_object_init(void *addr, const struct debug_obj_descr *descr)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_object_init_on_stack(void *addr, const struct debug_obj_descr *descr)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
debug_object_activate(void *addr, const struct debug_obj_descr *descr)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_object_deactivate(void *addr, const struct debug_obj_descr *descr)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_object_destroy(void *addr, const struct debug_obj_descr *descr)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_object_free(void *addr, const struct debug_obj_descr *descr)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_object_assert_init(void *addr, const struct debug_obj_descr *descr)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_objects_early_init(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_objects_mem_init(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
debug_check_no_obj_freed(const void *address, unsigned long size)
{
}
void init_timer_key(struct timer_list *timer, void (*func)(struct timer_list *),
		    unsigned int flags, const char *name,
		    struct lock_class_key *key);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
init_timer_on_stack_key(struct timer_list *timer,
			void (*func)(struct timer_list *), unsigned int flags,
			const char *name, struct lock_class_key *key)
{
	init_timer_key(timer, func, flags, name, key);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
destroy_timer_on_stack(struct timer_list *timer)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
timer_pending(const struct timer_list *timer)
{
	return !hlist_unhashed_lockless(&timer->entry);
}

extern void add_timer_on(struct timer_list *timer, int cpu);
extern int mod_timer(struct timer_list *timer, unsigned long expires);
extern int mod_timer_pending(struct timer_list *timer, unsigned long expires);
extern int timer_reduce(struct timer_list *timer, unsigned long expires);

extern void add_timer(struct timer_list *timer);
extern void add_timer_local(struct timer_list *timer);
extern void add_timer_global(struct timer_list *timer);

extern int try_to_del_timer_sync(struct timer_list *timer);
extern int timer_delete_sync(struct timer_list *timer);
extern int timer_delete(struct timer_list *timer);
extern int timer_shutdown_sync(struct timer_list *timer);
extern int timer_shutdown(struct timer_list *timer);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
del_timer_sync(struct timer_list *timer)
{
	return timer_delete_sync(timer);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
del_timer(struct timer_list *timer)
{
	return timer_delete(timer);
}

extern void init_timers(void);
struct hrtimer;
extern enum hrtimer_restart it_real_fn(struct hrtimer *);

unsigned long __round_jiffies(unsigned long j, int cpu);
unsigned long __round_jiffies_relative(unsigned long j, int cpu);
unsigned long round_jiffies(unsigned long j);
unsigned long round_jiffies_relative(unsigned long j);

unsigned long __round_jiffies_up(unsigned long j, int cpu);
unsigned long __round_jiffies_up_relative(unsigned long j, int cpu);
unsigned long round_jiffies_up(unsigned long j);
unsigned long round_jiffies_up_relative(unsigned long j);

int timers_prepare_cpu(unsigned int cpu);
int timers_dead_cpu(unsigned int cpu);

struct workqueue_struct;

struct work_struct;
typedef void (*work_func_t)(struct work_struct *work);
void delayed_work_timer_fn(struct timer_list *t);

struct work_struct {
	atomic_long_t data;
	struct list_head entry;
	work_func_t func;
};

enum work_bits {
	WORK_STRUCT_PENDING_BIT = 0,
	WORK_STRUCT_INACTIVE_BIT,
	WORK_STRUCT_PWQ_BIT,
	WORK_STRUCT_LINKED_BIT,

	WORK_STRUCT_FLAG_BITS,

	WORK_STRUCT_COLOR_SHIFT = WORK_STRUCT_FLAG_BITS,
	WORK_STRUCT_COLOR_BITS = 4,
	WORK_STRUCT_PWQ_SHIFT =
		WORK_STRUCT_COLOR_SHIFT + WORK_STRUCT_COLOR_BITS,
	WORK_OFFQ_FLAG_SHIFT = WORK_STRUCT_FLAG_BITS,
	WORK_OFFQ_BH_BIT = WORK_OFFQ_FLAG_SHIFT,
	WORK_OFFQ_FLAG_END,
	WORK_OFFQ_FLAG_BITS = WORK_OFFQ_FLAG_END - WORK_OFFQ_FLAG_SHIFT,

	WORK_OFFQ_DISABLE_SHIFT = WORK_OFFQ_FLAG_SHIFT + WORK_OFFQ_FLAG_BITS,
	WORK_OFFQ_DISABLE_BITS = 16,

	WORK_OFFQ_POOL_SHIFT = WORK_OFFQ_DISABLE_SHIFT + WORK_OFFQ_DISABLE_BITS,
	WORK_OFFQ_LEFT = 64 - WORK_OFFQ_POOL_SHIFT,
	WORK_OFFQ_POOL_BITS = WORK_OFFQ_LEFT <= 31 ? WORK_OFFQ_LEFT : 31,
};

enum work_flags {
	WORK_STRUCT_PENDING = 1 << WORK_STRUCT_PENDING_BIT,
	WORK_STRUCT_INACTIVE = 1 << WORK_STRUCT_INACTIVE_BIT,
	WORK_STRUCT_PWQ = 1 << WORK_STRUCT_PWQ_BIT,
	WORK_STRUCT_LINKED = 1 << WORK_STRUCT_LINKED_BIT,

	WORK_STRUCT_STATIC = 0,

};

enum wq_misc_consts {
	WORK_NR_COLORS = (1 << WORK_STRUCT_COLOR_BITS),

	WORK_CPU_UNBOUND = 64,

	WORK_BUSY_PENDING = 1 << 0,
	WORK_BUSY_RUNNING = 1 << 1,

	WORKER_DESC_LEN = 32,
};
struct delayed_work {
	struct work_struct work;
	struct timer_list timer;

	struct workqueue_struct *wq;
	int cpu;
};

struct rcu_work {
	struct work_struct work;
	struct callback_head rcu;

	struct workqueue_struct *wq;
};

enum wq_affn_scope {
	WQ_AFFN_DFL,
	WQ_AFFN_CPU,
	WQ_AFFN_SMT,
	WQ_AFFN_CACHE,
	WQ_AFFN_NUMA,
	WQ_AFFN_SYSTEM,

	WQ_AFFN_NR_TYPES,
};

struct workqueue_attrs {
	int nice;
	cpumask_var_t cpumask;
	cpumask_var_t __pod_cpumask;
	bool affn_strict;
	enum wq_affn_scope affn_scope;

	bool ordered;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct delayed_work *
to_delayed_work(struct work_struct *work)
{
	return ({
		void *__mptr = (void *)(work);
		_Static_assert(
			__builtin_types_compatible_p(
				typeof(*(work)),
				typeof(((struct delayed_work *)0)->work)) ||
				__builtin_types_compatible_p(typeof(*(work)),
							     typeof(void)),
			"pointer type mismatch in container_of()");
		((struct delayed_work *)(__mptr -
					 __builtin_offsetof(struct delayed_work,
							    work)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct rcu_work *
to_rcu_work(struct work_struct *work)
{
	return ({
		void *__mptr = (void *)(work);
		_Static_assert(__builtin_types_compatible_p(
				       typeof(*(work)),
				       typeof(((struct rcu_work *)0)->work)) ||
				       __builtin_types_compatible_p(
					       typeof(*(work)), typeof(void)),
			       "pointer type mismatch in container_of()");
		((struct rcu_work *)(__mptr - __builtin_offsetof(
						      struct rcu_work, work)));
	});
}

struct execute_work {
	struct work_struct work;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__init_work(struct work_struct *work, int onstack)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
destroy_work_on_stack(struct work_struct *work)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
destroy_delayed_work_on_stack(struct delayed_work *work)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
work_static(struct work_struct *work)
{
	return 0;
}
enum wq_flags {
	WQ_BH = 1 << 0,
	WQ_UNBOUND = 1 << 1,
	WQ_FREEZABLE = 1 << 2,
	WQ_MEM_RECLAIM = 1 << 3,
	WQ_HIGHPRI = 1 << 4,
	WQ_CPU_INTENSIVE = 1 << 5,
	WQ_SYSFS = 1 << 6,
	WQ_POWER_EFFICIENT = 1 << 7,

	__WQ_DESTROYING = 1 << 15,
	__WQ_DRAINING = 1 << 16,
	__WQ_ORDERED = 1 << 17,
	__WQ_LEGACY = 1 << 18,

	__WQ_BH_ALLOWS = WQ_BH | WQ_HIGHPRI,
};

enum wq_consts {
	WQ_MAX_ACTIVE = 512,
	WQ_UNBOUND_MAX_ACTIVE = WQ_MAX_ACTIVE,
	WQ_DFL_ACTIVE = WQ_MAX_ACTIVE / 2,

	WQ_DFL_MIN_ACTIVE = 8,
};
extern struct workqueue_struct *system_wq;
extern struct workqueue_struct *system_highpri_wq;
extern struct workqueue_struct *system_long_wq;
extern struct workqueue_struct *system_unbound_wq;
extern struct workqueue_struct *system_freezable_wq;
extern struct workqueue_struct *system_power_efficient_wq;
extern struct workqueue_struct *system_freezable_power_efficient_wq;
extern struct workqueue_struct *system_bh_wq;
extern struct workqueue_struct *system_bh_highpri_wq;

void workqueue_softirq_action(bool highpri);
void workqueue_softirq_dead(unsigned int cpu);
__attribute__((__format__(printf, 1, 4))) struct workqueue_struct *
alloc_workqueue(const char *fmt, unsigned int flags, int max_active, ...);
extern void destroy_workqueue(struct workqueue_struct *wq);

struct workqueue_attrs *alloc_workqueue_attrs(void);
void free_workqueue_attrs(struct workqueue_attrs *attrs);
int apply_workqueue_attrs(struct workqueue_struct *wq,
			  const struct workqueue_attrs *attrs);
extern int workqueue_unbound_exclude_cpumask(cpumask_var_t cpumask);

extern bool queue_work_on(int cpu, struct workqueue_struct *wq,
			  struct work_struct *work);
extern bool queue_work_node(int node, struct workqueue_struct *wq,
			    struct work_struct *work);
extern bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
				  struct delayed_work *work,
				  unsigned long delay);
extern bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,
				struct delayed_work *dwork,
				unsigned long delay);
extern bool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork);

extern void __flush_workqueue(struct workqueue_struct *wq);
extern void drain_workqueue(struct workqueue_struct *wq);

extern int schedule_on_each_cpu(work_func_t func);

int execute_in_process_context(work_func_t fn, struct execute_work *);

extern bool flush_work(struct work_struct *work);
extern bool cancel_work(struct work_struct *work);
extern bool cancel_work_sync(struct work_struct *work);

extern bool flush_delayed_work(struct delayed_work *dwork);
extern bool cancel_delayed_work(struct delayed_work *dwork);
extern bool cancel_delayed_work_sync(struct delayed_work *dwork);

extern bool disable_work(struct work_struct *work);
extern bool disable_work_sync(struct work_struct *work);
extern bool enable_work(struct work_struct *work);

extern bool disable_delayed_work(struct delayed_work *dwork);
extern bool disable_delayed_work_sync(struct delayed_work *dwork);
extern bool enable_delayed_work(struct delayed_work *dwork);

extern bool flush_rcu_work(struct rcu_work *rwork);

extern void workqueue_set_max_active(struct workqueue_struct *wq,
				     int max_active);
extern void workqueue_set_min_active(struct workqueue_struct *wq,
				     int min_active);
extern struct work_struct *current_work(void);
extern bool current_is_workqueue_rescuer(void);
extern bool workqueue_congested(int cpu, struct workqueue_struct *wq);
extern unsigned int work_busy(struct work_struct *work);
extern __attribute__((__format__(printf, 1, 2))) void
set_worker_desc(const char *fmt, ...);
extern void print_worker_info(const char *log_lvl, struct task_struct *task);
extern void show_all_workqueues(void);
extern void show_freezable_workqueues(void);
extern void show_one_workqueue(struct workqueue_struct *wq);
extern void wq_worker_comm(char *buf, size_t size, struct task_struct *task);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
queue_work(struct workqueue_struct *wq, struct work_struct *work)
{
	return queue_work_on(WORK_CPU_UNBOUND, wq, work);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
queue_delayed_work(struct workqueue_struct *wq, struct delayed_work *dwork,
		   unsigned long delay)
{
	return queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mod_delayed_work(struct workqueue_struct *wq, struct delayed_work *dwork,
		 unsigned long delay)
{
	return mod_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
schedule_work_on(int cpu, struct work_struct *work)
{
	return queue_work_on(cpu, system_wq, work);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
schedule_work(struct work_struct *work)
{
	return queue_work(system_wq, work);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
enable_and_queue_work(struct workqueue_struct *wq, struct work_struct *work)
{
	if (enable_work(work)) {
		queue_work(wq, work);
		return true;
	}
	return false;
}
extern void __warn_flushing_systemwide_wq(void) __attribute__((
	__warning__("Please avoid flushing system-wide workqueues.")));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
schedule_delayed_work_on(int cpu, struct delayed_work *dwork,
			 unsigned long delay)
{
	return queue_delayed_work_on(cpu, system_wq, dwork, delay);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
schedule_delayed_work(struct delayed_work *dwork, unsigned long delay)
{
	return queue_delayed_work(system_wq, dwork, delay);
}
long work_on_cpu_key(int cpu, long (*fn)(void *), void *arg,
		     struct lock_class_key *key);
long work_on_cpu_safe_key(int cpu, long (*fn)(void *), void *arg,
			  struct lock_class_key *key);
extern void freeze_workqueues_begin(void);
extern bool freeze_workqueues_busy(void);
extern void thaw_workqueues(void);

int workqueue_sysfs_register(struct workqueue_struct *wq);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
wq_watchdog_touch(int cpu)
{
}

int workqueue_prepare_cpu(unsigned int cpu);
int workqueue_online_cpu(unsigned int cpu);
int workqueue_offline_cpu(unsigned int cpu);

void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
workqueue_init_early(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
workqueue_init(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
workqueue_init_topology(void);
struct rcu_cblist {
	struct callback_head *head;
	struct callback_head **tail;
	long len;
};
struct rcu_segcblist {
	struct callback_head *head;
	struct callback_head **tails[4];
	unsigned long gp_seq[4];

	long len;

	long seglen[4];
	u8 flags;
};

struct srcu_struct;
int init_srcu_struct(struct srcu_struct *ssp);

struct srcu_node;
struct srcu_struct;

struct srcu_data {
	atomic_long_t srcu_lock_count[2];
	atomic_long_t srcu_unlock_count[2];
	int srcu_nmi_safety;

	spinlock_t lock __attribute__((__aligned__(1 << (6))));
	struct rcu_segcblist srcu_cblist;
	unsigned long srcu_gp_seq_needed;
	unsigned long srcu_gp_seq_needed_exp;
	bool srcu_cblist_invoking;
	struct timer_list delay_work;
	struct work_struct work;
	struct callback_head srcu_barrier_head;
	struct srcu_node *mynode;
	unsigned long grpmask;

	int cpu;
	struct srcu_struct *ssp;
};

struct srcu_node {
	spinlock_t lock;
	unsigned long srcu_have_cbs[4];

	unsigned long srcu_data_have_cbs[4];
	unsigned long srcu_gp_seq_needed_exp;
	struct srcu_node *srcu_parent;
	int grplo;
	int grphi;
};

struct srcu_usage {
	struct srcu_node *node;
	struct srcu_node *level[2 + 1];

	int srcu_size_state;
	struct mutex srcu_cb_mutex;
	spinlock_t lock;
	struct mutex srcu_gp_mutex;
	unsigned long srcu_gp_seq;
	unsigned long srcu_gp_seq_needed;
	unsigned long srcu_gp_seq_needed_exp;
	unsigned long srcu_gp_start;
	unsigned long srcu_last_gp_end;
	unsigned long srcu_size_jiffies;
	unsigned long srcu_n_lock_retries;
	unsigned long srcu_n_exp_nodelay;
	bool sda_is_static;
	unsigned long srcu_barrier_seq;
	struct mutex srcu_barrier_mutex;
	struct completion srcu_barrier_completion;

	atomic_t srcu_barrier_cpu_cnt;

	unsigned long reschedule_jiffies;
	unsigned long reschedule_count;
	struct delayed_work work;
	struct srcu_struct *srcu_ssp;
};

struct srcu_struct {
	unsigned int srcu_idx;
	struct srcu_data *sda;
	struct lockdep_map dep_map;
	struct srcu_usage *srcu_sup;
};
void synchronize_srcu_expedited(struct srcu_struct *ssp);
void srcu_barrier(struct srcu_struct *ssp);
void srcu_torture_stats_print(struct srcu_struct *ssp, char *tt, char *tf);

void call_srcu(struct srcu_struct *ssp, struct callback_head *head,
	       void (*func)(struct callback_head *head));
void cleanup_srcu_struct(struct srcu_struct *ssp);
int __srcu_read_lock(struct srcu_struct *ssp);
void __srcu_read_unlock(struct srcu_struct *ssp, int idx);
void synchronize_srcu(struct srcu_struct *ssp);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
get_completed_synchronize_srcu(void)
{
	return 0x1;
}

unsigned long get_state_synchronize_srcu(struct srcu_struct *ssp);
unsigned long start_poll_synchronize_srcu(struct srcu_struct *ssp);
bool poll_state_synchronize_srcu(struct srcu_struct *ssp, unsigned long cookie);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
same_state_synchronize_srcu(unsigned long oldstate1, unsigned long oldstate2)
{
	return oldstate1 == oldstate2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__srcu_read_lock_nmisafe(struct srcu_struct *ssp)
{
	return __srcu_read_lock(ssp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__srcu_read_unlock_nmisafe(struct srcu_struct *ssp, int idx)
{
	__srcu_read_unlock(ssp, idx);
}

void srcu_init(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
srcu_read_lock_held(const struct srcu_struct *ssp)
{
	return 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
srcu_check_nmi_safety(struct srcu_struct *ssp, bool nmi_safe)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
srcu_read_lock(struct srcu_struct *ssp)
{
	int retval;

	srcu_check_nmi_safety(ssp, false);
	retval = __srcu_read_lock(ssp);
	do {
	} while (0);
	return retval;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
srcu_read_lock_nmisafe(struct srcu_struct *ssp)
{
	int retval;

	srcu_check_nmi_safety(ssp, true);
	retval = __srcu_read_lock_nmisafe(ssp);
	do {
	} while (0);
	return retval;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((no_instrument_function)) int
srcu_read_lock_notrace(struct srcu_struct *ssp)
{
	int retval;

	srcu_check_nmi_safety(ssp, false);
	retval = __srcu_read_lock(ssp);
	return retval;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
srcu_down_read(struct srcu_struct *ssp)
{
	({
		int __ret_warn_on =
			!!(((preempt_count() &
			     (((1UL << (4)) - 1) << (((0 + 8) + 8) + 4)))));
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) |
						      ((1 << 1) | ((9) << 8));
				({
					asm volatile(
						"330"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"330"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(330));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/srcu.h"),
						  "i"(305), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"331"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"331"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(331));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	srcu_check_nmi_safety(ssp, false);
	return __srcu_read_lock(ssp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
srcu_read_unlock(struct srcu_struct *ssp, int idx)

{
	({
		int __ret_warn_on = !!(idx & ~0x1);
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) |
						      ((1 << 1) | ((9) << 8));
				({
					asm volatile(
						"332"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"332"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(332));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/srcu.h"),
						  "i"(320), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"333"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"333"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(333));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	srcu_check_nmi_safety(ssp, false);
	do {
	} while (0);
	__srcu_read_unlock(ssp, idx);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
srcu_read_unlock_nmisafe(struct srcu_struct *ssp, int idx)

{
	({
		int __ret_warn_on = !!(idx & ~0x1);
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) |
						      ((1 << 1) | ((9) << 8));
				({
					asm volatile(
						"334"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"334"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(334));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/srcu.h"),
						  "i"(336), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"335"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"335"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(335));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	srcu_check_nmi_safety(ssp, true);
	do {
	} while (0);
	__srcu_read_unlock_nmisafe(ssp, idx);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((no_instrument_function)) void
srcu_read_unlock_notrace(struct srcu_struct *ssp, int idx)
{
	srcu_check_nmi_safety(ssp, false);
	__srcu_read_unlock(ssp, idx);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
srcu_up_read(struct srcu_struct *ssp, int idx)

{
	({
		int __ret_warn_on = !!(idx & ~0x1);
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) |
						      ((1 << 1) | ((9) << 8));
				({
					asm volatile(
						"336"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"336"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(336));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/srcu.h"),
						  "i"(361), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"337"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"337"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(337));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	({
		int __ret_warn_on =
			!!(((preempt_count() &
			     (((1UL << (4)) - 1) << (((0 + 8) + 8) + 4)))));
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) |
						      ((1 << 1) | ((9) << 8));
				({
					asm volatile(
						"338"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"338"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(338));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/srcu.h"),
						  "i"(362), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"339"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"339"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(339));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	srcu_check_nmi_safety(ssp, false);
	__srcu_read_unlock(ssp, idx);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
smp_mb__after_srcu_read_unlock(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
smp_mb__after_srcu_read_lock(void)
{
}

typedef struct {
	struct srcu_struct *lock;
	int idx;
} class_srcu_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_srcu_destructor(class_srcu_t *_T)
{
	if (_T->lock) {
		srcu_read_unlock(_T->lock, _T->idx);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_srcu_lock_ptr(class_srcu_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_srcu_t
class_srcu_constructor(struct srcu_struct *l)
{
	class_srcu_t _t = { .lock = l }, *_T = &_t;
	_T->idx = srcu_read_lock(_T->lock);
	return _t;
}
struct notifier_block;

typedef int (*notifier_fn_t)(struct notifier_block *nb, unsigned long action,
			     void *data);

struct notifier_block {
	notifier_fn_t notifier_call;
	struct notifier_block *next;
	int priority;
};

struct atomic_notifier_head {
	spinlock_t lock;
	struct notifier_block *head;
};

struct blocking_notifier_head {
	struct rw_semaphore rwsem;
	struct notifier_block *head;
};

struct raw_notifier_head {
	struct notifier_block *head;
};

struct srcu_notifier_head {
	struct mutex mutex;
	struct srcu_usage srcuu;
	struct srcu_struct srcu;
	struct notifier_block *head;
};
extern void srcu_init_notifier_head(struct srcu_notifier_head *nh);
extern int atomic_notifier_chain_register(struct atomic_notifier_head *nh,
					  struct notifier_block *nb);
extern int blocking_notifier_chain_register(struct blocking_notifier_head *nh,
					    struct notifier_block *nb);
extern int raw_notifier_chain_register(struct raw_notifier_head *nh,
				       struct notifier_block *nb);
extern int srcu_notifier_chain_register(struct srcu_notifier_head *nh,
					struct notifier_block *nb);

extern int
atomic_notifier_chain_register_unique_prio(struct atomic_notifier_head *nh,
					   struct notifier_block *nb);
extern int
blocking_notifier_chain_register_unique_prio(struct blocking_notifier_head *nh,
					     struct notifier_block *nb);

extern int atomic_notifier_chain_unregister(struct atomic_notifier_head *nh,
					    struct notifier_block *nb);
extern int blocking_notifier_chain_unregister(struct blocking_notifier_head *nh,
					      struct notifier_block *nb);
extern int raw_notifier_chain_unregister(struct raw_notifier_head *nh,
					 struct notifier_block *nb);
extern int srcu_notifier_chain_unregister(struct srcu_notifier_head *nh,
					  struct notifier_block *nb);

extern int atomic_notifier_call_chain(struct atomic_notifier_head *nh,
				      unsigned long val, void *v);
extern int blocking_notifier_call_chain(struct blocking_notifier_head *nh,
					unsigned long val, void *v);
extern int raw_notifier_call_chain(struct raw_notifier_head *nh,
				   unsigned long val, void *v);
extern int srcu_notifier_call_chain(struct srcu_notifier_head *nh,
				    unsigned long val, void *v);

extern int
blocking_notifier_call_chain_robust(struct blocking_notifier_head *nh,
				    unsigned long val_up,
				    unsigned long val_down, void *v);
extern int raw_notifier_call_chain_robust(struct raw_notifier_head *nh,
					  unsigned long val_up,
					  unsigned long val_down, void *v);

extern bool
atomic_notifier_call_chain_is_empty(struct atomic_notifier_head *nh);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
notifier_from_errno(int err)
{
	if (err)
		return 0x8000 | (0x0001 - err);

	return 0x0001;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
notifier_to_errno(int ret)
{
	ret &= ~0x8000;
	return ret > 0x0001 ? 0x0001 - ret : 0;
}
extern struct blocking_notifier_head reboot_notifier_list;

typedef u8 uprobe_opcode_t;

struct uprobe_xol_ops;

struct arch_uprobe {
	union {
		u8 insn[16];
		u8 ixol[16];
	};

	const struct uprobe_xol_ops *ops;

	union {
		struct {
			s32 offs;
			u8 ilen;
			u8 opc1;
		} branch;
		struct {
			u8 fixups;
			u8 ilen;
		} defparam;
		struct {
			u8 reg_offset;
			u8 ilen;
		} push;
	};
};

struct arch_uprobe_task {
	unsigned long saved_scratch_register;

	unsigned int saved_trap_nr;
	unsigned int saved_tf;
};

enum uprobe_task_state {
	UTASK_RUNNING,
	UTASK_SSTEP,
	UTASK_SSTEP_ACK,
	UTASK_SSTEP_TRAPPED,
};

struct uprobe_task {
	enum uprobe_task_state state;

	union {
		struct {
			struct arch_uprobe_task autask;
			unsigned long vaddr;
		};

		struct {
			struct callback_head dup_xol_work;
			unsigned long dup_xol_addr;
		};
	};

	struct uprobe *active_uprobe;
	unsigned long xol_vaddr;

	struct arch_uprobe *auprobe;

	struct return_instance *return_instances;
	unsigned int depth;
};

struct return_instance {
	struct uprobe *uprobe;
	unsigned long func;
	unsigned long stack;
	unsigned long orig_ret_vaddr;
	bool chained;

	struct return_instance *next;
};

enum rp_check {
	RP_CHECK_CALL,
	RP_CHECK_CHAIN_CALL,
	RP_CHECK_RET,
};

struct xol_area;

struct uprobes_state {
	struct xol_area *xol_area;
};

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
uprobes_init(void);
extern int set_swbp(struct arch_uprobe *aup, struct mm_struct *mm,
		    unsigned long vaddr);
extern int set_orig_insn(struct arch_uprobe *aup, struct mm_struct *mm,
			 unsigned long vaddr);
extern bool is_swbp_insn(uprobe_opcode_t *insn);
extern bool is_trap_insn(uprobe_opcode_t *insn);
extern unsigned long uprobe_get_swbp_addr(struct pt_regs *regs);
extern unsigned long uprobe_get_trap_addr(struct pt_regs *regs);
extern int uprobe_write_opcode(struct arch_uprobe *auprobe,
			       struct mm_struct *mm, unsigned long vaddr,
			       uprobe_opcode_t);
extern struct uprobe *uprobe_register(struct inode *inode, loff_t offset,
				      loff_t ref_ctr_offset,
				      struct uprobe_consumer *uc);
extern int uprobe_apply(struct uprobe *uprobe, struct uprobe_consumer *uc,
			bool);
extern void uprobe_unregister_nosync(struct uprobe *uprobe,
				     struct uprobe_consumer *uc);
extern void uprobe_unregister_sync(void);
extern int uprobe_mmap(struct vm_area_struct *vma);
extern void uprobe_munmap(struct vm_area_struct *vma, unsigned long start,
			  unsigned long end);
extern void uprobe_start_dup_mmap(void);
extern void uprobe_end_dup_mmap(void);
extern void uprobe_dup_mmap(struct mm_struct *oldmm, struct mm_struct *newmm);
extern void uprobe_free_utask(struct task_struct *t);
extern void uprobe_copy_process(struct task_struct *t, unsigned long flags);
extern int uprobe_post_sstep_notifier(struct pt_regs *regs);
extern int uprobe_pre_sstep_notifier(struct pt_regs *regs);
extern void uprobe_notify_resume(struct pt_regs *regs);
extern bool uprobe_deny_signal(void);
extern bool arch_uprobe_skip_sstep(struct arch_uprobe *aup,
				   struct pt_regs *regs);
extern void uprobe_clear_state(struct mm_struct *mm);
extern int arch_uprobe_analyze_insn(struct arch_uprobe *aup,
				    struct mm_struct *mm, unsigned long addr);
extern int arch_uprobe_pre_xol(struct arch_uprobe *aup, struct pt_regs *regs);
extern int arch_uprobe_post_xol(struct arch_uprobe *aup, struct pt_regs *regs);
extern bool arch_uprobe_xol_was_trapped(struct task_struct *tsk);
extern int arch_uprobe_exception_notify(struct notifier_block *self,
					unsigned long val, void *data);
extern void arch_uprobe_abort_xol(struct arch_uprobe *aup,
				  struct pt_regs *regs);
extern unsigned long
arch_uretprobe_hijack_return_addr(unsigned long trampoline_vaddr,
				  struct pt_regs *regs);
extern bool arch_uretprobe_is_alive(struct return_instance *ret,
				    enum rp_check ctx, struct pt_regs *regs);
extern bool arch_uprobe_ignore(struct arch_uprobe *aup, struct pt_regs *regs);
extern void arch_uprobe_copy_ixol(struct page *page, unsigned long vaddr,
				  void *src, unsigned long len);
extern void uprobe_handle_trampoline(struct pt_regs *regs);
extern void *arch_uprobe_trampoline(unsigned long *psize);
extern unsigned long uprobe_get_trampoline_vaddr(void);

struct percpu_counter {
	raw_spinlock_t lock;
	s64 count;

	struct list_head list;

	s32 *counters;
};

extern int percpu_counter_batch;

int __percpu_counter_init_many(struct percpu_counter *fbc, s64 amount,
			       gfp_t gfp, u32 nr_counters,
			       struct lock_class_key *key);
void percpu_counter_destroy_many(struct percpu_counter *fbc, u32 nr_counters);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_counter_destroy(struct percpu_counter *fbc)
{
	percpu_counter_destroy_many(fbc, 1);
}

void percpu_counter_set(struct percpu_counter *fbc, s64 amount);
void percpu_counter_add_batch(struct percpu_counter *fbc, s64 amount,
			      s32 batch);
s64 __percpu_counter_sum(struct percpu_counter *fbc);
int __percpu_counter_compare(struct percpu_counter *fbc, s64 rhs, s32 batch);
bool __percpu_counter_limited_add(struct percpu_counter *fbc, s64 limit,
				  s64 amount, s32 batch);
void percpu_counter_sync(struct percpu_counter *fbc);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
percpu_counter_compare(struct percpu_counter *fbc, s64 rhs)
{
	return __percpu_counter_compare(fbc, rhs, percpu_counter_batch);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_counter_add(struct percpu_counter *fbc, s64 amount)
{
	percpu_counter_add_batch(fbc, amount, percpu_counter_batch);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
percpu_counter_limited_add(struct percpu_counter *fbc, s64 limit, s64 amount)
{
	return __percpu_counter_limited_add(fbc, limit, amount,
					    percpu_counter_batch);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_counter_add_local(struct percpu_counter *fbc, s64 amount)
{
	percpu_counter_add_batch(fbc, amount, ((int)(~0U >> 1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
percpu_counter_sum_positive(struct percpu_counter *fbc)
{
	s64 ret = __percpu_counter_sum(fbc);
	return ret < 0 ? 0 : ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
percpu_counter_sum(struct percpu_counter *fbc)
{
	return __percpu_counter_sum(fbc);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
percpu_counter_read(struct percpu_counter *fbc)
{
	return fbc->count;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
percpu_counter_read_positive(struct percpu_counter *fbc)
{
	s64 ret = ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_340(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(fbc->count) == sizeof(char) ||
			       sizeof(fbc->count) == sizeof(short) ||
			       sizeof(fbc->count) == sizeof(int) ||
			       sizeof(fbc->count) == sizeof(long)) ||
			      sizeof(fbc->count) == sizeof(long long)))
				__compiletime_assert_340();
		} while (0);
		(*(const volatile typeof(_Generic((fbc->count),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (fbc->count)))
			   *)&(fbc->count));
	});

	if (ret >= 0)
		return ret;
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
percpu_counter_initialized(struct percpu_counter *fbc)
{
	return (fbc->counters != ((void *)0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_counter_inc(struct percpu_counter *fbc)
{
	percpu_counter_add(fbc, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_counter_dec(struct percpu_counter *fbc)
{
	percpu_counter_add(fbc, -1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_counter_sub(struct percpu_counter *fbc, s64 amount)
{
	percpu_counter_add(fbc, -amount);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_counter_sub_local(struct percpu_counter *fbc, s64 amount)
{
	percpu_counter_add_local(fbc, -amount);
}

typedef struct {
	u64 ctx_id;
	atomic64_t tlb_gen;

	struct rw_semaphore ldt_usr_sem;
	struct ldt_struct *ldt;

	unsigned long flags;
	struct mutex lock;
	void *vdso;
	const struct vdso_image *vdso_image;

	atomic_t perf_rdpmc_allowed;

	u16 pkey_allocation_map;
	s16 execute_only_pkey;

} mm_context_t;

void leave_mm(void);

struct address_space;
struct mem_cgroup;
struct page {
	unsigned long flags;

	union {
		struct {
			union {
				struct list_head lru;

				struct {
					void *__filler;

					unsigned int mlock_count;
				};

				struct list_head buddy_list;
				struct list_head pcp_list;
			};

			struct address_space *mapping;
			union {
				unsigned long index;
				unsigned long share;
			};

			unsigned long private;
		};
		struct {
			unsigned long pp_magic;
			struct page_pool *pp;
			unsigned long _pp_mapping_pad;
			unsigned long dma_addr;
			atomic_long_t pp_ref_count;
		};
		struct {
			unsigned long compound_head;
		};
		struct {
			struct dev_pagemap *pgmap;
			void *zone_device_data;
		};

		struct callback_head callback_head;
	};

	union {
		unsigned int page_type;
		atomic_t _mapcount;
	};

	atomic_t _refcount;
} __attribute__((__aligned__(2 * sizeof(unsigned long))));
struct encoded_page;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct encoded_page *
encode_page(struct page *page, unsigned long flags)
{
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_341(void)
			__attribute__((__error__("BUILD_BUG_ON failed: "
						 "flags > ENCODED_PAGE_BITS")));
		if (!(!(flags > 3ul)))
			__compiletime_assert_341();
	} while (0);
	return (struct encoded_page *)(flags | (unsigned long)page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
encoded_page_flags(struct encoded_page *page)
{
	return 3ul & (unsigned long)page;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct page *
encoded_page_ptr(struct encoded_page *page)
{
	return (struct page *)(~3ul & (unsigned long)page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct encoded_page *
encode_nr_pages(unsigned long nr)
{
	((void)(sizeof((long)((nr << 2) >> 2 != nr))));
	return (struct encoded_page *)(nr << 2);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
encoded_nr_pages(struct encoded_page *page)
{
	return ((unsigned long)page) >> 2;
}

typedef struct {
	unsigned long val;
} swp_entry_t;
struct folio {
	union {
		struct {
			unsigned long flags;
			union {
				struct list_head lru;

				struct {
					void *__filler;

					unsigned int mlock_count;
				};
			};
			struct address_space *mapping;
			unsigned long index;
			union {
				void *private;
				swp_entry_t swap;
			};
			atomic_t _mapcount;
			atomic_t _refcount;
		};
		struct page page;
	};
	union {
		struct {
			unsigned long _flags_1;
			unsigned long _head_1;

			atomic_t _large_mapcount;
			atomic_t _entire_mapcount;
			atomic_t _nr_pages_mapped;
			atomic_t _pincount;

			unsigned int _folio_nr_pages;
		};
		struct page __page_1;
	};
	union {
		struct {
			unsigned long _flags_2;
			unsigned long _head_2;

			void *_hugetlb_subpool;
			void *_hugetlb_cgroup;
			void *_hugetlb_cgroup_rsvd;
			void *_hugetlb_hwpoison;
		};
		struct {
			unsigned long _flags_2a;
			unsigned long _head_2a;

			struct list_head _deferred_list;
		};
		struct page __page_2;
	};
};

_Static_assert(__builtin_offsetof(struct page, flags) ==
		       __builtin_offsetof(struct folio, flags),
	       "offsetof(struct page, flags) == offsetof(struct folio, flags)");
_Static_assert(__builtin_offsetof(struct page, lru) ==
		       __builtin_offsetof(struct folio, lru),
	       "offsetof(struct page, lru) == offsetof(struct folio, lru)");
_Static_assert(
	__builtin_offsetof(struct page, mapping) ==
		__builtin_offsetof(struct folio, mapping),
	"offsetof(struct page, mapping) == offsetof(struct folio, mapping)");
_Static_assert(
	__builtin_offsetof(struct page, compound_head) ==
		__builtin_offsetof(struct folio, lru),
	"offsetof(struct page, compound_head) == offsetof(struct folio, lru)");
_Static_assert(__builtin_offsetof(struct page, index) ==
		       __builtin_offsetof(struct folio, index),
	       "offsetof(struct page, index) == offsetof(struct folio, index)");
_Static_assert(
	__builtin_offsetof(struct page, private) ==
		__builtin_offsetof(struct folio, private),
	"offsetof(struct page, private) == offsetof(struct folio, private)");
_Static_assert(
	__builtin_offsetof(struct page, _mapcount) ==
		__builtin_offsetof(struct folio, _mapcount),
	"offsetof(struct page, _mapcount) == offsetof(struct folio, _mapcount)");
_Static_assert(
	__builtin_offsetof(struct page, _refcount) ==
		__builtin_offsetof(struct folio, _refcount),
	"offsetof(struct page, _refcount) == offsetof(struct folio, _refcount)");
_Static_assert(
	__builtin_offsetof(struct folio, _flags_1) ==
		__builtin_offsetof(struct page, flags) + sizeof(struct page),
	"offsetof(struct folio, _flags_1) == offsetof(struct page, flags) + sizeof(struct page)");
_Static_assert(
	__builtin_offsetof(struct folio, _head_1) ==
		__builtin_offsetof(struct page, compound_head) +
			sizeof(struct page),
	"offsetof(struct folio, _head_1) == offsetof(struct page, compound_head) + sizeof(struct page)");

_Static_assert(
	__builtin_offsetof(struct folio, _flags_2) ==
		__builtin_offsetof(struct page, flags) +
			2 * sizeof(struct page),
	"offsetof(struct folio, _flags_2) == offsetof(struct page, flags) + 2 * sizeof(struct page)");
_Static_assert(
	__builtin_offsetof(struct folio, _head_2) ==
		__builtin_offsetof(struct page, compound_head) +
			2 * sizeof(struct page),
	"offsetof(struct folio, _head_2) == offsetof(struct page, compound_head) + 2 * sizeof(struct page)");
_Static_assert(
	__builtin_offsetof(struct folio, _flags_2a) ==
		__builtin_offsetof(struct page, flags) +
			2 * sizeof(struct page),
	"offsetof(struct folio, _flags_2a) == offsetof(struct page, flags) + 2 * sizeof(struct page)");
_Static_assert(
	__builtin_offsetof(struct folio, _head_2a) ==
		__builtin_offsetof(struct page, compound_head) +
			2 * sizeof(struct page),
	"offsetof(struct folio, _head_2a) == offsetof(struct page, compound_head) + 2 * sizeof(struct page)");
struct ptdesc {
	unsigned long __page_flags;

	union {
		struct callback_head pt_rcu_head;
		struct list_head pt_list;
		struct {
			unsigned long _pt_pad_1;
			pgtable_t pmd_huge_pte;
		};
	};
	unsigned long __page_mapping;

	union {
		unsigned long pt_index;
		struct mm_struct *pt_mm;
		atomic_t pt_frag_refcount;
	};

	union {
		unsigned long _pt_pad_2;

		spinlock_t ptl;
	};
	unsigned int __page_type;
	atomic_t __page_refcount;
};

_Static_assert(
	__builtin_offsetof(struct page, flags) ==
		__builtin_offsetof(struct ptdesc, __page_flags),
	"offsetof(struct page, flags) == offsetof(struct ptdesc, __page_flags)");
_Static_assert(
	__builtin_offsetof(struct page, compound_head) ==
		__builtin_offsetof(struct ptdesc, pt_list),
	"offsetof(struct page, compound_head) == offsetof(struct ptdesc, pt_list)");
_Static_assert(
	__builtin_offsetof(struct page, compound_head) ==
		__builtin_offsetof(struct ptdesc, _pt_pad_1),
	"offsetof(struct page, compound_head) == offsetof(struct ptdesc, _pt_pad_1)");
_Static_assert(
	__builtin_offsetof(struct page, mapping) ==
		__builtin_offsetof(struct ptdesc, __page_mapping),
	"offsetof(struct page, mapping) == offsetof(struct ptdesc, __page_mapping)");
_Static_assert(
	__builtin_offsetof(struct page, index) ==
		__builtin_offsetof(struct ptdesc, pt_index),
	"offsetof(struct page, index) == offsetof(struct ptdesc, pt_index)");
_Static_assert(
	__builtin_offsetof(struct page, callback_head) ==
		__builtin_offsetof(struct ptdesc, pt_rcu_head),
	"offsetof(struct page, callback_head) == offsetof(struct ptdesc, pt_rcu_head)");
_Static_assert(
	__builtin_offsetof(struct page, page_type) ==
		__builtin_offsetof(struct ptdesc, __page_type),
	"offsetof(struct page, page_type) == offsetof(struct ptdesc, __page_type)");
_Static_assert(
	__builtin_offsetof(struct page, _refcount) ==
		__builtin_offsetof(struct ptdesc, __page_refcount),
	"offsetof(struct page, _refcount) == offsetof(struct ptdesc, __page_refcount)");

_Static_assert(sizeof(struct ptdesc) <= sizeof(struct page),
	       "sizeof(struct ptdesc) <= sizeof(struct page)");
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_page_private(struct page *page, unsigned long private)
{
	page->private = private;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
folio_get_private(struct folio *folio)
{
	return folio->private;
}

struct page_frag_cache {
	void *va;

	__u16 offset;
	__u16 size;

	unsigned int pagecnt_bias;
	bool pfmemalloc;
};

typedef unsigned long vm_flags_t;

struct vm_region {
	struct rb_node vm_rb;
	vm_flags_t vm_flags;
	unsigned long vm_start;
	unsigned long vm_end;
	unsigned long vm_top;
	unsigned long vm_pgoff;
	struct file *vm_file;

	int vm_usage;
	bool vm_icache_flushed : 1;
};
struct vm_userfaultfd_ctx {};

struct anon_vma_name {
	struct kref kref;

	char name[];
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct anon_vma_name *
anon_vma_name(struct vm_area_struct *vma)
{
	return ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct anon_vma_name *
anon_vma_name_alloc(const char *name)
{
	return ((void *)0);
}

struct vma_lock {
	struct rw_semaphore lock;
};

struct vma_numab_state {
	unsigned long next_scan;

	unsigned long pids_active_reset;
	unsigned long pids_active[2];

	int start_scan_seq;

	int prev_scan_seq;
};
struct vm_area_struct {
	union {
		struct {
			unsigned long vm_start;
			unsigned long vm_end;
		};

		struct callback_head vm_rcu;
	};

	struct mm_struct *vm_mm;
	pgprot_t vm_page_prot;

	union {
		const vm_flags_t vm_flags;
		vm_flags_t __vm_flags;
	};

	bool detached;
	int vm_lock_seq;

	struct vma_lock *vm_lock;

	struct {
		struct rb_node rb;
		unsigned long rb_subtree_last;
	} shared;

	struct list_head anon_vma_chain;

	struct anon_vma *anon_vma;

	const struct vm_operations_struct *vm_ops;

	unsigned long vm_pgoff;

	struct file *vm_file;
	void *vm_private_data;
	atomic_long_t swap_readahead_info;

	struct mempolicy *vm_policy;

	struct vm_userfaultfd_ctx vm_userfaultfd_ctx;
};
struct mm_cid {
	u64 time;
	int cid;
};

struct kioctx_table;
struct iommu_mm_data;
struct mm_struct {
	struct {
		struct {
			atomic_t mm_count;
		} __attribute__((__aligned__((1 << (6)))));

		struct maple_tree mm_mt;

		unsigned long mmap_base;
		unsigned long mmap_legacy_base;

		unsigned long mmap_compat_base;
		unsigned long mmap_compat_legacy_base;

		unsigned long task_size;
		pgd_t *pgd;
		atomic_t membarrier_state;
		atomic_t mm_users;
		struct mm_cid *pcpu_cid;

		unsigned long mm_cid_next_scan;

		atomic_long_t pgtables_bytes;

		int map_count;

		spinlock_t page_table_lock;
		struct rw_semaphore mmap_lock;

		struct list_head mmlist;
		int mm_lock_seq;

		unsigned long hiwater_rss;
		unsigned long hiwater_vm;

		unsigned long total_vm;
		unsigned long locked_vm;
		atomic64_t pinned_vm;
		unsigned long data_vm;
		unsigned long exec_vm;
		unsigned long stack_vm;
		unsigned long def_flags;

		seqcount_t write_protect_seq;

		spinlock_t arg_lock;

		unsigned long start_code, end_code, start_data, end_data;
		unsigned long start_brk, brk, start_stack;
		unsigned long arg_start, arg_end, env_start, env_end;

		unsigned long saved_auxv[(2 * (3 + 22 + 1))];

		struct percpu_counter rss_stat[NR_MM_COUNTERS];

		struct linux_binfmt *binfmt;

		mm_context_t context;

		unsigned long flags;

		spinlock_t ioctx_lock;
		struct kioctx_table *ioctx_table;
		struct user_namespace *user_ns;

		struct file *exe_file;

		struct mmu_notifier_subscriptions *notifier_subscriptions;
		atomic_t tlb_flush_pending;

		atomic_t tlb_flush_batched;

		struct uprobes_state uprobes_state;

		atomic_long_t hugetlb_usage;

		struct work_struct async_put_work;

		struct iommu_mm_data *iommu_mm;
	};

	unsigned long cpu_bitmap[];
};

extern struct mm_struct init_mm;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mm_init_cpumask(struct mm_struct *mm)
{
	unsigned long cpu_bitmap = (unsigned long)mm;

	cpu_bitmap += __builtin_offsetof(struct mm_struct, cpu_bitmap);
	cpumask_clear((struct cpumask *)cpu_bitmap);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) cpumask_t *
mm_cpumask(struct mm_struct *mm)
{
	return (struct cpumask *)&mm->cpu_bitmap;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_add_mm(struct mm_struct *mm)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_del_mm(struct mm_struct *mm)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_migrate_mm(struct mm_struct *mm)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_init_mm(struct mm_struct *mm)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_use_mm(struct mm_struct *mm)
{
}

struct vma_iterator {
	struct ma_state mas;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
vma_iter_init(struct vma_iterator *vmi, struct mm_struct *mm,
	      unsigned long addr)
{
	mas_init(&vmi->mas, &mm->mm_mt, addr);
}

enum mm_cid_state {
	MM_CID_UNSET = -1U,
	MM_CID_LAZY_PUT = (1U << 31),
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mm_cid_is_unset(int cid)
{
	return cid == MM_CID_UNSET;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mm_cid_is_lazy_put(int cid)
{
	return !mm_cid_is_unset(cid) && (cid & MM_CID_LAZY_PUT);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mm_cid_is_valid(int cid)
{
	return !(cid & MM_CID_LAZY_PUT);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mm_cid_set_lazy_put(int cid)
{
	return cid | MM_CID_LAZY_PUT;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mm_cid_clear_lazy_put(int cid)
{
	return cid & ~MM_CID_LAZY_PUT;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) cpumask_t *
mm_cidmask(struct mm_struct *mm)
{
	unsigned long cid_bitmap = (unsigned long)mm;

	cid_bitmap += __builtin_offsetof(struct mm_struct, cpu_bitmap);

	cid_bitmap += cpumask_size();
	return (struct cpumask *)cid_bitmap;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mm_init_cid(struct mm_struct *mm)
{
	int i;

	for (((i)) = 0;
	     ((i)) = find_next_bit(
		     (((((const struct cpumask *)&__cpu_possible_mask))->bits)),
		     (((unsigned int)64)), ((i))),
	    ((i)) < (((unsigned int)64));
	     ((i))++) {
		struct mm_cid *pcpu_cid = ({
			do {
				const void *__vpp_verify =
					(typeof((mm->pcpu_cid) + 0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			({
				unsigned long __ptr;
				__ptr = (unsigned long)((typeof(*(
					(mm->pcpu_cid))) *)((mm->pcpu_cid)));
				(typeof((typeof(*((mm->pcpu_cid))) *)((
					mm->pcpu_cid))))(__ptr +
							 (((__per_cpu_offset[(
								 i)]))));
			});
		});

		pcpu_cid->cid = MM_CID_UNSET;
		pcpu_cid->time = 0;
	}
	cpumask_clear(mm_cidmask(mm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mm_alloc_cid_noprof(struct mm_struct *mm)
{
	mm->pcpu_cid = ((typeof(struct mm_cid) *)pcpu_alloc_noprof(
		sizeof(struct mm_cid), __alignof__(struct mm_cid), false,
		(((gfp_t)(((((1UL))) << (___GFP_DIRECT_RECLAIM_BIT)) |
			  ((((1UL))) << (___GFP_KSWAPD_RECLAIM_BIT)))) |
		 ((gfp_t)((((1UL))) << (___GFP_IO_BIT))) |
		 ((gfp_t)((((1UL))) << (___GFP_FS_BIT))))));
	if (!mm->pcpu_cid)
		return -12;
	mm_init_cid(mm);
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mm_destroy_cid(struct mm_struct *mm)
{
	free_percpu(mm->pcpu_cid);
	mm->pcpu_cid = ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
mm_cid_size(void)
{
	return cpumask_size();
}
struct mmu_gather;
extern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm);
extern void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm);
extern void tlb_finish_mmu(struct mmu_gather *tlb);

struct vm_fault;

typedef unsigned int vm_fault_t;
enum vm_fault_reason {
	VM_FAULT_OOM = (vm_fault_t)0x000001,
	VM_FAULT_SIGBUS = (vm_fault_t)0x000002,
	VM_FAULT_MAJOR = (vm_fault_t)0x000004,
	VM_FAULT_HWPOISON = (vm_fault_t)0x000010,
	VM_FAULT_HWPOISON_LARGE = (vm_fault_t)0x000020,
	VM_FAULT_SIGSEGV = (vm_fault_t)0x000040,
	VM_FAULT_NOPAGE = (vm_fault_t)0x000100,
	VM_FAULT_LOCKED = (vm_fault_t)0x000200,
	VM_FAULT_RETRY = (vm_fault_t)0x000400,
	VM_FAULT_FALLBACK = (vm_fault_t)0x000800,
	VM_FAULT_DONE_COW = (vm_fault_t)0x001000,
	VM_FAULT_NEEDDSYNC = (vm_fault_t)0x002000,
	VM_FAULT_COMPLETED = (vm_fault_t)0x004000,
	VM_FAULT_HINDEX_MASK = (vm_fault_t)0x0f0000,
};
struct vm_special_mapping {
	const char *name;

	struct page **pages;

	vm_fault_t (*fault)(const struct vm_special_mapping *sm,
			    struct vm_area_struct *vma, struct vm_fault *vmf);

	int (*mremap)(const struct vm_special_mapping *sm,
		      struct vm_area_struct *new_vma);

	void (*close)(const struct vm_special_mapping *sm,
		      struct vm_area_struct *vma);
};

enum tlb_flush_reason {
	TLB_FLUSH_ON_TASK_SWITCH,
	TLB_REMOTE_SHOOTDOWN,
	TLB_LOCAL_SHOOTDOWN,
	TLB_LOCAL_MM_SHOOTDOWN,
	TLB_REMOTE_SEND_IPI,
	NR_TLB_FLUSH_REASONS,
};
enum fault_flag {
	FAULT_FLAG_WRITE = 1 << 0,
	FAULT_FLAG_MKWRITE = 1 << 1,
	FAULT_FLAG_ALLOW_RETRY = 1 << 2,
	FAULT_FLAG_RETRY_NOWAIT = 1 << 3,
	FAULT_FLAG_KILLABLE = 1 << 4,
	FAULT_FLAG_TRIED = 1 << 5,
	FAULT_FLAG_USER = 1 << 6,
	FAULT_FLAG_REMOTE = 1 << 7,
	FAULT_FLAG_INSTRUCTION = 1 << 8,
	FAULT_FLAG_INTERRUPTIBLE = 1 << 9,
	FAULT_FLAG_UNSHARE = 1 << 10,
	FAULT_FLAG_ORIG_PTE_VALID = 1 << 11,
	FAULT_FLAG_VMA_LOCK = 1 << 12,
};

typedef unsigned int zap_flags_t;

typedef int cydp_t;
enum {

	FOLL_WRITE = 1 << 0,

	FOLL_GET = 1 << 1,

	FOLL_DUMP = 1 << 2,

	FOLL_FORCE = 1 << 3,

	FOLL_NOWAIT = 1 << 4,

	FOLL_NOFAULT = 1 << 5,

	FOLL_HWPOISON = 1 << 6,

	FOLL_ANON = 1 << 7,

	FOLL_LONGTERM = 1 << 8,

	FOLL_SPLIT_PMD = 1 << 9,

	FOLL_PCI_P2PDMA = 1 << 10,

	FOLL_INTERRUPTIBLE = 1 << 11,
	FOLL_HONOR_NUMA_FAULT = 1 << 12,

};
enum pageflags {
	PG_locked,
	PG_writeback,
	PG_referenced,
	PG_uptodate,
	PG_dirty,
	PG_lru,
	PG_head,
	PG_waiters,
	PG_active,
	PG_workingset,
	PG_owner_priv_1,
	PG_owner_2,
	PG_arch_1,
	PG_reserved,
	PG_private,
	PG_private_2,
	PG_reclaim,
	PG_swapbacked,
	PG_unevictable,

	PG_mlocked,
	PG_arch_2,

	__NR_PAGEFLAGS,

	PG_readahead = PG_reclaim,

	PG_swapcache = PG_owner_priv_1,

	PG_checked = PG_owner_priv_1,
	PG_anon_exclusive = PG_owner_2,

	PG_mappedtodisk = PG_owner_2,

	PG_fscache = PG_private_2,

	PG_pinned = PG_owner_priv_1,

	PG_savepinned = PG_dirty,

	PG_foreign = PG_owner_priv_1,

	PG_xen_remapped = PG_owner_priv_1,

	PG_isolated = PG_reclaim,

	PG_reported = PG_uptodate,
	PG_has_hwpoisoned = PG_active,
	PG_large_rmappable = PG_workingset,
	PG_partially_mapped = PG_reclaim,
};

extern struct static_key_false hugetlb_optimize_vmemmap_key;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
const struct page *
page_fixed_fake_head(const struct page *page)
{
	if (!({
		    bool branch;
		    if (__builtin_types_compatible_p(
				typeof(*&hugetlb_optimize_vmemmap_key),
				struct static_key_true))
			    branch = arch_static_branch_jump(
				    &(&hugetlb_optimize_vmemmap_key)->key,
				    false);
		    else if (__builtin_types_compatible_p(
				     typeof(*&hugetlb_optimize_vmemmap_key),
				     struct static_key_false))
			    branch = arch_static_branch(
				    &(&hugetlb_optimize_vmemmap_key)->key,
				    false);
		    else
			    branch = ____wrong_branch_error();
		    __builtin_expect(!!(branch), 0);
	    }))
		return page;

	if (((((unsigned long)page) &
	      ((typeof((unsigned long)page))(((1UL) << 12)) - 1)) == 0) &&
	    ((__builtin_constant_p(PG_head) &&
	      __builtin_constant_p((uintptr_t)(&page->flags) !=
				   (uintptr_t)((void *)0)) &&
	      (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) &&
	      __builtin_constant_p(*(const unsigned long *)(&page->flags))) ?
		     const_test_bit(PG_head, &page->flags) :
		     _test_bit(PG_head, &page->flags))) {
		unsigned long head = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_342(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(page[1].compound_head) ==
					       sizeof(char) ||
				       sizeof(page[1].compound_head) ==
					       sizeof(short) ||
				       sizeof(page[1].compound_head) ==
					       sizeof(int) ||
				       sizeof(page[1].compound_head) ==
					       sizeof(long)) ||
				      sizeof(page[1].compound_head) ==
					      sizeof(long long)))
					__compiletime_assert_342();
			} while (0);
			(*(const volatile typeof(_Generic(
				(page[1].compound_head),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 page[1].compound_head)))
				   *)&(page[1].compound_head));
		});

		if (__builtin_expect(!!(head & 1), 1))
			return (const struct page *)(head - 1);
	}
	return page;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
page_is_fake_head(const struct page *page)
{
	return page_fixed_fake_head(page) != page;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
_compound_head(const struct page *page)
{
	unsigned long head = ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_343(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(page->compound_head) == sizeof(char) ||
			       sizeof(page->compound_head) == sizeof(short) ||
			       sizeof(page->compound_head) == sizeof(int) ||
			       sizeof(page->compound_head) == sizeof(long)) ||
			      sizeof(page->compound_head) == sizeof(long long)))
				__compiletime_assert_343();
		} while (0);
		(*(const volatile typeof(_Generic((page->compound_head),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (page->compound_head)))
			   *)&(page->compound_head));
	});

	if (__builtin_expect(!!(head & 1), 0))
		return head - 1;
	return (unsigned long)page_fixed_fake_head(page);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageTail(const struct page *page)
{
	return ({
		       do {
			       __attribute__((__noreturn__)) extern void
			       __compiletime_assert_344(void) __attribute__((__error__(
				       "Unsupported access size for {READ,WRITE}_ONCE().")));
			       if (!((sizeof(page->compound_head) ==
					      sizeof(char) ||
				      sizeof(page->compound_head) ==
					      sizeof(short) ||
				      sizeof(page->compound_head) ==
					      sizeof(int) ||
				      sizeof(page->compound_head) ==
					      sizeof(long)) ||
				     sizeof(page->compound_head) ==
					     sizeof(long long)))
				       __compiletime_assert_344();
		       } while (0);
		       (*(const volatile typeof(_Generic(
			       (page->compound_head),
							char: (char)0,
							unsigned char: (
								unsigned char)0,
							signed char: (
								signed char)0,
							unsigned short: (
								unsigned short)0,
							signed short: (
								signed short)0,
							unsigned int: (
								unsigned int)0,
							signed int: (
								signed int)0,
							unsigned long: (
								unsigned long)0,
							signed long: (
								signed long)0,
							unsigned long long: (
								unsigned long long)0,
							signed long long: (
								signed long long)0,
							default: (
								page->compound_head)))
				  *)&(page->compound_head));
	       }) & 1 ||
	       page_is_fake_head(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageCompound(const struct page *page)
{
	return ((__builtin_constant_p(PG_head) &&
		 __builtin_constant_p((uintptr_t)(&page->flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(const unsigned long *)(&page->flags))) ?
			const_test_bit(PG_head, &page->flags) :
			_test_bit(PG_head, &page->flags)) ||
	       ({
		       do {
			       __attribute__((__noreturn__)) extern void
			       __compiletime_assert_345(void) __attribute__((__error__(
				       "Unsupported access size for {READ,WRITE}_ONCE().")));
			       if (!((sizeof(page->compound_head) ==
					      sizeof(char) ||
				      sizeof(page->compound_head) ==
					      sizeof(short) ||
				      sizeof(page->compound_head) ==
					      sizeof(int) ||
				      sizeof(page->compound_head) ==
					      sizeof(long)) ||
				     sizeof(page->compound_head) ==
					     sizeof(long long)))
				       __compiletime_assert_345();
		       } while (0);
		       (*(const volatile typeof(_Generic(
			       (page->compound_head),
							char: (char)0,
							unsigned char: (
								unsigned char)0,
							signed char: (
								signed char)0,
							unsigned short: (
								unsigned short)0,
							signed short: (
								signed short)0,
							unsigned int: (
								unsigned int)0,
							signed int: (
								signed int)0,
							unsigned long: (
								unsigned long)0,
							signed long: (
								signed long)0,
							unsigned long long: (
								unsigned long long)0,
							signed long long: (
								signed long long)0,
							default: (
								page->compound_head)))
				  *)&(page->compound_head));
	       }) & 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
PagePoisoned(const struct page *page)
{
	return ({
		       do {
			       __attribute__((__noreturn__)) extern void
			       __compiletime_assert_346(void) __attribute__((__error__(
				       "Unsupported access size for {READ,WRITE}_ONCE().")));
			       if (!((sizeof(page->flags) == sizeof(char) ||
				      sizeof(page->flags) == sizeof(short) ||
				      sizeof(page->flags) == sizeof(int) ||
				      sizeof(page->flags) == sizeof(long)) ||
				     sizeof(page->flags) == sizeof(long long)))
				       __compiletime_assert_346();
		       } while (0);
		       (*(const volatile typeof(_Generic(
			       (page->flags),
							char: (char)0,
							unsigned char: (
								unsigned char)0,
							signed char: (
								signed char)0,
							unsigned short: (
								unsigned short)0,
							signed short: (
								signed short)0,
							unsigned int: (
								unsigned int)0,
							signed int: (
								signed int)0,
							unsigned long: (
								unsigned long)0,
							signed long: (
								signed long)0,
							unsigned long long: (
								unsigned long long)0,
							signed long long: (
								signed long long)0,
							default: (page->flags)))
				  *)&(page->flags));
	       }) == -1l;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
page_init_poison(struct page *page, size_t size)
{
}

static const unsigned long *const_folio_flags(const struct folio *folio,
					      unsigned n)
{
	const struct page *page = &folio->page;

	((void)(sizeof((long)(PageTail(page)))));
	((void)(sizeof(
		(long)(n > 0 &&
		       !((__builtin_constant_p(PG_head) &&
			  __builtin_constant_p((uintptr_t)(&page->flags) !=
					       (uintptr_t)((void *)0)) &&
			  (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) &&
			  __builtin_constant_p(
				  *(const unsigned long *)(&page->flags))) ?
				 const_test_bit(PG_head, &page->flags) :
				 _test_bit(PG_head, &page->flags))))));
	return &page[n].flags;
}

static unsigned long *folio_flags(struct folio *folio, unsigned n)
{
	struct page *page = &folio->page;

	((void)(sizeof((long)(PageTail(page)))));
	((void)(sizeof(
		(long)(n > 0 &&
		       !((__builtin_constant_p(PG_head) &&
			  __builtin_constant_p((uintptr_t)(&page->flags) !=
					       (uintptr_t)((void *)0)) &&
			  (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) &&
			  __builtin_constant_p(
				  *(const unsigned long *)(&page->flags))) ?
				 const_test_bit(PG_head, &page->flags) :
				 _test_bit(PG_head, &page->flags))))));
	return &page[n].flags;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_locked(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_locked) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_locked, const_folio_flags(folio, 0)) :
			_test_bit(PG_locked, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageLocked(const struct page *page)
{
	return ((__builtin_constant_p(PG_locked) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof(
						      (long)(0 &&
							     PageTail(page)))));
					      ({
						      ((void)(sizeof((
							      long)(PagePoisoned(
							      ((typeof(page))_compound_head(
								      page)))))));
						      ((typeof(page))
							       _compound_head(
								       page));
					      });
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof(
					      (long)(0 && PageTail(page)))));
				      ({
					      ((void)(sizeof((
						      long)(PagePoisoned((
						      (typeof(page))_compound_head(
							      page)))))));
					      ((typeof(page))_compound_head(
						      page));
				      });
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof(
						     (long)(0 &&
							    PageTail(page)))));
					     ({
						     ((void)(sizeof((
							     long)(PagePoisoned((
							     (typeof(page))_compound_head(
								     page)))))));
						     ((typeof(page))
							      _compound_head(
								      page));
					     });
				     })->flags))) ?
			const_test_bit(
				PG_locked,
				&({
					 ((void)(sizeof(
						 (long)(0 && PageTail(page)))));
					 ({
						 ((void)(sizeof((
							 long)(PagePoisoned((
							 (typeof(page))_compound_head(
								 page)))))));
						 ((typeof(page))_compound_head(
							 page));
					 });
				 })->flags) :
			_test_bit(
				PG_locked,
				&({
					 ((void)(sizeof(
						 (long)(0 && PageTail(page)))));
					 ({
						 ((void)(sizeof((
							 long)(PagePoisoned((
							 (typeof(page))_compound_head(
								 page)))))));
						 ((typeof(page))_compound_head(
							 page));
					 });
				 })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_locked(struct folio *folio)
{
	((__builtin_constant_p(PG_locked) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___set_bit(PG_locked, folio_flags(folio, 0)) :
		 ___set_bit(PG_locked, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__SetPageLocked(struct page *page)
{
	((__builtin_constant_p(PG_locked) &&
	  __builtin_constant_p(
		  (uintptr_t)(&({
				       ((void)(sizeof(
					       (long)(1 && PageTail(page)))));
				       ({
					       ((void)(sizeof((
						       long)(PagePoisoned((
						       (typeof(page))_compound_head(
							       page)))))));
					       ((typeof(page))_compound_head(
						       page));
				       });
			       })->flags) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(&({
			       ((void)(sizeof((long)(1 && PageTail(page)))));
			       ({
				       ((void)(sizeof((long)(PagePoisoned(
					       ((typeof(page))_compound_head(
						       page)))))));
				       ((typeof(page))_compound_head(page));
			       });
		       })->flags) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(*(
		  const unsigned long
			  *)(&({
				      ((void)(sizeof(
					      (long)(1 && PageTail(page)))));
				      ({
					      ((void)(sizeof((
						      long)(PagePoisoned((
						      (typeof(page))_compound_head(
							      page)))))));
					      ((typeof(page))_compound_head(
						      page));
				      });
			      })->flags))) ?
		 generic___set_bit(
			 PG_locked,
			 &({
				  ((void)(sizeof((long)(1 && PageTail(page)))));
				  ({
					  ((void)(sizeof((long)(PagePoisoned(
						  ((typeof(page))_compound_head(
							  page)))))));
					  ((typeof(page))_compound_head(page));
				  });
			  })->flags) :
		 ___set_bit(
			 PG_locked,
			 &({
				  ((void)(sizeof((long)(1 && PageTail(page)))));
				  ({
					  ((void)(sizeof((long)(PagePoisoned(
						  ((typeof(page))_compound_head(
							  page)))))));
					  ((typeof(page))_compound_head(page));
				  });
			  })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_locked(struct folio *folio)
{
	((__builtin_constant_p(PG_locked) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___clear_bit(PG_locked, folio_flags(folio, 0)) :
		 ___clear_bit(PG_locked, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageLocked(struct page *page)
{
	((__builtin_constant_p(PG_locked) &&
	  __builtin_constant_p(
		  (uintptr_t)(&({
				       ((void)(sizeof(
					       (long)(1 && PageTail(page)))));
				       ({
					       ((void)(sizeof((
						       long)(PagePoisoned((
						       (typeof(page))_compound_head(
							       page)))))));
					       ((typeof(page))_compound_head(
						       page));
				       });
			       })->flags) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(&({
			       ((void)(sizeof((long)(1 && PageTail(page)))));
			       ({
				       ((void)(sizeof((long)(PagePoisoned(
					       ((typeof(page))_compound_head(
						       page)))))));
				       ((typeof(page))_compound_head(page));
			       });
		       })->flags) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(*(
		  const unsigned long
			  *)(&({
				      ((void)(sizeof(
					      (long)(1 && PageTail(page)))));
				      ({
					      ((void)(sizeof((
						      long)(PagePoisoned((
						      (typeof(page))_compound_head(
							      page)))))));
					      ((typeof(page))_compound_head(
						      page));
				      });
			      })->flags))) ?
		 generic___clear_bit(
			 PG_locked,
			 &({
				  ((void)(sizeof((long)(1 && PageTail(page)))));
				  ({
					  ((void)(sizeof((long)(PagePoisoned(
						  ((typeof(page))_compound_head(
							  page)))))));
					  ((typeof(page))_compound_head(page));
				  });
			  })->flags) :
		 ___clear_bit(
			 PG_locked,
			 &({
				  ((void)(sizeof((long)(1 && PageTail(page)))));
				  ({
					  ((void)(sizeof((long)(PagePoisoned(
						  ((typeof(page))_compound_head(
							  page)))))));
					  ((typeof(page))_compound_head(page));
				  });
			  })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_waiters(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_waiters) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_waiters,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_waiters, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_waiters(struct folio *folio)
{
	set_bit(PG_waiters, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_waiters(struct folio *folio)
{
	clear_bit(PG_waiters, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_referenced(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_referenced) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_referenced,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_referenced, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_referenced(struct folio *folio)
{
	set_bit(PG_referenced, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_referenced(struct folio *folio)
{
	clear_bit(PG_referenced, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_referenced(struct folio *folio)
{
	return test_and_clear_bit(PG_referenced, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_referenced(struct folio *folio)
{
	((__builtin_constant_p(PG_referenced) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___set_bit(PG_referenced, folio_flags(folio, 0)) :
		 ___set_bit(PG_referenced, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_dirty(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_dirty) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_dirty, const_folio_flags(folio, 0)) :
			_test_bit(PG_dirty, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageDirty(const struct page *page)
{
	return ((__builtin_constant_p(PG_dirty) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof((
						      long)(PagePoisoned((
						      (typeof(page))_compound_head(
							      page)))))));
					      ((typeof(page))_compound_head(
						      page));
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof((long)(PagePoisoned(
					      ((typeof(page))_compound_head(
						      page)))))));
				      ((typeof(page))_compound_head(page));
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof((long)(PagePoisoned((
						     (typeof(page))_compound_head(
							     page)))))));
					     ((typeof(page))_compound_head(
						     page));
				     })->flags))) ?
			const_test_bit(
				PG_dirty,
				&({
					 ((void)(sizeof((long)(PagePoisoned(
						 ((typeof(page))_compound_head(
							 page)))))));
					 ((typeof(page))_compound_head(page));
				 })->flags) :
			_test_bit(PG_dirty,
				  &({
					   ((void)(sizeof((long)(PagePoisoned((
						   (typeof(page))_compound_head(
							   page)))))));
					   ((typeof(page))_compound_head(page));
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_dirty(struct folio *folio)
{
	set_bit(PG_dirty, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageDirty(struct page *page)
{
	set_bit(PG_dirty,
		&({
			 ((void)(sizeof((long)(PagePoisoned(
				 ((typeof(page))_compound_head(page)))))));
			 ((typeof(page))_compound_head(page));
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_dirty(struct folio *folio)
{
	clear_bit(PG_dirty, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageDirty(struct page *page)
{
	clear_bit(PG_dirty,
		  &({
			   ((void)(sizeof((long)(PagePoisoned(
				   ((typeof(page))_compound_head(page)))))));
			   ((typeof(page))_compound_head(page));
		   })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_set_dirty(struct folio *folio)
{
	return test_and_set_bit(PG_dirty, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestSetPageDirty(struct page *page)
{
	return test_and_set_bit(
		PG_dirty,
		&({
			 ((void)(sizeof((long)(PagePoisoned(
				 ((typeof(page))_compound_head(page)))))));
			 ((typeof(page))_compound_head(page));
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_dirty(struct folio *folio)
{
	return test_and_clear_bit(PG_dirty, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestClearPageDirty(struct page *page)
{
	return test_and_clear_bit(
		PG_dirty,
		&({
			 ((void)(sizeof((long)(PagePoisoned(
				 ((typeof(page))_compound_head(page)))))));
			 ((typeof(page))_compound_head(page));
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_dirty(struct folio *folio)
{
	((__builtin_constant_p(PG_dirty) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___clear_bit(PG_dirty, folio_flags(folio, 0)) :
		 ___clear_bit(PG_dirty, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageDirty(struct page *page)
{
	((__builtin_constant_p(PG_dirty) &&
	  __builtin_constant_p(
		  (uintptr_t)(&({
				       ((void)(sizeof((long)(PagePoisoned(
					       ((typeof(page))_compound_head(
						       page)))))));
				       ((typeof(page))_compound_head(page));
			       })->flags) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(&({
			       ((void)(sizeof((long)(PagePoisoned((
				       (typeof(page))_compound_head(page)))))));
			       ((typeof(page))_compound_head(page));
		       })->flags) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long
			    *)(&({
					((void)(sizeof((long)(PagePoisoned(
						((typeof(page))_compound_head(
							page)))))));
					((typeof(page))_compound_head(page));
				})->flags))) ?
		 generic___clear_bit(
			 PG_dirty,
			 &({
				  ((void)(sizeof((long)(PagePoisoned(
					  ((typeof(page))_compound_head(
						  page)))))));
				  ((typeof(page))_compound_head(page));
			  })->flags) :
		 ___clear_bit(PG_dirty,
			      &({
				       ((void)(sizeof((long)(PagePoisoned(
					       ((typeof(page))_compound_head(
						       page)))))));
				       ((typeof(page))_compound_head(page));
			       })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_lru(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_lru) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_lru, const_folio_flags(folio, 0)) :
			_test_bit(PG_lru, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageLRU(const struct page *page)
{
	return ((__builtin_constant_p(PG_lru) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof((
						      long)(PagePoisoned((
						      (typeof(page))_compound_head(
							      page)))))));
					      ((typeof(page))_compound_head(
						      page));
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof((long)(PagePoisoned(
					      ((typeof(page))_compound_head(
						      page)))))));
				      ((typeof(page))_compound_head(page));
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof((long)(PagePoisoned((
						     (typeof(page))_compound_head(
							     page)))))));
					     ((typeof(page))_compound_head(
						     page));
				     })->flags))) ?
			const_test_bit(
				PG_lru,
				&({
					 ((void)(sizeof((long)(PagePoisoned(
						 ((typeof(page))_compound_head(
							 page)))))));
					 ((typeof(page))_compound_head(page));
				 })->flags) :
			_test_bit(PG_lru,
				  &({
					   ((void)(sizeof((long)(PagePoisoned((
						   (typeof(page))_compound_head(
							   page)))))));
					   ((typeof(page))_compound_head(page));
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_lru(struct folio *folio)
{
	set_bit(PG_lru, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageLRU(struct page *page)
{
	set_bit(PG_lru,
		&({
			 ((void)(sizeof((long)(PagePoisoned(
				 ((typeof(page))_compound_head(page)))))));
			 ((typeof(page))_compound_head(page));
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_lru(struct folio *folio)
{
	clear_bit(PG_lru, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageLRU(struct page *page)
{
	clear_bit(PG_lru,
		  &({
			   ((void)(sizeof((long)(PagePoisoned(
				   ((typeof(page))_compound_head(page)))))));
			   ((typeof(page))_compound_head(page));
		   })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_lru(struct folio *folio)
{
	((__builtin_constant_p(PG_lru) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___clear_bit(PG_lru, folio_flags(folio, 0)) :
		 ___clear_bit(PG_lru, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageLRU(struct page *page)
{
	((__builtin_constant_p(PG_lru) &&
	  __builtin_constant_p(
		  (uintptr_t)(&({
				       ((void)(sizeof((long)(PagePoisoned(
					       ((typeof(page))_compound_head(
						       page)))))));
				       ((typeof(page))_compound_head(page));
			       })->flags) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(&({
			       ((void)(sizeof((long)(PagePoisoned((
				       (typeof(page))_compound_head(page)))))));
			       ((typeof(page))_compound_head(page));
		       })->flags) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long
			    *)(&({
					((void)(sizeof((long)(PagePoisoned(
						((typeof(page))_compound_head(
							page)))))));
					((typeof(page))_compound_head(page));
				})->flags))) ?
		 generic___clear_bit(
			 PG_lru, &({
					  ((void)(sizeof((long)(PagePoisoned(
						  ((typeof(page))_compound_head(
							  page)))))));
					  ((typeof(page))_compound_head(page));
				  })->flags) :
		 ___clear_bit(PG_lru,
			      &({
				       ((void)(sizeof((long)(PagePoisoned(
					       ((typeof(page))_compound_head(
						       page)))))));
				       ((typeof(page))_compound_head(page));
			       })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_lru(struct folio *folio)
{
	return test_and_clear_bit(PG_lru, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestClearPageLRU(struct page *page)
{
	return test_and_clear_bit(
		PG_lru,
		&({
			 ((void)(sizeof((long)(PagePoisoned(
				 ((typeof(page))_compound_head(page)))))));
			 ((typeof(page))_compound_head(page));
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_active(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_active) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_active, const_folio_flags(folio, 0)) :
			_test_bit(PG_active, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_active(struct folio *folio)
{
	set_bit(PG_active, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_active(struct folio *folio)
{
	clear_bit(PG_active, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_active(struct folio *folio)
{
	((__builtin_constant_p(PG_active) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___clear_bit(PG_active, folio_flags(folio, 0)) :
		 ___clear_bit(PG_active, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_active(struct folio *folio)
{
	return test_and_clear_bit(PG_active, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_workingset(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_workingset) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_workingset,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_workingset, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageWorkingset(const struct page *page)
{
	return ((__builtin_constant_p(PG_workingset) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof((
						      long)(PagePoisoned((
						      (typeof(page))_compound_head(
							      page)))))));
					      ((typeof(page))_compound_head(
						      page));
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof((long)(PagePoisoned(
					      ((typeof(page))_compound_head(
						      page)))))));
				      ((typeof(page))_compound_head(page));
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof((long)(PagePoisoned((
						     (typeof(page))_compound_head(
							     page)))))));
					     ((typeof(page))_compound_head(
						     page));
				     })->flags))) ?
			const_test_bit(
				PG_workingset,
				&({
					 ((void)(sizeof((long)(PagePoisoned(
						 ((typeof(page))_compound_head(
							 page)))))));
					 ((typeof(page))_compound_head(page));
				 })->flags) :
			_test_bit(PG_workingset,
				  &({
					   ((void)(sizeof((long)(PagePoisoned((
						   (typeof(page))_compound_head(
							   page)))))));
					   ((typeof(page))_compound_head(page));
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_workingset(struct folio *folio)
{
	set_bit(PG_workingset, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageWorkingset(struct page *page)
{
	set_bit(PG_workingset,
		&({
			 ((void)(sizeof((long)(PagePoisoned(
				 ((typeof(page))_compound_head(page)))))));
			 ((typeof(page))_compound_head(page));
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_workingset(struct folio *folio)
{
	clear_bit(PG_workingset, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageWorkingset(struct page *page)
{
	clear_bit(PG_workingset,
		  &({
			   ((void)(sizeof((long)(PagePoisoned(
				   ((typeof(page))_compound_head(page)))))));
			   ((typeof(page))_compound_head(page));
		   })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_workingset(struct folio *folio)
{
	return test_and_clear_bit(PG_workingset, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestClearPageWorkingset(struct page *page)
{
	return test_and_clear_bit(
		PG_workingset,
		&({
			 ((void)(sizeof((long)(PagePoisoned(
				 ((typeof(page))_compound_head(page)))))));
			 ((typeof(page))_compound_head(page));
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_checked(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_checked) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_checked,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_checked, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageChecked(const struct page *page)
{
	return ((__builtin_constant_p(PG_checked) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof(
						      (long)(0 &&
							     PageCompound(
								     page)))));
					      ({
						      ((void)(sizeof((
							      long)(PagePoisoned(
							      page)))));
						      page;
					      });
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof((
					      long)(0 && PageCompound(page)))));
				      ({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      });
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof(
						     (long)(0 &&
							    PageCompound(
								    page)))));
					     ({
						     ((void)(sizeof((
							     long)(PagePoisoned(
							     page)))));
						     page;
					     });
				     })->flags))) ?
			const_test_bit(
				PG_checked,
				&({
					 ((void)(sizeof(
						 (long)(0 &&
							PageCompound(page)))));
					 ({
						 ((void)(sizeof(
							 (long)(PagePoisoned(
								 page)))));
						 page;
					 });
				 })->flags) :
			_test_bit(PG_checked,
				  &({
					   ((void)(sizeof((
						   long)(0 &&
							 PageCompound(page)))));
					   ({
						   ((void)(sizeof(
							   (long)(PagePoisoned(
								   page)))));
						   page;
					   });
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_checked(struct folio *folio)
{
	set_bit(PG_checked, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageChecked(struct page *page)
{
	set_bit(PG_checked,
		&({
			 ((void)(sizeof((long)(1 && PageCompound(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(page)))));
				 page;
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_checked(struct folio *folio)
{
	clear_bit(PG_checked, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageChecked(struct page *page)
{
	clear_bit(PG_checked,
		  &({
			   ((void)(sizeof((long)(1 && PageCompound(page)))));
			   ({
				   ((void)(sizeof((long)(PagePoisoned(page)))));
				   page;
			   });
		   })->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_pinned(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_pinned) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_pinned, const_folio_flags(folio, 0)) :
			_test_bit(PG_pinned, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PagePinned(const struct page *page)
{
	return ((__builtin_constant_p(PG_pinned) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof(
						      (long)(0 &&
							     PageCompound(
								     page)))));
					      ({
						      ((void)(sizeof((
							      long)(PagePoisoned(
							      page)))));
						      page;
					      });
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof((
					      long)(0 && PageCompound(page)))));
				      ({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      });
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof(
						     (long)(0 &&
							    PageCompound(
								    page)))));
					     ({
						     ((void)(sizeof((
							     long)(PagePoisoned(
							     page)))));
						     page;
					     });
				     })->flags))) ?
			const_test_bit(
				PG_pinned,
				&({
					 ((void)(sizeof(
						 (long)(0 &&
							PageCompound(page)))));
					 ({
						 ((void)(sizeof(
							 (long)(PagePoisoned(
								 page)))));
						 page;
					 });
				 })->flags) :
			_test_bit(PG_pinned,
				  &({
					   ((void)(sizeof((
						   long)(0 &&
							 PageCompound(page)))));
					   ({
						   ((void)(sizeof(
							   (long)(PagePoisoned(
								   page)))));
						   page;
					   });
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_pinned(struct folio *folio)
{
	set_bit(PG_pinned, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPagePinned(struct page *page)
{
	set_bit(PG_pinned,
		&({
			 ((void)(sizeof((long)(1 && PageCompound(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(page)))));
				 page;
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_pinned(struct folio *folio)
{
	clear_bit(PG_pinned, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPagePinned(struct page *page)
{
	clear_bit(PG_pinned,
		  &({
			   ((void)(sizeof((long)(1 && PageCompound(page)))));
			   ({
				   ((void)(sizeof((long)(PagePoisoned(page)))));
				   page;
			   });
		   })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_set_pinned(struct folio *folio)
{
	return test_and_set_bit(PG_pinned, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestSetPagePinned(struct page *page)
{
	return test_and_set_bit(
		PG_pinned,
		&({
			 ((void)(sizeof((long)(1 && PageCompound(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(page)))));
				 page;
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_pinned(struct folio *folio)
{
	return test_and_clear_bit(PG_pinned, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestClearPagePinned(struct page *page)
{
	return test_and_clear_bit(
		PG_pinned,
		&({
			 ((void)(sizeof((long)(1 && PageCompound(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(page)))));
				 page;
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_savepinned(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_savepinned) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_savepinned,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_savepinned, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageSavePinned(const struct page *page)
{
	return ((__builtin_constant_p(PG_savepinned) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof(
						      (long)(0 &&
							     PageCompound(
								     page)))));
					      ({
						      ((void)(sizeof((
							      long)(PagePoisoned(
							      page)))));
						      page;
					      });
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof((
					      long)(0 && PageCompound(page)))));
				      ({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      });
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof(
						     (long)(0 &&
							    PageCompound(
								    page)))));
					     ({
						     ((void)(sizeof((
							     long)(PagePoisoned(
							     page)))));
						     page;
					     });
				     })->flags))) ?
			const_test_bit(
				PG_savepinned,
				&({
					 ((void)(sizeof(
						 (long)(0 &&
							PageCompound(page)))));
					 ({
						 ((void)(sizeof(
							 (long)(PagePoisoned(
								 page)))));
						 page;
					 });
				 })->flags) :
			_test_bit(PG_savepinned,
				  &({
					   ((void)(sizeof((
						   long)(0 &&
							 PageCompound(page)))));
					   ({
						   ((void)(sizeof(
							   (long)(PagePoisoned(
								   page)))));
						   page;
					   });
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_savepinned(struct folio *folio)
{
	set_bit(PG_savepinned, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageSavePinned(struct page *page)
{
	set_bit(PG_savepinned,
		&({
			 ((void)(sizeof((long)(1 && PageCompound(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(page)))));
				 page;
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_savepinned(struct folio *folio)
{
	clear_bit(PG_savepinned, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageSavePinned(struct page *page)
{
	clear_bit(PG_savepinned,
		  &({
			   ((void)(sizeof((long)(1 && PageCompound(page)))));
			   ({
				   ((void)(sizeof((long)(PagePoisoned(page)))));
				   page;
			   });
		   })->flags);
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_foreign(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_foreign) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_foreign,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_foreign, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageForeign(const struct page *page)
{
	return ((__builtin_constant_p(PG_foreign) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof(
						      (long)(0 &&
							     PageCompound(
								     page)))));
					      ({
						      ((void)(sizeof((
							      long)(PagePoisoned(
							      page)))));
						      page;
					      });
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof((
					      long)(0 && PageCompound(page)))));
				      ({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      });
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof(
						     (long)(0 &&
							    PageCompound(
								    page)))));
					     ({
						     ((void)(sizeof((
							     long)(PagePoisoned(
							     page)))));
						     page;
					     });
				     })->flags))) ?
			const_test_bit(
				PG_foreign,
				&({
					 ((void)(sizeof(
						 (long)(0 &&
							PageCompound(page)))));
					 ({
						 ((void)(sizeof(
							 (long)(PagePoisoned(
								 page)))));
						 page;
					 });
				 })->flags) :
			_test_bit(PG_foreign,
				  &({
					   ((void)(sizeof((
						   long)(0 &&
							 PageCompound(page)))));
					   ({
						   ((void)(sizeof(
							   (long)(PagePoisoned(
								   page)))));
						   page;
					   });
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_foreign(struct folio *folio)
{
	set_bit(PG_foreign, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageForeign(struct page *page)
{
	set_bit(PG_foreign,
		&({
			 ((void)(sizeof((long)(1 && PageCompound(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(page)))));
				 page;
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_foreign(struct folio *folio)
{
	clear_bit(PG_foreign, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageForeign(struct page *page)
{
	clear_bit(PG_foreign,
		  &({
			   ((void)(sizeof((long)(1 && PageCompound(page)))));
			   ({
				   ((void)(sizeof((long)(PagePoisoned(page)))));
				   page;
			   });
		   })->flags);
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_xen_remapped(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_xen_remapped) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_xen_remapped,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_xen_remapped,
				  const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageXenRemapped(const struct page *page)
{
	return ((__builtin_constant_p(PG_xen_remapped) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof(
						      (long)(0 &&
							     PageCompound(
								     page)))));
					      ({
						      ((void)(sizeof((
							      long)(PagePoisoned(
							      page)))));
						      page;
					      });
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof((
					      long)(0 && PageCompound(page)))));
				      ({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      });
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof(
						     (long)(0 &&
							    PageCompound(
								    page)))));
					     ({
						     ((void)(sizeof((
							     long)(PagePoisoned(
							     page)))));
						     page;
					     });
				     })->flags))) ?
			const_test_bit(
				PG_xen_remapped,
				&({
					 ((void)(sizeof(
						 (long)(0 &&
							PageCompound(page)))));
					 ({
						 ((void)(sizeof(
							 (long)(PagePoisoned(
								 page)))));
						 page;
					 });
				 })->flags) :
			_test_bit(PG_xen_remapped,
				  &({
					   ((void)(sizeof((
						   long)(0 &&
							 PageCompound(page)))));
					   ({
						   ((void)(sizeof(
							   (long)(PagePoisoned(
								   page)))));
						   page;
					   });
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_xen_remapped(struct folio *folio)
{
	set_bit(PG_xen_remapped, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageXenRemapped(struct page *page)
{
	set_bit(PG_xen_remapped,
		&({
			 ((void)(sizeof((long)(1 && PageCompound(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(page)))));
				 page;
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_xen_remapped(struct folio *folio)
{
	clear_bit(PG_xen_remapped, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageXenRemapped(struct page *page)
{
	clear_bit(PG_xen_remapped,
		  &({
			   ((void)(sizeof((long)(1 && PageCompound(page)))));
			   ({
				   ((void)(sizeof((long)(PagePoisoned(page)))));
				   page;
			   });
		   })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_xen_remapped(struct folio *folio)
{
	return test_and_clear_bit(PG_xen_remapped, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestClearPageXenRemapped(struct page *page)
{
	return test_and_clear_bit(
		PG_xen_remapped,
		&({
			 ((void)(sizeof((long)(1 && PageCompound(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(page)))));
				 page;
			 });
		 })->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_reserved(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_reserved) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_reserved,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_reserved, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageReserved(const struct page *page)
{
	return ((__builtin_constant_p(PG_reserved) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof(
						      (long)(0 &&
							     PageCompound(
								     page)))));
					      ({
						      ((void)(sizeof((
							      long)(PagePoisoned(
							      page)))));
						      page;
					      });
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof((
					      long)(0 && PageCompound(page)))));
				      ({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      });
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof(
						     (long)(0 &&
							    PageCompound(
								    page)))));
					     ({
						     ((void)(sizeof((
							     long)(PagePoisoned(
							     page)))));
						     page;
					     });
				     })->flags))) ?
			const_test_bit(
				PG_reserved,
				&({
					 ((void)(sizeof(
						 (long)(0 &&
							PageCompound(page)))));
					 ({
						 ((void)(sizeof(
							 (long)(PagePoisoned(
								 page)))));
						 page;
					 });
				 })->flags) :
			_test_bit(PG_reserved,
				  &({
					   ((void)(sizeof((
						   long)(0 &&
							 PageCompound(page)))));
					   ({
						   ((void)(sizeof(
							   (long)(PagePoisoned(
								   page)))));
						   page;
					   });
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_reserved(struct folio *folio)
{
	set_bit(PG_reserved, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageReserved(struct page *page)
{
	set_bit(PG_reserved,
		&({
			 ((void)(sizeof((long)(1 && PageCompound(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(page)))));
				 page;
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_reserved(struct folio *folio)
{
	clear_bit(PG_reserved, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageReserved(struct page *page)
{
	clear_bit(PG_reserved,
		  &({
			   ((void)(sizeof((long)(1 && PageCompound(page)))));
			   ({
				   ((void)(sizeof((long)(PagePoisoned(page)))));
				   page;
			   });
		   })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_reserved(struct folio *folio)
{
	((__builtin_constant_p(PG_reserved) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___clear_bit(PG_reserved, folio_flags(folio, 0)) :
		 ___clear_bit(PG_reserved, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageReserved(struct page *page)
{
	((__builtin_constant_p(PG_reserved) &&
	  __builtin_constant_p(
		  (uintptr_t)(&({
				       ((void)(sizeof((
					       long)(1 && PageCompound(page)))));
				       ({
					       ((void)(sizeof(
						       (long)(PagePoisoned(
							       page)))));
					       page;
				       });
			       })->flags) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(&({
			       ((void)(sizeof((long)(1 && PageCompound(page)))));
			       ({
				       ((void)(sizeof(
					       (long)(PagePoisoned(page)))));
				       page;
			       });
		       })->flags) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(*(
		  const unsigned long
			  *)(&({
				      ((void)(sizeof((
					      long)(1 && PageCompound(page)))));
				      ({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      });
			      })->flags))) ?
		 generic___clear_bit(
			 PG_reserved,
			 &({
				  ((void)(sizeof(
					  (long)(1 && PageCompound(page)))));
				  ({
					  ((void)(sizeof(
						  (long)(PagePoisoned(page)))));
					  page;
				  });
			  })->flags) :
		 ___clear_bit(
			 PG_reserved,
			 &({
				  ((void)(sizeof(
					  (long)(1 && PageCompound(page)))));
				  ({
					  ((void)(sizeof(
						  (long)(PagePoisoned(page)))));
					  page;
				  });
			  })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_reserved(struct folio *folio)
{
	((__builtin_constant_p(PG_reserved) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___set_bit(PG_reserved, folio_flags(folio, 0)) :
		 ___set_bit(PG_reserved, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__SetPageReserved(struct page *page)
{
	((__builtin_constant_p(PG_reserved) &&
	  __builtin_constant_p(
		  (uintptr_t)(&({
				       ((void)(sizeof((
					       long)(1 && PageCompound(page)))));
				       ({
					       ((void)(sizeof(
						       (long)(PagePoisoned(
							       page)))));
					       page;
				       });
			       })->flags) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(&({
			       ((void)(sizeof((long)(1 && PageCompound(page)))));
			       ({
				       ((void)(sizeof(
					       (long)(PagePoisoned(page)))));
				       page;
			       });
		       })->flags) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(*(
		  const unsigned long
			  *)(&({
				      ((void)(sizeof((
					      long)(1 && PageCompound(page)))));
				      ({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      });
			      })->flags))) ?
		 generic___set_bit(
			 PG_reserved,
			 &({
				  ((void)(sizeof(
					  (long)(1 && PageCompound(page)))));
				  ({
					  ((void)(sizeof(
						  (long)(PagePoisoned(page)))));
					  page;
				  });
			  })->flags) :
		 ___set_bit(PG_reserved,
			    &({
				     ((void)(sizeof(
					     (long)(1 && PageCompound(page)))));
				     ({
					     ((void)(sizeof((long)(PagePoisoned(
						     page)))));
					     page;
				     });
			     })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_swapbacked(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_swapbacked) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_swapbacked,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_swapbacked, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_swapbacked(struct folio *folio)
{
	set_bit(PG_swapbacked, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_swapbacked(struct folio *folio)
{
	clear_bit(PG_swapbacked, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_swapbacked(struct folio *folio)
{
	((__builtin_constant_p(PG_swapbacked) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___clear_bit(PG_swapbacked, folio_flags(folio, 0)) :
		 ___clear_bit(PG_swapbacked, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_swapbacked(struct folio *folio)
{
	((__builtin_constant_p(PG_swapbacked) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___set_bit(PG_swapbacked, folio_flags(folio, 0)) :
		 ___set_bit(PG_swapbacked, folio_flags(folio, 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_private(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_private) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_private,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_private, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PagePrivate(const struct page *page)
{
	return ((__builtin_constant_p(PG_private) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof(
					      (long)(PagePoisoned(page)))));
				      page;
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof((long)(PagePoisoned(
						     page)))));
					     page;
				     })->flags))) ?
			const_test_bit(
				PG_private,
				&({
					 ((void)(sizeof(
						 (long)(PagePoisoned(page)))));
					 page;
				 })->flags) :
			_test_bit(PG_private,
				  &({
					   ((void)(sizeof((
						   long)(PagePoisoned(page)))));
					   page;
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_private(struct folio *folio)
{
	set_bit(PG_private, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPagePrivate(struct page *page)
{
	set_bit(PG_private,
		&({
			 ((void)(sizeof((long)(PagePoisoned(page)))));
			 page;
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_private(struct folio *folio)
{
	clear_bit(PG_private, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPagePrivate(struct page *page)
{
	clear_bit(PG_private,
		  &({
			   ((void)(sizeof((long)(PagePoisoned(page)))));
			   page;
		   })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_private_2(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_private_2) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_private_2,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_private_2, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PagePrivate2(const struct page *page)
{
	return ((__builtin_constant_p(PG_private_2) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof(
					      (long)(PagePoisoned(page)))));
				      page;
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof((long)(PagePoisoned(
						     page)))));
					     page;
				     })->flags))) ?
			const_test_bit(
				PG_private_2,
				&({
					 ((void)(sizeof(
						 (long)(PagePoisoned(page)))));
					 page;
				 })->flags) :
			_test_bit(PG_private_2,
				  &({
					   ((void)(sizeof((
						   long)(PagePoisoned(page)))));
					   page;
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_private_2(struct folio *folio)
{
	set_bit(PG_private_2, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPagePrivate2(struct page *page)
{
	set_bit(PG_private_2,
		&({
			 ((void)(sizeof((long)(PagePoisoned(page)))));
			 page;
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_private_2(struct folio *folio)
{
	clear_bit(PG_private_2, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPagePrivate2(struct page *page)
{
	clear_bit(PG_private_2,
		  &({
			   ((void)(sizeof((long)(PagePoisoned(page)))));
			   page;
		   })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_set_private_2(struct folio *folio)
{
	return test_and_set_bit(PG_private_2, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestSetPagePrivate2(struct page *page)
{
	return test_and_set_bit(
		PG_private_2,
		&({
			 ((void)(sizeof((long)(PagePoisoned(page)))));
			 page;
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_private_2(struct folio *folio)
{
	return test_and_clear_bit(PG_private_2, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestClearPagePrivate2(struct page *page)
{
	return test_and_clear_bit(
		PG_private_2,
		&({
			 ((void)(sizeof((long)(PagePoisoned(page)))));
			 page;
		 })->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_owner_2(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_owner_2) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_owner_2,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_owner_2, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_owner_2(struct folio *folio)
{
	set_bit(PG_owner_2, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_owner_2(struct folio *folio)
{
	clear_bit(PG_owner_2, folio_flags(folio, 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_writeback(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_writeback) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_writeback,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_writeback, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageWriteback(const struct page *page)
{
	return ((__builtin_constant_p(PG_writeback) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof(
						      (long)(0 &&
							     PageTail(page)))));
					      ({
						      ((void)(sizeof((
							      long)(PagePoisoned(
							      ((typeof(page))_compound_head(
								      page)))))));
						      ((typeof(page))
							       _compound_head(
								       page));
					      });
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof(
					      (long)(0 && PageTail(page)))));
				      ({
					      ((void)(sizeof((
						      long)(PagePoisoned((
						      (typeof(page))_compound_head(
							      page)))))));
					      ((typeof(page))_compound_head(
						      page));
				      });
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof(
						     (long)(0 &&
							    PageTail(page)))));
					     ({
						     ((void)(sizeof((
							     long)(PagePoisoned((
							     (typeof(page))_compound_head(
								     page)))))));
						     ((typeof(page))
							      _compound_head(
								      page));
					     });
				     })->flags))) ?
			const_test_bit(
				PG_writeback,
				&({
					 ((void)(sizeof(
						 (long)(0 && PageTail(page)))));
					 ({
						 ((void)(sizeof((
							 long)(PagePoisoned((
							 (typeof(page))_compound_head(
								 page)))))));
						 ((typeof(page))_compound_head(
							 page));
					 });
				 })->flags) :
			_test_bit(
				PG_writeback,
				&({
					 ((void)(sizeof(
						 (long)(0 && PageTail(page)))));
					 ({
						 ((void)(sizeof((
							 long)(PagePoisoned((
							 (typeof(page))_compound_head(
								 page)))))));
						 ((typeof(page))_compound_head(
							 page));
					 });
				 })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_set_writeback(struct folio *folio)
{
	return test_and_set_bit(PG_writeback, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestSetPageWriteback(struct page *page)
{
	return test_and_set_bit(
		PG_writeback,
		&({
			 ((void)(sizeof((long)(1 && PageTail(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(((
					 typeof(page))_compound_head(page)))))));
				 ((typeof(page))_compound_head(page));
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_writeback(struct folio *folio)
{
	return test_and_clear_bit(PG_writeback, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestClearPageWriteback(struct page *page)
{
	return test_and_clear_bit(
		PG_writeback,
		&({
			 ((void)(sizeof((long)(1 && PageTail(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(((
					 typeof(page))_compound_head(page)))))));
				 ((typeof(page))_compound_head(page));
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_mappedtodisk(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_mappedtodisk) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_mappedtodisk,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_mappedtodisk,
				  const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageMappedToDisk(const struct page *page)
{
	return ((__builtin_constant_p(PG_mappedtodisk) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof(
						      (long)(0 &&
							     PageTail(page)))));
					      ({
						      ((void)(sizeof((
							      long)(PagePoisoned(
							      ((typeof(page))_compound_head(
								      page)))))));
						      ((typeof(page))
							       _compound_head(
								       page));
					      });
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof(
					      (long)(0 && PageTail(page)))));
				      ({
					      ((void)(sizeof((
						      long)(PagePoisoned((
						      (typeof(page))_compound_head(
							      page)))))));
					      ((typeof(page))_compound_head(
						      page));
				      });
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof(
						     (long)(0 &&
							    PageTail(page)))));
					     ({
						     ((void)(sizeof((
							     long)(PagePoisoned((
							     (typeof(page))_compound_head(
								     page)))))));
						     ((typeof(page))
							      _compound_head(
								      page));
					     });
				     })->flags))) ?
			const_test_bit(
				PG_mappedtodisk,
				&({
					 ((void)(sizeof(
						 (long)(0 && PageTail(page)))));
					 ({
						 ((void)(sizeof((
							 long)(PagePoisoned((
							 (typeof(page))_compound_head(
								 page)))))));
						 ((typeof(page))_compound_head(
							 page));
					 });
				 })->flags) :
			_test_bit(
				PG_mappedtodisk,
				&({
					 ((void)(sizeof(
						 (long)(0 && PageTail(page)))));
					 ({
						 ((void)(sizeof((
							 long)(PagePoisoned((
							 (typeof(page))_compound_head(
								 page)))))));
						 ((typeof(page))_compound_head(
							 page));
					 });
				 })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_mappedtodisk(struct folio *folio)
{
	set_bit(PG_mappedtodisk, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageMappedToDisk(struct page *page)
{
	set_bit(PG_mappedtodisk,
		&({
			 ((void)(sizeof((long)(1 && PageTail(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(((
					 typeof(page))_compound_head(page)))))));
				 ((typeof(page))_compound_head(page));
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_mappedtodisk(struct folio *folio)
{
	clear_bit(PG_mappedtodisk, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageMappedToDisk(struct page *page)
{
	clear_bit(PG_mappedtodisk,
		  &({
			   ((void)(sizeof((long)(1 && PageTail(page)))));
			   ({
				   ((void)(sizeof((long)(PagePoisoned(
					   ((typeof(page))_compound_head(
						   page)))))));
				   ((typeof(page))_compound_head(page));
			   });
		   })->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_reclaim(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_reclaim) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_reclaim,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_reclaim, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageReclaim(const struct page *page)
{
	return ((__builtin_constant_p(PG_reclaim) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof(
						      (long)(0 &&
							     PageTail(page)))));
					      ({
						      ((void)(sizeof((
							      long)(PagePoisoned(
							      ((typeof(page))_compound_head(
								      page)))))));
						      ((typeof(page))
							       _compound_head(
								       page));
					      });
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof(
					      (long)(0 && PageTail(page)))));
				      ({
					      ((void)(sizeof((
						      long)(PagePoisoned((
						      (typeof(page))_compound_head(
							      page)))))));
					      ((typeof(page))_compound_head(
						      page));
				      });
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof(
						     (long)(0 &&
							    PageTail(page)))));
					     ({
						     ((void)(sizeof((
							     long)(PagePoisoned((
							     (typeof(page))_compound_head(
								     page)))))));
						     ((typeof(page))
							      _compound_head(
								      page));
					     });
				     })->flags))) ?
			const_test_bit(
				PG_reclaim,
				&({
					 ((void)(sizeof(
						 (long)(0 && PageTail(page)))));
					 ({
						 ((void)(sizeof((
							 long)(PagePoisoned((
							 (typeof(page))_compound_head(
								 page)))))));
						 ((typeof(page))_compound_head(
							 page));
					 });
				 })->flags) :
			_test_bit(
				PG_reclaim,
				&({
					 ((void)(sizeof(
						 (long)(0 && PageTail(page)))));
					 ({
						 ((void)(sizeof((
							 long)(PagePoisoned((
							 (typeof(page))_compound_head(
								 page)))))));
						 ((typeof(page))_compound_head(
							 page));
					 });
				 })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_reclaim(struct folio *folio)
{
	set_bit(PG_reclaim, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageReclaim(struct page *page)
{
	set_bit(PG_reclaim,
		&({
			 ((void)(sizeof((long)(1 && PageTail(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(((
					 typeof(page))_compound_head(page)))))));
				 ((typeof(page))_compound_head(page));
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_reclaim(struct folio *folio)
{
	clear_bit(PG_reclaim, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageReclaim(struct page *page)
{
	clear_bit(PG_reclaim,
		  &({
			   ((void)(sizeof((long)(1 && PageTail(page)))));
			   ({
				   ((void)(sizeof((long)(PagePoisoned(
					   ((typeof(page))_compound_head(
						   page)))))));
				   ((typeof(page))_compound_head(page));
			   });
		   })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_reclaim(struct folio *folio)
{
	return test_and_clear_bit(PG_reclaim, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
TestClearPageReclaim(struct page *page)
{
	return test_and_clear_bit(
		PG_reclaim,
		&({
			 ((void)(sizeof((long)(1 && PageTail(page)))));
			 ({
				 ((void)(sizeof((long)(PagePoisoned(((
					 typeof(page))_compound_head(page)))))));
				 ((typeof(page))_compound_head(page));
			 });
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_readahead(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_readahead) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_readahead,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_readahead, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_readahead(struct folio *folio)
{
	set_bit(PG_readahead, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_readahead(struct folio *folio)
{
	clear_bit(PG_readahead, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_readahead(struct folio *folio)
{
	return test_and_clear_bit(PG_readahead, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_highmem(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
PageHighMem(const struct page *page)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_set_highmem(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
SetPageHighMem(struct page *page)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_clear_highmem(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ClearPageHighMem(struct page *page)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_swapcache(const struct folio *folio)
{
	return folio_test_swapbacked(folio) &&
	       ((__builtin_constant_p(PG_swapcache) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_swapcache,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_swapcache, const_folio_flags(folio, 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_swapcache(struct folio *folio)
{
	set_bit(PG_swapcache, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_swapcache(struct folio *folio)
{
	clear_bit(PG_swapcache, folio_flags(folio, 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_unevictable(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_unevictable) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_unevictable,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_unevictable, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_unevictable(struct folio *folio)
{
	set_bit(PG_unevictable, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_unevictable(struct folio *folio)
{
	clear_bit(PG_unevictable, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_unevictable(struct folio *folio)
{
	((__builtin_constant_p(PG_unevictable) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___clear_bit(PG_unevictable, folio_flags(folio, 0)) :
		 ___clear_bit(PG_unevictable, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_unevictable(struct folio *folio)
{
	return test_and_clear_bit(PG_unevictable, folio_flags(folio, 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_mlocked(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_mlocked) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_mlocked,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_mlocked, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_mlocked(struct folio *folio)
{
	set_bit(PG_mlocked, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_mlocked(struct folio *folio)
{
	clear_bit(PG_mlocked, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_mlocked(struct folio *folio)
{
	((__builtin_constant_p(PG_mlocked) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___clear_bit(PG_mlocked, folio_flags(folio, 0)) :
		 ___clear_bit(PG_mlocked, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_clear_mlocked(struct folio *folio)
{
	return test_and_clear_bit(PG_mlocked, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_set_mlocked(struct folio *folio)
{
	return test_and_set_bit(PG_mlocked, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_hwpoison(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
PageHWPoison(const struct page *page)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_set_hwpoison(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
SetPageHWPoison(struct page *page)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_clear_hwpoison(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ClearPageHWPoison(struct page *page)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_young(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_set_young(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_clear_young(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_clear_young(struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_idle(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_set_idle(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_clear_idle(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_reported(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_reported) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_reported,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_reported, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageReported(const struct page *page)
{
	return ((__builtin_constant_p(PG_reported) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof(
						      (long)(0 &&
							     PageCompound(
								     page)))));
					      ({
						      ((void)(sizeof((
							      long)(PagePoisoned(
							      page)))));
						      page;
					      });
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof((
					      long)(0 && PageCompound(page)))));
				      ({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      });
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof(
						     (long)(0 &&
							    PageCompound(
								    page)))));
					     ({
						     ((void)(sizeof((
							     long)(PagePoisoned(
							     page)))));
						     page;
					     });
				     })->flags))) ?
			const_test_bit(
				PG_reported,
				&({
					 ((void)(sizeof(
						 (long)(0 &&
							PageCompound(page)))));
					 ({
						 ((void)(sizeof(
							 (long)(PagePoisoned(
								 page)))));
						 page;
					 });
				 })->flags) :
			_test_bit(PG_reported,
				  &({
					   ((void)(sizeof((
						   long)(0 &&
							 PageCompound(page)))));
					   ({
						   ((void)(sizeof(
							   (long)(PagePoisoned(
								   page)))));
						   page;
					   });
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_reported(struct folio *folio)
{
	((__builtin_constant_p(PG_reported) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___set_bit(PG_reported, folio_flags(folio, 0)) :
		 ___set_bit(PG_reported, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__SetPageReported(struct page *page)
{
	((__builtin_constant_p(PG_reported) &&
	  __builtin_constant_p(
		  (uintptr_t)(&({
				       ((void)(sizeof((
					       long)(1 && PageCompound(page)))));
				       ({
					       ((void)(sizeof(
						       (long)(PagePoisoned(
							       page)))));
					       page;
				       });
			       })->flags) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(&({
			       ((void)(sizeof((long)(1 && PageCompound(page)))));
			       ({
				       ((void)(sizeof(
					       (long)(PagePoisoned(page)))));
				       page;
			       });
		       })->flags) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(*(
		  const unsigned long
			  *)(&({
				      ((void)(sizeof((
					      long)(1 && PageCompound(page)))));
				      ({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      });
			      })->flags))) ?
		 generic___set_bit(
			 PG_reported,
			 &({
				  ((void)(sizeof(
					  (long)(1 && PageCompound(page)))));
				  ({
					  ((void)(sizeof(
						  (long)(PagePoisoned(page)))));
					  page;
				  });
			  })->flags) :
		 ___set_bit(PG_reported,
			    &({
				     ((void)(sizeof(
					     (long)(1 && PageCompound(page)))));
				     ({
					     ((void)(sizeof((long)(PagePoisoned(
						     page)))));
					     page;
				     });
			     })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_reported(struct folio *folio)
{
	((__builtin_constant_p(PG_reported) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___clear_bit(PG_reported, folio_flags(folio, 0)) :
		 ___clear_bit(PG_reported, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageReported(struct page *page)
{
	((__builtin_constant_p(PG_reported) &&
	  __builtin_constant_p(
		  (uintptr_t)(&({
				       ((void)(sizeof((
					       long)(1 && PageCompound(page)))));
				       ({
					       ((void)(sizeof(
						       (long)(PagePoisoned(
							       page)))));
					       page;
				       });
			       })->flags) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(&({
			       ((void)(sizeof((long)(1 && PageCompound(page)))));
			       ({
				       ((void)(sizeof(
					       (long)(PagePoisoned(page)))));
				       page;
			       });
		       })->flags) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(*(
		  const unsigned long
			  *)(&({
				      ((void)(sizeof((
					      long)(1 && PageCompound(page)))));
				      ({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      });
			      })->flags))) ?
		 generic___clear_bit(
			 PG_reported,
			 &({
				  ((void)(sizeof(
					  (long)(1 && PageCompound(page)))));
				  ({
					  ((void)(sizeof(
						  (long)(PagePoisoned(page)))));
					  page;
				  });
			  })->flags) :
		 ___clear_bit(
			 PG_reported,
			 &({
				  ((void)(sizeof(
					  (long)(1 && PageCompound(page)))));
				  ({
					  ((void)(sizeof(
						  (long)(PagePoisoned(page)))));
					  page;
				  });
			  })->flags));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_vmemmap_self_hosted(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
PageVmemmapSelfHosted(const struct page *page)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_set_vmemmap_self_hosted(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
SetPageVmemmapSelfHosted(struct page *page)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_clear_vmemmap_self_hosted(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ClearPageVmemmapSelfHosted(struct page *page)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_mapping_flags(const struct folio *folio)
{
	return ((unsigned long)folio->mapping & (0x1 | 0x2)) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
PageMappingFlags(const struct page *page)
{
	return ((unsigned long)page->mapping & (0x1 | 0x2)) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_anon(const struct folio *folio)
{
	return ((unsigned long)folio->mapping & 0x1) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
PageAnon(const struct page *page)
{
	return folio_test_anon(
		(_Generic((page),
			 const struct page *: (
				  const struct folio *)_compound_head(page),
			 struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__folio_test_movable(const struct folio *folio)
{
	return ((unsigned long)folio->mapping & (0x1 | 0x2)) == 0x2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
__PageMovable(const struct page *page)
{
	return ((unsigned long)page->mapping & (0x1 | 0x2)) == 0x2;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_ksm(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
PageKsm(const struct page *page)
{
	return 0;
}

u64 stable_page_flags(const struct page *page);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_xor_flags_has_waiters(struct folio *folio, unsigned long mask)
{
	return xor_unlock_is_negative_byte(mask, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_uptodate(const struct folio *folio)
{
	bool ret =
		((__builtin_constant_p(PG_uptodate) &&
		  __builtin_constant_p(
			  (uintptr_t)(const_folio_flags(folio, 0)) !=
			  (uintptr_t)((void *)0)) &&
		  (uintptr_t)(const_folio_flags(folio, 0)) !=
			  (uintptr_t)((void *)0) &&
		  __builtin_constant_p(*(
			  const unsigned long *)(const_folio_flags(folio, 0)))) ?
			 const_test_bit(PG_uptodate,
					const_folio_flags(folio, 0)) :
			 _test_bit(PG_uptodate, const_folio_flags(folio, 0)));
	if (ret)
		do {
			do {
			} while (0);
			do {
				do {
				} while (0);
				__asm__ __volatile__("" : : : "memory");
			} while (0);
		} while (0);

	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
PageUptodate(const struct page *page)
{
	return folio_test_uptodate(
		(_Generic((page),
			 const struct page *: (
				  const struct folio *)_compound_head(page),
			 struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_mark_uptodate(struct folio *folio)
{
	do {
		do {
		} while (0);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	((__builtin_constant_p(PG_uptodate) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___set_bit(PG_uptodate, folio_flags(folio, 0)) :
		 ___set_bit(PG_uptodate, folio_flags(folio, 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_mark_uptodate(struct folio *folio)
{
	do {
		do {
		} while (0);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	set_bit(PG_uptodate, folio_flags(folio, 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__SetPageUptodate(struct page *page)
{
	__folio_mark_uptodate((struct folio *)page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageUptodate(struct page *page)
{
	folio_mark_uptodate((struct folio *)page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_uptodate(struct folio *folio)
{
	clear_bit(PG_uptodate, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageUptodate(struct page *page)
{
	clear_bit(PG_uptodate,
		  &({
			   ((void)(sizeof((long)(1 && PageTail(page)))));
			   ({
				   ((void)(sizeof((long)(PagePoisoned(
					   ((typeof(page))_compound_head(
						   page)))))));
				   ((typeof(page))_compound_head(page));
			   });
		   })->flags);
}

void __folio_start_writeback(struct folio *folio, bool keep_write);
void set_page_writeback(struct page *page);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_head(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_head) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_head, const_folio_flags(folio, 0)) :
			_test_bit(PG_head, const_folio_flags(folio, 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageHead(const struct page *page)
{
	({
		((void)(sizeof((long)(PagePoisoned(page)))));
		page;
	});
	return ((__builtin_constant_p(PG_head) &&
		 __builtin_constant_p((uintptr_t)(&page->flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(const unsigned long *)(&page->flags))) ?
			const_test_bit(PG_head, &page->flags) :
			_test_bit(PG_head, &page->flags)) &&
	       !page_is_fake_head(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_head(struct folio *folio)
{
	((__builtin_constant_p(PG_head) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___set_bit(PG_head, folio_flags(folio, 0)) :
		 ___set_bit(PG_head, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__SetPageHead(struct page *page)
{
	((__builtin_constant_p(PG_head) &&
	  __builtin_constant_p(
		  (uintptr_t)(&({
				       ((void)(sizeof(
					       (long)(PagePoisoned(page)))));
				       page;
			       })->flags) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(&({
			       ((void)(sizeof((long)(PagePoisoned(page)))));
			       page;
		       })->flags) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(&({
						    ((void)(sizeof(
							    (long)(PagePoisoned(
								    page)))));
						    page;
					    })->flags))) ?
		 generic___set_bit(
			 PG_head,
			 &({
				  ((void)(sizeof((long)(PagePoisoned(page)))));
				  page;
			  })->flags) :
		 ___set_bit(
			 PG_head,
			 &({
				  ((void)(sizeof((long)(PagePoisoned(page)))));
				  page;
			  })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_head(struct folio *folio)
{
	((__builtin_constant_p(PG_head) &&
	  __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) !=
			       (uintptr_t)((void *)0)) &&
	  (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(folio_flags(folio, 0)))) ?
		 generic___clear_bit(PG_head, folio_flags(folio, 0)) :
		 ___clear_bit(PG_head, folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageHead(struct page *page)
{
	((__builtin_constant_p(PG_head) &&
	  __builtin_constant_p(
		  (uintptr_t)(&({
				       ((void)(sizeof(
					       (long)(PagePoisoned(page)))));
				       page;
			       })->flags) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(&({
			       ((void)(sizeof((long)(PagePoisoned(page)))));
			       page;
		       })->flags) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(&({
						    ((void)(sizeof(
							    (long)(PagePoisoned(
								    page)))));
						    page;
					    })->flags))) ?
		 generic___clear_bit(
			 PG_head,
			 &({
				  ((void)(sizeof((long)(PagePoisoned(page)))));
				  page;
			  })->flags) :
		 ___clear_bit(
			 PG_head,
			 &({
				  ((void)(sizeof((long)(PagePoisoned(page)))));
				  page;
			  })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_head(struct folio *folio)
{
	clear_bit(PG_head, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageHead(struct page *page)
{
	clear_bit(PG_head,
		  &({
			   ((void)(sizeof((long)(PagePoisoned(page)))));
			   page;
		   })->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_large(const struct folio *folio)
{
	return folio_test_head(folio);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
set_compound_head(struct page *page, struct page *head)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_347(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(page->compound_head) == sizeof(char) ||
			       sizeof(page->compound_head) == sizeof(short) ||
			       sizeof(page->compound_head) == sizeof(int) ||
			       sizeof(page->compound_head) == sizeof(long)) ||
			      sizeof(page->compound_head) == sizeof(long long)))
				__compiletime_assert_347();
		} while (0);
		do {
			*(volatile typeof(page->compound_head) *)&(
				page->compound_head) =
				((unsigned long)head + 1);
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
clear_compound_head(struct page *page)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_348(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(page->compound_head) == sizeof(char) ||
			       sizeof(page->compound_head) == sizeof(short) ||
			       sizeof(page->compound_head) == sizeof(int) ||
			       sizeof(page->compound_head) == sizeof(long)) ||
			      sizeof(page->compound_head) == sizeof(long long)))
				__compiletime_assert_348();
		} while (0);
		do {
			*(volatile typeof(page->compound_head) *)&(
				page->compound_head) = (0);
		} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_large_rmappable(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_set_large_rmappable(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_clear_large_rmappable(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_partially_mapped(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__folio_set_partially_mapped(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__folio_clear_partially_mapped(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_transhuge(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
PageTransHuge(const struct page *page)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_transcompound(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
PageTransCompound(const struct page *page)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_transcompoundmap(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
PageTransCompoundMap(const struct page *page)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_transtail(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
PageTransTail(const struct page *page)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_has_hwpoisoned(const struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
PageHasHWPoisoned(const struct page *page)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_set_has_hwpoisoned(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
SetPageHasHWPoisoned(struct page *page)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
folio_clear_has_hwpoisoned(struct folio *folio)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ClearPageHasHWPoisoned(struct page *page)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_set_has_hwpoisoned(struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
TestSetPageHasHWPoisoned(struct page *page)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_test_clear_has_hwpoisoned(struct folio *folio)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
TestClearPageHasHWPoisoned(struct page *page)
{
	return 0;
}
enum pagetype {

	PGTY_buddy = 0xf0,
	PGTY_offline = 0xf1,
	PGTY_table = 0xf2,
	PGTY_guard = 0xf3,
	PGTY_hugetlb = 0xf4,
	PGTY_slab = 0xf5,
	PGTY_zsmalloc = 0xf6,
	PGTY_unaccepted = 0xf7,

	PGTY_mapcount_underflow = 0xff
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
page_type_has_type(int page_type)
{
	return page_type < (PGTY_mapcount_underflow << 24);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
page_mapcount_is_type(unsigned int mapcount)
{
	return page_type_has_type(mapcount - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
page_has_type(const struct page *page)
{
	return page_mapcount_is_type(({
		__kcsan_disable_current();
		__auto_type __v = (page->page_type);
		__kcsan_enable_current();
		__v;
	}));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_buddy(const struct folio *folio)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (folio->page.page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_buddy;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_buddy(struct folio *folio)
{
	if (folio_test_buddy(folio))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (folio->page.page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	folio->page.page_type = (unsigned int)PGTY_buddy << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_buddy(struct folio *folio)
{
	if (folio->page.page_type == (~0U))
		return;
	((void)(sizeof((long)(!folio_test_buddy(folio)))));
	folio->page.page_type = (~0U);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageBuddy(const struct page *page)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (page->page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_buddy;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__SetPageBuddy(struct page *page)
{
	if (PageBuddy(page))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (page->page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	page->page_type = (unsigned int)PGTY_buddy << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageBuddy(struct page *page)
{
	if (page->page_type == (~0U))
		return;
	((void)(sizeof((long)(!PageBuddy(page)))));
	page->page_type = (~0U);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_offline(const struct folio *folio)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (folio->page.page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_offline;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_offline(struct folio *folio)
{
	if (folio_test_offline(folio))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (folio->page.page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	folio->page.page_type = (unsigned int)PGTY_offline << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_offline(struct folio *folio)
{
	if (folio->page.page_type == (~0U))
		return;
	((void)(sizeof((long)(!folio_test_offline(folio)))));
	folio->page.page_type = (~0U);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageOffline(const struct page *page)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (page->page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_offline;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__SetPageOffline(struct page *page)
{
	if (PageOffline(page))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (page->page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	page->page_type = (unsigned int)PGTY_offline << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageOffline(struct page *page)
{
	if (page->page_type == (~0U))
		return;
	((void)(sizeof((long)(!PageOffline(page)))));
	page->page_type = (~0U);
}

extern void page_offline_freeze(void);
extern void page_offline_thaw(void);
extern void page_offline_begin(void);
extern void page_offline_end(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_pgtable(const struct folio *folio)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (folio->page.page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_table;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_pgtable(struct folio *folio)
{
	if (folio_test_pgtable(folio))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (folio->page.page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	folio->page.page_type = (unsigned int)PGTY_table << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_pgtable(struct folio *folio)
{
	if (folio->page.page_type == (~0U))
		return;
	((void)(sizeof((long)(!folio_test_pgtable(folio)))));
	folio->page.page_type = (~0U);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageTable(const struct page *page)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (page->page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_table;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__SetPageTable(struct page *page)
{
	if (PageTable(page))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (page->page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	page->page_type = (unsigned int)PGTY_table << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageTable(struct page *page)
{
	if (page->page_type == (~0U))
		return;
	((void)(sizeof((long)(!PageTable(page)))));
	page->page_type = (~0U);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_guard(const struct folio *folio)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (folio->page.page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_guard;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_guard(struct folio *folio)
{
	if (folio_test_guard(folio))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (folio->page.page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	folio->page.page_type = (unsigned int)PGTY_guard << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_guard(struct folio *folio)
{
	if (folio->page.page_type == (~0U))
		return;
	((void)(sizeof((long)(!folio_test_guard(folio)))));
	folio->page.page_type = (~0U);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageGuard(const struct page *page)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (page->page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_guard;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__SetPageGuard(struct page *page)
{
	if (PageGuard(page))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (page->page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	page->page_type = (unsigned int)PGTY_guard << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageGuard(struct page *page)
{
	if (page->page_type == (~0U))
		return;
	((void)(sizeof((long)(!PageGuard(page)))));
	page->page_type = (~0U);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_slab(const struct folio *folio)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (folio->page.page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_slab;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_slab(struct folio *folio)
{
	if (folio_test_slab(folio))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (folio->page.page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	folio->page.page_type = (unsigned int)PGTY_slab << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_slab(struct folio *folio)
{
	if (folio->page.page_type == (~0U))
		return;
	((void)(sizeof((long)(!folio_test_slab(folio)))));
	folio->page.page_type = (~0U);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
PageSlab(const struct page *page)
{
	return folio_test_slab(
		(_Generic((page),
			 const struct page *: (
				  const struct folio *)_compound_head(page),
			 struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_hugetlb(const struct folio *folio)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (folio->page.page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_hugetlb;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_hugetlb(struct folio *folio)
{
	if (folio_test_hugetlb(folio))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (folio->page.page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	folio->page.page_type = (unsigned int)PGTY_hugetlb << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_hugetlb(struct folio *folio)
{
	if (folio->page.page_type == (~0U))
		return;
	((void)(sizeof((long)(!folio_test_hugetlb(folio)))));
	folio->page.page_type = (~0U);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_zsmalloc(const struct folio *folio)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (folio->page.page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_zsmalloc;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_zsmalloc(struct folio *folio)
{
	if (folio_test_zsmalloc(folio))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (folio->page.page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	folio->page.page_type = (unsigned int)PGTY_zsmalloc << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_zsmalloc(struct folio *folio)
{
	if (folio->page.page_type == (~0U))
		return;
	((void)(sizeof((long)(!folio_test_zsmalloc(folio)))));
	folio->page.page_type = (~0U);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageZsmalloc(const struct page *page)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (page->page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_zsmalloc;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__SetPageZsmalloc(struct page *page)
{
	if (PageZsmalloc(page))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (page->page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	page->page_type = (unsigned int)PGTY_zsmalloc << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageZsmalloc(struct page *page)
{
	if (page->page_type == (~0U))
		return;
	((void)(sizeof((long)(!PageZsmalloc(page)))));
	page->page_type = (~0U);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_unaccepted(const struct folio *folio)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (folio->page.page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_unaccepted;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_set_unaccepted(struct folio *folio)
{
	if (folio_test_unaccepted(folio))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (folio->page.page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	folio->page.page_type = (unsigned int)PGTY_unaccepted << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__folio_clear_unaccepted(struct folio *folio)
{
	if (folio->page.page_type == (~0U))
		return;
	((void)(sizeof((long)(!folio_test_unaccepted(folio)))));
	folio->page.page_type = (~0U);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageUnaccepted(const struct page *page)
{
	return ({
		       __kcsan_disable_current();
		       __auto_type __v = (page->page_type >> 24);
		       __kcsan_enable_current();
		       __v;
	       }) == PGTY_unaccepted;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__SetPageUnaccepted(struct page *page)
{
	if (PageUnaccepted(page))
		return;
	((void)(sizeof((long)(({
				      __kcsan_disable_current();
				      __auto_type __v = (page->page_type);
				      __kcsan_enable_current();
				      __v;
			      }) != (~0U)))));
	page->page_type = (unsigned int)PGTY_unaccepted << 24;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageUnaccepted(struct page *page)
{
	if (page->page_type == (~0U))
		return;
	((void)(sizeof((long)(!PageUnaccepted(page)))));
	page->page_type = (~0U);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
PageHuge(const struct page *page)
{
	return folio_test_hugetlb(
		(_Generic((page),
			 const struct page *: (
				  const struct folio *)_compound_head(page),
			 struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_page_hwpoison(const struct page *page)
{
	const struct folio *folio;

	if (PageHWPoison(page))
		return true;
	folio = (_Generic((page),
			 const struct page *: (
				  const struct folio *)_compound_head(page),
			 struct page *: (struct folio *)_compound_head(page)));
	return folio_test_hugetlb(folio) && PageHWPoison(&folio->page);
}

bool is_free_buddy_page(const struct page *page);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
folio_test_isolated(const struct folio *folio)
{
	return ((__builtin_constant_p(PG_isolated) &&
		 __builtin_constant_p((uintptr_t)(const_folio_flags(folio, 0)) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(const_folio_flags(folio, 0)) !=
			 (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long *)(const_folio_flags(folio, 0)))) ?
			const_test_bit(PG_isolated,
				       const_folio_flags(folio, 0)) :
			_test_bit(PG_isolated, const_folio_flags(folio, 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageIsolated(const struct page *page)
{
	return ((__builtin_constant_p(PG_isolated) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof(
					      (long)(PagePoisoned(page)))));
				      page;
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof((long)(PagePoisoned(
						     page)))));
					     page;
				     })->flags))) ?
			const_test_bit(
				PG_isolated,
				&({
					 ((void)(sizeof(
						 (long)(PagePoisoned(page)))));
					 page;
				 })->flags) :
			_test_bit(PG_isolated,
				  &({
					   ((void)(sizeof((
						   long)(PagePoisoned(page)))));
					   page;
				   })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_set_isolated(struct folio *folio)
{
	set_bit(PG_isolated, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageIsolated(struct page *page)
{
	set_bit(PG_isolated,
		&({
			 ((void)(sizeof((long)(PagePoisoned(page)))));
			 page;
		 })->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
folio_clear_isolated(struct folio *folio)
{
	clear_bit(PG_isolated, folio_flags(folio, 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageIsolated(struct page *page)
{
	clear_bit(PG_isolated,
		  &({
			   ((void)(sizeof((long)(PagePoisoned(page)))));
			   page;
		   })->flags);
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
PageAnonExclusive(const struct page *page)
{
	((void)(sizeof((long)(!PageAnon(page)))));

	if (PageHuge(page))
		page = ((typeof(page))_compound_head(page));
	return ((__builtin_constant_p(PG_anon_exclusive) &&
		 __builtin_constant_p(
			 (uintptr_t)(&({
					      ((void)(sizeof((long)(PagePoisoned(
						      page)))));
					      page;
				      })->flags) != (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&({
				      ((void)(sizeof(
					      (long)(PagePoisoned(page)))));
				      page;
			      })->flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(
			 const unsigned long
				 *)(&({
					     ((void)(sizeof((long)(PagePoisoned(
						     page)))));
					     page;
				     })->flags))) ?
			const_test_bit(
				PG_anon_exclusive,
				&({
					 ((void)(sizeof(
						 (long)(PagePoisoned(page)))));
					 page;
				 })->flags) :
			_test_bit(PG_anon_exclusive,
				  &({
					   ((void)(sizeof((
						   long)(PagePoisoned(page)))));
					   page;
				   })->flags));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
SetPageAnonExclusive(struct page *page)
{
	((void)(sizeof((long)(!PageAnon(page) || PageKsm(page)))));
	((void)(sizeof((long)(PageHuge(page) && !PageHead(page)))));
	set_bit(PG_anon_exclusive,
		&({
			 ((void)(sizeof((long)(PagePoisoned(page)))));
			 page;
		 })->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
ClearPageAnonExclusive(struct page *page)
{
	((void)(sizeof((long)(!PageAnon(page) || PageKsm(page)))));
	((void)(sizeof((long)(PageHuge(page) && !PageHead(page)))));
	clear_bit(PG_anon_exclusive,
		  &({
			   ((void)(sizeof((long)(PagePoisoned(page)))));
			   page;
		   })->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__ClearPageAnonExclusive(struct page *page)
{
	((void)(sizeof((long)(!PageAnon(page)))));
	((void)(sizeof((long)(PageHuge(page) && !PageHead(page)))));
	((__builtin_constant_p(PG_anon_exclusive) &&
	  __builtin_constant_p(
		  (uintptr_t)(&({
				       ((void)(sizeof(
					       (long)(PagePoisoned(page)))));
				       page;
			       })->flags) != (uintptr_t)((void *)0)) &&
	  (uintptr_t)(&({
			       ((void)(sizeof((long)(PagePoisoned(page)))));
			       page;
		       })->flags) != (uintptr_t)((void *)0) &&
	  __builtin_constant_p(
		  *(const unsigned long *)(&({
						    ((void)(sizeof(
							    (long)(PagePoisoned(
								    page)))));
						    page;
					    })->flags))) ?
		 generic___clear_bit(
			 PG_anon_exclusive,
			 &({
				  ((void)(sizeof((long)(PagePoisoned(page)))));
				  page;
			  })->flags) :
		 ___clear_bit(
			 PG_anon_exclusive,
			 &({
				  ((void)(sizeof((long)(PagePoisoned(page)))));
				  page;
			  })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
folio_has_private(const struct folio *folio)
{
	return !!(folio->flags & (1UL << PG_private | 1UL << PG_private_2));
}

typedef struct {
} local_lock_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
local_lock_acquire(local_lock_t *l)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
local_lock_release(local_lock_t *l)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
local_lock_debug_init(local_lock_t *l)
{
}
typedef local_lock_t *class_local_lock_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_local_lock_destructor(local_lock_t **p)
{
	local_lock_t *_T = *p;
	if (_T) {
		do {
			local_lock_release(({
				do {
					const void *__vpp_verify =
						(typeof((_T) + 0))((void *)0);
					(void)__vpp_verify;
				} while (0);
				({
					unsigned long tcp_ptr__ = ({
						u64 pfo_val__;
						asm("mov"
						    "q "
						    "%%"
						    "gs"
						    ":"
						    "%"
						    "[var]"
						    ", "
						    "%[val]"
						    : [val] "="
							    "r"(pfo_val__)
						    : [var] "m"((*(
							    typeof(*(&(
								    this_cpu_off)))
								    *)(uintptr_t)(&(
							    this_cpu_off)))));
						(typeof(this_cpu_off))(unsigned long)
							pfo_val__;
					});
					tcp_ptr__ += (unsigned long)(_T);
					(typeof(*(_T)) *)tcp_ptr__;
				});
			}));
			do {
				__asm__ __volatile__("" : : : "memory");
				if (__builtin_expect(
					    !!(__preempt_count_dec_and_test()),
					    0))
					do {
						static void *__attribute__((
							__used__))
						__attribute__((__section__(
							".discard.addressable")))
						__UNIQUE_ID___addressable___SCK__preempt_schedule349 =
							(void *)(uintptr_t)&__SCK__preempt_schedule;
						;
						asm volatile(
							"call "
							"__SCT__preempt_schedule"
							: "+r"(current_stack_pointer));
					} while (0);
			} while (0);
		} while (0);
	};
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) local_lock_t *
class_local_lock_constructor(local_lock_t *_T)
{
	local_lock_t *t = ({
		do {
			do {
				__preempt_count_add(1);
				__asm__ __volatile__("" : : : "memory");
			} while (0);
			local_lock_acquire(({
				do {
					const void *__vpp_verify =
						(typeof((_T) + 0))((void *)0);
					(void)__vpp_verify;
				} while (0);
				({
					unsigned long tcp_ptr__ = ({
						u64 pfo_val__;
						asm("mov"
						    "q "
						    "%%"
						    "gs"
						    ":"
						    "%"
						    "[var]"
						    ", "
						    "%[val]"
						    : [val] "="
							    "r"(pfo_val__)
						    : [var] "m"((*(
							    typeof(*(&(
								    this_cpu_off)))
								    *)(uintptr_t)(&(
							    this_cpu_off)))));
						(typeof(this_cpu_off))(unsigned long)
							pfo_val__;
					});
					tcp_ptr__ += (unsigned long)(_T);
					(typeof(*(_T)) *)tcp_ptr__;
				});
			}));
		} while (0);
		_T;
	});
	return t;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_local_lock_lock_ptr(class_local_lock_t *_T)
{
	return *_T;
}

typedef local_lock_t *class_local_lock_irq_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_local_lock_irq_destructor(local_lock_t **p)
{
	local_lock_t *_T = *p;
	if (_T) {
		do {
			local_lock_release(({
				do {
					const void *__vpp_verify =
						(typeof((_T) + 0))((void *)0);
					(void)__vpp_verify;
				} while (0);
				({
					unsigned long tcp_ptr__ = ({
						u64 pfo_val__;
						asm("mov"
						    "q "
						    "%%"
						    "gs"
						    ":"
						    "%"
						    "[var]"
						    ", "
						    "%[val]"
						    : [val] "="
							    "r"(pfo_val__)
						    : [var] "m"((*(
							    typeof(*(&(
								    this_cpu_off)))
								    *)(uintptr_t)(&(
							    this_cpu_off)))));
						(typeof(this_cpu_off))(unsigned long)
							pfo_val__;
					});
					tcp_ptr__ += (unsigned long)(_T);
					(typeof(*(_T)) *)tcp_ptr__;
				});
			}));
			do {
				arch_local_irq_enable();
			} while (0);
		} while (0);
	};
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) local_lock_t *
class_local_lock_irq_constructor(local_lock_t *_T)
{
	local_lock_t *t = ({
		do {
			do {
				arch_local_irq_disable();
			} while (0);
			local_lock_acquire(({
				do {
					const void *__vpp_verify =
						(typeof((_T) + 0))((void *)0);
					(void)__vpp_verify;
				} while (0);
				({
					unsigned long tcp_ptr__ = ({
						u64 pfo_val__;
						asm("mov"
						    "q "
						    "%%"
						    "gs"
						    ":"
						    "%"
						    "[var]"
						    ", "
						    "%[val]"
						    : [val] "="
							    "r"(pfo_val__)
						    : [var] "m"((*(
							    typeof(*(&(
								    this_cpu_off)))
								    *)(uintptr_t)(&(
							    this_cpu_off)))));
						(typeof(this_cpu_off))(unsigned long)
							pfo_val__;
					});
					tcp_ptr__ += (unsigned long)(_T);
					(typeof(*(_T)) *)tcp_ptr__;
				});
			}));
		} while (0);
		_T;
	});
	return t;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_local_lock_irq_lock_ptr(class_local_lock_irq_t *_T)
{
	return *_T;
}

typedef struct {
	local_lock_t *lock;
	unsigned long flags;
} class_local_lock_irqsave_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_local_lock_irqsave_destructor(class_local_lock_irqsave_t *_T)
{
	if (_T->lock) {
		do {
			local_lock_release(({
				do {
					const void *__vpp_verify =
						(typeof((_T->lock) +
							0))((void *)0);
					(void)__vpp_verify;
				} while (0);
				({
					unsigned long tcp_ptr__ = ({
						u64 pfo_val__;
						asm("mov"
						    "q "
						    "%%"
						    "gs"
						    ":"
						    "%"
						    "[var]"
						    ", "
						    "%[val]"
						    : [val] "="
							    "r"(pfo_val__)
						    : [var] "m"((*(
							    typeof(*(&(
								    this_cpu_off)))
								    *)(uintptr_t)(&(
							    this_cpu_off)))));
						(typeof(this_cpu_off))(unsigned long)
							pfo_val__;
					});
					tcp_ptr__ += (unsigned long)(_T->lock);
					(typeof(*(_T->lock)) *)tcp_ptr__;
				});
			}));
			do {
				do {
					({
						unsigned long __dummy;
						typeof(_T->flags) __dummy2;
						(void)(&__dummy == &__dummy2);
						1;
					});
					do {
					} while (0);
					arch_local_irq_restore(_T->flags);
				} while (0);
			} while (0);
		} while (0);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_local_lock_irqsave_lock_ptr(class_local_lock_irqsave_t *_T)
{
	return _T->lock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) class_local_lock_irqsave_t
class_local_lock_irqsave_constructor(local_lock_t *l)
{
	class_local_lock_irqsave_t _t = { .lock = l }, *_T = &_t;
	do {
		do {
			do {
				({
					unsigned long __dummy;
					typeof(_T->flags) __dummy2;
					(void)(&__dummy == &__dummy2);
					1;
				});
				_T->flags = arch_local_irq_save();
			} while (0);
		} while (0);
		local_lock_acquire(({
			do {
				const void *__vpp_verify =
					(typeof((_T->lock) + 0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			({
				unsigned long tcp_ptr__ = ({
					u64 pfo_val__;
					asm("mov"
					    "q "
					    "%%"
					    "gs"
					    ":"
					    "%"
					    "[var]"
					    ", "
					    "%[val]"
					    : [val] "="
						    "r"(pfo_val__)
					    : [var] "m"((
						    *(typeof(*(&(this_cpu_off)))
							      *)(uintptr_t)(&(
							    this_cpu_off)))));
					(typeof(this_cpu_off))(unsigned long)
						pfo_val__;
				});
				tcp_ptr__ += (unsigned long)(_T->lock);
				(typeof(*(_T->lock)) *)tcp_ptr__;
			});
		}));
	} while (0);
	return _t;
}
typedef local_lock_t *class_local_lock_nested_bh_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_local_lock_nested_bh_destructor(local_lock_t **p)
{
	local_lock_t *_T = *p;
	if (_T) {
		local_lock_release(({
			do {
				const void *__vpp_verify =
					(typeof((_T) + 0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			({
				unsigned long tcp_ptr__ = ({
					u64 pfo_val__;
					asm("mov"
					    "q "
					    "%%"
					    "gs"
					    ":"
					    "%"
					    "[var]"
					    ", "
					    "%[val]"
					    : [val] "="
						    "r"(pfo_val__)
					    : [var] "m"((
						    *(typeof(*(&(this_cpu_off)))
							      *)(uintptr_t)(&(
							    this_cpu_off)))));
					(typeof(this_cpu_off))(unsigned long)
						pfo_val__;
				});
				tcp_ptr__ += (unsigned long)(_T);
				(typeof(*(_T)) *)tcp_ptr__;
			});
		}));
	};
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) local_lock_t *
class_local_lock_nested_bh_constructor(local_lock_t *_T)
{
	local_lock_t *t = ({
		do {
			do {
			} while (0);
			local_lock_acquire(({
				do {
					const void *__vpp_verify =
						(typeof((_T) + 0))((void *)0);
					(void)__vpp_verify;
				} while (0);
				({
					unsigned long tcp_ptr__ = ({
						u64 pfo_val__;
						asm("mov"
						    "q "
						    "%%"
						    "gs"
						    ":"
						    "%"
						    "[var]"
						    ", "
						    "%[val]"
						    : [val] "="
							    "r"(pfo_val__)
						    : [var] "m"((*(
							    typeof(*(&(
								    this_cpu_off)))
								    *)(uintptr_t)(&(
							    this_cpu_off)))));
						(typeof(this_cpu_off))(unsigned long)
							pfo_val__;
					});
					tcp_ptr__ += (unsigned long)(_T);
					(typeof(*(_T)) *)tcp_ptr__;
				});
			}));
		} while (0);
		_T;
	});
	return t;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_local_lock_nested_bh_lock_ptr(class_local_lock_nested_bh_t *_T)
{
	return *_T;
}

struct lruvec;

extern atomic_t zswap_stored_pages;
struct zswap_lruvec_state {};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
zswap_store(struct folio *folio)
{
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
zswap_load(struct folio *folio)
{
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
zswap_invalidate(swp_entry_t swp)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
zswap_swapon(int type, unsigned long nr_pages)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
zswap_swapoff(int type)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
zswap_memcg_offline_cleanup(struct mem_cgroup *memcg)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
zswap_lruvec_state_init(struct lruvec *lruvec)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
zswap_folio_swapin(struct folio *folio)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
zswap_is_enabled(void)
{
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
zswap_never_enabled(void)
{
	return true;
}
enum migratetype {
	MIGRATE_UNMOVABLE,
	MIGRATE_MOVABLE,
	MIGRATE_RECLAIMABLE,
	MIGRATE_PCPTYPES,
	MIGRATE_HIGHATOMIC = MIGRATE_PCPTYPES,
	MIGRATE_TYPES
};

extern const char *const migratetype_names[MIGRATE_TYPES];
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_migrate_movable(int mt)
{
	return false || mt == MIGRATE_MOVABLE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
migratetype_is_mergeable(int mt)
{
	return mt < MIGRATE_PCPTYPES;
}

extern int page_group_by_mobility_disabled;
struct free_area {
	struct list_head free_list[MIGRATE_TYPES];
	unsigned long nr_free;
};

struct pglist_data;

enum numa_stat_item {
	NUMA_HIT,
	NUMA_MISS,
	NUMA_FOREIGN,
	NUMA_INTERLEAVE_HIT,
	NUMA_LOCAL,
	NUMA_OTHER,
	NR_VM_NUMA_EVENT_ITEMS
};

enum zone_stat_item {

	NR_FREE_PAGES,
	NR_ZONE_LRU_BASE,
	NR_ZONE_INACTIVE_ANON = NR_ZONE_LRU_BASE,
	NR_ZONE_ACTIVE_ANON,
	NR_ZONE_INACTIVE_FILE,
	NR_ZONE_ACTIVE_FILE,
	NR_ZONE_UNEVICTABLE,
	NR_ZONE_WRITE_PENDING,
	NR_MLOCK,

	NR_BOUNCE,

	NR_FREE_CMA_PAGES,

	NR_VM_ZONE_STAT_ITEMS
};

enum node_stat_item {
	NR_LRU_BASE,
	NR_INACTIVE_ANON = NR_LRU_BASE,
	NR_ACTIVE_ANON,
	NR_INACTIVE_FILE,
	NR_ACTIVE_FILE,
	NR_UNEVICTABLE,
	NR_SLAB_RECLAIMABLE_B,
	NR_SLAB_UNRECLAIMABLE_B,
	NR_ISOLATED_ANON,
	NR_ISOLATED_FILE,
	WORKINGSET_NODES,
	WORKINGSET_REFAULT_BASE,
	WORKINGSET_REFAULT_ANON = WORKINGSET_REFAULT_BASE,
	WORKINGSET_REFAULT_FILE,
	WORKINGSET_ACTIVATE_BASE,
	WORKINGSET_ACTIVATE_ANON = WORKINGSET_ACTIVATE_BASE,
	WORKINGSET_ACTIVATE_FILE,
	WORKINGSET_RESTORE_BASE,
	WORKINGSET_RESTORE_ANON = WORKINGSET_RESTORE_BASE,
	WORKINGSET_RESTORE_FILE,
	WORKINGSET_NODERECLAIM,
	NR_ANON_MAPPED,
	NR_FILE_MAPPED,

	NR_FILE_PAGES,
	NR_FILE_DIRTY,
	NR_WRITEBACK,
	NR_WRITEBACK_TEMP,
	NR_SHMEM,
	NR_SHMEM_THPS,
	NR_SHMEM_PMDMAPPED,
	NR_FILE_THPS,
	NR_FILE_PMDMAPPED,
	NR_ANON_THPS,
	NR_VMSCAN_WRITE,
	NR_VMSCAN_IMMEDIATE,
	NR_DIRTIED,
	NR_WRITTEN,
	NR_THROTTLED_WRITTEN,
	NR_KERNEL_MISC_RECLAIMABLE,
	NR_FOLL_PIN_ACQUIRED,
	NR_FOLL_PIN_RELEASED,
	NR_KERNEL_STACK_KB,

	NR_PAGETABLE,
	NR_SECONDARY_PAGETABLE,

	NR_IOMMU_PAGES,

	NR_SWAPCACHE,

	PGDEMOTE_KSWAPD,
	PGDEMOTE_DIRECT,
	PGDEMOTE_KHUGEPAGED,
	NR_VM_NODE_STAT_ITEMS
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
vmstat_item_print_in_thp(enum node_stat_item item)
{
	if (!0)
		return false;

	return item == NR_ANON_THPS || item == NR_FILE_THPS ||
	       item == NR_SHMEM_THPS || item == NR_SHMEM_PMDMAPPED ||
	       item == NR_FILE_PMDMAPPED;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
vmstat_item_in_bytes(int idx)
{
	return (idx == NR_SLAB_RECLAIMABLE_B || idx == NR_SLAB_UNRECLAIMABLE_B);
}
enum lru_list {
	LRU_INACTIVE_ANON = 0,
	LRU_ACTIVE_ANON = 0 + 1,
	LRU_INACTIVE_FILE = 0 + 2,
	LRU_ACTIVE_FILE = 0 + 2 + 1,
	LRU_UNEVICTABLE,
	NR_LRU_LISTS
};

enum vmscan_throttle_state {
	VMSCAN_THROTTLE_WRITEBACK,
	VMSCAN_THROTTLE_ISOLATED,
	VMSCAN_THROTTLE_NOPROGRESS,
	VMSCAN_THROTTLE_CONGESTED,
	NR_VMSCAN_THROTTLE,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_file_lru(enum lru_list lru)
{
	return (lru == LRU_INACTIVE_FILE || lru == LRU_ACTIVE_FILE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_active_lru(enum lru_list lru)
{
	return (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);
}

enum lruvec_flags {
	LRUVEC_CGROUP_CONGESTED,
	LRUVEC_NODE_CONGESTED,
};
struct lruvec;
struct page_vma_mapped_walk;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_init_pgdat(struct pglist_data *pgdat)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_init_lruvec(struct lruvec *lruvec)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
lru_gen_look_around(struct page_vma_mapped_walk *pvmw)
{
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_init_memcg(struct mem_cgroup *memcg)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_exit_memcg(struct mem_cgroup *memcg)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_online_memcg(struct mem_cgroup *memcg)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_offline_memcg(struct mem_cgroup *memcg)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_release_memcg(struct mem_cgroup *memcg)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lru_gen_soft_reclaim(struct mem_cgroup *memcg, int nid)
{
}

struct lruvec {
	struct list_head lists[NR_LRU_LISTS];

	spinlock_t lru_lock;

	unsigned long anon_cost;
	unsigned long file_cost;

	atomic_long_t nonresident_age;

	unsigned long refaults[2];

	unsigned long flags;
	struct zswap_lruvec_state zswap_lruvec_state;
};

typedef unsigned isolate_mode_t;

enum zone_watermarks {
	WMARK_MIN,
	WMARK_LOW,
	WMARK_HIGH,
	WMARK_PROMO,
	NR_WMARK
};
struct per_cpu_pages {
	spinlock_t lock;
	int count;
	int high;
	int high_min;
	int high_max;
	int batch;
	u8 flags;
	u8 alloc_factor;

	u8 expire;

	short free_count;

	struct list_head lists[((MIGRATE_PCPTYPES * (3 + 1)) + 0)];
} __attribute__((__aligned__((1 << (6)))));

struct per_cpu_zonestat {
	s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
	s8 stat_threshold;

	unsigned long vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];
};

struct per_cpu_nodestat {
	s8 stat_threshold;
	s8 vm_node_stat_diff[NR_VM_NODE_STAT_ITEMS];
};

enum zone_type {
	ZONE_DMA,

	ZONE_DMA32,

	ZONE_NORMAL,
	ZONE_MOVABLE,

	__MAX_NR_ZONES

};

struct zone {
	unsigned long _watermark[NR_WMARK];
	unsigned long watermark_boost;

	unsigned long nr_reserved_highatomic;
	unsigned long nr_free_highatomic;
	long lowmem_reserve[4];

	int node;

	struct pglist_data *zone_pgdat;
	struct per_cpu_pages *per_cpu_pageset;
	struct per_cpu_zonestat *per_cpu_zonestats;

	int pageset_high_min;
	int pageset_high_max;
	int pageset_batch;
	unsigned long zone_start_pfn;
	atomic_long_t managed_pages;
	unsigned long spanned_pages;
	unsigned long present_pages;

	const char *name;
	int initialized;

	struct cacheline_padding _pad1_;

	struct free_area free_area[(10 + 1)];

	unsigned long flags;

	spinlock_t lock;

	struct cacheline_padding _pad2_;

	unsigned long percpu_drift_mark;

	unsigned long compact_cached_free_pfn;

	unsigned long compact_cached_migrate_pfn[2];
	unsigned long compact_init_migrate_pfn;
	unsigned long compact_init_free_pfn;
	unsigned int compact_considered;
	unsigned int compact_defer_shift;
	int compact_order_failed;

	bool compact_blockskip_flush;

	bool contiguous;

	struct cacheline_padding _pad3_;

	atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];
	atomic_long_t vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];
} __attribute__((__aligned__(1 << (6))));

enum pgdat_flags {
	PGDAT_DIRTY,

	PGDAT_WRITEBACK,

	PGDAT_RECLAIM_LOCKED,
};

enum zone_flags {
	ZONE_BOOSTED_WATERMARK,

	ZONE_RECLAIM_ACTIVE,
	ZONE_BELOW_HIGH,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
wmark_pages(const struct zone *z, enum zone_watermarks w)
{
	return z->_watermark[w] + z->watermark_boost;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
min_wmark_pages(const struct zone *z)
{
	return wmark_pages(z, WMARK_MIN);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
low_wmark_pages(const struct zone *z)
{
	return wmark_pages(z, WMARK_LOW);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
high_wmark_pages(const struct zone *z)
{
	return wmark_pages(z, WMARK_HIGH);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
promo_wmark_pages(const struct zone *z)
{
	return wmark_pages(z, WMARK_PROMO);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
zone_managed_pages(struct zone *zone)
{
	return (unsigned long)atomic_long_read(&zone->managed_pages);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
zone_cma_pages(struct zone *zone)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
zone_end_pfn(const struct zone *zone)
{
	return zone->zone_start_pfn + zone->spanned_pages;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
zone_spans_pfn(const struct zone *zone, unsigned long pfn)
{
	return zone->zone_start_pfn <= pfn && pfn < zone_end_pfn(zone);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
zone_is_initialized(struct zone *zone)
{
	return zone->initialized;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
zone_is_empty(struct zone *zone)
{
	return zone->spanned_pages == 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) enum zone_type
page_zonenum(const struct page *page)
{
	do {
		kcsan_set_access_mask(
			((1UL << 2) - 1)
			<< (((((sizeof(unsigned long) * 8) - 0) - 6) - 2) *
			    (2 != 0)));
		__kcsan_check_access(&(page->flags), sizeof(page->flags),
				     (1 << 3));
		kcsan_set_access_mask(0);
		kcsan_atomic_next(1);
	} while (0);
	return (page->flags >>
		(((((sizeof(unsigned long) * 8) - 0) - 6) - 2) * (2 != 0))) &
	       ((1UL << 2) - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) enum zone_type
folio_zonenum(const struct folio *folio)
{
	return page_zonenum(&folio->page);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_zone_device_page(const struct page *page)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
zone_device_pages_have_same_pgmap(const struct page *a, const struct page *b)
{
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_is_zone_device(const struct folio *folio)
{
	return is_zone_device_page(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_zone_movable_page(const struct page *page)
{
	return page_zonenum(page) == ZONE_MOVABLE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
folio_is_zone_movable(const struct folio *folio)
{
	return folio_zonenum(folio) == ZONE_MOVABLE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
zone_intersects(struct zone *zone, unsigned long start_pfn,
		unsigned long nr_pages)
{
	if (zone_is_empty(zone))
		return false;
	if (start_pfn >= zone_end_pfn(zone) ||
	    start_pfn + nr_pages <= zone->zone_start_pfn)
		return false;

	return true;
}
enum {
	ZONELIST_FALLBACK,

	ZONELIST_NOFALLBACK,

	MAX_ZONELISTS
};

struct zoneref {
	struct zone *zone;
	int zone_idx;
};
struct zonelist {
	struct zoneref _zonerefs[((1 << 6) * 4) + 1];
};

extern struct page *mem_map;
typedef struct pglist_data {
	struct zone node_zones[4];

	struct zonelist node_zonelists[MAX_ZONELISTS];

	int nr_zones;
	unsigned long node_start_pfn;
	unsigned long node_present_pages;
	unsigned long node_spanned_pages;

	int node_id;
	wait_queue_head_t kswapd_wait;
	wait_queue_head_t pfmemalloc_wait;

	wait_queue_head_t reclaim_wait[NR_VMSCAN_THROTTLE];

	atomic_t nr_writeback_throttled;
	unsigned long nr_reclaim_start;

	struct task_struct *kswapd;
	int kswapd_order;
	enum zone_type kswapd_highest_zoneidx;

	int kswapd_failures;

	int kcompactd_max_order;
	enum zone_type kcompactd_highest_zoneidx;
	wait_queue_head_t kcompactd_wait;
	struct task_struct *kcompactd;
	bool proactive_compact_trigger;

	unsigned long totalreserve_pages;

	unsigned long min_unmapped_pages;
	unsigned long min_slab_pages;

	struct cacheline_padding _pad1_;
	struct lruvec __lruvec;

	unsigned long flags;
	struct cacheline_padding _pad2_;

	struct per_cpu_nodestat *per_cpu_nodestats;
	atomic_long_t vm_stat[NR_VM_NODE_STAT_ITEMS];

	struct memory_tier *memtier;

} pg_data_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
pgdat_end_pfn(pg_data_t *pgdat)
{
	return pgdat->node_start_pfn + pgdat->node_spanned_pages;
}

struct page;
struct zone;
struct pglist_data;
struct mem_section;
struct memory_group;
struct resource;
struct vmem_altmap;
struct dev_pagemap;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned
zone_span_seqbegin(struct zone *zone)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
zone_span_seqretry(struct zone *zone, unsigned iv)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
zone_span_writelock(struct zone *zone)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
zone_span_writeunlock(struct zone *zone)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
zone_seqlock_init(struct zone *zone)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
try_online_node(int nid)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
get_online_mems(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
put_online_mems(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mem_hotplug_begin(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mem_hotplug_done(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
movable_node_is_enabled(void)
{
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mhp_supports_memmap_on_memory(void)
{
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
pgdat_kswapd_lock(pg_data_t *pgdat)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
pgdat_kswapd_unlock(pg_data_t *pgdat)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
pgdat_kswapd_lock_init(pg_data_t *pgdat)
{
}

struct range arch_get_mappable_range(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
pgdat_resize_lock(struct pglist_data *p, unsigned long *f)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
pgdat_resize_unlock(struct pglist_data *p, unsigned long *f)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
pgdat_resize_init(struct pglist_data *pgdat)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
try_offline_node(int nid)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
offline_pages(unsigned long start_pfn, unsigned long nr_pages,
	      struct zone *zone, struct memory_group *group)
{
	return -22;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
remove_memory(u64 start, u64 size)
{
	return -16;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__remove_memory(u64 start, u64 size)
{
}

void build_all_zonelists(pg_data_t *pgdat);
void wakeup_kswapd(struct zone *zone, gfp_t gfp_mask, int order,
		   enum zone_type highest_zoneidx);
bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
			 int highest_zoneidx, unsigned int alloc_flags,
			 long free_pages);
bool zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
		       int highest_zoneidx, unsigned int alloc_flags);
bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
			    unsigned long mark, int highest_zoneidx);

enum meminit_context {
	MEMINIT_EARLY,
	MEMINIT_HOTPLUG,
};

extern void init_currently_empty_zone(struct zone *zone,
				      unsigned long start_pfn,
				      unsigned long size);

extern void lruvec_init(struct lruvec *lruvec);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct pglist_data *
lruvec_pgdat(struct lruvec *lruvec)
{
	return ({
		void *__mptr = (void *)(lruvec);
		_Static_assert(
			__builtin_types_compatible_p(
				typeof(*(lruvec)),
				typeof(((struct pglist_data *)0)->__lruvec)) ||
				__builtin_types_compatible_p(typeof(*(lruvec)),
							     typeof(void)),
			"pointer type mismatch in container_of()");
		((struct pglist_data *)(__mptr -
					__builtin_offsetof(struct pglist_data,
							   __lruvec)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
local_memory_node(int node_id)
{
	return node_id;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
zone_is_zone_device(struct zone *zone)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
managed_zone(struct zone *zone)
{
	return zone_managed_pages(zone);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
populated_zone(struct zone *zone)
{
	return zone->present_pages;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
zone_to_nid(struct zone *zone)
{
	return zone->node;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
zone_set_nid(struct zone *zone, int nid)
{
	zone->node = nid;
}
extern int movable_zone;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
is_highmem_idx(enum zone_type idx)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
is_highmem(struct zone *zone)
{
	return is_highmem_idx(((zone) - (zone)->zone_pgdat->node_zones));
}

bool has_managed_dma(void);

extern struct pglist_data *first_online_pgdat(void);
extern struct pglist_data *next_online_pgdat(struct pglist_data *pgdat);
extern struct zone *next_zone(struct zone *zone);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct zone *
zonelist_zone(struct zoneref *zoneref)
{
	return zoneref->zone;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
zonelist_zone_idx(struct zoneref *zoneref)
{
	return zoneref->zone_idx;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
zonelist_node_idx(struct zoneref *zoneref)
{
	return zone_to_nid(zoneref->zone);
}

struct zoneref *__next_zones_zonelist(struct zoneref *z,
				      enum zone_type highest_zoneidx,
				      nodemask_t *nodes);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct zoneref *
next_zones_zonelist(struct zoneref *z, enum zone_type highest_zoneidx,
		    nodemask_t *nodes)
{
	if (__builtin_expect(
		    !!(!nodes && zonelist_zone_idx(z) <= highest_zoneidx), 1))
		return z;
	return __next_zones_zonelist(z, highest_zoneidx, nodes);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct zoneref *
first_zones_zonelist(struct zonelist *zonelist, enum zone_type highest_zoneidx,
		     nodemask_t *nodes)
{
	return next_zones_zonelist(zonelist->_zonerefs, highest_zoneidx, nodes);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
movable_only_nodes(nodemask_t *nodes)
{
	struct zonelist *zonelist;
	struct zoneref *z;
	int nid;

	if (__nodes_empty(&(*nodes), (1 << 6)))
		return false;

	nid = __first_node(&(*nodes));
	zonelist = &(node_data[nid])->node_zonelists[ZONELIST_FALLBACK];
	z = first_zones_zonelist(zonelist, ZONE_NORMAL, nodes);
	return (!zonelist_zone(z)) ? true : false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
pfn_to_section_nr(unsigned long pfn)
{
	return pfn >> (27 - 12);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
section_nr_to_pfn(unsigned long sec)
{
	return sec << (27 - 12);
}
struct mem_section_usage {
	struct callback_head rcu;

	unsigned long subsection_map[(
		(((1UL << (27 - 21))) + ((sizeof(long) * 8)) - 1) /
		((sizeof(long) * 8)))];

	unsigned long pageblock_flags[0];
};

void subsection_map_init(unsigned long pfn, unsigned long nr_pages);

struct page;
struct page_ext;
struct mem_section {
	unsigned long section_mem_map;

	struct mem_section_usage *usage;
};
extern struct mem_section **mem_section;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long *
section_to_usemap(struct mem_section *ms)
{
	return ms->usage->pageblock_flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct mem_section *
__nr_to_section(unsigned long nr)
{
	unsigned long root =
		((nr) / (((1UL) << 12) / sizeof(struct mem_section)));

	if (__builtin_expect(
		    !!(root >=
		       ((((1UL
			   << (((__builtin_constant_p((16 * 32 + 16)) &&
						 (((((16 * 32 + 16)) >> 5) ==
							   (0) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    ((1 << ((0 * 32 + 1) &
							    31))))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (1) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (2) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (3) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    ((1 << ((3 * 32 + 2) & 31)) |
						     (1 << ((3 * 32 + 3) & 31)) |
						     (1 << ((3 * 32 + 1) &
							    31))))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (4) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    (0))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (5) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (6) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (7) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    (0))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (8) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    ((1 << ((8 * 32 + 16) & 31)) |
						     (1 << ((8 * 32 + 22) &
							    31))))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (9) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    ((1 << ((9 * 32 + 2) &
							    31))))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (10) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (11) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    (0 | 0 | 0 | 0 |
						     (1 << ((11 * 32 + 23) &
							    31))))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (12) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    ((1 << ((12 * 32 + 17) &
							    31)) |
						     (1 << ((12 * 32 + 26) &
							    31))))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (13) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (14) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (15) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (16) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    (0 | 0 | 0 | 0 |
						     (1 << ((16 * 32 + 29) &
							    31))))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (17) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (18) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    (0))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (19) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    ((1 << ((19 * 32 + 4) &
							    31))))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (20) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (21) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((int)(sizeof(struct {
							  int : (-!!(22 != 22));
						  }))) ||
						  ((int)(sizeof(struct {
							  int : (-!!(22 != 22));
						  })))) ?
					 0 :
					 (__builtin_constant_p((
						  __builtin_constant_p(
							  (16 * 32 + 16)) &&
								  (((((16 * 32 +
								       16)) >>
								     5) == (0) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     ((1
								       << ((0 * 32 +
									    0) &
									   31)) |
								      (1 << ((0 * 32 +
									      3)) &
								       31) |
								      (1
								       << ((0 * 32 +
									    5) &
									   31)) |
								      (1
								       << ((0 * 32 +
									    6) &
									   31)) |
								      (1
								       << ((0 * 32 +
									    8) &
									   31)) |
								      (1 << ((0 * 32 +
									      13)) &
								       31) |
								      (1
								       << ((0 * 32 +
									    24) &
									   31)) |
								      (1
								       << ((0 * 32 +
									    15) &
									   31)) |
								      (1
								       << ((0 * 32 +
									    25) &
									   31)) |
								      (1
								       << ((0 * 32 +
									    26) &
									   31))))) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (1) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     ((1
								       << ((1 * 32 +
									    29) &
									   31)) |
								      0))) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (2) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (3) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     ((1
								       << ((3 * 32 +
									    20) &
									   31))))) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (4) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     (0))) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (5) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (6) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (7) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (8) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (9) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (10) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (11) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (12) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (13) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (14) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (15) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (16) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (17) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (18) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (19) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (20) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((((16 * 32 +
								       16)) >>
								     5) == (21) &&
								    (1UL << (((16 * 32 +
									       16)) &
									     31) &
								     0)) ||
								   ((int)(sizeof(struct {
									   int : (-!!(22 !=
										      22));
								   }))) ||
								   ((int)(sizeof(struct {
									   int : (-!!(22 !=
										      22));
								   })))) ?
							  1 :
							  arch_test_bit(
								  (16 * 32 + 16),
								  (unsigned long
									   *)((&boot_cpu_data)
										      ->x86_capability)))) ?
						  (__builtin_constant_p(
							   (16 * 32 + 16)) &&
								   (((((16 * 32 +
									16)) >>
								      5) == (0) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      ((1
									<< ((0 * 32 +
									     0) &
									    31)) |
								       (1 << ((0 * 32 +
									       3)) &
									31) |
								       (1
									<< ((0 * 32 +
									     5) &
									    31)) |
								       (1
									<< ((0 * 32 +
									     6) &
									    31)) |
								       (1
									<< ((0 * 32 +
									     8) &
									    31)) |
								       (1 << ((0 * 32 +
									       13)) &
									31) |
								       (1
									<< ((0 * 32 +
									     24) &
									    31)) |
								       (1
									<< ((0 * 32 +
									     15) &
									    31)) |
								       (1
									<< ((0 * 32 +
									     25) &
									    31)) |
								       (1
									<< ((0 * 32 +
									     26) &
									    31))))) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (1) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      ((1
									<< ((1 * 32 +
									     29) &
									    31)) |
								       0))) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (2) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (3) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      ((1
									<< ((3 * 32 +
									     20) &
									    31))))) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (4) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      (0))) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (5) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (6) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (7) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (8) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (9) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (10) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (11) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (12) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (13) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (14) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (15) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (16) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (17) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (18) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (19) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (20) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((((16 * 32 +
									16)) >>
								      5) == (21) &&
								     (1UL << (((16 * 32 +
										16)) &
									      31) &
								      0)) ||
								    ((int)(sizeof(struct {
									    int : (-!!(22 !=
										       22));
								    }))) ||
								    ((int)(sizeof(struct {
									    int : (-!!(22 !=
										       22));
								    })))) ?
							   1 :
							   arch_test_bit(
								   (16 * 32 + 16),
								   (unsigned long
									    *)((&boot_cpu_data)
										       ->x86_capability))) :
						  _static_cpu_has(
							  (16 * 32 + 16)))) ?
					52 :
					46) -
			       27))) +
			 ((((1UL) << 12) / sizeof(struct mem_section))) - 1) /
			((((1UL) << 12) / sizeof(struct mem_section))))),
		    0))
		return ((void *)0);

	if (!mem_section || !mem_section[root])
		return ((void *)0);

	return &mem_section[root]
			   [nr &
			    ((((1UL) << 12) / sizeof(struct mem_section)) - 1)];
}
extern size_t mem_section_usage_size(void);
enum {
	SECTION_MARKED_PRESENT_BIT,
	SECTION_HAS_MEM_MAP_BIT,
	SECTION_IS_ONLINE_BIT,
	SECTION_IS_EARLY_BIT,

	SECTION_MAP_LAST_BIT,
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct page *
__section_mem_map_addr(struct mem_section *section)
{
	unsigned long map = section->section_mem_map;
	map &= (~(((((1UL))) << (SECTION_MAP_LAST_BIT)) - 1));
	return (struct page *)map;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
present_section(struct mem_section *section)
{
	return (section && (section->section_mem_map &
			    ((((1UL))) << (SECTION_MARKED_PRESENT_BIT))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
present_section_nr(unsigned long nr)
{
	return present_section(__nr_to_section(nr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
valid_section(struct mem_section *section)
{
	return (section && (section->section_mem_map &
			    ((((1UL))) << (SECTION_HAS_MEM_MAP_BIT))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
early_section(struct mem_section *section)
{
	return (section && (section->section_mem_map &
			    ((((1UL))) << (SECTION_IS_EARLY_BIT))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
valid_section_nr(unsigned long nr)
{
	return valid_section(__nr_to_section(nr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
online_section(struct mem_section *section)
{
	return (section && (section->section_mem_map &
			    ((((1UL))) << (SECTION_IS_ONLINE_BIT))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
online_device_section(struct mem_section *section)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
online_section_nr(unsigned long nr)
{
	return online_section(__nr_to_section(nr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct mem_section *
__pfn_to_section(unsigned long pfn)
{
	return __nr_to_section(pfn_to_section_nr(pfn));
}

extern unsigned long __highest_present_section_nr;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
subsection_map_index(unsigned long pfn)
{
	return (pfn & ~((~((1UL << (27 - 12)) - 1)))) / (1UL << (21 - 12));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pfn_section_valid(struct mem_section *ms, unsigned long pfn)
{
	int idx = subsection_map_index(pfn);
	struct mem_section_usage *usage = ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_350(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(ms->usage) == sizeof(char) ||
			       sizeof(ms->usage) == sizeof(short) ||
			       sizeof(ms->usage) == sizeof(int) ||
			       sizeof(ms->usage) == sizeof(long)) ||
			      sizeof(ms->usage) == sizeof(long long)))
				__compiletime_assert_350();
		} while (0);
		(*(const volatile typeof(_Generic((ms->usage),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (ms->usage)))
			   *)&(ms->usage));
	});

	return usage ? ((__builtin_constant_p(idx) &&
			 __builtin_constant_p(
				 (uintptr_t)(usage->subsection_map) !=
				 (uintptr_t)((void *)0)) &&
			 (uintptr_t)(usage->subsection_map) !=
				 (uintptr_t)((void *)0) &&
			 __builtin_constant_p(*(
				 const unsigned long *)(usage->subsection_map))) ?
				const_test_bit(idx, usage->subsection_map) :
				_test_bit(idx, usage->subsection_map)) :
		       0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pfn_valid(unsigned long pfn)
{
	struct mem_section *ms;
	int ret;

	if (((unsigned long)((((phys_addr_t)(pfn) << 12)) >> 12)) != pfn)
		return 0;

	if (pfn_to_section_nr(pfn) >=
	    (1UL
	     << (((__builtin_constant_p((16 * 32 + 16)) &&
				   (((((16 * 32 + 16)) >> 5) == (0) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((0 * 32 + 1) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (1) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (2) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (3) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((3 * 32 + 2) & 31)) |
				       (1 << ((3 * 32 + 3) & 31)) |
				       (1 << ((3 * 32 + 1) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (4) &&
				     (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
				    ((((16 * 32 + 16)) >> 5) == (5) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (6) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (7) &&
				     (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
				    ((((16 * 32 + 16)) >> 5) == (8) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((8 * 32 + 16) & 31)) |
				       (1 << ((8 * 32 + 22) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (9) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((9 * 32 + 2) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (10) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (11) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      (0 | 0 | 0 | 0 |
				       (1 << ((11 * 32 + 23) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (12) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((12 * 32 + 17) & 31)) |
				       (1 << ((12 * 32 + 26) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (13) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (14) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (15) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (16) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      (0 | 0 | 0 | 0 |
				       (1 << ((16 * 32 + 29) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (17) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (18) &&
				     (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
				    ((((16 * 32 + 16)) >> 5) == (19) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((19 * 32 + 4) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (20) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (21) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((int)(sizeof(struct {
					    int : (-!!(22 != 22));
				    }))) ||
				    ((int)(sizeof(struct {
					    int : (-!!(22 != 22));
				    })))) ?
			   0 :
			   (__builtin_constant_p((
				    __builtin_constant_p((16 * 32 + 16)) &&
						    (((((16 * 32 + 16)) >> 5) ==
							      (0) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       ((1 << ((0 * 32 + 0) &
							       31)) |
							(1 << ((0 * 32 + 3)) &
							 31) |
							(1 << ((0 * 32 + 5) &
							       31)) |
							(1 << ((0 * 32 + 6) &
							       31)) |
							(1 << ((0 * 32 + 8) &
							       31)) |
							(1 << ((0 * 32 + 13)) &
							 31) |
							(1 << ((0 * 32 + 24) &
							       31)) |
							(1 << ((0 * 32 + 15) &
							       31)) |
							(1 << ((0 * 32 + 25) &
							       31)) |
							(1 << ((0 * 32 + 26) &
							       31))))) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (1) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       ((1 << ((1 * 32 + 29) &
							       31)) |
							0))) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (2) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (3) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       ((1 << ((3 * 32 + 20) &
							       31))))) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (4) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       (0))) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (5) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (6) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (7) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (8) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (9) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (10) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (11) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (12) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (13) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (14) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (15) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (16) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (17) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (18) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (19) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (20) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (21) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((int)(sizeof(struct {
							     int : (-!!(22 !=
									22));
						     }))) ||
						     ((int)(sizeof(struct {
							     int : (-!!(22 !=
									22));
						     })))) ?
					    1 :
					    arch_test_bit(
						    (16 * 32 + 16),
						    (unsigned long
							     *)((&boot_cpu_data)
									->x86_capability)))) ?
				    (__builtin_constant_p((16 * 32 + 16)) &&
						     (((((16 * 32 + 16)) >>
							5) == (0) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							((1 << ((0 * 32 + 0) &
								31)) |
							 (1 << ((0 * 32 + 3)) &
							  31) |
							 (1 << ((0 * 32 + 5) &
								31)) |
							 (1 << ((0 * 32 + 6) &
								31)) |
							 (1 << ((0 * 32 + 8) &
								31)) |
							 (1 << ((0 * 32 + 13)) &
							  31) |
							 (1 << ((0 * 32 + 24) &
								31)) |
							 (1 << ((0 * 32 + 15) &
								31)) |
							 (1 << ((0 * 32 + 25) &
								31)) |
							 (1 << ((0 * 32 + 26) &
								31))))) ||
						      ((((16 * 32 + 16)) >>
							5) == (1) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							((1 << ((1 * 32 + 29) &
								31)) |
							 0))) ||
						      ((((16 * 32 + 16)) >>
							5) == (2) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (3) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							((1 << ((3 * 32 + 20) &
								31))))) ||
						      ((((16 * 32 + 16)) >>
							5) == (4) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							(0))) ||
						      ((((16 * 32 + 16)) >>
							5) == (5) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (6) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (7) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (8) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (9) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (10) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (11) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (12) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (13) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (14) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (15) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (16) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (17) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (18) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (19) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (20) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (21) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((int)(sizeof(struct {
							      int : (-!!(22 !=
									 22));
						      }))) ||
						      ((int)(sizeof(struct {
							      int : (-!!(22 !=
									 22));
						      })))) ?
					     1 :
					     arch_test_bit(
						     (16 * 32 + 16),
						     (unsigned long
							      *)((&boot_cpu_data)
									 ->x86_capability))) :
				    _static_cpu_has((16 * 32 + 16)))) ?
			  52 :
			  46) -
		 27)))
		return 0;
	ms = __pfn_to_section(pfn);
	rcu_read_lock_sched();
	if (!valid_section(ms)) {
		rcu_read_unlock_sched();
		return 0;
	}

	ret = early_section(ms) || pfn_section_valid(ms, pfn);
	rcu_read_unlock_sched();

	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pfn_in_present_section(unsigned long pfn)
{
	if (pfn_to_section_nr(pfn) >=
	    (1UL
	     << (((__builtin_constant_p((16 * 32 + 16)) &&
				   (((((16 * 32 + 16)) >> 5) == (0) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((0 * 32 + 1) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (1) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (2) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (3) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((3 * 32 + 2) & 31)) |
				       (1 << ((3 * 32 + 3) & 31)) |
				       (1 << ((3 * 32 + 1) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (4) &&
				     (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
				    ((((16 * 32 + 16)) >> 5) == (5) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (6) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (7) &&
				     (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
				    ((((16 * 32 + 16)) >> 5) == (8) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((8 * 32 + 16) & 31)) |
				       (1 << ((8 * 32 + 22) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (9) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((9 * 32 + 2) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (10) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (11) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      (0 | 0 | 0 | 0 |
				       (1 << ((11 * 32 + 23) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (12) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((12 * 32 + 17) & 31)) |
				       (1 << ((12 * 32 + 26) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (13) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (14) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (15) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (16) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      (0 | 0 | 0 | 0 |
				       (1 << ((16 * 32 + 29) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (17) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (18) &&
				     (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
				    ((((16 * 32 + 16)) >> 5) == (19) &&
				     (1UL << (((16 * 32 + 16)) & 31) &
				      ((1 << ((19 * 32 + 4) & 31))))) ||
				    ((((16 * 32 + 16)) >> 5) == (20) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((((16 * 32 + 16)) >> 5) == (21) &&
				     (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				    ((int)(sizeof(struct {
					    int : (-!!(22 != 22));
				    }))) ||
				    ((int)(sizeof(struct {
					    int : (-!!(22 != 22));
				    })))) ?
			   0 :
			   (__builtin_constant_p((
				    __builtin_constant_p((16 * 32 + 16)) &&
						    (((((16 * 32 + 16)) >> 5) ==
							      (0) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       ((1 << ((0 * 32 + 0) &
							       31)) |
							(1 << ((0 * 32 + 3)) &
							 31) |
							(1 << ((0 * 32 + 5) &
							       31)) |
							(1 << ((0 * 32 + 6) &
							       31)) |
							(1 << ((0 * 32 + 8) &
							       31)) |
							(1 << ((0 * 32 + 13)) &
							 31) |
							(1 << ((0 * 32 + 24) &
							       31)) |
							(1 << ((0 * 32 + 15) &
							       31)) |
							(1 << ((0 * 32 + 25) &
							       31)) |
							(1 << ((0 * 32 + 26) &
							       31))))) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (1) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       ((1 << ((1 * 32 + 29) &
							       31)) |
							0))) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (2) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (3) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       ((1 << ((3 * 32 + 20) &
							       31))))) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (4) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       (0))) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (5) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (6) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (7) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (8) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (9) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (10) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (11) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (12) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (13) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (14) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (15) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (16) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (17) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (18) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (19) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (20) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((((16 * 32 + 16)) >> 5) ==
							      (21) &&
						      (1UL << (((16 * 32 + 16)) &
							       31) &
						       0)) ||
						     ((int)(sizeof(struct {
							     int : (-!!(22 !=
									22));
						     }))) ||
						     ((int)(sizeof(struct {
							     int : (-!!(22 !=
									22));
						     })))) ?
					    1 :
					    arch_test_bit(
						    (16 * 32 + 16),
						    (unsigned long
							     *)((&boot_cpu_data)
									->x86_capability)))) ?
				    (__builtin_constant_p((16 * 32 + 16)) &&
						     (((((16 * 32 + 16)) >>
							5) == (0) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							((1 << ((0 * 32 + 0) &
								31)) |
							 (1 << ((0 * 32 + 3)) &
							  31) |
							 (1 << ((0 * 32 + 5) &
								31)) |
							 (1 << ((0 * 32 + 6) &
								31)) |
							 (1 << ((0 * 32 + 8) &
								31)) |
							 (1 << ((0 * 32 + 13)) &
							  31) |
							 (1 << ((0 * 32 + 24) &
								31)) |
							 (1 << ((0 * 32 + 15) &
								31)) |
							 (1 << ((0 * 32 + 25) &
								31)) |
							 (1 << ((0 * 32 + 26) &
								31))))) ||
						      ((((16 * 32 + 16)) >>
							5) == (1) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							((1 << ((1 * 32 + 29) &
								31)) |
							 0))) ||
						      ((((16 * 32 + 16)) >>
							5) == (2) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (3) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							((1 << ((3 * 32 + 20) &
								31))))) ||
						      ((((16 * 32 + 16)) >>
							5) == (4) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							(0))) ||
						      ((((16 * 32 + 16)) >>
							5) == (5) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (6) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (7) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (8) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (9) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (10) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (11) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (12) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (13) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (14) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (15) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (16) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (17) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (18) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (19) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (20) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((((16 * 32 + 16)) >>
							5) == (21) &&
						       (1UL << (((16 * 32 + 16)) &
								31) &
							0)) ||
						      ((int)(sizeof(struct {
							      int : (-!!(22 !=
									 22));
						      }))) ||
						      ((int)(sizeof(struct {
							      int : (-!!(22 !=
									 22));
						      })))) ?
					     1 :
					     arch_test_bit(
						     (16 * 32 + 16),
						     (unsigned long
							      *)((&boot_cpu_data)
									 ->x86_capability))) :
				    _static_cpu_has((16 * 32 + 16)))) ?
			  52 :
			  46) -
		 27)))
		return 0;
	return present_section(__pfn_to_section(pfn));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
next_present_section_nr(unsigned long section_nr)
{
	while (++section_nr <= __highest_present_section_nr) {
		if (present_section_nr(section_nr))
			return section_nr;
	}

	return -1;
}
void sparse_init(void);
void topology_normalize_cpu_scale(void);
int topology_update_cpu_topology(void);

struct device_node;
bool topology_parse_cpu_capacity(struct device_node *cpu_node, int cpu);

extern __attribute__((section(".data..percpu"
			      ""))) __typeof__(unsigned long) cpu_scale;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
topology_get_cpu_scale(int cpu)
{
	return (*({
		do {
			const void *__vpp_verify =
				(typeof((&(cpu_scale)) + 0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((
				typeof(*((&(cpu_scale)))) *)((&(cpu_scale))));
			(typeof((typeof(*((&(cpu_scale)))) *)((
				&(cpu_scale)))))(__ptr +
						 (((__per_cpu_offset[(cpu)]))));
		});
	}));
}

void topology_set_cpu_scale(unsigned int cpu, unsigned long capacity);

extern __attribute__((section(".data..percpu"
			      ""))) __typeof__(unsigned long) capacity_freq_ref;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
topology_get_freq_ref(int cpu)
{
	return (*({
		do {
			const void *__vpp_verify =
				(typeof((&(capacity_freq_ref)) + 0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((
				typeof(*((&(capacity_freq_ref)))) *)((
				&(capacity_freq_ref))));
			(typeof((typeof(*((&(capacity_freq_ref)))) *)((
				&(capacity_freq_ref)))))(__ptr +
							 (((__per_cpu_offset[(
								 cpu)]))));
		});
	}));
}

extern __attribute__((section(".data..percpu"
			      ""))) __typeof__(unsigned long) arch_freq_scale;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
topology_get_freq_scale(int cpu)
{
	return (*({
		do {
			const void *__vpp_verify =
				(typeof((&(arch_freq_scale)) + 0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((typeof(*((
				&(arch_freq_scale)))) *)((&(arch_freq_scale))));
			(typeof((typeof(*((&(arch_freq_scale)))) *)((
				&(arch_freq_scale)))))(__ptr +
						       (((__per_cpu_offset[(
							       cpu)]))));
		});
	}));
}

void topology_set_freq_scale(const struct cpumask *cpus, unsigned long cur_freq,
			     unsigned long max_freq);
bool topology_scale_freq_invariant(void);

enum scale_freq_source {
	SCALE_FREQ_SOURCE_CPUFREQ = 0,
	SCALE_FREQ_SOURCE_ARCH,
	SCALE_FREQ_SOURCE_CPPC,
};

struct scale_freq_data {
	enum scale_freq_source source;
	void (*set_freq_scale)(void);
};

void topology_scale_freq_tick(void);
void topology_set_scale_freq_source(struct scale_freq_data *data,
				    const struct cpumask *cpus);
void topology_clear_scale_freq_source(enum scale_freq_source source,
				      const struct cpumask *cpus);

extern __attribute__((section(".data..percpu"
			      ""))) __typeof__(unsigned long) hw_pressure;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
topology_get_hw_pressure(int cpu)
{
	return (*({
		do {
			const void *__vpp_verify =
				(typeof((&(hw_pressure)) + 0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((typeof(*(
				(&(hw_pressure)))) *)((&(hw_pressure))));
			(typeof((typeof(*((&(hw_pressure)))) *)((&(
				hw_pressure)))))(__ptr +
						 (((__per_cpu_offset[(cpu)]))));
		});
	}));
}

void topology_update_hw_pressure(const struct cpumask *cpus,
				 unsigned long capped_freq);

struct cpu_topology {
	int thread_id;
	int core_id;
	int cluster_id;
	int package_id;
	cpumask_t thread_sibling;
	cpumask_t core_sibling;
	cpumask_t cluster_sibling;
	cpumask_t llc_sibling;
};

struct mpf_intel {
	char signature[4];
	unsigned int physptr;
	unsigned char length;
	unsigned char specification;
	unsigned char checksum;
	unsigned char feature1;
	unsigned char feature2;
	unsigned char feature3;
	unsigned char feature4;
	unsigned char feature5;
};

struct mpc_table {
	char signature[4];
	unsigned short length;
	char spec;
	char checksum;
	char oem[8];
	char productid[12];
	unsigned int oemptr;
	unsigned short oemsize;
	unsigned short oemcount;
	unsigned int lapic;
	unsigned int reserved;
};
struct mpc_cpu {
	unsigned char type;
	unsigned char apicid;
	unsigned char apicver;
	unsigned char cpuflag;
	unsigned int cpufeature;
	unsigned int featureflag;
	unsigned int reserved[2];
};

struct mpc_bus {
	unsigned char type;
	unsigned char busid;
	unsigned char bustype[6];
};
struct mpc_ioapic {
	unsigned char type;
	unsigned char apicid;
	unsigned char apicver;
	unsigned char flags;
	unsigned int apicaddr;
};

struct mpc_intsrc {
	unsigned char type;
	unsigned char irqtype;
	unsigned short irqflag;
	unsigned char srcbus;
	unsigned char srcbusirq;
	unsigned char dstapic;
	unsigned char dstirq;
};

enum mp_irq_source_types { mp_INT = 0, mp_NMI = 1, mp_SMI = 2, mp_ExtINT = 3 };
struct mpc_lintsrc {
	unsigned char type;
	unsigned char irqtype;
	unsigned short irqflag;
	unsigned char srcbusid;
	unsigned char srcbusirq;
	unsigned char destapic;
	unsigned char destapiclint;
};

struct mpc_oemtable {
	char signature[4];
	unsigned short length;
	char rev;
	char checksum;
	char mpc[8];
};
enum mp_bustype {
	MP_BUS_ISA = 1,
	MP_BUS_EISA,
	MP_BUS_PCI,
};

struct ghcb;
struct mpc_bus;
struct mpc_cpu;
struct pt_regs;
struct mpc_table;
struct cpuinfo_x86;
struct irq_domain;
struct x86_init_mpparse {
	void (*setup_ioapic_ids)(void);
	void (*find_mptable)(void);
	void (*early_parse_smp_cfg)(void);
	void (*parse_smp_cfg)(void);
};
struct x86_init_resources {
	void (*probe_roms)(void);
	void (*reserve_resources)(void);
	char *(*memory_setup)(void);
	void (*dmi_setup)(void);
};
struct x86_init_irqs {
	void (*pre_vector_init)(void);
	void (*intr_init)(void);
	void (*intr_mode_select)(void);
	void (*intr_mode_init)(void);
	struct irq_domain *(*create_pci_msi_domain)(void);
};

struct x86_init_oem {
	void (*arch_setup)(void);
	void (*banner)(void);
};
struct x86_init_paging {
	void (*pagetable_init)(void);
};
struct x86_init_timers {
	void (*setup_percpu_clockev)(void);
	void (*timer_init)(void);
	void (*wallclock_init)(void);
};

struct x86_init_iommu {
	int (*iommu_init)(void);
};
struct x86_init_pci {
	int (*arch_init)(void);
	int (*init)(void);
	void (*init_irq)(void);
	void (*fixup_irqs)(void);
};
struct x86_hyper_init {
	void (*init_platform)(void);
	void (*guest_late_init)(void);
	bool (*x2apic_available)(void);
	bool (*msi_ext_dest_id)(void);
	void (*init_mem_mapping)(void);
	void (*init_after_bootmem)(void);
};

struct x86_init_acpi {
	void (*set_root_pointer)(u64 addr);
	u64 (*get_root_pointer)(void);
	void (*reduced_hw_early_init)(void);
};
struct x86_guest {
	int (*enc_status_change_prepare)(unsigned long vaddr, int npages,
					 bool enc);
	int (*enc_status_change_finish)(unsigned long vaddr, int npages,
					bool enc);
	bool (*enc_tlb_flush_required)(bool enc);
	bool (*enc_cache_flush_required)(void);
	void (*enc_kexec_begin)(void);
	void (*enc_kexec_finish)(void);
};

struct x86_init_ops {
	struct x86_init_resources resources;
	struct x86_init_mpparse mpparse;
	struct x86_init_irqs irqs;
	struct x86_init_oem oem;
	struct x86_init_paging paging;
	struct x86_init_timers timers;
	struct x86_init_iommu iommu;
	struct x86_init_pci pci;
	struct x86_hyper_init hyper;
	struct x86_init_acpi acpi;
};
struct x86_cpuinit_ops {
	void (*setup_percpu_clockev)(void);
	void (*early_percpu_clock_init)(void);
	void (*fixup_cpu_id)(struct cpuinfo_x86 *c, int node);
	bool parallel_bringup;
};

struct timespec64;
struct x86_legacy_devices {
	int pnpbios;
};
enum x86_legacy_i8042_state {
	X86_LEGACY_I8042_PLATFORM_ABSENT,
	X86_LEGACY_I8042_FIRMWARE_ABSENT,
	X86_LEGACY_I8042_EXPECTED_PRESENT,
};
struct x86_legacy_features {
	enum x86_legacy_i8042_state i8042;
	int rtc;
	int warm_reset;
	int no_vga;
	int reserve_bios_regions;
	struct x86_legacy_devices devices;
};
struct x86_hyper_runtime {
	void (*pin_vcpu)(int cpu);
	void (*sev_es_hcall_prepare)(struct ghcb *ghcb, struct pt_regs *regs);
	bool (*sev_es_hcall_finish)(struct ghcb *ghcb, struct pt_regs *regs);
	bool (*is_private_mmio)(u64 addr);
};
struct x86_platform_ops {
	unsigned long (*calibrate_cpu)(void);
	unsigned long (*calibrate_tsc)(void);
	void (*get_wallclock)(struct timespec64 *ts);
	int (*set_wallclock)(const struct timespec64 *ts);
	void (*iommu_shutdown)(void);
	bool (*is_untracked_pat_range)(u64 start, u64 end);
	void (*nmi_init)(void);
	unsigned char (*get_nmi_reason)(void);
	void (*save_sched_clock_state)(void);
	void (*restore_sched_clock_state)(void);
	void (*apic_post_init)(void);
	struct x86_legacy_features legacy;
	void (*set_legacy_features)(void);
	void (*realmode_reserve)(void);
	void (*realmode_init)(void);
	struct x86_hyper_runtime hyper;
	struct x86_guest guest;
};

struct x86_apic_ops {
	unsigned int (*io_apic_read)(unsigned int apic, unsigned int reg);
	void (*restore)(void);
};

extern struct x86_init_ops x86_init;
extern struct x86_cpuinit_ops x86_cpuinit;
extern struct x86_platform_ops x86_platform;
extern struct x86_msi_ops x86_msi;
extern struct x86_apic_ops x86_apic_ops;

extern void x86_early_init_platform_quirks(void);
extern void x86_init_noop(void);
extern void x86_init_uint_noop(unsigned int unused);
extern bool bool_x86_init_noop(void);
extern void x86_op_int_noop(int cpu);
extern bool x86_pnpbios_disabled(void);
extern int set_rtc_noop(const struct timespec64 *now);
extern void get_rtc_noop(struct timespec64 *now);

extern int pic_mode;
extern unsigned long mp_bus_not_pci[(((256) + ((sizeof(long) * 8)) - 1) /
				     ((sizeof(long) * 8)))];

extern u32 boot_cpu_physical_apicid;
extern u8 boot_cpu_apic_version;

extern int smp_found_config;

extern void e820__memblock_alloc_reserved_mpc_new(void);
extern int enable_update_mptable;
extern void mpparse_find_mptable(void);
extern void mpparse_parse_early_smp_config(void);
extern void mpparse_parse_smp_config(void);
extern unsigned long phys_cpu_present_map[(
	((32768) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
reset_phys_cpu_present_map(u32 apicid)
{
	bitmap_zero(phys_cpu_present_map, 32768);
	set_bit(apicid, phys_cpu_present_map);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
copy_phys_cpu_present_map(unsigned long *dst)
{
	bitmap_copy(dst, phys_cpu_present_map, 32768);
}

extern __attribute__((section(".data..percpu"
			      ""))) __typeof__(int) x86_cpu_to_node_map;
extern __typeof__(int) *x86_cpu_to_node_map_early_ptr;
extern __typeof__(int) x86_cpu_to_node_map_early_map[];
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
early_cpu_to_node(int cpu)
{
	return *((x86_cpu_to_node_map_early_ptr) ? &(x86_cpu_to_node_map_early_ptr)[cpu] : &(*({
		do {
			const void *__vpp_verify =
				(typeof((&(x86_cpu_to_node_map)) +
					0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((
				typeof(*((&(x86_cpu_to_node_map)))) *)((
				&(x86_cpu_to_node_map))));
			(typeof((typeof(*((&(x86_cpu_to_node_map)))) *)((
				&(x86_cpu_to_node_map)))))(__ptr +
							   (((__per_cpu_offset[(
								   cpu)]))));
		});
	})));
}

extern cpumask_var_t node_to_cpumask_map[(1 << 6)];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const struct cpumask *
cpumask_of_node(int node)
{
	return node_to_cpumask_map[node];
}

extern void setup_node_to_cpumask_map(void);

extern int __node_distance(int, int);

enum x86_topology_domains {
	TOPO_SMT_DOMAIN,
	TOPO_CORE_DOMAIN,
	TOPO_MODULE_DOMAIN,
	TOPO_TILE_DOMAIN,
	TOPO_DIE_DOMAIN,
	TOPO_DIEGRP_DOMAIN,
	TOPO_PKG_DOMAIN,
	TOPO_MAX_DOMAIN,
};

struct x86_topology_system {
	unsigned int dom_shifts[TOPO_MAX_DOMAIN];
	unsigned int dom_size[TOPO_MAX_DOMAIN];
};

extern struct x86_topology_system x86_topo_system;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
topology_get_domain_size(enum x86_topology_domains dom)
{
	return x86_topo_system.dom_size[dom];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
topology_get_domain_shift(enum x86_topology_domains dom)
{
	return dom == TOPO_SMT_DOMAIN ? 0 : x86_topo_system.dom_shifts[dom - 1];
}

extern const struct cpumask *cpu_coregroup_mask(int cpu);
extern const struct cpumask *cpu_clustergroup_mask(int cpu);
extern unsigned int __max_dies_per_package;
extern unsigned int __max_logical_packages;
extern unsigned int __max_threads_per_core;
extern unsigned int __num_threads_per_package;
extern unsigned int __num_cores_per_package;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
topology_max_packages(void)
{
	return __max_logical_packages;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
topology_max_dies_per_package(void)
{
	return __max_dies_per_package;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
topology_num_cores_per_package(void)
{
	return __num_cores_per_package;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
topology_num_threads_per_package(void)
{
	return __num_threads_per_package;
}

int topology_get_logical_id(u32 apicid, enum x86_topology_domains at_level);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
topology_phys_to_logical_pkg(unsigned int pkg)
{
	return topology_get_logical_id(
		pkg << x86_topo_system.dom_shifts[TOPO_PKG_DOMAIN],
		TOPO_PKG_DOMAIN);
}

extern int __max_smt_threads;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
topology_max_smt_threads(void)
{
	return __max_smt_threads;
}

enum cpuhp_smt_control {
	CPU_SMT_ENABLED,
	CPU_SMT_DISABLED,
	CPU_SMT_FORCE_DISABLED,
	CPU_SMT_NOT_SUPPORTED,
	CPU_SMT_NOT_IMPLEMENTED,
};

extern enum cpuhp_smt_control cpu_smt_control;
extern unsigned int cpu_smt_num_threads;
extern void cpu_smt_disable(bool force);
extern void cpu_smt_set_num_threads(unsigned int num_threads,
				    unsigned int max_threads);
extern bool cpu_smt_possible(void);
extern int cpuhp_smt_enable(void);
extern int cpuhp_smt_disable(enum cpuhp_smt_control ctrlval);

extern unsigned int __amd_nodes_per_pkg;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
topology_amd_nodes_per_pkg(void)
{
	return __amd_nodes_per_pkg;
}

extern struct cpumask __cpu_primary_thread_mask;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
topology_is_primary_thread(unsigned int cpu)
{
	return cpumask_test_cpu(
		cpu, ((const struct cpumask *)&__cpu_primary_thread_mask));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
arch_fix_phys_package_id(int num, u32 slot)
{
}

struct pci_bus;
int x86_pci_root_bus_node(int bus);
void x86_pci_root_bus_resources(int bus, struct list_head *resources);

extern bool x86_topology_update;

extern __attribute__((
	section(".data..percpu"
		"..read_mostly"))) __typeof__(int) sched_core_priority;
extern unsigned int __attribute__((
	__section__(".data..read_mostly"))) sysctl_sched_itmt_enabled;

void sched_set_itmt_core_prio(int prio, int core_cpu);

int sched_set_itmt_support(void);

void sched_clear_itmt_support(void);
extern struct static_key_false arch_scale_freq_key;

extern __attribute__((section(".data..percpu"
			      ""))) __typeof__(unsigned long) arch_freq_scale;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) long
arch_scale_freq_capacity(int cpu)
{
	return (*({
		do {
			const void *__vpp_verify =
				(typeof((&(arch_freq_scale)) + 0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((typeof(*((
				&(arch_freq_scale)))) *)((&(arch_freq_scale))));
			(typeof((typeof(*((&(arch_freq_scale)))) *)((
				&(arch_freq_scale)))))(__ptr +
						       (((__per_cpu_offset[(
							       cpu)]))));
		});
	}));
}

bool arch_enable_hybrid_capacity_scale(void);
void arch_set_cpu_capacity(int cpu, unsigned long cap, unsigned long max_cap,
			   unsigned long cap_freq, unsigned long base_freq);

unsigned long arch_scale_cpu_capacity(int cpu);

extern void arch_set_max_freq_ratio(bool turbo_disabled);
extern void freq_invariance_set_perf_ratio(u64 ratio, bool turbo_disabled);
extern void arch_scale_freq_tick(void);
int arch_update_cpu_topology(void);
extern int __attribute__((
	__section__(".data..read_mostly"))) node_reclaim_distance;

extern __attribute__((section(".data..percpu"
			      ""))) __typeof__(int) numa_node;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
numa_node_id(void)
{
	return ({
		typeof(numa_node) pscr_ret__;
		do {
			const void *__vpp_verify =
				(typeof((&(numa_node)) + 0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		switch (sizeof(numa_node)) {
		case 1:
			pscr_ret__ = ({
				u8 pfo_val__;
				asm("mov"
				    "b "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "q"(pfo_val__)
				    : [var] "m"((*(typeof(*(&(numa_node)))
							   *)(uintptr_t)(&(
					    numa_node)))));
				(typeof(numa_node))(unsigned long)pfo_val__;
			});
			break;
		case 2:
			pscr_ret__ = ({
				u16 pfo_val__;
				asm("mov"
				    "w "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "r"(pfo_val__)
				    : [var] "m"((*(typeof(*(&(numa_node)))
							   *)(uintptr_t)(&(
					    numa_node)))));
				(typeof(numa_node))(unsigned long)pfo_val__;
			});
			break;
		case 4:
			pscr_ret__ = ({
				u32 pfo_val__;
				asm("mov"
				    "l "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "r"(pfo_val__)
				    : [var] "m"((*(typeof(*(&(numa_node)))
							   *)(uintptr_t)(&(
					    numa_node)))));
				(typeof(numa_node))(unsigned long)pfo_val__;
			});
			break;
		case 8:
			pscr_ret__ = ({
				u64 pfo_val__;
				asm("mov"
				    "q "
				    "%%"
				    "gs"
				    ":"
				    "%"
				    "[var]"
				    ", "
				    "%[val]"
				    : [val] "="
					    "r"(pfo_val__)
				    : [var] "m"((*(typeof(*(&(numa_node)))
							   *)(uintptr_t)(&(
					    numa_node)))));
				(typeof(numa_node))(unsigned long)pfo_val__;
			});
			break;
		default:
			__bad_size_call_parameter();
			break;
		}
		pscr_ret__;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
cpu_to_node(int cpu)
{
	return (*({
		do {
			const void *__vpp_verify =
				(typeof((&(numa_node)) + 0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((
				typeof(*((&(numa_node)))) *)((&(numa_node))));
			(typeof((typeof(*((&(numa_node)))) *)((
				&(numa_node)))))(__ptr +
						 (((__per_cpu_offset[(cpu)]))));
		});
	}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_numa_node(int node)
{
	do {
		do {
			const void *__vpp_verify =
				(typeof((&(numa_node)) + 0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		switch (sizeof(numa_node)) {
		case 1:
			do {
				u8 pto_val__ =
					((u8)(((unsigned long)node) & 0xff));
				if (0) {
					typeof(numa_node) pto_tmp__;
					pto_tmp__ = (node);
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"b "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((*(typeof(*(&(numa_node)))
								*)(uintptr_t)(&(
						numa_node))))
					: [val] "qi"(pto_val__));
			} while (0);
			break;
		case 2:
			do {
				u16 pto_val__ =
					((u16)(((unsigned long)node) & 0xffff));
				if (0) {
					typeof(numa_node) pto_tmp__;
					pto_tmp__ = (node);
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"w "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((*(typeof(*(&(numa_node)))
								*)(uintptr_t)(&(
						numa_node))))
					: [val] "ri"(pto_val__));
			} while (0);
			break;
		case 4:
			do {
				u32 pto_val__ = ((u32)(((unsigned long)node) &
						       0xffffffff));
				if (0) {
					typeof(numa_node) pto_tmp__;
					pto_tmp__ = (node);
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"l "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((*(typeof(*(&(numa_node)))
								*)(uintptr_t)(&(
						numa_node))))
					: [val] "ri"(pto_val__));
			} while (0);
			break;
		case 8:
			do {
				u64 pto_val__ = ((u64)(node));
				if (0) {
					typeof(numa_node) pto_tmp__;
					pto_tmp__ = (node);
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"q "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((*(typeof(*(&(numa_node)))
								*)(uintptr_t)(&(
						numa_node))))
					: [val] "re"(pto_val__));
			} while (0);
			break;
		default:
			__bad_size_call_parameter();
			break;
		}
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_cpu_numa_node(int cpu, int node)
{
	(*({
		do {
			const void *__vpp_verify =
				(typeof((&(numa_node)) + 0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((
				typeof(*((&(numa_node)))) *)((&(numa_node))));
			(typeof((typeof(*((&(numa_node)))) *)((
				&(numa_node)))))(__ptr +
						 (((__per_cpu_offset[(cpu)]))));
		});
	})) = node;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
numa_mem_id(void)
{
	return numa_node_id();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
cpu_to_mem(int cpu)
{
	return cpu_to_node(cpu);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const struct cpumask *
cpu_smt_mask(int cpu)
{
	return ((*({
		do {
			const void *__vpp_verify =
				(typeof((&(cpu_sibling_map)) + 0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		({
			unsigned long __ptr;
			__ptr = (unsigned long)((typeof(*((
				&(cpu_sibling_map)))) *)((&(cpu_sibling_map))));
			(typeof((typeof(*((&(cpu_sibling_map)))) *)((
				&(cpu_sibling_map)))))(__ptr +
						       (((__per_cpu_offset[(
							       cpu)]))));
		});
	})));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const struct cpumask *
cpu_cpu_mask(int cpu)
{
	return cpumask_of_node(cpu_to_node(cpu));
}

int sched_numa_find_nth_cpu(const struct cpumask *cpus, int cpu, int node);
extern const struct cpumask *sched_numa_hop_mask(unsigned int node,
						 unsigned int hops);

struct vm_area_struct;
struct mempolicy;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
gfp_migratetype(const gfp_t gfp_flags)
{
	((void)(sizeof(
		(long)((gfp_flags &
			(((gfp_t)((((1UL))) << (___GFP_RECLAIMABLE_BIT))) |
			 ((gfp_t)((((1UL))) << (___GFP_MOVABLE_BIT))))) ==
		       (((gfp_t)((((1UL))) << (___GFP_RECLAIMABLE_BIT))) |
			((gfp_t)((((1UL))) << (___GFP_MOVABLE_BIT))))))));
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_351(void) __attribute__((__error__(
			"BUILD_BUG_ON failed: "
			"(1UL << GFP_MOVABLE_SHIFT) != ___GFP_MOVABLE")));
		if (!(!((1UL << 3) != ((((1UL))) << (___GFP_MOVABLE_BIT)))))
			__compiletime_assert_351();
	} while (0);
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_352(void) __attribute__((__error__(
			"BUILD_BUG_ON failed: "
			"(___GFP_MOVABLE >> GFP_MOVABLE_SHIFT) != MIGRATE_MOVABLE")));
		if (!(!((((((1UL))) << (___GFP_MOVABLE_BIT)) >> 3) !=
			MIGRATE_MOVABLE)))
			__compiletime_assert_352();
	} while (0);
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_353(void) __attribute__((__error__(
			"BUILD_BUG_ON failed: "
			"(___GFP_RECLAIMABLE >> GFP_MOVABLE_SHIFT) != MIGRATE_RECLAIMABLE")));
		if (!(!((((((1UL))) << (___GFP_RECLAIMABLE_BIT)) >> 3) !=
			MIGRATE_RECLAIMABLE)))
			__compiletime_assert_353();
	} while (0);
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_354(void) __attribute__((__error__(
			"BUILD_BUG_ON failed: "
			"((___GFP_MOVABLE | ___GFP_RECLAIMABLE) >> GFP_MOVABLE_SHIFT) != MIGRATE_HIGHATOMIC")));
		if (!(!(((((((1UL))) << (___GFP_MOVABLE_BIT)) |
			  ((((1UL))) << (___GFP_RECLAIMABLE_BIT))) >>
			 3) != MIGRATE_HIGHATOMIC)))
			__compiletime_assert_354();
	} while (0);

	if (__builtin_expect(!!(page_group_by_mobility_disabled), 0))
		return MIGRATE_UNMOVABLE;

	return (unsigned long)(gfp_flags &
			       (((gfp_t)((((1UL)))
					 << (___GFP_RECLAIMABLE_BIT))) |
				((gfp_t)((((1UL))) << (___GFP_MOVABLE_BIT))))) >>
	       3;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
gfpflags_allow_blocking(const gfp_t gfp_flags)
{
	return !!(gfp_flags &
		  ((gfp_t)((((1UL))) << (___GFP_DIRECT_RECLAIM_BIT))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) enum zone_type
gfp_zone(gfp_t flags)
{
	enum zone_type z;
	int bit = (int)(flags & (((gfp_t)((((1UL))) << (___GFP_DMA_BIT))) |
				 ((gfp_t)((((1UL))) << (___GFP_HIGHMEM_BIT))) |
				 ((gfp_t)((((1UL))) << (___GFP_DMA32_BIT))) |
				 ((gfp_t)((((1UL))) << (___GFP_MOVABLE_BIT)))));

	z = (((ZONE_NORMAL << 0 * 2) |
	      (ZONE_DMA << ((((1UL))) << (___GFP_DMA_BIT)) * 2) |
	      (ZONE_NORMAL << ((((1UL))) << (___GFP_HIGHMEM_BIT)) * 2) |
	      (ZONE_DMA32 << ((((1UL))) << (___GFP_DMA32_BIT)) * 2) |
	      (ZONE_NORMAL << ((((1UL))) << (___GFP_MOVABLE_BIT)) * 2) |
	      (ZONE_DMA << (((((1UL))) << (___GFP_MOVABLE_BIT)) |
			    ((((1UL))) << (___GFP_DMA_BIT))) *
				   2) |
	      (ZONE_MOVABLE << (((((1UL))) << (___GFP_MOVABLE_BIT)) |
				((((1UL))) << (___GFP_HIGHMEM_BIT))) *
				       2) |
	      (ZONE_DMA32 << (((((1UL))) << (___GFP_MOVABLE_BIT)) |
			      ((((1UL))) << (___GFP_DMA32_BIT))) *
				     2)) >>
	     (bit * 2)) &
	    ((1 << 2) - 1);
	((void)(sizeof((long)(((1 << (((((1UL))) << (___GFP_DMA_BIT)) |
				      ((((1UL))) << (___GFP_HIGHMEM_BIT))) |
				1 << (((((1UL))) << (___GFP_DMA_BIT)) |
				      ((((1UL))) << (___GFP_DMA32_BIT))) |
				1 << (((((1UL))) << (___GFP_DMA32_BIT)) |
				      ((((1UL))) << (___GFP_HIGHMEM_BIT))) |
				1 << (((((1UL))) << (___GFP_DMA_BIT)) |
				      ((((1UL))) << (___GFP_DMA32_BIT)) |
				      ((((1UL))) << (___GFP_HIGHMEM_BIT))) |
				1 << (((((1UL))) << (___GFP_MOVABLE_BIT)) |
				      ((((1UL))) << (___GFP_HIGHMEM_BIT)) |
				      ((((1UL))) << (___GFP_DMA_BIT))) |
				1 << (((((1UL))) << (___GFP_MOVABLE_BIT)) |
				      ((((1UL))) << (___GFP_DMA32_BIT)) |
				      ((((1UL))) << (___GFP_DMA_BIT))) |
				1 << (((((1UL))) << (___GFP_MOVABLE_BIT)) |
				      ((((1UL))) << (___GFP_DMA32_BIT)) |
				      ((((1UL))) << (___GFP_HIGHMEM_BIT))) |
				1 << (((((1UL))) << (___GFP_MOVABLE_BIT)) |
				      ((((1UL))) << (___GFP_DMA32_BIT)) |
				      ((((1UL))) << (___GFP_DMA_BIT)) |
				      ((((1UL))) << (___GFP_HIGHMEM_BIT)))) >>
			       bit) &
			      1))));
	return z;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
gfp_zonelist(gfp_t flags)
{
	if (__builtin_expect(!!(flags &
				((gfp_t)((((1UL))) << (___GFP_THISNODE_BIT)))),
			     0))
		return ZONELIST_NOFALLBACK;

	return ZONELIST_FALLBACK;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) gfp_t
gfp_nested_mask(gfp_t flags)
{
	return ((flags &
		 ((((gfp_t)(((((1UL))) << (___GFP_DIRECT_RECLAIM_BIT)) |
			    ((((1UL))) << (___GFP_KSWAPD_RECLAIM_BIT)))) |
		   ((gfp_t)((((1UL))) << (___GFP_IO_BIT))) |
		   ((gfp_t)((((1UL))) << (___GFP_FS_BIT)))) |
		  (((gfp_t)((((1UL))) << (___GFP_HIGH_BIT))) |
		   ((gfp_t)((((1UL))) << (___GFP_KSWAPD_RECLAIM_BIT)))) |
		  ((gfp_t)0))) |
		(((gfp_t)((((1UL))) << (___GFP_NORETRY_BIT))) |
		 ((gfp_t)((((1UL))) << (___GFP_NOMEMALLOC_BIT))) |
		 ((gfp_t)((((1UL))) << (___GFP_NOWARN_BIT)))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct zonelist *
node_zonelist(int nid, gfp_t flags)
{
	return (node_data[nid])->node_zonelists + gfp_zonelist(flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
arch_free_page(struct page *page, int order)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
arch_alloc_page(struct page *page, int order)
{
}

struct page *__alloc_pages_noprof(gfp_t gfp, unsigned int order,
				  int preferred_nid, nodemask_t *nodemask);

struct folio *__folio_alloc_noprof(gfp_t gfp, unsigned int order,
				   int preferred_nid, nodemask_t *nodemask);

unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,
				      nodemask_t *nodemask, int nr_pages,
				      struct list_head *page_list,
				      struct page **page_array);

unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,
						      unsigned long nr_pages,
						      struct page **page_array);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
alloc_pages_bulk_array_node_noprof(gfp_t gfp, int nid, unsigned long nr_pages,
				   struct page **page_array)
{
	if (nid == (-1))
		nid = numa_mem_id();

	return alloc_pages_bulk_noprof(gfp, nid, ((void *)0), nr_pages,
				       ((void *)0), page_array);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
warn_if_node_offline(int this_node, gfp_t gfp_mask)
{
	gfp_t warn_gfp = gfp_mask &
			 (((gfp_t)((((1UL))) << (___GFP_THISNODE_BIT))) |
			  ((gfp_t)((((1UL))) << (___GFP_NOWARN_BIT))));

	if (warn_gfp != (((gfp_t)((((1UL))) << (___GFP_THISNODE_BIT))) |
			 ((gfp_t)((((1UL))) << (___GFP_NOWARN_BIT)))))
		return;

	if (node_state((this_node), N_ONLINE))
		return;

	({
		do {
		} while (0);
		_printk("\001"
			"4"
			"%pGg allocation from offline node %d\n",
			&gfp_mask, this_node);
	});
	dump_stack();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct page *
__alloc_pages_node_noprof(int nid, gfp_t gfp_mask, unsigned int order)
{
	((void)(sizeof((long)(nid < 0 || nid >= (1 << 6)))));
	warn_if_node_offline(nid, gfp_mask);

	return __alloc_pages_noprof(gfp_mask, order, nid, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct folio *
__folio_alloc_node_noprof(gfp_t gfp, unsigned int order, int nid)
{
	((void)(sizeof((long)(nid < 0 || nid >= (1 << 6)))));
	warn_if_node_offline(nid, gfp);

	return __folio_alloc_noprof(gfp, order, nid, ((void *)0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct page *
alloc_pages_node_noprof(int nid, gfp_t gfp_mask, unsigned int order)
{
	if (nid == (-1))
		nid = numa_mem_id();

	return __alloc_pages_node_noprof(nid, gfp_mask, order);
}

struct page *alloc_pages_noprof(gfp_t gfp, unsigned int order);
struct page *alloc_pages_mpol_noprof(gfp_t gfp, unsigned int order,
				     struct mempolicy *mpol, unsigned long ilx,
				     int nid);
struct folio *folio_alloc_noprof(gfp_t gfp, unsigned int order);
struct folio *folio_alloc_mpol_noprof(gfp_t gfp, unsigned int order,
				      struct mempolicy *mpol, unsigned long ilx,
				      int nid);
struct folio *vma_alloc_folio_noprof(gfp_t gfp, int order,
				     struct vm_area_struct *vma,
				     unsigned long addr, bool hugepage);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct page *
alloc_page_vma_noprof(gfp_t gfp, struct vm_area_struct *vma, unsigned long addr)
{
	struct folio *folio = vma_alloc_folio_noprof(gfp, 0, vma, addr, false);

	return &folio->page;
}

extern unsigned long get_free_pages_noprof(gfp_t gfp_mask, unsigned int order);

extern unsigned long get_zeroed_page_noprof(gfp_t gfp_mask);

void *alloc_pages_exact_noprof(size_t size, gfp_t gfp_mask)
	__attribute__((__alloc_size__(1))) __attribute__((__malloc__));

void free_pages_exact(void *virt, size_t size);

__attribute__((__section__(".init.text"))) __attribute__((__cold__)) void *
alloc_pages_exact_nid_noprof(int nid, size_t size, gfp_t gfp_mask)
	__attribute__((__alloc_size__(2))) __attribute__((__malloc__));
extern void __free_pages(struct page *page, unsigned int order);
extern void free_pages(unsigned long addr, unsigned int order);

struct page_frag_cache;
void page_frag_cache_drain(struct page_frag_cache *nc);
extern void __page_frag_cache_drain(struct page *page, unsigned int count);
void *__page_frag_alloc_align(struct page_frag_cache *nc, unsigned int fragsz,
			      gfp_t gfp_mask, unsigned int align_mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
page_frag_alloc_align(struct page_frag_cache *nc, unsigned int fragsz,
		      gfp_t gfp_mask, unsigned int align)
{
	({
		int __ret_warn_on = !!(!is_power_of_2(align));
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) |
						      ((1 << 1) | ((9) << 8));
				({
					asm volatile(
						"355"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"355"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(355));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/gfp.h"),
						  "i"(384), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"356"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"356"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(356));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	return __page_frag_alloc_align(nc, fragsz, gfp_mask, -align);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
page_frag_alloc(struct page_frag_cache *nc, unsigned int fragsz, gfp_t gfp_mask)
{
	return __page_frag_alloc_align(nc, fragsz, gfp_mask, ~0u);
}

extern void page_frag_free(void *addr);

void page_alloc_init_cpuhp(void);
int decay_pcp_high(struct zone *zone, struct per_cpu_pages *pcp);
void drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp);
void drain_all_pages(struct zone *zone);
void drain_local_pages(struct zone *zone);

void page_alloc_init_late(void);
void setup_pcp_cacheinfo(unsigned int cpu);
extern gfp_t gfp_allowed_mask;

bool gfp_pfmemalloc_allowed(gfp_t gfp_mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
gfp_has_io_fs(gfp_t gfp)
{
	return (gfp & (((gfp_t)((((1UL))) << (___GFP_IO_BIT))) |
		       ((gfp_t)((((1UL))) << (___GFP_FS_BIT))))) ==
	       (((gfp_t)((((1UL))) << (___GFP_IO_BIT))) |
		((gfp_t)((((1UL))) << (___GFP_FS_BIT))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
gfp_compaction_allowed(gfp_t gfp_mask)
{
	return 1 && (gfp_mask & ((gfp_t)((((1UL))) << (___GFP_IO_BIT))));
}

extern gfp_t vma_thp_gfp_mask(struct vm_area_struct *vma);
void free_contig_range(unsigned long pfn, unsigned long nr_pages);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct folio *
folio_alloc_gigantic_noprof(int order, gfp_t gfp, int nid, nodemask_t *node)
{
	return ((void *)0);
}

struct percpu_ref;
typedef void(percpu_ref_func_t)(struct percpu_ref *);

enum {
	__PERCPU_REF_ATOMIC = 1LU << 0,
	__PERCPU_REF_DEAD = 1LU << 1,
	__PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,

	__PERCPU_REF_FLAG_BITS = 2,
};

enum {

	PERCPU_REF_INIT_ATOMIC = 1 << 0,

	PERCPU_REF_INIT_DEAD = 1 << 1,

	PERCPU_REF_ALLOW_REINIT = 1 << 2,
};

struct percpu_ref_data {
	atomic_long_t count;
	percpu_ref_func_t *release;
	percpu_ref_func_t *confirm_switch;
	bool force_atomic : 1;
	bool allow_reinit : 1;
	struct callback_head rcu;
	struct percpu_ref *ref;
};

struct percpu_ref {
	unsigned long percpu_count_ptr;

	struct percpu_ref_data *data;
};

int __attribute__((__warn_unused_result__))
percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,
		unsigned int flags, gfp_t gfp);
void percpu_ref_exit(struct percpu_ref *ref);
void percpu_ref_switch_to_atomic(struct percpu_ref *ref,
				 percpu_ref_func_t *confirm_switch);
void percpu_ref_switch_to_atomic_sync(struct percpu_ref *ref);
void percpu_ref_switch_to_percpu(struct percpu_ref *ref);
void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
				 percpu_ref_func_t *confirm_kill);
void percpu_ref_resurrect(struct percpu_ref *ref);
void percpu_ref_reinit(struct percpu_ref *ref);
bool percpu_ref_is_zero(struct percpu_ref *ref);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_ref_kill(struct percpu_ref *ref)
{
	percpu_ref_kill_and_confirm(ref, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__ref_is_percpu(struct percpu_ref *ref, unsigned long **percpu_countp)
{
	unsigned long percpu_ptr;
	percpu_ptr = ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_357(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(ref->percpu_count_ptr) == sizeof(char) ||
			       sizeof(ref->percpu_count_ptr) == sizeof(short) ||
			       sizeof(ref->percpu_count_ptr) == sizeof(int) ||
			       sizeof(ref->percpu_count_ptr) == sizeof(long)) ||
			      sizeof(ref->percpu_count_ptr) ==
				      sizeof(long long)))
				__compiletime_assert_357();
		} while (0);
		(*(const volatile typeof(_Generic((ref->percpu_count_ptr),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (
							  ref->percpu_count_ptr)))
			   *)&(ref->percpu_count_ptr));
	});

	if (__builtin_expect(!!(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD), 0))
		return false;

	*percpu_countp = (unsigned long *)percpu_ptr;
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
{
	unsigned long *percpu_count;

	rcu_read_lock();

	if (__ref_is_percpu(ref, &percpu_count))
		do {
			do {
				const void *__vpp_verify =
					(typeof((&(*percpu_count)) +
						0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			switch (sizeof(*percpu_count)) {
			case 1:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(nr) &&
						 ((nr) == 1 || (nr) == -1)) ?
							(int)(nr) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (nr);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u8 pto_val__ = ((
								u8)(((unsigned long)
									     nr) &
								    0xff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ =
									(nr);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"b "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "qi"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 2:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(nr) &&
						 ((nr) == 1 || (nr) == -1)) ?
							(int)(nr) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (nr);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u16 pto_val__ = ((
								u16)(((unsigned long)
									      nr) &
								     0xffff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ =
									(nr);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"w "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 4:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(nr) &&
						 ((nr) == 1 || (nr) == -1)) ?
							(int)(nr) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (nr);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u32 pto_val__ = ((
								u32)(((unsigned long)
									      nr) &
								     0xffffffff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ =
									(nr);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"l "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 8:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(nr) &&
						 ((nr) == 1 || (nr) == -1)) ?
							(int)(nr) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (nr);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u64 pto_val__ =
								((u64)(nr));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ =
									(nr);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"q "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "re"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			default:
				__bad_size_call_parameter();
				break;
			}
		} while (0);
	else
		atomic_long_add(nr, &ref->data->count);

	rcu_read_unlock();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_ref_get(struct percpu_ref *ref)
{
	percpu_ref_get_many(ref, 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
percpu_ref_tryget_many(struct percpu_ref *ref, unsigned long nr)
{
	unsigned long *percpu_count;
	bool ret;

	rcu_read_lock();

	if (__ref_is_percpu(ref, &percpu_count)) {
		do {
			do {
				const void *__vpp_verify =
					(typeof((&(*percpu_count)) +
						0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			switch (sizeof(*percpu_count)) {
			case 1:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(nr) &&
						 ((nr) == 1 || (nr) == -1)) ?
							(int)(nr) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (nr);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u8 pto_val__ = ((
								u8)(((unsigned long)
									     nr) &
								    0xff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ =
									(nr);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"b "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "qi"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 2:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(nr) &&
						 ((nr) == 1 || (nr) == -1)) ?
							(int)(nr) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (nr);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u16 pto_val__ = ((
								u16)(((unsigned long)
									      nr) &
								     0xffff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ =
									(nr);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"w "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 4:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(nr) &&
						 ((nr) == 1 || (nr) == -1)) ?
							(int)(nr) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (nr);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u32 pto_val__ = ((
								u32)(((unsigned long)
									      nr) &
								     0xffffffff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ =
									(nr);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"l "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 8:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(nr) &&
						 ((nr) == 1 || (nr) == -1)) ?
							(int)(nr) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (nr);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u64 pto_val__ =
								((u64)(nr));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ =
									(nr);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"q "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "re"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			default:
				__bad_size_call_parameter();
				break;
			}
		} while (0);
		ret = true;
	} else {
		ret = atomic_long_add_unless(&ref->data->count, nr, 0);
	}

	rcu_read_unlock();

	return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
percpu_ref_tryget(struct percpu_ref *ref)
{
	return percpu_ref_tryget_many(ref, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
percpu_ref_tryget_live_rcu(struct percpu_ref *ref)
{
	unsigned long *percpu_count;
	bool ret = false;

	({
		int __ret_warn_on = !!(!rcu_read_lock_held());
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) |
						      ((1 << 1) | ((9) << 8));
				({
					asm volatile(
						"358"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"358"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(358));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/percpu-refcount.h"),
						  "i"(280), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"359"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"359"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(359));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});

	if (__builtin_expect(!!(__ref_is_percpu(ref, &percpu_count)), 1)) {
		do {
			do {
				const void *__vpp_verify =
					(typeof((&(*percpu_count)) +
						0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			switch (sizeof(*percpu_count)) {
			case 1:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u8 pto_val__ = ((
								u8)(((unsigned long)1) &
								    0xff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"b "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "qi"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 2:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u16 pto_val__ = ((
								u16)(((unsigned long)1) &
								     0xffff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"w "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 4:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u32 pto_val__ = ((
								u32)(((unsigned long)1) &
								     0xffffffff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"l "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 8:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u64 pto_val__ =
								((u64)(1));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"q "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "re"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			default:
				__bad_size_call_parameter();
				break;
			}
		} while (0);
		ret = true;
	} else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
		ret = atomic_long_inc_not_zero(&ref->data->count);
	}
	return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
percpu_ref_tryget_live(struct percpu_ref *ref)
{
	bool ret = false;

	rcu_read_lock();
	ret = percpu_ref_tryget_live_rcu(ref);
	rcu_read_unlock();
	return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
{
	unsigned long *percpu_count;

	rcu_read_lock();

	if (__ref_is_percpu(ref, &percpu_count))
		do {
			do {
				const void *__vpp_verify =
					(typeof((&(*percpu_count)) +
						0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			switch (sizeof(*percpu_count)) {
			case 1:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*percpu_count))(nr)) &&
						 ((-(typeof(*percpu_count))(nr)) ==
							  1 ||
						  (-(typeof(*percpu_count))(nr)) ==
							  -1)) ?
							(int)(-(typeof(*percpu_count))(nr)) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*percpu_count))(nr));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u8 pto_val__ = ((
								u8)(((unsigned long)-(
									    typeof(*percpu_count))(nr)) &
								    0xff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*percpu_count))(nr));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"b "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "qi"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 2:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*percpu_count))(nr)) &&
						 ((-(typeof(*percpu_count))(nr)) ==
							  1 ||
						  (-(typeof(*percpu_count))(nr)) ==
							  -1)) ?
							(int)(-(typeof(*percpu_count))(nr)) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*percpu_count))(nr));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u16 pto_val__ = ((
								u16)(((unsigned long)-(
									     typeof(*percpu_count))(nr)) &
								     0xffff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*percpu_count))(nr));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"w "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 4:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*percpu_count))(nr)) &&
						 ((-(typeof(*percpu_count))(nr)) ==
							  1 ||
						  (-(typeof(*percpu_count))(nr)) ==
							  -1)) ?
							(int)(-(typeof(*percpu_count))(nr)) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*percpu_count))(nr));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u32 pto_val__ = ((
								u32)(((unsigned long)-(
									     typeof(*percpu_count))(nr)) &
								     0xffffffff));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*percpu_count))(nr));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"l "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 8:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*percpu_count))(nr)) &&
						 ((-(typeof(*percpu_count))(nr)) ==
							  1 ||
						  (-(typeof(*percpu_count))(nr)) ==
							  -1)) ?
							(int)(-(typeof(*percpu_count))(nr)) :
							0;
					if (0) {
						typeof((*percpu_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*percpu_count))(nr));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count))))));
						});
					else
						do {
							u64 pto_val__ = ((
								u64)(-(
								typeof(*percpu_count))(nr)));
							if (0) {
								typeof((*percpu_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*percpu_count))(nr));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"q "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*percpu_count))))
										*)(uintptr_t)(&(
									(*percpu_count)))))
								: [val] "re"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			default:
				__bad_size_call_parameter();
				break;
			}
		} while (0);
	else if (__builtin_expect(
			 !!(atomic_long_sub_and_test(nr, &ref->data->count)),
			 0))
		ref->data->release(ref);

	rcu_read_unlock();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_ref_put(struct percpu_ref *ref)
{
	percpu_ref_put_many(ref, 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
percpu_ref_is_dying(struct percpu_ref *ref)
{
	return ref->percpu_count_ptr & __PERCPU_REF_DEAD;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
__hash_32_generic(u32 val)
{
	return val * 0x61C88647;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
hash_32(u32 val, unsigned int bits)
{
	return __hash_32_generic(val) >> (32 - bits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u32
hash_64_generic(u64 val, unsigned int bits)
{
	return val * 0x61C8864680B583EBull >> (64 - bits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
hash_ptr(const void *ptr, unsigned int bits)
{
	return hash_64_generic((unsigned long)ptr, bits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
hash32_ptr(const void *ptr)
{
	unsigned long val = (unsigned long)ptr;

	val ^= (val >> 32);

	return (u32)val;
}

enum _slab_flag_bits {
	_SLAB_CONSISTENCY_CHECKS,
	_SLAB_RED_ZONE,
	_SLAB_POISON,
	_SLAB_KMALLOC,
	_SLAB_HWCACHE_ALIGN,
	_SLAB_CACHE_DMA,
	_SLAB_CACHE_DMA32,
	_SLAB_STORE_USER,
	_SLAB_PANIC,
	_SLAB_TYPESAFE_BY_RCU,
	_SLAB_TRACE,

	_SLAB_NOLEAKTRACE,
	_SLAB_NO_MERGE,
	_SLAB_NO_USER_FLAGS,

	_SLAB_RECLAIM_ACCOUNT,

	_SLAB_OBJECT_POISON,
	_SLAB_CMPXCHG_DOUBLE,

	_SLAB_FLAGS_LAST_BIT
};
typedef struct {
	unsigned long v;
} freeptr_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kasan_enabled(void)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kasan_hw_tags_enabled(void)
{
	return false;
}

struct kmem_cache;
struct page;
struct slab;
struct vm_struct;
struct task_struct;
typedef unsigned int kasan_vmalloc_flags_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kasan_add_zero_shadow(void *start, unsigned long size)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_remove_zero_shadow(void *start, unsigned long size)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_enable_current(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_disable_current(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kasan_has_integrated_init(void)
{
	return kasan_hw_tags_enabled();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_unpoison_range(const void *address, size_t size)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_poison_pages(struct page *page, unsigned int order, bool init)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kasan_unpoison_pages(struct page *page, unsigned int order, bool init)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_poison_slab(struct slab *slab)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_unpoison_new_object(struct kmem_cache *cache, void *object)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_poison_new_object(struct kmem_cache *cache, void *object)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
{
	return (void *)object;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kasan_slab_pre_free(struct kmem_cache *s, void *object)
{
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kasan_slab_free(struct kmem_cache *s, void *object, bool init,
		bool still_accessible)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_kfree_large(void *ptr)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
kasan_slab_alloc(struct kmem_cache *s, void *object, gfp_t flags, bool init)
{
	return object;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
kasan_kmalloc(struct kmem_cache *s, const void *object, size_t size,
	      gfp_t flags)
{
	return (void *)object;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
{
	return (void *)ptr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
kasan_krealloc(const void *object, size_t new_size, gfp_t flags)
{
	return (void *)object;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kasan_mempool_poison_pages(struct page *page, unsigned int order)
{
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_mempool_unpoison_pages(struct page *page, unsigned int order)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kasan_mempool_poison_object(void *ptr)
{
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_mempool_unpoison_object(void *ptr, size_t size)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kasan_check_byte(const void *address)
{
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_unpoison_task_stack(struct task_struct *task)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_unpoison_task_stack_below(const void *watermark)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
kasan_metadata_size(struct kmem_cache *cache, bool in_object)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
		   slab_flags_t *flags)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_cache_shrink(struct kmem_cache *cache)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_cache_shutdown(struct kmem_cache *cache)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_record_aux_stack(void *ptr)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_record_aux_stack_noalloc(void *ptr)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
kasan_reset_tag(const void *addr)
{
	return (void *)addr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_init_sw_tags(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_init_hw_tags_cpu(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_init_hw_tags(void)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_populate_early_vm_area_shadow(void *start, unsigned long size)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kasan_populate_vmalloc(unsigned long start, unsigned long size)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_release_vmalloc(unsigned long start, unsigned long end,
		      unsigned long free_region_start,
		      unsigned long free_region_end)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
kasan_unpoison_vmalloc(const void *start, unsigned long size,
		       kasan_vmalloc_flags_t flags)
{
	return (void *)start;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_poison_vmalloc(const void *start, unsigned long size)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kasan_alloc_module_shadow(void *addr, size_t size, gfp_t gfp_mask)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_free_module_shadow(const struct vm_struct *vm)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kasan_non_canonical_hook(unsigned long addr)
{
}

struct list_lru;
struct mem_cgroup;

bool slab_is_available(void);
struct kmem_cache_args {
	unsigned int align;

	unsigned int useroffset;

	unsigned int usersize;
	unsigned int freeptr_offset;

	bool use_freeptr_offset;
	void (*ctor)(void *);
};

struct kmem_cache *__kmem_cache_create_args(const char *name,
					    unsigned int object_size,
					    struct kmem_cache_args *args,
					    slab_flags_t flags);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kmem_cache *
__kmem_cache_create(const char *name, unsigned int size, unsigned int align,
		    slab_flags_t flags, void (*ctor)(void *))
{
	struct kmem_cache_args kmem_args = {
		.align = align,
		.ctor = ctor,
	};

	return __kmem_cache_create_args(name, size, &kmem_args, flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kmem_cache *
kmem_cache_create_usercopy(const char *name, unsigned int size,
			   unsigned int align, slab_flags_t flags,
			   unsigned int useroffset, unsigned int usersize,
			   void (*ctor)(void *))
{
	struct kmem_cache_args kmem_args = {
		.align = align,
		.ctor = ctor,
		.useroffset = useroffset,
		.usersize = usersize,
	};

	return __kmem_cache_create_args(name, size, &kmem_args, flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kmem_cache *
__kmem_cache_default_args(const char *name, unsigned int size,
			  struct kmem_cache_args *args, slab_flags_t flags)
{
	struct kmem_cache_args kmem_default_args = {};

	if (({
		    int __ret_warn_on = !!(args);
		    if (__builtin_expect(!!(__ret_warn_on), 0))
			    do {
				    __auto_type __flags =
					    (1 << 0) | ((1 << 1) | ((9) << 8));
				    ({
					    asm volatile(
						    "360"
						    ": nop\n\t"
						    ".pushsection .discard.instr_begin\n\t"
						    ".long "
						    "360"
						    "b - .\n\t"
						    ".popsection\n\t"
						    :
						    : "i"(360));
				    });
				    do {
					    asm __inline volatile(
						    "1:\t"
						    ".byte 0x0f, 0x0b"
						    "\n"
						    ".pushsection __bug_table,\"aw\"\n"
						    "2:\t"
						    ".long "
						    "1b"
						    " - ."
						    "\t# bug_entry::bug_addr\n"
						    "\t"
						    ".long "
						    "%c0"
						    " - ."
						    "\t# bug_entry::file\n"
						    "\t.word %c1"
						    "\t# bug_entry::line\n"
						    "\t.word %c2"
						    "\t# bug_entry::flags\n"
						    "\t.org 2b+%c3\n"
						    ".popsection\n"
						    "998:\n\t"
						    ".pushsection .discard.reachable\n\t"
						    ".long 998b\n\t"
						    ".popsection\n\t"
						    :
						    : "i"("include/linux/slab.h"),
						      "i"(373), "i"(__flags),
						      "i"(sizeof(
							      struct bug_entry)));
				    } while (0);
				    ({
					    asm volatile(
						    "361"
						    ": nop\n\t"
						    ".pushsection .discard.instr_end\n\t"
						    ".long "
						    "361"
						    "b - .\n\t"
						    ".popsection\n\t"
						    :
						    : "i"(361));
				    });
			    } while (0);
		    __builtin_expect(!!(__ret_warn_on), 0);
	    }))
		return ERR_PTR(-22);

	return __kmem_cache_create_args(name, size, &kmem_default_args, flags);
}
void kmem_cache_destroy(struct kmem_cache *s);
int kmem_cache_shrink(struct kmem_cache *s);
void *__attribute__((__warn_unused_result__))
krealloc_noprof(const void *objp, size_t new_size, gfp_t flags)
	__attribute__((__alloc_size__(2)));

void kfree(const void *objp);
void kfree_sensitive(const void *objp);
size_t __ksize(const void *objp);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__free_kfree(void *p)
{
	void *_T = *(void **)p;
	if (!IS_ERR_OR_NULL(_T))
		kfree(_T);
}
size_t ksize(const void *objp);

bool kmem_dump_obj(void *object);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
arch_slab_minalign(void)
{
	return __alignof__(unsigned long long);
}
enum kmalloc_cache_type {
	KMALLOC_NORMAL = 0,

	KMALLOC_CGROUP = KMALLOC_NORMAL,

	KMALLOC_RANDOM_START = KMALLOC_NORMAL,
	KMALLOC_RANDOM_END = KMALLOC_RANDOM_START + 0,

	KMALLOC_RECLAIM,

	KMALLOC_DMA,

	NR_KMALLOC_TYPES
};

typedef struct kmem_cache *kmem_buckets[(12 + 1) + 1];

extern kmem_buckets kmalloc_caches[NR_KMALLOC_TYPES];
extern unsigned long random_kmalloc_seed;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) enum kmalloc_cache_type
kmalloc_type(gfp_t flags, unsigned long caller)
{
	if (__builtin_expect(
		    !!((flags &
			(((gfp_t)((((1UL))) << (___GFP_RECLAIMABLE_BIT))) |
			 (1 ? ((gfp_t)((((1UL))) << (___GFP_DMA_BIT))) : 0) |
			 (0 ? ((gfp_t)((((1UL))) << (___GFP_ACCOUNT_BIT))) :
			      0))) == 0),
		    1))

		return KMALLOC_NORMAL;
	if (1 && (flags & ((gfp_t)((((1UL))) << (___GFP_DMA_BIT)))))
		return KMALLOC_DMA;
	if (!0 || (flags & ((gfp_t)((((1UL))) << (___GFP_RECLAIMABLE_BIT)))))
		return KMALLOC_RECLAIM;
	else
		return KMALLOC_CGROUP;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned int
__kmalloc_index(size_t size, bool size_is_constant)
{
	if (!size)
		return 0;

	if (size <= (1 << 3))
		return 3;

	if ((1 << 3) <= 32 && size > 64 && size <= 96)
		return 1;
	if ((1 << 3) <= 64 && size > 128 && size <= 192)
		return 2;
	if (size <= 8)
		return 3;
	if (size <= 16)
		return 4;
	if (size <= 32)
		return 5;
	if (size <= 64)
		return 6;
	if (size <= 128)
		return 7;
	if (size <= 256)
		return 8;
	if (size <= 512)
		return 9;
	if (size <= 1024)
		return 10;
	if (size <= 2 * 1024)
		return 11;
	if (size <= 4 * 1024)
		return 12;
	if (size <= 8 * 1024)
		return 13;
	if (size <= 16 * 1024)
		return 14;
	if (size <= 32 * 1024)
		return 15;
	if (size <= 64 * 1024)
		return 16;
	if (size <= 128 * 1024)
		return 17;
	if (size <= 256 * 1024)
		return 18;
	if (size <= 512 * 1024)
		return 19;
	if (size <= 1024 * 1024)
		return 20;
	if (size <= 2 * 1024 * 1024)
		return 21;

	if (!0 && size_is_constant)
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_362(void) __attribute__((__error__(
				"unexpected size in kmalloc_index()")));
			if (!(!(1)))
				__compiletime_assert_362();
		} while (0);
	else
		do {
			({
				asm volatile(
					"363"
					": nop\n\t"
					".pushsection .discard.instr_begin\n\t"
					".long "
					"363"
					"b - .\n\t"
					".popsection\n\t"
					:
					: "i"(363));
			});
			do {
				asm __inline volatile(
					"1:\t"
					".byte 0x0f, 0x0b"
					"\n"
					".pushsection __bug_table,\"aw\"\n"
					"2:\t"
					".long "
					"1b"
					" - ."
					"\t# bug_entry::bug_addr\n"
					"\t"
					".long "
					"%c0"
					" - ."
					"\t# bug_entry::file\n"
					"\t.word %c1"
					"\t# bug_entry::line\n"
					"\t.word %c2"
					"\t# bug_entry::flags\n"
					"\t.org 2b+%c3\n"
					".popsection\n"
					""
					:
					: "i"("include/linux/slab.h"), "i"(690),
					  "i"(0),
					  "i"(sizeof(struct bug_entry)));
			} while (0);
			__builtin_unreachable();
		} while (0);

	return -1;
}
_Static_assert(12 <= 20, "PAGE_SHIFT <= 20");
void *kmem_cache_alloc_noprof(struct kmem_cache *cachep, gfp_t flags)
	__attribute__((__assume_aligned__(__alignof__(unsigned long long))))
	__attribute__((__malloc__));

void *kmem_cache_alloc_lru_noprof(struct kmem_cache *s, struct list_lru *lru,
				  gfp_t gfpflags)
	__attribute__((__assume_aligned__(__alignof__(unsigned long long))))
	__attribute__((__malloc__));
bool kmem_cache_charge(void *objp, gfp_t gfpflags);
void kmem_cache_free(struct kmem_cache *s, void *objp);

kmem_buckets *kmem_buckets_create(const char *name, slab_flags_t flags,
				  unsigned int useroffset,
				  unsigned int usersize, void (*ctor)(void *));
void kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p);

int kmem_cache_alloc_bulk_noprof(struct kmem_cache *s, gfp_t flags, size_t size,
				 void **p);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
kfree_bulk(size_t size, void **p)
{
	kmem_cache_free_bulk(((void *)0), size, p);
}

void *kmem_cache_alloc_node_noprof(struct kmem_cache *s, gfp_t flags, int node)
	__attribute__((__assume_aligned__(__alignof__(unsigned long long))))
	__attribute__((__malloc__));
void *__kmalloc_noprof(size_t size, gfp_t flags)
	__attribute__((__assume_aligned__(__alignof__(unsigned long long))))
	__attribute__((__alloc_size__(1))) __attribute__((__malloc__));

void *__kmalloc_node_noprof(size_t(size), gfp_t flags, int node)
	__attribute__((__assume_aligned__(__alignof__(unsigned long long))))
	__attribute__((__alloc_size__(1))) __attribute__((__malloc__));

void *__kmalloc_cache_noprof(struct kmem_cache *s, gfp_t flags, size_t size)
	__attribute__((__assume_aligned__(__alignof__(unsigned long long))))
	__attribute__((__alloc_size__(3))) __attribute__((__malloc__));

void *__kmalloc_cache_node_noprof(struct kmem_cache *s, gfp_t gfpflags,
				  int node, size_t size)
	__attribute__((__assume_aligned__(__alignof__(unsigned long long))))
	__attribute__((__alloc_size__(4))) __attribute__((__malloc__));

void *__kmalloc_large_noprof(size_t size, gfp_t flags)
	__attribute__((__assume_aligned__(((1UL) << 12))))
	__attribute__((__alloc_size__(1))) __attribute__((__malloc__));

void *__kmalloc_large_node_noprof(size_t size, gfp_t flags, int node)
	__attribute__((__assume_aligned__(((1UL) << 12))))
	__attribute__((__alloc_size__(1))) __attribute__((__malloc__));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *
kmalloc_noprof(size_t size, gfp_t flags)
{
	if (__builtin_constant_p(size) && size) {
		unsigned int index;

		if (size > (1UL << (12 + 1)))
			return __kmalloc_large_noprof(size, flags);

		index = __kmalloc_index(size, true);
		return __kmalloc_cache_noprof(
			kmalloc_caches[kmalloc_type(
				flags, (unsigned long)__builtin_return_address(
					       0))][index],
			flags, size);
	}
	return __kmalloc_noprof(size, flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *
kmalloc_node_noprof(size_t size, gfp_t flags, int node)
{
	if (__builtin_constant_p(size) && size) {
		unsigned int index;

		if (size > (1UL << (12 + 1)))
			return __kmalloc_large_node_noprof(size, flags, node);

		index = __kmalloc_index(size, true);
		return __kmalloc_cache_node_noprof(
			kmalloc_caches[kmalloc_type(
				flags, (unsigned long)__builtin_return_address(
					       0))][index],
			flags, node, size);
	}
	return __kmalloc_node_noprof((size), flags, node);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__alloc_size__(1, 2)))
__attribute__((__malloc__)) void *
kmalloc_array_noprof(size_t n, size_t size, gfp_t flags)
{
	size_t bytes;

	if (__builtin_expect(!!(__must_check_overflow(
				     __builtin_mul_overflow(n, size, &bytes))),
			     0))
		return ((void *)0);
	if (__builtin_constant_p(n) && __builtin_constant_p(size))
		return kmalloc_noprof(bytes, flags);
	return kmalloc_noprof(bytes, flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__alloc_size__(2,
			      3))) void *__attribute__((__warn_unused_result__))
krealloc_array_noprof(void *p, size_t new_n, size_t new_size, gfp_t flags)
{
	size_t bytes;

	if (__builtin_expect(!!(__must_check_overflow(__builtin_mul_overflow(
				     new_n, new_size, &bytes))),
			     0))
		return ((void *)0);

	return krealloc_noprof(p, bytes, flags);
}
void *__kmalloc_node_track_caller_noprof(size_t(size), gfp_t flags, int node,
					 unsigned long caller)
	__attribute__((__alloc_size__(1))) __attribute__((__malloc__));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__alloc_size__(1, 2)))
__attribute__((__malloc__)) void *
kmalloc_array_node_noprof(size_t n, size_t size, gfp_t flags, int node)
{
	size_t bytes;

	if (__builtin_expect(!!(__must_check_overflow(
				     __builtin_mul_overflow(n, size, &bytes))),
			     0))
		return ((void *)0);
	if (__builtin_constant_p(n) && __builtin_constant_p(size))
		return kmalloc_node_noprof(bytes, flags, node);
	return __kmalloc_node_noprof((bytes), flags, node);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__alloc_size__(1)))
__attribute__((__malloc__)) void *
kzalloc_noprof(size_t size, gfp_t flags)
{
	return kmalloc_noprof(
		size, flags | ((gfp_t)((((1UL))) << (___GFP_ZERO_BIT))));
}

void *__kvmalloc_node_noprof(size_t(size), gfp_t flags, int node)
	__attribute__((__alloc_size__(1))) __attribute__((__malloc__));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__alloc_size__(1, 2)))
__attribute__((__malloc__)) void *
kvmalloc_array_node_noprof(size_t n, size_t size, gfp_t flags, int node)
{
	size_t bytes;

	if (__builtin_expect(!!(__must_check_overflow(
				     __builtin_mul_overflow(n, size, &bytes))),
			     0))
		return ((void *)0);

	return __kvmalloc_node_noprof((bytes), flags, node);
}
void *kvrealloc_noprof(const void *p, size_t size, gfp_t flags)
	__attribute__((__alloc_size__(2)));

extern void kvfree(const void *addr);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__free_kvfree(void *p)
{
	void *_T = *(void **)p;
	if (!IS_ERR_OR_NULL(_T))
		kvfree(_T);
}

extern void kvfree_sensitive(const void *addr, size_t len);

unsigned int kmem_cache_size(struct kmem_cache *s);
size_t kmalloc_size_roundup(size_t size);

void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
kmem_cache_init_late(void);
struct crypto_tfm;
struct crypto_type;
struct module;

typedef void (*crypto_completion_t)(void *req, int err);
struct crypto_async_request {
	struct list_head list;
	crypto_completion_t complete;
	void *data;
	struct crypto_tfm *tfm;

	u32 flags;
};
struct cipher_alg {
	unsigned int cia_min_keysize;
	unsigned int cia_max_keysize;
	int (*cia_setkey)(struct crypto_tfm *tfm, const u8 *key,
			  unsigned int keylen);
	void (*cia_encrypt)(struct crypto_tfm *tfm, u8 *dst, const u8 *src);
	void (*cia_decrypt)(struct crypto_tfm *tfm, u8 *dst, const u8 *src);
};
struct compress_alg {
	int (*coa_compress)(struct crypto_tfm *tfm, const u8 *src,
			    unsigned int slen, u8 *dst, unsigned int *dlen);
	int (*coa_decompress)(struct crypto_tfm *tfm, const u8 *src,
			      unsigned int slen, u8 *dst, unsigned int *dlen);
};
struct crypto_alg {
	struct list_head cra_list;
	struct list_head cra_users;

	u32 cra_flags;
	unsigned int cra_blocksize;
	unsigned int cra_ctxsize;
	unsigned int cra_alignmask;

	int cra_priority;
	refcount_t cra_refcnt;

	char cra_name[128];
	char cra_driver_name[128];

	const struct crypto_type *cra_type;

	union {
		struct cipher_alg cipher;
		struct compress_alg compress;
	} cra_u;

	int (*cra_init)(struct crypto_tfm *tfm);
	void (*cra_exit)(struct crypto_tfm *tfm);
	void (*cra_destroy)(struct crypto_alg *alg);

	struct module *cra_module;
} __attribute__((__aligned__(__alignof__(unsigned long long))));

struct crypto_wait {
	struct completion completion;
	int err;
};
void crypto_req_done(void *req, int err);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
crypto_wait_req(int err, struct crypto_wait *wait)
{
	switch (err) {
	case -115:
	case -16:
		wait_for_completion(&wait->completion);
		reinit_completion(&wait->completion);
		err = wait->err;
		break;
	}

	return err;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_init_wait(struct crypto_wait *wait)
{
	init_completion(&wait->completion);
}

int crypto_has_alg(const char *name, u32 type, u32 mask);

struct crypto_tfm {
	refcount_t refcnt;

	u32 crt_flags;

	int node;

	void (*exit)(struct crypto_tfm *tfm);

	struct crypto_alg *__crt_alg;

	void *__crt_ctx[]
		__attribute__((__aligned__(__alignof__(unsigned long long))));
};

struct crypto_comp {
	struct crypto_tfm base;
};

struct crypto_tfm *crypto_alloc_base(const char *alg_name, u32 type, u32 mask);
void crypto_destroy_tfm(void *mem, struct crypto_tfm *tfm);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_free_tfm(struct crypto_tfm *tfm)
{
	return crypto_destroy_tfm(tfm, tfm);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const char *
crypto_tfm_alg_name(struct crypto_tfm *tfm)
{
	return tfm->__crt_alg->cra_name;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const char *
crypto_tfm_alg_driver_name(struct crypto_tfm *tfm)
{
	return tfm->__crt_alg->cra_driver_name;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_tfm_alg_blocksize(struct crypto_tfm *tfm)
{
	return tfm->__crt_alg->cra_blocksize;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_tfm_alg_alignmask(struct crypto_tfm *tfm)
{
	return tfm->__crt_alg->cra_alignmask;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
crypto_tfm_get_flags(struct crypto_tfm *tfm)
{
	return tfm->crt_flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_tfm_set_flags(struct crypto_tfm *tfm, u32 flags)
{
	tfm->crt_flags |= flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_tfm_clear_flags(struct crypto_tfm *tfm, u32 flags)
{
	tfm->crt_flags &= ~flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_tfm_ctx_alignment(void)
{
	struct crypto_tfm *tfm;
	return __alignof__(tfm->__crt_ctx);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_comp *
__crypto_comp_cast(struct crypto_tfm *tfm)
{
	return (struct crypto_comp *)tfm;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_comp *
crypto_alloc_comp(const char *alg_name, u32 type, u32 mask)
{
	type &= ~0x0000000f;
	type |= 0x00000002;
	mask |= 0x0000000f;

	return __crypto_comp_cast(crypto_alloc_base(alg_name, type, mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_tfm *
crypto_comp_tfm(struct crypto_comp *tfm)
{
	return &tfm->base;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_free_comp(struct crypto_comp *tfm)
{
	crypto_free_tfm(crypto_comp_tfm(tfm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
crypto_has_comp(const char *alg_name, u32 type, u32 mask)
{
	type &= ~0x0000000f;
	type |= 0x00000002;
	mask |= 0x0000000f;

	return crypto_has_alg(alg_name, type, mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const char *
crypto_comp_name(struct crypto_comp *tfm)
{
	return crypto_tfm_alg_name(crypto_comp_tfm(tfm));
}

int crypto_comp_compress(struct crypto_comp *tfm, const u8 *src,
			 unsigned int slen, u8 *dst, unsigned int *dlen);

int crypto_comp_decompress(struct crypto_comp *tfm, const u8 *src,
			   unsigned int slen, u8 *dst, unsigned int *dlen);
struct crypto_aead;
struct crypto_instance;
struct module;
struct notifier_block;
struct rtattr;
struct scatterlist;
struct seq_file;
struct sk_buff;

struct crypto_type {
	unsigned int (*ctxsize)(struct crypto_alg *alg, u32 type, u32 mask);
	unsigned int (*extsize)(struct crypto_alg *alg);
	int (*init_tfm)(struct crypto_tfm *tfm);
	void (*show)(struct seq_file *m, struct crypto_alg *alg);
	int (*report)(struct sk_buff *skb, struct crypto_alg *alg);
	void (*free)(struct crypto_instance *inst);

	unsigned int type;
	unsigned int maskclear;
	unsigned int maskset;
	unsigned int tfmsize;
};

struct crypto_instance {
	struct crypto_alg alg;

	struct crypto_template *tmpl;

	union {
		struct hlist_node list;

		struct crypto_spawn *spawns;
	};

	struct work_struct free_work;

	void *__ctx[]
		__attribute__((__aligned__(__alignof__(unsigned long long))));
};

struct crypto_template {
	struct list_head list;
	struct hlist_head instances;
	struct module *module;

	int (*create)(struct crypto_template *tmpl, struct rtattr **tb);

	char name[128];
};

struct crypto_spawn {
	struct list_head list;
	struct crypto_alg *alg;
	union {
		struct crypto_instance *inst;

		struct crypto_spawn *next;
	};
	const struct crypto_type *frontend;
	u32 mask;
	bool dead;
	bool registered;
};

struct crypto_queue {
	struct list_head list;
	struct list_head *backlog;

	unsigned int qlen;
	unsigned int max_qlen;
};

struct scatter_walk {
	struct scatterlist *sg;
	unsigned int offset;
};

struct crypto_attr_alg {
	char name[128];
};

struct crypto_attr_type {
	u32 type;
	u32 mask;
};

int crypto_register_alg(struct crypto_alg *alg);
void crypto_unregister_alg(struct crypto_alg *alg);
int crypto_register_algs(struct crypto_alg *algs, int count);
void crypto_unregister_algs(struct crypto_alg *algs, int count);

void crypto_mod_put(struct crypto_alg *alg);

int crypto_register_template(struct crypto_template *tmpl);
int crypto_register_templates(struct crypto_template *tmpls, int count);
void crypto_unregister_template(struct crypto_template *tmpl);
void crypto_unregister_templates(struct crypto_template *tmpls, int count);
struct crypto_template *crypto_lookup_template(const char *name);

int crypto_register_instance(struct crypto_template *tmpl,
			     struct crypto_instance *inst);
void crypto_unregister_instance(struct crypto_instance *inst);

int crypto_grab_spawn(struct crypto_spawn *spawn, struct crypto_instance *inst,
		      const char *name, u32 type, u32 mask);
void crypto_drop_spawn(struct crypto_spawn *spawn);
struct crypto_tfm *crypto_spawn_tfm(struct crypto_spawn *spawn, u32 type,
				    u32 mask);
void *crypto_spawn_tfm2(struct crypto_spawn *spawn);

struct crypto_attr_type *crypto_get_attr_type(struct rtattr **tb);
int crypto_check_attr_type(struct rtattr **tb, u32 type, u32 *mask_ret);
const char *crypto_attr_alg_name(struct rtattr *rta);
int crypto_inst_setname(struct crypto_instance *inst, const char *name,
			struct crypto_alg *alg);

void crypto_init_queue(struct crypto_queue *queue, unsigned int max_qlen);
int crypto_enqueue_request(struct crypto_queue *queue,
			   struct crypto_async_request *request);
void crypto_enqueue_request_head(struct crypto_queue *queue,
				 struct crypto_async_request *request);
struct crypto_async_request *crypto_dequeue_request(struct crypto_queue *queue);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_queue_len(struct crypto_queue *queue)
{
	return queue->qlen;
}

void crypto_inc(u8 *a, unsigned int size);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
crypto_tfm_ctx(struct crypto_tfm *tfm)
{
	return tfm->__crt_ctx;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
crypto_tfm_ctx_align(struct crypto_tfm *tfm, unsigned int align)
{
	if (align <= crypto_tfm_ctx_alignment())
		align = 1;

	return ((typeof(crypto_tfm_ctx(
		tfm)))(((((unsigned long)(crypto_tfm_ctx(tfm)))) +
			((__typeof__(((unsigned long)(crypto_tfm_ctx(tfm)))))((
				 (align))) -
			 1)) &
		       ~((__typeof__(((unsigned long)(crypto_tfm_ctx(tfm)))))((
				 (align))) -
			 1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_dma_align(void)
{
	return __alignof__(unsigned long long);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_dma_padding(void)
{
	return (crypto_dma_align() - 1) & ~(crypto_tfm_ctx_alignment() - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
crypto_tfm_ctx_dma(struct crypto_tfm *tfm)
{
	return crypto_tfm_ctx_align(tfm, crypto_dma_align());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_instance *
crypto_tfm_alg_instance(struct crypto_tfm *tfm)
{
	return ({
		void *__mptr = (void *)(tfm->__crt_alg);
		_Static_assert(
			__builtin_types_compatible_p(
				typeof(*(tfm->__crt_alg)),
				typeof(((struct crypto_instance *)0)->alg)) ||
				__builtin_types_compatible_p(
					typeof(*(tfm->__crt_alg)),
					typeof(void)),
			"pointer type mismatch in container_of()");
		((struct crypto_instance *)(__mptr -
					    __builtin_offsetof(
						    struct crypto_instance,
						    alg)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
crypto_instance_ctx(struct crypto_instance *inst)
{
	return inst->__ctx;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_async_request *
crypto_get_backlog(struct crypto_queue *queue)
{
	return queue->backlog == &queue->list ? ((void *)0) : ({
		void *__mptr = (void *)(queue->backlog);
		_Static_assert(__builtin_types_compatible_p(
				       typeof(*(queue->backlog)),
				       typeof(((struct crypto_async_request *)0)
						      ->list)) ||
				       __builtin_types_compatible_p(
					       typeof(*(queue->backlog)),
					       typeof(void)),
			       "pointer type mismatch in container_of()");
		((struct crypto_async_request
			  *)(__mptr -
			     __builtin_offsetof(struct crypto_async_request,
						list)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
crypto_requires_off(struct crypto_attr_type *algt, u32 off)
{
	return (algt->type ^ off) & algt->mask & off;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
crypto_algt_inherited_mask(struct crypto_attr_type *algt)
{
	return crypto_requires_off(algt,
				   (0x00000080 | 0x00000100 | 0x00010000));
}

int crypto_register_notifier(struct notifier_block *nb);
int crypto_unregister_notifier(struct notifier_block *nb);

enum {
	CRYPTO_MSG_ALG_REQUEST,
	CRYPTO_MSG_ALG_REGISTER,
	CRYPTO_MSG_ALG_LOADED,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_request_complete(struct crypto_async_request *req, int err)
{
	req->complete(req->data, err);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
crypto_tfm_alg_type(struct crypto_tfm *tfm)
{
	return tfm->__crt_alg->cra_flags & 0x0000000f;
}
struct crypto_ahash;
struct hash_alg_common {
	unsigned int digestsize;
	unsigned int statesize;
	struct crypto_alg base;
};

struct ahash_request {
	struct crypto_async_request base;

	unsigned int nbytes;
	struct scatterlist *src;
	u8 *result;

	void *priv;

	void *__ctx[]
		__attribute__((__aligned__(__alignof__(unsigned long long))));
};
struct ahash_alg {
	int (*init)(struct ahash_request *req);
	int (*update)(struct ahash_request *req);
	int (*final)(struct ahash_request *req);
	int (*finup)(struct ahash_request *req);
	int (*digest)(struct ahash_request *req);
	int (*export)(struct ahash_request *req, void *out);
	int (*import)(struct ahash_request *req, const void *in);
	int (*setkey)(struct crypto_ahash *tfm, const u8 *key,
		      unsigned int keylen);
	int (*init_tfm)(struct crypto_ahash *tfm);
	void (*exit_tfm)(struct crypto_ahash *tfm);
	int (*clone_tfm)(struct crypto_ahash *dst, struct crypto_ahash *src);

	struct hash_alg_common halg;
};

struct shash_desc {
	struct crypto_shash *tfm;
	void *__ctx[]
		__attribute__((__aligned__(__alignof__(unsigned long long))));
};
struct shash_alg {
	int (*init)(struct shash_desc *desc);
	int (*update)(struct shash_desc *desc, const u8 *data,
		      unsigned int len);
	int (*final)(struct shash_desc *desc, u8 *out);
	int (*finup)(struct shash_desc *desc, const u8 *data, unsigned int len,
		     u8 *out);
	int (*digest)(struct shash_desc *desc, const u8 *data, unsigned int len,
		      u8 *out);
	int (*export)(struct shash_desc *desc, void *out);
	int (*import)(struct shash_desc *desc, const void *in);
	int (*setkey)(struct crypto_shash *tfm, const u8 *key,
		      unsigned int keylen);
	int (*init_tfm)(struct crypto_shash *tfm);
	void (*exit_tfm)(struct crypto_shash *tfm);
	int (*clone_tfm)(struct crypto_shash *dst, struct crypto_shash *src);

	unsigned int descsize;

	union {
		struct {
			unsigned int digestsize;
			unsigned int statesize;
			struct crypto_alg base;
		};
		struct hash_alg_common halg;
	};
};

struct crypto_ahash {
	bool using_shash;
	unsigned int statesize;
	unsigned int reqsize;
	struct crypto_tfm base;
};

struct crypto_shash {
	unsigned int descsize;
	struct crypto_tfm base;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_ahash *
__crypto_ahash_cast(struct crypto_tfm *tfm)
{
	return ({
		void *__mptr = (void *)(tfm);
		_Static_assert(
			__builtin_types_compatible_p(
				typeof(*(tfm)),
				typeof(((struct crypto_ahash *)0)->base)) ||
				__builtin_types_compatible_p(typeof(*(tfm)),
							     typeof(void)),
			"pointer type mismatch in container_of()");
		((struct crypto_ahash *)(__mptr -
					 __builtin_offsetof(struct crypto_ahash,
							    base)));
	});
}
struct crypto_ahash *crypto_alloc_ahash(const char *alg_name, u32 type,
					u32 mask);

struct crypto_ahash *crypto_clone_ahash(struct crypto_ahash *tfm);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_tfm *
crypto_ahash_tfm(struct crypto_ahash *tfm)
{
	return &tfm->base;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_free_ahash(struct crypto_ahash *tfm)
{
	crypto_destroy_tfm(tfm, crypto_ahash_tfm(tfm));
}
int crypto_has_ahash(const char *alg_name, u32 type, u32 mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const char *
crypto_ahash_alg_name(struct crypto_ahash *tfm)
{
	return crypto_tfm_alg_name(crypto_ahash_tfm(tfm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const char *
crypto_ahash_driver_name(struct crypto_ahash *tfm)
{
	return crypto_tfm_alg_driver_name(crypto_ahash_tfm(tfm));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_ahash_blocksize(struct crypto_ahash *tfm)
{
	return crypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct hash_alg_common *
__crypto_hash_alg_common(struct crypto_alg *alg)
{
	return ({
		void *__mptr = (void *)(alg);
		_Static_assert(
			__builtin_types_compatible_p(
				typeof(*(alg)),
				typeof(((struct hash_alg_common *)0)->base)) ||
				__builtin_types_compatible_p(typeof(*(alg)),
							     typeof(void)),
			"pointer type mismatch in container_of()");
		((struct hash_alg_common *)(__mptr -
					    __builtin_offsetof(
						    struct hash_alg_common,
						    base)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct hash_alg_common *
crypto_hash_alg_common(struct crypto_ahash *tfm)
{
	return __crypto_hash_alg_common(crypto_ahash_tfm(tfm)->__crt_alg);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_ahash_digestsize(struct crypto_ahash *tfm)
{
	return crypto_hash_alg_common(tfm)->digestsize;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_ahash_statesize(struct crypto_ahash *tfm)
{
	return tfm->statesize;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
crypto_ahash_get_flags(struct crypto_ahash *tfm)
{
	return crypto_tfm_get_flags(crypto_ahash_tfm(tfm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_ahash_set_flags(struct crypto_ahash *tfm, u32 flags)
{
	crypto_tfm_set_flags(crypto_ahash_tfm(tfm), flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_ahash_clear_flags(struct crypto_ahash *tfm, u32 flags)
{
	crypto_tfm_clear_flags(crypto_ahash_tfm(tfm), flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_ahash *
crypto_ahash_reqtfm(struct ahash_request *req)
{
	return __crypto_ahash_cast(req->base.tfm);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_ahash_reqsize(struct crypto_ahash *tfm)
{
	return tfm->reqsize;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
ahash_request_ctx(struct ahash_request *req)
{
	return req->__ctx;
}
int crypto_ahash_setkey(struct crypto_ahash *tfm, const u8 *key,
			unsigned int keylen);
int crypto_ahash_finup(struct ahash_request *req);
int crypto_ahash_final(struct ahash_request *req);
int crypto_ahash_digest(struct ahash_request *req);
int crypto_ahash_export(struct ahash_request *req, void *out);
int crypto_ahash_import(struct ahash_request *req, const void *in);
int crypto_ahash_init(struct ahash_request *req);
int crypto_ahash_update(struct ahash_request *req);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ahash_request_set_tfm(struct ahash_request *req, struct crypto_ahash *tfm)
{
	req->base.tfm = crypto_ahash_tfm(tfm);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct ahash_request *
ahash_request_alloc_noprof(struct crypto_ahash *tfm, gfp_t gfp)
{
	struct ahash_request *req;

	req = kmalloc_noprof(
		sizeof(struct ahash_request) + crypto_ahash_reqsize(tfm), gfp);

	if (__builtin_expect(!!(req), 1))
		ahash_request_set_tfm(req, tfm);

	return req;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ahash_request_free(struct ahash_request *req)
{
	kfree_sensitive(req);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ahash_request_zero(struct ahash_request *req)
{
	memzero_explicit(req, sizeof(*req) + crypto_ahash_reqsize(
						     crypto_ahash_reqtfm(req)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct ahash_request *
ahash_request_cast(struct crypto_async_request *req)
{
	return ({
		void *__mptr = (void *)(req);
		_Static_assert(
			__builtin_types_compatible_p(
				typeof(*(req)),
				typeof(((struct ahash_request *)0)->base)) ||
				__builtin_types_compatible_p(typeof(*(req)),
							     typeof(void)),
			"pointer type mismatch in container_of()");
		((struct ahash_request *)(__mptr -
					  __builtin_offsetof(
						  struct ahash_request, base)));
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ahash_request_set_callback(struct ahash_request *req, u32 flags,
			   crypto_completion_t compl, void *data)
{
	req->base.complete = compl;
	req->base.data = data;
	req->base.flags = flags;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ahash_request_set_crypt(struct ahash_request *req, struct scatterlist *src,
			u8 *result, unsigned int nbytes)
{
	req->src = src;
	req->nbytes = nbytes;
	req->result = result;
}
struct crypto_shash *crypto_alloc_shash(const char *alg_name, u32 type,
					u32 mask);

struct crypto_shash *crypto_clone_shash(struct crypto_shash *tfm);

int crypto_has_shash(const char *alg_name, u32 type, u32 mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_tfm *
crypto_shash_tfm(struct crypto_shash *tfm)
{
	return &tfm->base;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_free_shash(struct crypto_shash *tfm)
{
	crypto_destroy_tfm(tfm, crypto_shash_tfm(tfm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const char *
crypto_shash_alg_name(struct crypto_shash *tfm)
{
	return crypto_tfm_alg_name(crypto_shash_tfm(tfm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const char *
crypto_shash_driver_name(struct crypto_shash *tfm)
{
	return crypto_tfm_alg_driver_name(crypto_shash_tfm(tfm));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_shash_blocksize(struct crypto_shash *tfm)
{
	return crypto_tfm_alg_blocksize(crypto_shash_tfm(tfm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct shash_alg *
__crypto_shash_alg(struct crypto_alg *alg)
{
	return ({
		void *__mptr = (void *)(alg);
		_Static_assert(__builtin_types_compatible_p(
				       typeof(*(alg)),
				       typeof(((struct shash_alg *)0)->base)) ||
				       __builtin_types_compatible_p(
					       typeof(*(alg)), typeof(void)),
			       "pointer type mismatch in container_of()");
		((struct shash_alg *)(__mptr -
				      __builtin_offsetof(struct shash_alg,
							 base)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct shash_alg *
crypto_shash_alg(struct crypto_shash *tfm)
{
	return __crypto_shash_alg(crypto_shash_tfm(tfm)->__crt_alg);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_shash_digestsize(struct crypto_shash *tfm)
{
	return crypto_shash_alg(tfm)->digestsize;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_shash_statesize(struct crypto_shash *tfm)
{
	return crypto_shash_alg(tfm)->statesize;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
crypto_shash_get_flags(struct crypto_shash *tfm)
{
	return crypto_tfm_get_flags(crypto_shash_tfm(tfm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_shash_set_flags(struct crypto_shash *tfm, u32 flags)
{
	crypto_tfm_set_flags(crypto_shash_tfm(tfm), flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_shash_clear_flags(struct crypto_shash *tfm, u32 flags)
{
	crypto_tfm_clear_flags(crypto_shash_tfm(tfm), flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
crypto_shash_descsize(struct crypto_shash *tfm)
{
	return tfm->descsize;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
shash_desc_ctx(struct shash_desc *desc)
{
	return desc->__ctx;
}
int crypto_shash_setkey(struct crypto_shash *tfm, const u8 *key,
			unsigned int keylen);
int crypto_shash_digest(struct shash_desc *desc, const u8 *data,
			unsigned int len, u8 *out);
int crypto_shash_tfm_digest(struct crypto_shash *tfm, const u8 *data,
			    unsigned int len, u8 *out);
int crypto_shash_export(struct shash_desc *desc, void *out);
int crypto_shash_import(struct shash_desc *desc, const void *in);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
crypto_shash_init(struct shash_desc *desc)
{
	struct crypto_shash *tfm = desc->tfm;

	if (crypto_shash_get_flags(tfm) & 0x00000001)
		return -126;

	return crypto_shash_alg(tfm)->init(desc);
}
int crypto_shash_update(struct shash_desc *desc, const u8 *data,
			unsigned int len);
int crypto_shash_final(struct shash_desc *desc, u8 *out);
int crypto_shash_finup(struct shash_desc *desc, const u8 *data,
		       unsigned int len, u8 *out);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
shash_desc_zero(struct shash_desc *desc)
{
	memzero_explicit(desc,
			 sizeof(*desc) + crypto_shash_descsize(desc->tfm));
}

struct ahash_request;
struct scatterlist;

struct crypto_hash_walk {
	char *data;

	unsigned int offset;
	unsigned int flags;

	struct page *pg;
	unsigned int entrylen;

	unsigned int total;
	struct scatterlist *sg;
};

struct ahash_instance {
	void (*free)(struct ahash_instance *inst);
	union {
		struct {
			char head[__builtin_offsetof(struct ahash_alg,
						     halg.base)];
			struct crypto_instance base;
		} s;
		struct ahash_alg alg;
	};
};

struct shash_instance {
	void (*free)(struct shash_instance *inst);
	union {
		struct {
			char head[__builtin_offsetof(struct shash_alg, base)];
			struct crypto_instance base;
		} s;
		struct shash_alg alg;
	};
};

struct crypto_ahash_spawn {
	struct crypto_spawn base;
};

struct crypto_shash_spawn {
	struct crypto_spawn base;
};

int crypto_hash_walk_done(struct crypto_hash_walk *walk, int err);
int crypto_hash_walk_first(struct ahash_request *req,
			   struct crypto_hash_walk *walk);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
crypto_hash_walk_last(struct crypto_hash_walk *walk)
{
	return !(walk->entrylen | walk->total);
}

int crypto_register_ahash(struct ahash_alg *alg);
void crypto_unregister_ahash(struct ahash_alg *alg);
int crypto_register_ahashes(struct ahash_alg *algs, int count);
void crypto_unregister_ahashes(struct ahash_alg *algs, int count);
int ahash_register_instance(struct crypto_template *tmpl,
			    struct ahash_instance *inst);

int shash_no_setkey(struct crypto_shash *tfm, const u8 *key,
		    unsigned int keylen);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
crypto_shash_alg_has_setkey(struct shash_alg *alg)
{
	return alg->setkey != shash_no_setkey;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
crypto_shash_alg_needs_key(struct shash_alg *alg)
{
	return crypto_shash_alg_has_setkey(alg) &&
	       !(alg->base.cra_flags & 0x00004000);
}

int crypto_grab_ahash(struct crypto_ahash_spawn *spawn,
		      struct crypto_instance *inst, const char *name, u32 type,
		      u32 mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_drop_ahash(struct crypto_ahash_spawn *spawn)
{
	crypto_drop_spawn(&spawn->base);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct hash_alg_common *
crypto_spawn_ahash_alg(struct crypto_ahash_spawn *spawn)
{
	return __crypto_hash_alg_common(spawn->base.alg);
}

int crypto_register_shash(struct shash_alg *alg);
void crypto_unregister_shash(struct shash_alg *alg);
int crypto_register_shashes(struct shash_alg *algs, int count);
void crypto_unregister_shashes(struct shash_alg *algs, int count);
int shash_register_instance(struct crypto_template *tmpl,
			    struct shash_instance *inst);
void shash_free_singlespawn_instance(struct shash_instance *inst);

int crypto_grab_shash(struct crypto_shash_spawn *spawn,
		      struct crypto_instance *inst, const char *name, u32 type,
		      u32 mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_drop_shash(struct crypto_shash_spawn *spawn)
{
	crypto_drop_spawn(&spawn->base);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct shash_alg *
crypto_spawn_shash_alg(struct crypto_shash_spawn *spawn)
{
	return __crypto_shash_alg(spawn->base.alg);
}

int shash_ahash_update(struct ahash_request *req, struct shash_desc *desc);
int shash_ahash_finup(struct ahash_request *req, struct shash_desc *desc);
int shash_ahash_digest(struct ahash_request *req, struct shash_desc *desc);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
crypto_ahash_ctx(struct crypto_ahash *tfm)
{
	return crypto_tfm_ctx(crypto_ahash_tfm(tfm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
crypto_ahash_ctx_dma(struct crypto_ahash *tfm)
{
	return crypto_tfm_ctx_dma(crypto_ahash_tfm(tfm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct ahash_alg *
__crypto_ahash_alg(struct crypto_alg *alg)
{
	return ({
		void *__mptr = (void *)(__crypto_hash_alg_common(alg));
		_Static_assert(
			__builtin_types_compatible_p(
				typeof(*(__crypto_hash_alg_common(alg))),
				typeof(((struct ahash_alg *)0)->halg)) ||
				__builtin_types_compatible_p(
					typeof(*(__crypto_hash_alg_common(alg))),
					typeof(void)),
			"pointer type mismatch in container_of()");
		((struct ahash_alg *)(__mptr -
				      __builtin_offsetof(struct ahash_alg,
							 halg)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct ahash_alg *
crypto_ahash_alg(struct crypto_ahash *hash)
{
	return ({
		void *__mptr = (void *)(crypto_hash_alg_common(hash));
		_Static_assert(
			__builtin_types_compatible_p(
				typeof(*(crypto_hash_alg_common(hash))),
				typeof(((struct ahash_alg *)0)->halg)) ||
				__builtin_types_compatible_p(
					typeof(*(crypto_hash_alg_common(hash))),
					typeof(void)),
			"pointer type mismatch in container_of()");
		((struct ahash_alg *)(__mptr -
				      __builtin_offsetof(struct ahash_alg,
							 halg)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_ahash_set_statesize(struct crypto_ahash *tfm, unsigned int size)
{
	tfm->statesize = size;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_ahash_set_reqsize(struct crypto_ahash *tfm, unsigned int reqsize)
{
	tfm->reqsize = reqsize;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
crypto_ahash_set_reqsize_dma(struct crypto_ahash *ahash, unsigned int reqsize)
{
	reqsize += crypto_dma_align() & ~(crypto_tfm_ctx_alignment() - 1);
	ahash->reqsize = reqsize;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_instance *
ahash_crypto_instance(struct ahash_instance *inst)
{
	return &inst->s.base;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct ahash_instance *
ahash_instance(struct crypto_instance *inst)
{
	return ({
		void *__mptr = (void *)(inst);
		_Static_assert(
			__builtin_types_compatible_p(
				typeof(*(inst)),
				typeof(((struct ahash_instance *)0)->s.base)) ||
				__builtin_types_compatible_p(typeof(*(inst)),
							     typeof(void)),
			"pointer type mismatch in container_of()");
		((struct ahash_instance *)(__mptr -
					   __builtin_offsetof(
						   struct ahash_instance,
						   s.base)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct ahash_instance *
ahash_alg_instance(struct crypto_ahash *ahash)
{
	return ahash_instance(crypto_tfm_alg_instance(&ahash->base));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
ahash_instance_ctx(struct ahash_instance *inst)
{
	return crypto_instance_ctx(ahash_crypto_instance(inst));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
ahash_request_ctx_dma(struct ahash_request *req)
{
	unsigned int align = crypto_dma_align();

	if (align <= crypto_tfm_ctx_alignment())
		align = 1;

	return ((typeof(ahash_request_ctx(
		req)))(((((unsigned long)(ahash_request_ctx(req)))) +
			((__typeof__(((unsigned long)(ahash_request_ctx(
				 req)))))(((align))) -
			 1)) &
		       ~((__typeof__(((unsigned long)(ahash_request_ctx(
				 req)))))(((align))) -
			 1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ahash_request_complete(struct ahash_request *req, int err)
{
	crypto_request_complete(&req->base, err);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
ahash_request_flags(struct ahash_request *req)
{
	return req->base.flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_ahash *
crypto_spawn_ahash(struct crypto_ahash_spawn *spawn)
{
	return crypto_spawn_tfm2(&spawn->base);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
ahash_enqueue_request(struct crypto_queue *queue, struct ahash_request *request)
{
	return crypto_enqueue_request(queue, &request->base);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct ahash_request *
ahash_dequeue_request(struct crypto_queue *queue)
{
	return ahash_request_cast(crypto_dequeue_request(queue));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
crypto_shash_ctx(struct crypto_shash *tfm)
{
	return crypto_tfm_ctx(&tfm->base);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_instance *
shash_crypto_instance(struct shash_instance *inst)
{
	return &inst->s.base;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct shash_instance *
shash_instance(struct crypto_instance *inst)
{
	return ({
		void *__mptr = (void *)(inst);
		_Static_assert(
			__builtin_types_compatible_p(
				typeof(*(inst)),
				typeof(((struct shash_instance *)0)->s.base)) ||
				__builtin_types_compatible_p(typeof(*(inst)),
							     typeof(void)),
			"pointer type mismatch in container_of()");
		((struct shash_instance *)(__mptr -
					   __builtin_offsetof(
						   struct shash_instance,
						   s.base)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct shash_instance *
shash_alg_instance(struct crypto_shash *shash)
{
	return shash_instance(crypto_tfm_alg_instance(&shash->base));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
shash_instance_ctx(struct shash_instance *inst)
{
	return crypto_instance_ctx(shash_crypto_instance(inst));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_shash *
crypto_spawn_shash(struct crypto_shash_spawn *spawn)
{
	return crypto_spawn_tfm2(&spawn->base);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct crypto_shash *
__crypto_shash_cast(struct crypto_tfm *tfm)
{
	return ({
		void *__mptr = (void *)(tfm);
		_Static_assert(
			__builtin_types_compatible_p(
				typeof(*(tfm)),
				typeof(((struct crypto_shash *)0)->base)) ||
				__builtin_types_compatible_p(typeof(*(tfm)),
							     typeof(void)),
			"pointer type mismatch in container_of()");
		((struct crypto_shash *)(__mptr -
					 __builtin_offsetof(struct crypto_shash,
							    base)));
	});
}
extern const u8 sha224_zero_message_hash[28];

extern const u8 sha256_zero_message_hash[32];

extern const u8 sha384_zero_message_hash[48];

extern const u8 sha512_zero_message_hash[64];

struct sha256_state {
	u32 state[32 / 4];
	u64 count;
	u8 buf[64];
};

struct sha512_state {
	u64 state[64 / 8];
	u64 count[2];
	u8 buf[128];
};

struct shash_desc;

extern int crypto_sha256_update(struct shash_desc *desc, const u8 *data,
				unsigned int len);

extern int crypto_sha256_finup(struct shash_desc *desc, const u8 *data,
			       unsigned int len, u8 *hash);

extern int crypto_sha512_update(struct shash_desc *desc, const u8 *data,
				unsigned int len);

extern int crypto_sha512_finup(struct shash_desc *desc, const u8 *data,
			       unsigned int len, u8 *hash);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sha256_init(struct sha256_state *sctx)
{
	sctx->state[0] = 0x6a09e667UL;
	sctx->state[1] = 0xbb67ae85UL;
	sctx->state[2] = 0x3c6ef372UL;
	sctx->state[3] = 0xa54ff53aUL;
	sctx->state[4] = 0x510e527fUL;
	sctx->state[5] = 0x9b05688cUL;
	sctx->state[6] = 0x1f83d9abUL;
	sctx->state[7] = 0x5be0cd19UL;
	sctx->count = 0;
}
void sha256_update(struct sha256_state *sctx, const u8 *data, unsigned int len);
void sha256_final(struct sha256_state *sctx, u8 *out);
void sha256(const u8 *data, unsigned int len, u8 *out);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sha224_init(struct sha256_state *sctx)
{
	sctx->state[0] = 0xc1059ed8UL;
	sctx->state[1] = 0x367cd507UL;
	sctx->state[2] = 0x3070dd17UL;
	sctx->state[3] = 0xf70e5939UL;
	sctx->state[4] = 0xffc00b31UL;
	sctx->state[5] = 0x68581511UL;
	sctx->state[6] = 0x64f98fa7UL;
	sctx->state[7] = 0xbefa4fa4UL;
	sctx->count = 0;
}

void sha224_final(struct sha256_state *sctx, u8 *out);

typedef void(sha256_block_fn)(struct sha256_state *sst, u8 const *src,
			      int blocks);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sha224_base_init(struct shash_desc *desc)
{
	struct sha256_state *sctx = shash_desc_ctx(desc);

	sha224_init(sctx);
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sha256_base_init(struct shash_desc *desc)
{
	struct sha256_state *sctx = shash_desc_ctx(desc);

	sha256_init(sctx);
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
lib_sha256_base_do_update(struct sha256_state *sctx, const u8 *data,
			  unsigned int len, sha256_block_fn *block_fn)
{
	unsigned int partial = sctx->count % 64;

	sctx->count += len;

	if (__builtin_expect(!!((partial + len) >= 64), 0)) {
		int blocks;

		if (partial) {
			int p = 64 - partial;

			({
				const size_t __fortify_size = (size_t)(p);
				const size_t __p_size =
					(__builtin_dynamic_object_size(
						sctx->buf + partial, 0));
				const size_t __q_size =
					(__builtin_dynamic_object_size(data,
								       0));
				const size_t __p_size_field =
					(__builtin_dynamic_object_size(
						sctx->buf + partial, 1));
				const size_t __q_size_field =
					(__builtin_dynamic_object_size(data,
								       1));
				({
					bool __ret_do_once =
						!!(fortify_memcpy_chk(
							__fortify_size,
							__p_size, __q_size,
							__p_size_field,
							__q_size_field,
							FORTIFY_FUNC_memcpy));
					if (({
						    static bool __attribute__((
							    __section__(
								    ".data.once")))
						    __already_done;
						    bool __ret_cond =
							    !!(__ret_do_once);
						    bool __ret_once = false;
						    if (__builtin_expect(
								!!(__ret_cond &&
								   !__already_done),
								0)) {
							    __already_done =
								    true;
							    __ret_once = true;
						    }
						    __builtin_expect(
							    !!(__ret_once), 0);
					    }))
						({
							int __ret_warn_on =
								!!(1);
							if (__builtin_expect(
								    !!(__ret_warn_on),
								    0))
								do {
									({
										asm volatile(
											"364"
											": nop\n\t"
											".pushsection .discard.instr_begin\n\t"
											".long "
											"364"
											"b - .\n\t"
											".popsection\n\t"
											:
											: "i"(364));
									});
									__warn_printk(
										"memcpy"
										": detected field-spanning write (size %zu) of single %s (size %zu)\n",
										__fortify_size,
										"field \""
										"sctx->buf + partial"
										"\" at "
										"include/crypto/sha256_base.h"
										":"
										"52",
										__p_size_field);
									do {
										__auto_type __flags =
											(1
											 << 0) |
											((1
											  << 3) |
											 ((9)
											  << 8));
										({
											asm volatile(
												"365"
												": nop\n\t"
												".pushsection .discard.instr_begin\n\t"
												".long "
												"365"
												"b - .\n\t"
												".popsection\n\t"
												:
												: "i"(365));
										});
										do {
											asm __inline volatile(
												"1:\t"
												".byte 0x0f, 0x0b"
												"\n"
												".pushsection __bug_table,\"aw\"\n"
												"2:\t"
												".long "
												"1b"
												" - ."
												"\t# bug_entry::bug_addr\n"
												"\t"
												".long "
												"%c0"
												" - ."
												"\t# bug_entry::file\n"
												"\t.word %c1"
												"\t# bug_entry::line\n"
												"\t.word %c2"
												"\t# bug_entry::flags\n"
												"\t.org 2b+%c3\n"
												".popsection\n"
												"998:\n\t"
												".pushsection .discard.reachable\n\t"
												".long 998b\n\t"
												".popsection\n\t"
												:
												: "i"("include/crypto/sha256_base.h"),
												  "i"(52),
												  "i"(__flags),
												  "i"(sizeof(
													  struct bug_entry)));
										} while (
											0);
										({
											asm volatile(
												"366"
												": nop\n\t"
												".pushsection .discard.instr_end\n\t"
												".long "
												"366"
												"b - .\n\t"
												".popsection\n\t"
												:
												: "i"(366));
										});
									} while (
										0);
									({
										asm volatile(
											"367"
											": nop\n\t"
											".pushsection .discard.instr_end\n\t"
											".long "
											"367"
											"b - .\n\t"
											".popsection\n\t"
											:
											: "i"(367));
									});
								} while (0);
							__builtin_expect(
								!!(__ret_warn_on),
								0);
						});
					__builtin_expect(!!(__ret_do_once), 0);
				});
				__builtin_memcpy(sctx->buf + partial, data,
						 __fortify_size);
			});
			data += p;
			len -= p;

			block_fn(sctx, sctx->buf, 1);
		}

		blocks = len / 64;
		len %= 64;

		if (blocks) {
			block_fn(sctx, data, blocks);
			data += blocks * 64;
		}
		partial = 0;
	}
	if (len)
		({
			const size_t __fortify_size = (size_t)(len);
			const size_t __p_size = (__builtin_dynamic_object_size(
				sctx->buf + partial, 0));
			const size_t __q_size =
				(__builtin_dynamic_object_size(data, 0));
			const size_t __p_size_field =
				(__builtin_dynamic_object_size(
					sctx->buf + partial, 1));
			const size_t __q_size_field =
				(__builtin_dynamic_object_size(data, 1));
			({
				bool __ret_do_once = !!(fortify_memcpy_chk(
					__fortify_size, __p_size, __q_size,
					__p_size_field, __q_size_field,
					FORTIFY_FUNC_memcpy));
				if (({
					    static bool __attribute__((
						    __section__(".data.once")))
					    __already_done;
					    bool __ret_cond = !!(__ret_do_once);
					    bool __ret_once = false;
					    if (__builtin_expect(
							!!(__ret_cond &&
							   !__already_done),
							0)) {
						    __already_done = true;
						    __ret_once = true;
					    }
					    __builtin_expect(!!(__ret_once), 0);
				    }))
					({
						int __ret_warn_on = !!(1);
						if (__builtin_expect(
							    !!(__ret_warn_on),
							    0))
							do {
								({
									asm volatile(
										"368"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"368"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(368));
								});
								__warn_printk(
									"memcpy"
									": detected field-spanning write (size %zu) of single %s (size %zu)\n",
									__fortify_size,
									"field \""
									"sctx->buf + partial"
									"\" at "
									"include/crypto/sha256_base.h"
									":"
									"69",
									__p_size_field);
								do {
									__auto_type __flags =
										(1
										 << 0) |
										((1
										  << 3) |
										 ((9)
										  << 8));
									({
										asm volatile(
											"369"
											": nop\n\t"
											".pushsection .discard.instr_begin\n\t"
											".long "
											"369"
											"b - .\n\t"
											".popsection\n\t"
											:
											: "i"(369));
									});
									do {
										asm __inline volatile(
											"1:\t"
											".byte 0x0f, 0x0b"
											"\n"
											".pushsection __bug_table,\"aw\"\n"
											"2:\t"
											".long "
											"1b"
											" - ."
											"\t# bug_entry::bug_addr\n"
											"\t"
											".long "
											"%c0"
											" - ."
											"\t# bug_entry::file\n"
											"\t.word %c1"
											"\t# bug_entry::line\n"
											"\t.word %c2"
											"\t# bug_entry::flags\n"
											"\t.org 2b+%c3\n"
											".popsection\n"
											"998:\n\t"
											".pushsection .discard.reachable\n\t"
											".long 998b\n\t"
											".popsection\n\t"
											:
											: "i"("include/crypto/sha256_base.h"),
											  "i"(69),
											  "i"(__flags),
											  "i"(sizeof(
												  struct bug_entry)));
									} while (
										0);
									({
										asm volatile(
											"370"
											": nop\n\t"
											".pushsection .discard.instr_end\n\t"
											".long "
											"370"
											"b - .\n\t"
											".popsection\n\t"
											:
											: "i"(370));
									});
								} while (0);
								({
									asm volatile(
										"371"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"371"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(371));
								});
							} while (0);
						__builtin_expect(
							!!(__ret_warn_on), 0);
					});
				__builtin_expect(!!(__ret_do_once), 0);
			});
			__builtin_memcpy(sctx->buf + partial, data,
					 __fortify_size);
		});

	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sha256_base_do_update(struct shash_desc *desc, const u8 *data, unsigned int len,
		      sha256_block_fn *block_fn)
{
	struct sha256_state *sctx = shash_desc_ctx(desc);

	return lib_sha256_base_do_update(sctx, data, len, block_fn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
lib_sha256_base_do_finalize(struct sha256_state *sctx,
			    sha256_block_fn *block_fn)
{
	const int bit_offset = 64 - sizeof(__be64);
	__be64 *bits = (__be64 *)(sctx->buf + bit_offset);
	unsigned int partial = sctx->count % 64;

	sctx->buf[partial++] = 0x80;
	if (partial > bit_offset) {
		({
			size_t __fortify_size = (size_t)(64 - partial);
			fortify_memset_chk(__fortify_size,
					   __builtin_dynamic_object_size(
						   sctx->buf + partial, 0),
					   __builtin_dynamic_object_size(
						   sctx->buf + partial, 1)),
				__builtin_memset(sctx->buf + partial, 0x0,
						 __fortify_size);
		});
		partial = 0;

		block_fn(sctx, sctx->buf, 1);
	}

	({
		size_t __fortify_size = (size_t)(bit_offset - partial);
		fortify_memset_chk(
			__fortify_size,
			__builtin_dynamic_object_size(sctx->buf + partial, 0),
			__builtin_dynamic_object_size(sctx->buf + partial, 1)),
			__builtin_memset(sctx->buf + partial, 0x0,
					 __fortify_size);
	});
	*bits = ((__be64)(__u64)__builtin_bswap64((__u64)((sctx->count << 3))));
	block_fn(sctx, sctx->buf, 1);

	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sha256_base_do_finalize(struct shash_desc *desc, sha256_block_fn *block_fn)
{
	struct sha256_state *sctx = shash_desc_ctx(desc);

	return lib_sha256_base_do_finalize(sctx, block_fn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
lib_sha256_base_finish(struct sha256_state *sctx, u8 *out,
		       unsigned int digest_size)
{
	__be32 *digest = (__be32 *)out;
	int i;

	for (i = 0; digest_size > 0; i++, digest_size -= sizeof(__be32))
		put_unaligned_be32(sctx->state[i], digest++);

	memzero_explicit(sctx, sizeof(*sctx));
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sha256_base_finish(struct shash_desc *desc, u8 *out)
{
	unsigned int digest_size = crypto_shash_digestsize(desc->tfm);
	struct sha256_state *sctx = shash_desc_ctx(desc);

	return lib_sha256_base_finish(sctx, out, digest_size);
}

struct stat {
	__kernel_ulong_t st_dev;
	__kernel_ulong_t st_ino;
	__kernel_ulong_t st_nlink;

	unsigned int st_mode;
	unsigned int st_uid;
	unsigned int st_gid;
	unsigned int __pad0;
	__kernel_ulong_t st_rdev;
	__kernel_long_t st_size;
	__kernel_long_t st_blksize;
	__kernel_long_t st_blocks;

	__kernel_ulong_t st_atime;
	__kernel_ulong_t st_atime_nsec;
	__kernel_ulong_t st_mtime;
	__kernel_ulong_t st_mtime_nsec;
	__kernel_ulong_t st_ctime;
	__kernel_ulong_t st_ctime_nsec;
	__kernel_long_t __unused[3];
};
struct __old_kernel_stat {
	unsigned short st_dev;
	unsigned short st_ino;
	unsigned short st_mode;
	unsigned short st_nlink;
	unsigned short st_uid;
	unsigned short st_gid;
	unsigned short st_rdev;

	unsigned int st_size;
	unsigned int st_atime;
	unsigned int st_mtime;
	unsigned int st_ctime;
};
struct statx_timestamp {
	__s64 tv_sec;
	__u32 tv_nsec;
	__s32 __reserved;
};
struct statx {
	__u32 stx_mask;
	__u32 stx_blksize;
	__u64 stx_attributes;

	__u32 stx_nlink;
	__u32 stx_uid;
	__u32 stx_gid;
	__u16 stx_mode;
	__u16 __spare0[1];

	__u64 stx_ino;
	__u64 stx_size;
	__u64 stx_blocks;
	__u64 stx_attributes_mask;

	struct statx_timestamp stx_atime;
	struct statx_timestamp stx_btime;
	struct statx_timestamp stx_ctime;
	struct statx_timestamp stx_mtime;

	__u32 stx_rdev_major;
	__u32 stx_rdev_minor;
	__u32 stx_dev_major;
	__u32 stx_dev_minor;

	__u64 stx_mnt_id;
	__u32 stx_dio_mem_align;
	__u32 stx_dio_offset_align;

	__u64 stx_subvol;
	__u32 stx_atomic_write_unit_min;
	__u32 stx_atomic_write_unit_max;

	__u32 stx_atomic_write_segments_max;
	__u32 __spare1[1];

	__u64 __spare3[9];
};
extern int overflowuid;
extern int overflowgid;

extern void __bad_uid(void);
extern void __bad_gid(void);
extern int fs_overflowuid;
extern int fs_overflowgid;

struct user_namespace;
extern struct user_namespace init_user_ns;
struct uid_gid_map;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) uid_t
__kuid_val(kuid_t uid)
{
	return uid.val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) gid_t
__kgid_val(kgid_t gid)
{
	return gid.val;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
uid_eq(kuid_t left, kuid_t right)
{
	return __kuid_val(left) == __kuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
gid_eq(kgid_t left, kgid_t right)
{
	return __kgid_val(left) == __kgid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
uid_gt(kuid_t left, kuid_t right)
{
	return __kuid_val(left) > __kuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
gid_gt(kgid_t left, kgid_t right)
{
	return __kgid_val(left) > __kgid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
uid_gte(kuid_t left, kuid_t right)
{
	return __kuid_val(left) >= __kuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
gid_gte(kgid_t left, kgid_t right)
{
	return __kgid_val(left) >= __kgid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
uid_lt(kuid_t left, kuid_t right)
{
	return __kuid_val(left) < __kuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
gid_lt(kgid_t left, kgid_t right)
{
	return __kgid_val(left) < __kgid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
uid_lte(kuid_t left, kuid_t right)
{
	return __kuid_val(left) <= __kuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
gid_lte(kgid_t left, kgid_t right)
{
	return __kgid_val(left) <= __kgid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
uid_valid(kuid_t uid)
{
	return __kuid_val(uid) != (uid_t)-1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
gid_valid(kgid_t gid)
{
	return __kgid_val(gid) != (gid_t)-1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kuid_t
make_kuid(struct user_namespace *from, uid_t uid)
{
	return (kuid_t){ uid };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kgid_t
make_kgid(struct user_namespace *from, gid_t gid)
{
	return (kgid_t){ gid };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) uid_t
from_kuid(struct user_namespace *to, kuid_t kuid)
{
	return __kuid_val(kuid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) gid_t
from_kgid(struct user_namespace *to, kgid_t kgid)
{
	return __kgid_val(kgid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) uid_t
from_kuid_munged(struct user_namespace *to, kuid_t kuid)
{
	uid_t uid = from_kuid(to, kuid);
	if (uid == (uid_t)-1)
		uid = overflowuid;
	return uid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) gid_t
from_kgid_munged(struct user_namespace *to, kgid_t kgid)
{
	gid_t gid = from_kgid(to, kgid);
	if (gid == (gid_t)-1)
		gid = overflowgid;
	return gid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kuid_has_mapping(struct user_namespace *ns, kuid_t uid)
{
	return uid_valid(uid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kgid_has_mapping(struct user_namespace *ns, kgid_t gid)
{
	return gid_valid(gid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
map_id_down(struct uid_gid_map *map, u32 id)
{
	return id;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
map_id_up(struct uid_gid_map *map, u32 id)
{
	return id;
}

struct kstat {
	u32 result_mask;
	umode_t mode;
	unsigned int nlink;
	uint32_t blksize;
	u64 attributes;
	u64 attributes_mask;
	u64 ino;
	dev_t dev;
	dev_t rdev;
	kuid_t uid;
	kgid_t gid;
	loff_t size;
	struct timespec64 atime;
	struct timespec64 mtime;
	struct timespec64 ctime;
	struct timespec64 btime;
	u64 blocks;
	u64 mnt_id;
	u32 dio_mem_align;
	u32 dio_offset_align;
	u64 change_cookie;
	u64 subvol;
	u32 atomic_write_unit_min;
	u32 atomic_write_unit_max;
	u32 atomic_write_segments_max;
};

struct vm_area_struct;
int build_id_parse(struct vm_area_struct *vma, unsigned char *build_id,
		   __u32 *size);
int build_id_parse_nofault(struct vm_area_struct *vma, unsigned char *build_id,
			   __u32 *size);
int build_id_parse_buf(const void *buf, unsigned char *build_id, u32 buf_size);

extern unsigned char vmlinux_build_id[20];
void init_vmlinux_build_id(void);

struct __sysctl_args {
	int *name;
	int nlen;
	void *oldval;
	size_t *oldlenp;
	void *newval;
	size_t newlen;
	unsigned long __unused[4];
};

enum {
	CTL_KERN = 1,
	CTL_VM = 2,
	CTL_NET = 3,
	CTL_PROC = 4,
	CTL_FS = 5,
	CTL_DEBUG = 6,
	CTL_DEV = 7,
	CTL_BUS = 8,
	CTL_ABI = 9,
	CTL_CPU = 10,
	CTL_ARLAN = 254,
	CTL_S390DBF = 5677,
	CTL_SUNRPC = 7249,
	CTL_PM = 9899,
	CTL_FRV = 9898,
};

enum { CTL_BUS_ISA = 1 };

enum {
	INOTIFY_MAX_USER_INSTANCES = 1,
	INOTIFY_MAX_USER_WATCHES = 2,
	INOTIFY_MAX_QUEUED_EVENTS = 3
};

enum {
	KERN_OSTYPE = 1,
	KERN_OSRELEASE = 2,
	KERN_OSREV = 3,
	KERN_VERSION = 4,
	KERN_SECUREMASK = 5,
	KERN_PROF = 6,
	KERN_NODENAME = 7,
	KERN_DOMAINNAME = 8,

	KERN_PANIC = 15,
	KERN_REALROOTDEV = 16,

	KERN_SPARC_REBOOT = 21,
	KERN_CTLALTDEL = 22,
	KERN_PRINTK = 23,
	KERN_NAMETRANS = 24,
	KERN_PPC_HTABRECLAIM = 25,
	KERN_PPC_ZEROPAGED = 26,
	KERN_PPC_POWERSAVE_NAP = 27,
	KERN_MODPROBE = 28,
	KERN_SG_BIG_BUFF = 29,
	KERN_ACCT = 30,
	KERN_PPC_L2CR = 31,

	KERN_RTSIGNR = 32,
	KERN_RTSIGMAX = 33,

	KERN_SHMMAX = 34,
	KERN_MSGMAX = 35,
	KERN_MSGMNB = 36,
	KERN_MSGPOOL = 37,
	KERN_SYSRQ = 38,
	KERN_MAX_THREADS = 39,
	KERN_RANDOM = 40,
	KERN_SHMALL = 41,
	KERN_MSGMNI = 42,
	KERN_SEM = 43,
	KERN_SPARC_STOP_A = 44,
	KERN_SHMMNI = 45,
	KERN_OVERFLOWUID = 46,
	KERN_OVERFLOWGID = 47,
	KERN_SHMPATH = 48,
	KERN_HOTPLUG = 49,
	KERN_IEEE_EMULATION_WARNINGS = 50,
	KERN_S390_USER_DEBUG_LOGGING = 51,
	KERN_CORE_USES_PID = 52,
	KERN_TAINTED = 53,
	KERN_CADPID = 54,
	KERN_PIDMAX = 55,
	KERN_CORE_PATTERN = 56,
	KERN_PANIC_ON_OOPS = 57,
	KERN_HPPA_PWRSW = 58,
	KERN_HPPA_UNALIGNED = 59,
	KERN_PRINTK_RATELIMIT = 60,
	KERN_PRINTK_RATELIMIT_BURST = 61,
	KERN_PTY = 62,
	KERN_NGROUPS_MAX = 63,
	KERN_SPARC_SCONS_PWROFF = 64,
	KERN_HZ_TIMER = 65,
	KERN_UNKNOWN_NMI_PANIC = 66,
	KERN_BOOTLOADER_TYPE = 67,
	KERN_RANDOMIZE = 68,
	KERN_SETUID_DUMPABLE = 69,
	KERN_SPIN_RETRY = 70,
	KERN_ACPI_VIDEO_FLAGS = 71,
	KERN_IA64_UNALIGNED = 72,
	KERN_COMPAT_LOG = 73,
	KERN_MAX_LOCK_DEPTH = 74,
	KERN_NMI_WATCHDOG = 75,
	KERN_PANIC_ON_NMI = 76,
	KERN_PANIC_ON_WARN = 77,
	KERN_PANIC_PRINT = 78,
};

enum {
	VM_UNUSED1 = 1,
	VM_UNUSED2 = 2,
	VM_UNUSED3 = 3,
	VM_UNUSED4 = 4,
	VM_OVERCOMMIT_MEMORY = 5,
	VM_UNUSED5 = 6,
	VM_UNUSED7 = 7,
	VM_UNUSED8 = 8,
	VM_UNUSED9 = 9,
	VM_PAGE_CLUSTER = 10,
	VM_DIRTY_BACKGROUND = 11,
	VM_DIRTY_RATIO = 12,
	VM_DIRTY_WB_CS = 13,
	VM_DIRTY_EXPIRE_CS = 14,
	VM_NR_PDFLUSH_THREADS = 15,
	VM_OVERCOMMIT_RATIO = 16,
	VM_PAGEBUF = 17,
	VM_HUGETLB_PAGES = 18,
	VM_SWAPPINESS = 19,
	VM_LOWMEM_RESERVE_RATIO = 20,
	VM_MIN_FREE_KBYTES = 21,
	VM_MAX_MAP_COUNT = 22,
	VM_LAPTOP_MODE = 23,
	VM_BLOCK_DUMP = 24,
	VM_HUGETLB_GROUP = 25,
	VM_VFS_CACHE_PRESSURE = 26,
	VM_LEGACY_VA_LAYOUT = 27,
	VM_SWAP_TOKEN_TIMEOUT = 28,
	VM_DROP_PAGECACHE = 29,
	VM_PERCPU_PAGELIST_FRACTION = 30,
	VM_ZONE_RECLAIM_MODE = 31,
	VM_MIN_UNMAPPED = 32,
	VM_PANIC_ON_OOM = 33,
	VM_VDSO_ENABLED = 34,
	VM_MIN_SLAB = 35,
};

enum {
	NET_CORE = 1,
	NET_ETHER = 2,
	NET_802 = 3,
	NET_UNIX = 4,
	NET_IPV4 = 5,
	NET_IPX = 6,
	NET_ATALK = 7,
	NET_NETROM = 8,
	NET_AX25 = 9,
	NET_BRIDGE = 10,
	NET_ROSE = 11,
	NET_IPV6 = 12,
	NET_X25 = 13,
	NET_TR = 14,
	NET_DECNET = 15,
	NET_ECONET = 16,
	NET_SCTP = 17,
	NET_LLC = 18,
	NET_NETFILTER = 19,
	NET_DCCP = 20,
	NET_IRDA = 412,
};

enum {
	RANDOM_POOLSIZE = 1,
	RANDOM_ENTROPY_COUNT = 2,
	RANDOM_READ_THRESH = 3,
	RANDOM_WRITE_THRESH = 4,
	RANDOM_BOOT_ID = 5,
	RANDOM_UUID = 6
};

enum { PTY_MAX = 1, PTY_NR = 2 };

enum { BUS_ISA_MEM_BASE = 1, BUS_ISA_PORT_BASE = 2, BUS_ISA_PORT_SHIFT = 3 };

enum {
	NET_CORE_WMEM_MAX = 1,
	NET_CORE_RMEM_MAX = 2,
	NET_CORE_WMEM_DEFAULT = 3,
	NET_CORE_RMEM_DEFAULT = 4,

	NET_CORE_MAX_BACKLOG = 6,
	NET_CORE_FASTROUTE = 7,
	NET_CORE_MSG_COST = 8,
	NET_CORE_MSG_BURST = 9,
	NET_CORE_OPTMEM_MAX = 10,
	NET_CORE_HOT_LIST_LENGTH = 11,
	NET_CORE_DIVERT_VERSION = 12,
	NET_CORE_NO_CONG_THRESH = 13,
	NET_CORE_NO_CONG = 14,
	NET_CORE_LO_CONG = 15,
	NET_CORE_MOD_CONG = 16,
	NET_CORE_DEV_WEIGHT = 17,
	NET_CORE_SOMAXCONN = 18,
	NET_CORE_BUDGET = 19,
	NET_CORE_AEVENT_ETIME = 20,
	NET_CORE_AEVENT_RSEQTH = 21,
	NET_CORE_WARNINGS = 22,
};

enum {
	NET_UNIX_DESTROY_DELAY = 1,
	NET_UNIX_DELETE_DELAY = 2,
	NET_UNIX_MAX_DGRAM_QLEN = 3,
};

enum {
	NET_NF_CONNTRACK_MAX = 1,
	NET_NF_CONNTRACK_TCP_TIMEOUT_SYN_SENT = 2,
	NET_NF_CONNTRACK_TCP_TIMEOUT_SYN_RECV = 3,
	NET_NF_CONNTRACK_TCP_TIMEOUT_ESTABLISHED = 4,
	NET_NF_CONNTRACK_TCP_TIMEOUT_FIN_WAIT = 5,
	NET_NF_CONNTRACK_TCP_TIMEOUT_CLOSE_WAIT = 6,
	NET_NF_CONNTRACK_TCP_TIMEOUT_LAST_ACK = 7,
	NET_NF_CONNTRACK_TCP_TIMEOUT_TIME_WAIT = 8,
	NET_NF_CONNTRACK_TCP_TIMEOUT_CLOSE = 9,
	NET_NF_CONNTRACK_UDP_TIMEOUT = 10,
	NET_NF_CONNTRACK_UDP_TIMEOUT_STREAM = 11,
	NET_NF_CONNTRACK_ICMP_TIMEOUT = 12,
	NET_NF_CONNTRACK_GENERIC_TIMEOUT = 13,
	NET_NF_CONNTRACK_BUCKETS = 14,
	NET_NF_CONNTRACK_LOG_INVALID = 15,
	NET_NF_CONNTRACK_TCP_TIMEOUT_MAX_RETRANS = 16,
	NET_NF_CONNTRACK_TCP_LOOSE = 17,
	NET_NF_CONNTRACK_TCP_BE_LIBERAL = 18,
	NET_NF_CONNTRACK_TCP_MAX_RETRANS = 19,
	NET_NF_CONNTRACK_SCTP_TIMEOUT_CLOSED = 20,
	NET_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_WAIT = 21,
	NET_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_ECHOED = 22,
	NET_NF_CONNTRACK_SCTP_TIMEOUT_ESTABLISHED = 23,
	NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_SENT = 24,
	NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_RECD = 25,
	NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_ACK_SENT = 26,
	NET_NF_CONNTRACK_COUNT = 27,
	NET_NF_CONNTRACK_ICMPV6_TIMEOUT = 28,
	NET_NF_CONNTRACK_FRAG6_TIMEOUT = 29,
	NET_NF_CONNTRACK_FRAG6_LOW_THRESH = 30,
	NET_NF_CONNTRACK_FRAG6_HIGH_THRESH = 31,
	NET_NF_CONNTRACK_CHECKSUM = 32,
};

enum {

	NET_IPV4_FORWARD = 8,
	NET_IPV4_DYNADDR = 9,

	NET_IPV4_CONF = 16,
	NET_IPV4_NEIGH = 17,
	NET_IPV4_ROUTE = 18,
	NET_IPV4_FIB_HASH = 19,
	NET_IPV4_NETFILTER = 20,

	NET_IPV4_TCP_TIMESTAMPS = 33,
	NET_IPV4_TCP_WINDOW_SCALING = 34,
	NET_IPV4_TCP_SACK = 35,
	NET_IPV4_TCP_RETRANS_COLLAPSE = 36,
	NET_IPV4_DEFAULT_TTL = 37,
	NET_IPV4_AUTOCONFIG = 38,
	NET_IPV4_NO_PMTU_DISC = 39,
	NET_IPV4_TCP_SYN_RETRIES = 40,
	NET_IPV4_IPFRAG_HIGH_THRESH = 41,
	NET_IPV4_IPFRAG_LOW_THRESH = 42,
	NET_IPV4_IPFRAG_TIME = 43,
	NET_IPV4_TCP_MAX_KA_PROBES = 44,
	NET_IPV4_TCP_KEEPALIVE_TIME = 45,
	NET_IPV4_TCP_KEEPALIVE_PROBES = 46,
	NET_IPV4_TCP_RETRIES1 = 47,
	NET_IPV4_TCP_RETRIES2 = 48,
	NET_IPV4_TCP_FIN_TIMEOUT = 49,
	NET_IPV4_IP_MASQ_DEBUG = 50,
	NET_TCP_SYNCOOKIES = 51,
	NET_TCP_STDURG = 52,
	NET_TCP_RFC1337 = 53,
	NET_TCP_SYN_TAILDROP = 54,
	NET_TCP_MAX_SYN_BACKLOG = 55,
	NET_IPV4_LOCAL_PORT_RANGE = 56,
	NET_IPV4_ICMP_ECHO_IGNORE_ALL = 57,
	NET_IPV4_ICMP_ECHO_IGNORE_BROADCASTS = 58,
	NET_IPV4_ICMP_SOURCEQUENCH_RATE = 59,
	NET_IPV4_ICMP_DESTUNREACH_RATE = 60,
	NET_IPV4_ICMP_TIMEEXCEED_RATE = 61,
	NET_IPV4_ICMP_PARAMPROB_RATE = 62,
	NET_IPV4_ICMP_ECHOREPLY_RATE = 63,
	NET_IPV4_ICMP_IGNORE_BOGUS_ERROR_RESPONSES = 64,
	NET_IPV4_IGMP_MAX_MEMBERSHIPS = 65,
	NET_TCP_TW_RECYCLE = 66,
	NET_IPV4_ALWAYS_DEFRAG = 67,
	NET_IPV4_TCP_KEEPALIVE_INTVL = 68,
	NET_IPV4_INET_PEER_THRESHOLD = 69,
	NET_IPV4_INET_PEER_MINTTL = 70,
	NET_IPV4_INET_PEER_MAXTTL = 71,
	NET_IPV4_INET_PEER_GC_MINTIME = 72,
	NET_IPV4_INET_PEER_GC_MAXTIME = 73,
	NET_TCP_ORPHAN_RETRIES = 74,
	NET_TCP_ABORT_ON_OVERFLOW = 75,
	NET_TCP_SYNACK_RETRIES = 76,
	NET_TCP_MAX_ORPHANS = 77,
	NET_TCP_MAX_TW_BUCKETS = 78,
	NET_TCP_FACK = 79,
	NET_TCP_REORDERING = 80,
	NET_TCP_ECN = 81,
	NET_TCP_DSACK = 82,
	NET_TCP_MEM = 83,
	NET_TCP_WMEM = 84,
	NET_TCP_RMEM = 85,
	NET_TCP_APP_WIN = 86,
	NET_TCP_ADV_WIN_SCALE = 87,
	NET_IPV4_NONLOCAL_BIND = 88,
	NET_IPV4_ICMP_RATELIMIT = 89,
	NET_IPV4_ICMP_RATEMASK = 90,
	NET_TCP_TW_REUSE = 91,
	NET_TCP_FRTO = 92,
	NET_TCP_LOW_LATENCY = 93,
	NET_IPV4_IPFRAG_SECRET_INTERVAL = 94,
	NET_IPV4_IGMP_MAX_MSF = 96,
	NET_TCP_NO_METRICS_SAVE = 97,
	NET_TCP_DEFAULT_WIN_SCALE = 105,
	NET_TCP_MODERATE_RCVBUF = 106,
	NET_TCP_TSO_WIN_DIVISOR = 107,
	NET_TCP_BIC_BETA = 108,
	NET_IPV4_ICMP_ERRORS_USE_INBOUND_IFADDR = 109,
	NET_TCP_CONG_CONTROL = 110,
	NET_TCP_ABC = 111,
	NET_IPV4_IPFRAG_MAX_DIST = 112,
	NET_TCP_MTU_PROBING = 113,
	NET_TCP_BASE_MSS = 114,
	NET_IPV4_TCP_WORKAROUND_SIGNED_WINDOWS = 115,
	NET_TCP_DMA_COPYBREAK = 116,
	NET_TCP_SLOW_START_AFTER_IDLE = 117,
	NET_CIPSOV4_CACHE_ENABLE = 118,
	NET_CIPSOV4_CACHE_BUCKET_SIZE = 119,
	NET_CIPSOV4_RBM_OPTFMT = 120,
	NET_CIPSOV4_RBM_STRICTVALID = 121,
	NET_TCP_AVAIL_CONG_CONTROL = 122,
	NET_TCP_ALLOWED_CONG_CONTROL = 123,
	NET_TCP_MAX_SSTHRESH = 124,
	NET_TCP_FRTO_RESPONSE = 125,
};

enum {
	NET_IPV4_ROUTE_FLUSH = 1,
	NET_IPV4_ROUTE_MIN_DELAY = 2,
	NET_IPV4_ROUTE_MAX_DELAY = 3,
	NET_IPV4_ROUTE_GC_THRESH = 4,
	NET_IPV4_ROUTE_MAX_SIZE = 5,
	NET_IPV4_ROUTE_GC_MIN_INTERVAL = 6,
	NET_IPV4_ROUTE_GC_TIMEOUT = 7,
	NET_IPV4_ROUTE_GC_INTERVAL = 8,
	NET_IPV4_ROUTE_REDIRECT_LOAD = 9,
	NET_IPV4_ROUTE_REDIRECT_NUMBER = 10,
	NET_IPV4_ROUTE_REDIRECT_SILENCE = 11,
	NET_IPV4_ROUTE_ERROR_COST = 12,
	NET_IPV4_ROUTE_ERROR_BURST = 13,
	NET_IPV4_ROUTE_GC_ELASTICITY = 14,
	NET_IPV4_ROUTE_MTU_EXPIRES = 15,
	NET_IPV4_ROUTE_MIN_PMTU = 16,
	NET_IPV4_ROUTE_MIN_ADVMSS = 17,
	NET_IPV4_ROUTE_SECRET_INTERVAL = 18,
	NET_IPV4_ROUTE_GC_MIN_INTERVAL_MS = 19,
};

enum {
	NET_PROTO_CONF_ALL = -2,
	NET_PROTO_CONF_DEFAULT = -3

};

enum {
	NET_IPV4_CONF_FORWARDING = 1,
	NET_IPV4_CONF_MC_FORWARDING = 2,
	NET_IPV4_CONF_PROXY_ARP = 3,
	NET_IPV4_CONF_ACCEPT_REDIRECTS = 4,
	NET_IPV4_CONF_SECURE_REDIRECTS = 5,
	NET_IPV4_CONF_SEND_REDIRECTS = 6,
	NET_IPV4_CONF_SHARED_MEDIA = 7,
	NET_IPV4_CONF_RP_FILTER = 8,
	NET_IPV4_CONF_ACCEPT_SOURCE_ROUTE = 9,
	NET_IPV4_CONF_BOOTP_RELAY = 10,
	NET_IPV4_CONF_LOG_MARTIANS = 11,
	NET_IPV4_CONF_TAG = 12,
	NET_IPV4_CONF_ARPFILTER = 13,
	NET_IPV4_CONF_MEDIUM_ID = 14,
	NET_IPV4_CONF_NOXFRM = 15,
	NET_IPV4_CONF_NOPOLICY = 16,
	NET_IPV4_CONF_FORCE_IGMP_VERSION = 17,
	NET_IPV4_CONF_ARP_ANNOUNCE = 18,
	NET_IPV4_CONF_ARP_IGNORE = 19,
	NET_IPV4_CONF_PROMOTE_SECONDARIES = 20,
	NET_IPV4_CONF_ARP_ACCEPT = 21,
	NET_IPV4_CONF_ARP_NOTIFY = 22,
	NET_IPV4_CONF_ARP_EVICT_NOCARRIER = 23,
};

enum {
	NET_IPV4_NF_CONNTRACK_MAX = 1,
	NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_SYN_SENT = 2,
	NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_SYN_RECV = 3,
	NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_ESTABLISHED = 4,
	NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_FIN_WAIT = 5,
	NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_CLOSE_WAIT = 6,
	NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_LAST_ACK = 7,
	NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_TIME_WAIT = 8,
	NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_CLOSE = 9,
	NET_IPV4_NF_CONNTRACK_UDP_TIMEOUT = 10,
	NET_IPV4_NF_CONNTRACK_UDP_TIMEOUT_STREAM = 11,
	NET_IPV4_NF_CONNTRACK_ICMP_TIMEOUT = 12,
	NET_IPV4_NF_CONNTRACK_GENERIC_TIMEOUT = 13,
	NET_IPV4_NF_CONNTRACK_BUCKETS = 14,
	NET_IPV4_NF_CONNTRACK_LOG_INVALID = 15,
	NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_MAX_RETRANS = 16,
	NET_IPV4_NF_CONNTRACK_TCP_LOOSE = 17,
	NET_IPV4_NF_CONNTRACK_TCP_BE_LIBERAL = 18,
	NET_IPV4_NF_CONNTRACK_TCP_MAX_RETRANS = 19,
	NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_CLOSED = 20,
	NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_WAIT = 21,
	NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_ECHOED = 22,
	NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_ESTABLISHED = 23,
	NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_SENT = 24,
	NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_RECD = 25,
	NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_ACK_SENT = 26,
	NET_IPV4_NF_CONNTRACK_COUNT = 27,
	NET_IPV4_NF_CONNTRACK_CHECKSUM = 28,
};

enum {
	NET_IPV6_CONF = 16,
	NET_IPV6_NEIGH = 17,
	NET_IPV6_ROUTE = 18,
	NET_IPV6_ICMP = 19,
	NET_IPV6_BINDV6ONLY = 20,
	NET_IPV6_IP6FRAG_HIGH_THRESH = 21,
	NET_IPV6_IP6FRAG_LOW_THRESH = 22,
	NET_IPV6_IP6FRAG_TIME = 23,
	NET_IPV6_IP6FRAG_SECRET_INTERVAL = 24,
	NET_IPV6_MLD_MAX_MSF = 25,
};

enum {
	NET_IPV6_ROUTE_FLUSH = 1,
	NET_IPV6_ROUTE_GC_THRESH = 2,
	NET_IPV6_ROUTE_MAX_SIZE = 3,
	NET_IPV6_ROUTE_GC_MIN_INTERVAL = 4,
	NET_IPV6_ROUTE_GC_TIMEOUT = 5,
	NET_IPV6_ROUTE_GC_INTERVAL = 6,
	NET_IPV6_ROUTE_GC_ELASTICITY = 7,
	NET_IPV6_ROUTE_MTU_EXPIRES = 8,
	NET_IPV6_ROUTE_MIN_ADVMSS = 9,
	NET_IPV6_ROUTE_GC_MIN_INTERVAL_MS = 10
};

enum {
	NET_IPV6_FORWARDING = 1,
	NET_IPV6_HOP_LIMIT = 2,
	NET_IPV6_MTU = 3,
	NET_IPV6_ACCEPT_RA = 4,
	NET_IPV6_ACCEPT_REDIRECTS = 5,
	NET_IPV6_AUTOCONF = 6,
	NET_IPV6_DAD_TRANSMITS = 7,
	NET_IPV6_RTR_SOLICITS = 8,
	NET_IPV6_RTR_SOLICIT_INTERVAL = 9,
	NET_IPV6_RTR_SOLICIT_DELAY = 10,
	NET_IPV6_USE_TEMPADDR = 11,
	NET_IPV6_TEMP_VALID_LFT = 12,
	NET_IPV6_TEMP_PREFERED_LFT = 13,
	NET_IPV6_REGEN_MAX_RETRY = 14,
	NET_IPV6_MAX_DESYNC_FACTOR = 15,
	NET_IPV6_MAX_ADDRESSES = 16,
	NET_IPV6_FORCE_MLD_VERSION = 17,
	NET_IPV6_ACCEPT_RA_DEFRTR = 18,
	NET_IPV6_ACCEPT_RA_PINFO = 19,
	NET_IPV6_ACCEPT_RA_RTR_PREF = 20,
	NET_IPV6_RTR_PROBE_INTERVAL = 21,
	NET_IPV6_ACCEPT_RA_RT_INFO_MAX_PLEN = 22,
	NET_IPV6_PROXY_NDP = 23,
	NET_IPV6_ACCEPT_SOURCE_ROUTE = 25,
	NET_IPV6_ACCEPT_RA_FROM_LOCAL = 26,
	NET_IPV6_ACCEPT_RA_RT_INFO_MIN_PLEN = 27,
	NET_IPV6_RA_DEFRTR_METRIC = 28,
	__NET_IPV6_MAX
};

enum { NET_IPV6_ICMP_RATELIMIT = 1, NET_IPV6_ICMP_ECHO_IGNORE_ALL = 2 };

enum {
	NET_NEIGH_MCAST_SOLICIT = 1,
	NET_NEIGH_UCAST_SOLICIT = 2,
	NET_NEIGH_APP_SOLICIT = 3,
	NET_NEIGH_RETRANS_TIME = 4,
	NET_NEIGH_REACHABLE_TIME = 5,
	NET_NEIGH_DELAY_PROBE_TIME = 6,
	NET_NEIGH_GC_STALE_TIME = 7,
	NET_NEIGH_UNRES_QLEN = 8,
	NET_NEIGH_PROXY_QLEN = 9,
	NET_NEIGH_ANYCAST_DELAY = 10,
	NET_NEIGH_PROXY_DELAY = 11,
	NET_NEIGH_LOCKTIME = 12,
	NET_NEIGH_GC_INTERVAL = 13,
	NET_NEIGH_GC_THRESH1 = 14,
	NET_NEIGH_GC_THRESH2 = 15,
	NET_NEIGH_GC_THRESH3 = 16,
	NET_NEIGH_RETRANS_TIME_MS = 17,
	NET_NEIGH_REACHABLE_TIME_MS = 18,
	NET_NEIGH_INTERVAL_PROBE_TIME_MS = 19,
};

enum {
	NET_DCCP_DEFAULT = 1,
};

enum { NET_IPX_PPROP_BROADCASTING = 1, NET_IPX_FORWARDING = 2 };

enum {
	NET_LLC2 = 1,
	NET_LLC_STATION = 2,
};

enum {
	NET_LLC2_TIMEOUT = 1,
};

enum {
	NET_LLC_STATION_ACK_TIMEOUT = 1,
};

enum {
	NET_LLC2_ACK_TIMEOUT = 1,
	NET_LLC2_P_TIMEOUT = 2,
	NET_LLC2_REJ_TIMEOUT = 3,
	NET_LLC2_BUSY_TIMEOUT = 4,
};

enum {
	NET_ATALK_AARP_EXPIRY_TIME = 1,
	NET_ATALK_AARP_TICK_TIME = 2,
	NET_ATALK_AARP_RETRANSMIT_LIMIT = 3,
	NET_ATALK_AARP_RESOLVE_TIME = 4
};

enum {
	NET_NETROM_DEFAULT_PATH_QUALITY = 1,
	NET_NETROM_OBSOLESCENCE_COUNT_INITIALISER = 2,
	NET_NETROM_NETWORK_TTL_INITIALISER = 3,
	NET_NETROM_TRANSPORT_TIMEOUT = 4,
	NET_NETROM_TRANSPORT_MAXIMUM_TRIES = 5,
	NET_NETROM_TRANSPORT_ACKNOWLEDGE_DELAY = 6,
	NET_NETROM_TRANSPORT_BUSY_DELAY = 7,
	NET_NETROM_TRANSPORT_REQUESTED_WINDOW_SIZE = 8,
	NET_NETROM_TRANSPORT_NO_ACTIVITY_TIMEOUT = 9,
	NET_NETROM_ROUTING_CONTROL = 10,
	NET_NETROM_LINK_FAILS_COUNT = 11,
	NET_NETROM_RESET = 12
};

enum {
	NET_AX25_IP_DEFAULT_MODE = 1,
	NET_AX25_DEFAULT_MODE = 2,
	NET_AX25_BACKOFF_TYPE = 3,
	NET_AX25_CONNECT_MODE = 4,
	NET_AX25_STANDARD_WINDOW = 5,
	NET_AX25_EXTENDED_WINDOW = 6,
	NET_AX25_T1_TIMEOUT = 7,
	NET_AX25_T2_TIMEOUT = 8,
	NET_AX25_T3_TIMEOUT = 9,
	NET_AX25_IDLE_TIMEOUT = 10,
	NET_AX25_N2 = 11,
	NET_AX25_PACLEN = 12,
	NET_AX25_PROTOCOL = 13,
	NET_AX25_DAMA_SLAVE_TIMEOUT = 14
};

enum {
	NET_ROSE_RESTART_REQUEST_TIMEOUT = 1,
	NET_ROSE_CALL_REQUEST_TIMEOUT = 2,
	NET_ROSE_RESET_REQUEST_TIMEOUT = 3,
	NET_ROSE_CLEAR_REQUEST_TIMEOUT = 4,
	NET_ROSE_ACK_HOLD_BACK_TIMEOUT = 5,
	NET_ROSE_ROUTING_CONTROL = 6,
	NET_ROSE_LINK_FAIL_TIMEOUT = 7,
	NET_ROSE_MAX_VCS = 8,
	NET_ROSE_WINDOW_SIZE = 9,
	NET_ROSE_NO_ACTIVITY_TIMEOUT = 10
};

enum {
	NET_X25_RESTART_REQUEST_TIMEOUT = 1,
	NET_X25_CALL_REQUEST_TIMEOUT = 2,
	NET_X25_RESET_REQUEST_TIMEOUT = 3,
	NET_X25_CLEAR_REQUEST_TIMEOUT = 4,
	NET_X25_ACK_HOLD_BACK_TIMEOUT = 5,
	NET_X25_FORWARD = 6
};

enum { NET_TR_RIF_TIMEOUT = 1 };

enum {
	NET_DECNET_NODE_TYPE = 1,
	NET_DECNET_NODE_ADDRESS = 2,
	NET_DECNET_NODE_NAME = 3,
	NET_DECNET_DEFAULT_DEVICE = 4,
	NET_DECNET_TIME_WAIT = 5,
	NET_DECNET_DN_COUNT = 6,
	NET_DECNET_DI_COUNT = 7,
	NET_DECNET_DR_COUNT = 8,
	NET_DECNET_DST_GC_INTERVAL = 9,
	NET_DECNET_CONF = 10,
	NET_DECNET_NO_FC_MAX_CWND = 11,
	NET_DECNET_MEM = 12,
	NET_DECNET_RMEM = 13,
	NET_DECNET_WMEM = 14,
	NET_DECNET_DEBUG_LEVEL = 255
};

enum {
	NET_DECNET_CONF_LOOPBACK = -2,
	NET_DECNET_CONF_DDCMP = -3,
	NET_DECNET_CONF_PPP = -4,
	NET_DECNET_CONF_X25 = -5,
	NET_DECNET_CONF_GRE = -6,
	NET_DECNET_CONF_ETHER = -7

};

enum {
	NET_DECNET_CONF_DEV_PRIORITY = 1,
	NET_DECNET_CONF_DEV_T1 = 2,
	NET_DECNET_CONF_DEV_T2 = 3,
	NET_DECNET_CONF_DEV_T3 = 4,
	NET_DECNET_CONF_DEV_FORWARDING = 5,
	NET_DECNET_CONF_DEV_BLKSIZE = 6,
	NET_DECNET_CONF_DEV_STATE = 7
};

enum {
	NET_SCTP_RTO_INITIAL = 1,
	NET_SCTP_RTO_MIN = 2,
	NET_SCTP_RTO_MAX = 3,
	NET_SCTP_RTO_ALPHA = 4,
	NET_SCTP_RTO_BETA = 5,
	NET_SCTP_VALID_COOKIE_LIFE = 6,
	NET_SCTP_ASSOCIATION_MAX_RETRANS = 7,
	NET_SCTP_PATH_MAX_RETRANS = 8,
	NET_SCTP_MAX_INIT_RETRANSMITS = 9,
	NET_SCTP_HB_INTERVAL = 10,
	NET_SCTP_PRESERVE_ENABLE = 11,
	NET_SCTP_MAX_BURST = 12,
	NET_SCTP_ADDIP_ENABLE = 13,
	NET_SCTP_PRSCTP_ENABLE = 14,
	NET_SCTP_SNDBUF_POLICY = 15,
	NET_SCTP_SACK_TIMEOUT = 16,
	NET_SCTP_RCVBUF_POLICY = 17,
};

enum {
	NET_BRIDGE_NF_CALL_ARPTABLES = 1,
	NET_BRIDGE_NF_CALL_IPTABLES = 2,
	NET_BRIDGE_NF_CALL_IP6TABLES = 3,
	NET_BRIDGE_NF_FILTER_VLAN_TAGGED = 4,
	NET_BRIDGE_NF_FILTER_PPPOE_TAGGED = 5,
};

enum {
	FS_NRINODE = 1,
	FS_STATINODE = 2,
	FS_MAXINODE = 3,
	FS_NRDQUOT = 4,
	FS_MAXDQUOT = 5,
	FS_NRFILE = 6,
	FS_MAXFILE = 7,
	FS_DENTRY = 8,
	FS_NRSUPER = 9,
	FS_MAXSUPER = 10,
	FS_OVERFLOWUID = 11,
	FS_OVERFLOWGID = 12,
	FS_LEASES = 13,
	FS_DIR_NOTIFY = 14,
	FS_LEASE_TIME = 15,
	FS_DQSTATS = 16,
	FS_XFS = 17,
	FS_AIO_NR = 18,
	FS_AIO_MAX_NR = 19,
	FS_INOTIFY = 20,
	FS_OCFS2 = 988,
};

enum {
	FS_DQ_LOOKUPS = 1,
	FS_DQ_DROPS = 2,
	FS_DQ_READS = 3,
	FS_DQ_WRITES = 4,
	FS_DQ_CACHE_HITS = 5,
	FS_DQ_ALLOCATED = 6,
	FS_DQ_FREE = 7,
	FS_DQ_SYNCS = 8,
	FS_DQ_WARNINGS = 9,
};

enum {
	DEV_CDROM = 1,
	DEV_HWMON = 2,
	DEV_PARPORT = 3,
	DEV_RAID = 4,
	DEV_MAC_HID = 5,
	DEV_SCSI = 6,
	DEV_IPMI = 7,
};

enum {
	DEV_CDROM_INFO = 1,
	DEV_CDROM_AUTOCLOSE = 2,
	DEV_CDROM_AUTOEJECT = 3,
	DEV_CDROM_DEBUG = 4,
	DEV_CDROM_LOCK = 5,
	DEV_CDROM_CHECK_MEDIA = 6
};

enum { DEV_PARPORT_DEFAULT = -3 };

enum { DEV_RAID_SPEED_LIMIT_MIN = 1, DEV_RAID_SPEED_LIMIT_MAX = 2 };

enum { DEV_PARPORT_DEFAULT_TIMESLICE = 1, DEV_PARPORT_DEFAULT_SPINTIME = 2 };

enum {
	DEV_PARPORT_SPINTIME = 1,
	DEV_PARPORT_BASE_ADDR = 2,
	DEV_PARPORT_IRQ = 3,
	DEV_PARPORT_DMA = 4,
	DEV_PARPORT_MODES = 5,
	DEV_PARPORT_DEVICES = 6,
	DEV_PARPORT_AUTOPROBE = 16
};

enum {
	DEV_PARPORT_DEVICES_ACTIVE = -3,
};

enum {
	DEV_PARPORT_DEVICE_TIMESLICE = 1,
};

enum {
	DEV_MAC_HID_KEYBOARD_SENDS_LINUX_KEYCODES = 1,
	DEV_MAC_HID_KEYBOARD_LOCK_KEYCODES = 2,
	DEV_MAC_HID_MOUSE_BUTTON_EMULATION = 3,
	DEV_MAC_HID_MOUSE_BUTTON2_KEYCODE = 4,
	DEV_MAC_HID_MOUSE_BUTTON3_KEYCODE = 5,
	DEV_MAC_HID_ADB_MOUSE_SENDS_KEYCODES = 6
};

enum {
	DEV_SCSI_LOGGING_LEVEL = 1,
};

enum {
	DEV_IPMI_POWEROFF_POWERCYCLE = 1,
};

enum {
	ABI_DEFHANDLER_COFF = 1,
	ABI_DEFHANDLER_ELF = 2,
	ABI_DEFHANDLER_LCALL7 = 3,
	ABI_DEFHANDLER_LIBCSO = 4,
	ABI_TRACE = 5,
	ABI_FAKE_UTSNAME = 6,
};

struct completion;
struct ctl_table;
struct nsproxy;
struct ctl_table_root;
struct ctl_table_header;
struct ctl_dir;
extern const int sysctl_vals[];

extern const unsigned long sysctl_long_vals[];

typedef int proc_handler(const struct ctl_table *ctl, int write, void *buffer,
			 size_t *lenp, loff_t *ppos);

int proc_dostring(const struct ctl_table *, int, void *, size_t *, loff_t *);
int proc_dobool(const struct ctl_table *table, int write, void *buffer,
		size_t *lenp, loff_t *ppos);
int proc_dointvec(const struct ctl_table *, int, void *, size_t *, loff_t *);
int proc_douintvec(const struct ctl_table *, int, void *, size_t *, loff_t *);
int proc_dointvec_minmax(const struct ctl_table *, int, void *, size_t *,
			 loff_t *);
int proc_douintvec_minmax(const struct ctl_table *table, int write,
			  void *buffer, size_t *lenp, loff_t *ppos);
int proc_dou8vec_minmax(const struct ctl_table *table, int write, void *buffer,
			size_t *lenp, loff_t *ppos);
int proc_dointvec_jiffies(const struct ctl_table *, int, void *, size_t *,
			  loff_t *);
int proc_dointvec_ms_jiffies_minmax(const struct ctl_table *table, int write,
				    void *buffer, size_t *lenp, loff_t *ppos);
int proc_dointvec_userhz_jiffies(const struct ctl_table *, int, void *,
				 size_t *, loff_t *);
int proc_dointvec_ms_jiffies(const struct ctl_table *, int, void *, size_t *,
			     loff_t *);
int proc_doulongvec_minmax(const struct ctl_table *, int, void *, size_t *,
			   loff_t *);
int proc_doulongvec_ms_jiffies_minmax(const struct ctl_table *table, int,
				      void *, size_t *, loff_t *);
int proc_do_large_bitmap(const struct ctl_table *, int, void *, size_t *,
			 loff_t *);
int proc_do_static_key(const struct ctl_table *table, int write, void *buffer,
		       size_t *lenp, loff_t *ppos);
struct ctl_table_poll {
	atomic_t event;
	wait_queue_head_t wait;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
proc_sys_poll_event(struct ctl_table_poll *poll)
{
	return (void *)(unsigned long)atomic_read(&poll->event);
}
struct ctl_table {
	const char *procname;
	void *data;
	int maxlen;
	umode_t mode;
	proc_handler *proc_handler;
	struct ctl_table_poll *poll;
	void *extra1;
	void *extra2;
};

struct ctl_node {
	struct rb_node node;
	struct ctl_table_header *header;
};
struct ctl_table_header {
	union {
		struct {
			struct ctl_table *ctl_table;
			int ctl_table_size;
			int used;
			int count;
			int nreg;
		};
		struct callback_head rcu;
	};
	struct completion *unregistering;
	const struct ctl_table *ctl_table_arg;
	struct ctl_table_root *root;
	struct ctl_table_set *set;
	struct ctl_dir *parent;
	struct ctl_node *node;
	struct hlist_head inodes;

	enum {
		SYSCTL_TABLE_TYPE_DEFAULT,
		SYSCTL_TABLE_TYPE_PERMANENTLY_EMPTY,
	} type;
};

struct ctl_dir {
	struct ctl_table_header header;
	struct rb_root root;
};

struct ctl_table_set {
	int (*is_seen)(struct ctl_table_set *);
	struct ctl_dir dir;
};

struct ctl_table_root {
	struct ctl_table_set default_set;
	struct ctl_table_set *(*lookup)(struct ctl_table_root *root);
	void (*set_ownership)(struct ctl_table_header *head, kuid_t *uid,
			      kgid_t *gid);
	int (*permissions)(struct ctl_table_header *head,
			   const struct ctl_table *table);
};

void proc_sys_poll_notify(struct ctl_table_poll *poll);

extern void setup_sysctl_set(struct ctl_table_set *p,
			     struct ctl_table_root *root,
			     int (*is_seen)(struct ctl_table_set *));
extern void retire_sysctl_set(struct ctl_table_set *set);

struct ctl_table_header *__register_sysctl_table(struct ctl_table_set *set,
						 const char *path,
						 struct ctl_table *table,
						 size_t table_size);
struct ctl_table_header *register_sysctl_sz(const char *path,
					    struct ctl_table *table,
					    size_t table_size);
void unregister_sysctl_table(struct ctl_table_header *table);

extern int sysctl_init_bases(void);
extern void __register_sysctl_init(const char *path, struct ctl_table *table,
				   const char *table_name, size_t table_size);

extern struct ctl_table_header *register_sysctl_mount_point(const char *path);

void do_sysctl_args(void);
bool sysctl_is_alias(char *param);
int do_proc_douintvec(const struct ctl_table *table, int write, void *buffer,
		      size_t *lenp, loff_t *ppos,
		      int (*conv)(unsigned long *lvalp, unsigned int *valp,
				  int write, void *data),
		      void *data);

extern int pwrsw_enabled;
extern int unaligned_enabled;
extern int unaligned_dump_stack;
extern int no_unaligned_warning;
int sysctl_max_threads(const struct ctl_table *table, int write, void *buffer,
		       size_t *lenp, loff_t *ppos);

struct cred;
struct file;

struct subprocess_info {
	struct work_struct work;
	struct completion *complete;
	const char *path;
	char **argv;
	char **envp;
	int wait;
	int retval;
	int (*init)(struct subprocess_info *info, struct cred *new);
	void (*cleanup)(struct subprocess_info *info);
	void *data;
};

extern int call_usermodehelper(const char *path, char **argv, char **envp,
			       int wait);

extern struct subprocess_info *call_usermodehelper_setup(
	const char *path, char **argv, char **envp, gfp_t gfp_mask,
	int (*init)(struct subprocess_info *info, struct cred *new),
	void (*cleanup)(struct subprocess_info *), void *data);

extern int call_usermodehelper_exec(struct subprocess_info *info, int wait);

enum umh_disable_depth {
	UMH_ENABLED = 0,
	UMH_FREEZING,
	UMH_DISABLED,
};

extern int __usermodehelper_disable(enum umh_disable_depth depth);
extern void __usermodehelper_set_disable_depth(enum umh_disable_depth depth);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
usermodehelper_disable(void)
{
	return __usermodehelper_disable(UMH_DISABLED);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
usermodehelper_enable(void)
{
	__usermodehelper_set_disable_depth(UMH_ENABLED);
}

extern int usermodehelper_read_trylock(void);
extern long usermodehelper_read_lock_wait(long timeout);
extern void usermodehelper_read_unlock(void);
extern char modprobe_path[];

extern __attribute__((__format__(printf, 2, 3))) int
__request_module(bool wait, const char *name, ...);

struct rhash_head {
	struct rhash_head *next;
};

struct rhlist_head {
	struct rhash_head rhead;
	struct rhlist_head *next;
};

struct bucket_table;

struct rhashtable_compare_arg {
	struct rhashtable *ht;
	const void *key;
};

typedef u32 (*rht_hashfn_t)(const void *data, u32 len, u32 seed);
typedef u32 (*rht_obj_hashfn_t)(const void *data, u32 len, u32 seed);
typedef int (*rht_obj_cmpfn_t)(struct rhashtable_compare_arg *arg,
			       const void *obj);
struct rhashtable_params {
	u16 nelem_hint;
	u16 key_len;
	u16 key_offset;
	u16 head_offset;
	unsigned int max_size;
	u16 min_size;
	bool automatic_shrinking;
	rht_hashfn_t hashfn;
	rht_obj_hashfn_t obj_hashfn;
	rht_obj_cmpfn_t obj_cmpfn;
};
struct rhashtable {
	struct bucket_table *tbl;
	unsigned int key_len;
	unsigned int max_elems;
	struct rhashtable_params p;
	bool rhlist;
	struct work_struct run_work;
	struct mutex mutex;
	spinlock_t lock;
	atomic_t nelems;
};

struct rhltable {
	struct rhashtable ht;
};

struct rhashtable_walker {
	struct list_head list;
	struct bucket_table *tbl;
};
struct rhashtable_iter {
	struct rhashtable *ht;
	struct rhash_head *p;
	struct rhlist_head *list;
	struct rhashtable_walker walker;
	unsigned int slot;
	unsigned int skip;
	bool end_of_table;
};

int rhashtable_init_noprof(struct rhashtable *ht,
			   const struct rhashtable_params *params);

int rhltable_init_noprof(struct rhltable *hlt,
			 const struct rhashtable_params *params);
struct ipc_perm {
	__kernel_key_t key;
	__kernel_uid_t uid;
	__kernel_gid_t gid;
	__kernel_uid_t cuid;
	__kernel_gid_t cgid;
	__kernel_mode_t mode;
	unsigned short seq;
};

struct ipc64_perm {
	__kernel_key_t key;
	__kernel_uid32_t uid;
	__kernel_gid32_t gid;
	__kernel_uid32_t cuid;
	__kernel_gid32_t cgid;
	__kernel_mode_t mode;

	unsigned char __pad1[4 - sizeof(__kernel_mode_t)];
	unsigned short seq;
	unsigned short __pad2;
	__kernel_ulong_t __unused1;
	__kernel_ulong_t __unused2;
};
struct ipc_kludge {
	struct msgbuf *msgp;
	long msgtyp;
};

struct kern_ipc_perm {
	spinlock_t lock;
	bool deleted;
	int id;
	key_t key;
	kuid_t uid;
	kgid_t gid;
	kuid_t cuid;
	kgid_t cgid;
	umode_t mode;
	unsigned long seq;
	void *security;

	struct rhash_head khtnode;

	struct callback_head rcu;
	refcount_t refcount;
} __attribute__((__aligned__((1 << (6)))));
struct semid_ds {
	struct ipc_perm sem_perm;
	__kernel_old_time_t sem_otime;
	__kernel_old_time_t sem_ctime;
	struct sem *sem_base;
	struct sem_queue *sem_pending;
	struct sem_queue **sem_pending_last;
	struct sem_undo *undo;
	unsigned short sem_nsems;
};

struct semid64_ds {
	struct ipc64_perm sem_perm;

	__kernel_long_t sem_otime;
	__kernel_ulong_t __unused1;
	__kernel_long_t sem_ctime;
	__kernel_ulong_t __unused2;

	__kernel_ulong_t sem_nsems;
	__kernel_ulong_t __unused3;
	__kernel_ulong_t __unused4;
};

struct sembuf {
	unsigned short sem_num;
	short sem_op;
	short sem_flg;
};

union semun {
	int val;
	struct semid_ds *buf;
	unsigned short *array;
	struct seminfo *__buf;
	void *__pad;
};

struct seminfo {
	int semmap;
	int semmni;
	int semmns;
	int semmnu;
	int semmsl;
	int semopm;
	int semume;
	int semusz;
	int semvmx;
	int semaem;
};

struct task_struct;

extern int copy_semundo(unsigned long clone_flags, struct task_struct *tsk);
extern void exit_sem(struct task_struct *tsk);

struct iovec {
	void *iov_base;
	__kernel_size_t iov_len;
};

struct dmabuf_cmsg {
	__u64 frag_offset;

	__u32 frag_size;
	__u32 frag_token;

	__u32 dmabuf_id;
	__u32 flags;
};

struct dmabuf_token {
	__u32 token_start;
	__u32 token_count;
};

struct page;
struct folio_queue;

typedef unsigned int iov_iter_extraction_t;

struct kvec {
	void *iov_base;
	size_t iov_len;
};

enum iter_type {

	ITER_UBUF,
	ITER_IOVEC,
	ITER_BVEC,
	ITER_KVEC,
	ITER_FOLIOQ,
	ITER_XARRAY,
	ITER_DISCARD,
};

struct iov_iter_state {
	size_t iov_offset;
	size_t count;
	unsigned long nr_segs;
};

struct iov_iter {
	u8 iter_type;
	bool nofault;
	bool data_source;
	size_t iov_offset;
	union {
		struct iovec __ubuf_iovec;
		struct {
			union {
				const struct iovec *__iov;
				const struct kvec *kvec;
				const struct bio_vec *bvec;
				const struct folio_queue *folioq;
				struct xarray *xarray;
				void *ubuf;
			};
			size_t count;
		};
	};
	union {
		unsigned long nr_segs;
		u8 folioq_slot;
		loff_t xarray_start;
	};
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const struct iovec *
iter_iov(const struct iov_iter *iter)
{
	if (iter->iter_type == ITER_UBUF)
		return (const struct iovec *)&iter->__ubuf_iovec;
	return iter->__iov;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) enum iter_type
iov_iter_type(const struct iov_iter *i)
{
	return i->iter_type;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
iov_iter_save_state(struct iov_iter *iter, struct iov_iter_state *state)
{
	state->iov_offset = iter->iov_offset;
	state->count = iter->count;
	state->nr_segs = iter->nr_segs;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
iter_is_ubuf(const struct iov_iter *i)
{
	return iov_iter_type(i) == ITER_UBUF;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
iter_is_iovec(const struct iov_iter *i)
{
	return iov_iter_type(i) == ITER_IOVEC;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
iov_iter_is_kvec(const struct iov_iter *i)
{
	return iov_iter_type(i) == ITER_KVEC;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
iov_iter_is_bvec(const struct iov_iter *i)
{
	return iov_iter_type(i) == ITER_BVEC;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
iov_iter_is_discard(const struct iov_iter *i)
{
	return iov_iter_type(i) == ITER_DISCARD;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
iov_iter_is_folioq(const struct iov_iter *i)
{
	return iov_iter_type(i) == ITER_FOLIOQ;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
iov_iter_is_xarray(const struct iov_iter *i)
{
	return iov_iter_type(i) == ITER_XARRAY;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned char
iov_iter_rw(const struct iov_iter *i)
{
	return i->data_source ? 1 : 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
user_backed_iter(const struct iov_iter *i)
{
	return iter_is_ubuf(i) || iter_is_iovec(i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
iov_length(const struct iovec *iov, unsigned long nr_segs)
{
	unsigned long seg;
	size_t ret = 0;

	for (seg = 0; seg < nr_segs; seg++)
		ret += iov[seg].iov_len;
	return ret;
}

size_t copy_page_from_iter_atomic(struct page *page, size_t offset,
				  size_t bytes, struct iov_iter *i);
void iov_iter_advance(struct iov_iter *i, size_t bytes);
void iov_iter_revert(struct iov_iter *i, size_t bytes);
size_t fault_in_iov_iter_readable(const struct iov_iter *i, size_t bytes);
size_t fault_in_iov_iter_writeable(const struct iov_iter *i, size_t bytes);
size_t iov_iter_single_seg_count(const struct iov_iter *i);
size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
			 struct iov_iter *i);
size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
			   struct iov_iter *i);

size_t _copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i);
size_t _copy_from_iter(void *addr, size_t bytes, struct iov_iter *i);
size_t _copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
copy_folio_to_iter(struct folio *folio, size_t offset, size_t bytes,
		   struct iov_iter *i)
{
	return copy_page_to_iter(&folio->page, offset, bytes, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
copy_folio_from_iter(struct folio *folio, size_t offset, size_t bytes,
		     struct iov_iter *i)
{
	return copy_page_from_iter(&folio->page, offset, bytes, i);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
copy_folio_from_iter_atomic(struct folio *folio, size_t offset, size_t bytes,
			    struct iov_iter *i)
{
	return copy_page_from_iter_atomic(&folio->page, offset, bytes, i);
}

size_t copy_page_to_iter_nofault(struct page *page, unsigned offset,
				 size_t bytes, struct iov_iter *i);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) size_t
copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i)
{
	if (check_copy_size(addr, bytes, true))
		return _copy_to_iter(addr, bytes, i);
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) size_t
copy_from_iter(void *addr, size_t bytes, struct iov_iter *i)
{
	if (check_copy_size(addr, bytes, false))
		return _copy_from_iter(addr, bytes, i);
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) bool
copy_to_iter_full(const void *addr, size_t bytes, struct iov_iter *i)
{
	size_t copied = copy_to_iter(addr, bytes, i);
	if (__builtin_expect(!!(copied == bytes), 1))
		return true;
	iov_iter_revert(i, copied);
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) bool
copy_from_iter_full(void *addr, size_t bytes, struct iov_iter *i)
{
	size_t copied = copy_from_iter(addr, bytes, i);
	if (__builtin_expect(!!(copied == bytes), 1))
		return true;
	iov_iter_revert(i, copied);
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) size_t
copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i)
{
	if (check_copy_size(addr, bytes, false))
		return _copy_from_iter_nocache(addr, bytes, i);
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) bool
copy_from_iter_full_nocache(void *addr, size_t bytes, struct iov_iter *i)
{
	size_t copied = copy_from_iter_nocache(addr, bytes, i);
	if (__builtin_expect(!!(copied == bytes), 1))
		return true;
	iov_iter_revert(i, copied);
	return false;
}
size_t _copy_from_iter_flushcache(void *addr, size_t bytes, struct iov_iter *i);

size_t _copy_mc_to_iter(const void *addr, size_t bytes, struct iov_iter *i);

size_t iov_iter_zero(size_t bytes, struct iov_iter *);
bool iov_iter_is_aligned(const struct iov_iter *i, unsigned addr_mask,
			 unsigned len_mask);
unsigned long iov_iter_alignment(const struct iov_iter *i);
unsigned long iov_iter_gap_alignment(const struct iov_iter *i);
void iov_iter_init(struct iov_iter *i, unsigned int direction,
		   const struct iovec *iov, unsigned long nr_segs,
		   size_t count);
void iov_iter_kvec(struct iov_iter *i, unsigned int direction,
		   const struct kvec *kvec, unsigned long nr_segs,
		   size_t count);
void iov_iter_bvec(struct iov_iter *i, unsigned int direction,
		   const struct bio_vec *bvec, unsigned long nr_segs,
		   size_t count);
void iov_iter_discard(struct iov_iter *i, unsigned int direction, size_t count);
void iov_iter_folio_queue(struct iov_iter *i, unsigned int direction,
			  const struct folio_queue *folioq,
			  unsigned int first_slot, unsigned int offset,
			  size_t count);
void iov_iter_xarray(struct iov_iter *i, unsigned int direction,
		     struct xarray *xarray, loff_t start, size_t count);
ssize_t iov_iter_get_pages2(struct iov_iter *i, struct page **pages,
			    size_t maxsize, unsigned maxpages, size_t *start);
ssize_t iov_iter_get_pages_alloc2(struct iov_iter *i, struct page ***pages,
				  size_t maxsize, size_t *start);
int iov_iter_npages(const struct iov_iter *i, int maxpages);
void iov_iter_restore(struct iov_iter *i, struct iov_iter_state *state);

const void *dup_iter(struct iov_iter *new, struct iov_iter *old, gfp_t flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
iov_iter_count(const struct iov_iter *i)
{
	return i->count;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
iov_iter_truncate(struct iov_iter *i, u64 count)
{
	if (i->count > count)
		i->count = count;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
iov_iter_reexpand(struct iov_iter *i, size_t count)
{
	i->count = count;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
iov_iter_npages_cap(struct iov_iter *i, int maxpages, size_t max_bytes)
{
	size_t shorted = 0;
	int npages;

	if (iov_iter_count(i) > max_bytes) {
		shorted = iov_iter_count(i) - max_bytes;
		iov_iter_truncate(i, max_bytes);
	}
	npages = iov_iter_npages(i, maxpages);
	if (shorted)
		iov_iter_reexpand(i, iov_iter_count(i) + shorted);

	return npages;
}

struct iovec *iovec_from_user(const struct iovec *uvector,
			      unsigned long nr_segs, unsigned long fast_segs,
			      struct iovec *fast_iov, bool compat);
ssize_t import_iovec(int type, const struct iovec *uvec, unsigned nr_segs,
		     unsigned fast_segs, struct iovec **iovp,
		     struct iov_iter *i);
ssize_t __import_iovec(int type, const struct iovec *uvec, unsigned nr_segs,
		       unsigned fast_segs, struct iovec **iovp,
		       struct iov_iter *i, bool compat);
int import_ubuf(int type, void *buf, size_t len, struct iov_iter *i);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
iov_iter_ubuf(struct iov_iter *i, unsigned int direction, void *buf,
	      size_t count)
{
	({
		int __ret_warn_on = !!(direction & ~(0 | 1));
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) | (((9) << 8));
				({
					asm volatile(
						"372"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"372"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(372));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/uio.h"),
						  "i"(368), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"373"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"373"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(373));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	*i = (struct iov_iter){ .iter_type = ITER_UBUF,
				.data_source = direction,
				.ubuf = buf,
				.count = count,
				.nr_segs = 1 };
}

ssize_t iov_iter_extract_pages(struct iov_iter *i, struct page ***pages,
			       size_t maxsize, unsigned int maxpages,
			       iov_iter_extraction_t extraction_flags,
			       size_t *offset0);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
iov_iter_extract_will_pin(const struct iov_iter *iter)
{
	return user_backed_iter(iter);
}

struct sg_table;
ssize_t extract_iter_to_sg(struct iov_iter *iter, size_t len,
			   struct sg_table *sgtable, unsigned int sg_max,
			   iov_iter_extraction_t extraction_flags);

typedef unsigned short __kernel_sa_family_t;

struct __kernel_sockaddr_storage {
	union {
		struct {
			__kernel_sa_family_t ss_family;

			char __data[128 - sizeof(unsigned short)];
		};
		void *__align;
	};
};

struct file;
struct pid;
struct cred;
struct socket;
struct sock;
struct sk_buff;
struct proto_accept_arg;

struct seq_file;
extern void socket_seq_show(struct seq_file *seq);

typedef __kernel_sa_family_t sa_family_t;

struct sockaddr {
	sa_family_t sa_family;
	union {
		char sa_data_min[14];
		struct {
			struct {
			} __empty_sa_data;
			char sa_data[];
		};
	};
};

struct linger {
	int l_onoff;
	int l_linger;
};
struct msghdr {
	void *msg_name;
	int msg_namelen;

	int msg_inq;

	struct iov_iter msg_iter;

	union {
		void *msg_control;
		void *msg_control_user;
	};
	bool msg_control_is_user : 1;
	bool msg_get_inq : 1;
	unsigned int msg_flags;
	__kernel_size_t msg_controllen;
	struct kiocb *msg_iocb;
	struct ubuf_info *msg_ubuf;
	int (*sg_from_iter)(struct sk_buff *skb, struct iov_iter *from,
			    size_t length);
};

struct user_msghdr {
	void *msg_name;
	int msg_namelen;
	struct iovec *msg_iov;
	__kernel_size_t msg_iovlen;
	void *msg_control;
	__kernel_size_t msg_controllen;
	unsigned int msg_flags;
};

struct mmsghdr {
	struct user_msghdr msg_hdr;
	unsigned int msg_len;
};

struct cmsghdr {
	__kernel_size_t cmsg_len;
	int cmsg_level;
	int cmsg_type;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct cmsghdr *
__cmsg_nxthdr(void *__ctl, __kernel_size_t __size, struct cmsghdr *__cmsg)
{
	struct cmsghdr *__ptr;

	__ptr = (struct cmsghdr *)(((unsigned char *)__cmsg) +
				   (((__cmsg->cmsg_len) + sizeof(long) - 1) &
				    ~(sizeof(long) - 1)));
	if ((unsigned long)((char *)(__ptr + 1) - (char *)__ctl) > __size)
		return (struct cmsghdr *)0;

	return __ptr;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct cmsghdr *
cmsg_nxthdr(struct msghdr *__msg, struct cmsghdr *__cmsg)
{
	return __cmsg_nxthdr(__msg->msg_control, __msg->msg_controllen, __cmsg);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
msg_data_left(struct msghdr *msg)
{
	return iov_iter_count(&msg->msg_iter);
}
struct ucred {
	__u32 pid;
	__u32 uid;
	__u32 gid;
};
extern int move_addr_to_kernel(void *uaddr, int ulen,
			       struct __kernel_sockaddr_storage *kaddr);
extern int put_cmsg(struct msghdr *, int level, int type, int len, void *data);

struct timespec64;
struct __kernel_timespec;
struct old_timespec32;

struct scm_timestamping_internal {
	struct timespec64 ts[3];
};

extern void put_cmsg_scm_timestamping64(struct msghdr *msg,
					struct scm_timestamping_internal *tss);
extern void put_cmsg_scm_timestamping(struct msghdr *msg,
				      struct scm_timestamping_internal *tss);

extern long __sys_recvmsg(int fd, struct user_msghdr *msg, unsigned int flags,
			  bool forbid_cmsg_compat);
extern long __sys_sendmsg(int fd, struct user_msghdr *msg, unsigned int flags,
			  bool forbid_cmsg_compat);
extern int __sys_recvmmsg(int fd, struct mmsghdr *mmsg, unsigned int vlen,
			  unsigned int flags, struct __kernel_timespec *timeout,
			  struct old_timespec32 *timeout32);
extern int __sys_sendmmsg(int fd, struct mmsghdr *mmsg, unsigned int vlen,
			  unsigned int flags, bool forbid_cmsg_compat);
extern long __sys_sendmsg_sock(struct socket *sock, struct msghdr *msg,
			       unsigned int flags);
extern long __sys_recvmsg_sock(struct socket *sock, struct msghdr *msg,
			       struct user_msghdr *umsg, struct sockaddr *uaddr,
			       unsigned int flags);
extern int __copy_msghdr(struct msghdr *kmsg, struct user_msghdr *umsg,
			 struct sockaddr **save_addr);

extern int __sys_recvfrom(int fd, void *ubuf, size_t size, unsigned int flags,
			  struct sockaddr *addr, int *addr_len);
extern int __sys_sendto(int fd, void *buff, size_t len, unsigned int flags,
			struct sockaddr *addr, int addr_len);
extern struct file *do_accept(struct file *file, struct proto_accept_arg *arg,
			      struct sockaddr *upeer_sockaddr,
			      int *upeer_addrlen, int flags);
extern int __sys_accept4(int fd, struct sockaddr *upeer_sockaddr,
			 int *upeer_addrlen, int flags);
extern int __sys_socket(int family, int type, int protocol);
extern struct file *__sys_socket_file(int family, int type, int protocol);
extern int __sys_bind(int fd, struct sockaddr *umyaddr, int addrlen);
extern int __sys_bind_socket(struct socket *sock,
			     struct __kernel_sockaddr_storage *address,
			     int addrlen);
extern int __sys_connect_file(struct file *file,
			      struct __kernel_sockaddr_storage *addr,
			      int addrlen, int file_flags);
extern int __sys_connect(int fd, struct sockaddr *uservaddr, int addrlen);
extern int __sys_listen(int fd, int backlog);
extern int __sys_listen_socket(struct socket *sock, int backlog);
extern int __sys_getsockname(int fd, struct sockaddr *usockaddr,
			     int *usockaddr_len);
extern int __sys_getpeername(int fd, struct sockaddr *usockaddr,
			     int *usockaddr_len);
extern int __sys_socketpair(int family, int type, int protocol, int *usockvec);
extern int __sys_shutdown_sock(struct socket *sock, int how);
extern int __sys_shutdown(int fd, int how);
typedef struct {
	unsigned int clock_rate;
	unsigned int clock_type;
	unsigned short loopback;
} sync_serial_settings;

typedef struct {
	unsigned int clock_rate;
	unsigned int clock_type;
	unsigned short loopback;
	unsigned int slot_map;
} te1_settings;

typedef struct {
	unsigned short encoding;
	unsigned short parity;
} raw_hdlc_proto;

typedef struct {
	unsigned int t391;
	unsigned int t392;
	unsigned int n391;
	unsigned int n392;
	unsigned int n393;
	unsigned short lmi;
	unsigned short dce;
} fr_proto;

typedef struct {
	unsigned int dlci;
} fr_proto_pvc;

typedef struct {
	unsigned int dlci;
	char master[16];
} fr_proto_pvc_info;

typedef struct {
	unsigned int interval;
	unsigned int timeout;
} cisco_proto;

typedef struct {
	unsigned short dce;
	unsigned int modulo;
	unsigned int window;
	unsigned int t1;
	unsigned int t2;
	unsigned int n2;
} x25_hdlc_proto;
enum net_device_flags {

	IFF_UP = 1 << 0,
	IFF_BROADCAST = 1 << 1,
	IFF_DEBUG = 1 << 2,
	IFF_LOOPBACK = 1 << 3,
	IFF_POINTOPOINT = 1 << 4,
	IFF_NOTRAILERS = 1 << 5,
	IFF_RUNNING = 1 << 6,
	IFF_NOARP = 1 << 7,
	IFF_PROMISC = 1 << 8,
	IFF_ALLMULTI = 1 << 9,
	IFF_MASTER = 1 << 10,
	IFF_SLAVE = 1 << 11,
	IFF_MULTICAST = 1 << 12,
	IFF_PORTSEL = 1 << 13,
	IFF_AUTOMEDIA = 1 << 14,
	IFF_DYNAMIC = 1 << 15,

	IFF_LOWER_UP = 1 << 16,
	IFF_DORMANT = 1 << 17,
	IFF_ECHO = 1 << 18,

};
enum {
	IF_OPER_UNKNOWN,
	IF_OPER_NOTPRESENT,
	IF_OPER_DOWN,
	IF_OPER_LOWERLAYERDOWN,
	IF_OPER_TESTING,
	IF_OPER_DORMANT,
	IF_OPER_UP,
};

enum {
	IF_LINK_MODE_DEFAULT,
	IF_LINK_MODE_DORMANT,
	IF_LINK_MODE_TESTING,
};
struct ifmap {
	unsigned long mem_start;
	unsigned long mem_end;
	unsigned short base_addr;
	unsigned char irq;
	unsigned char dma;
	unsigned char port;
};

struct if_settings {
	unsigned int type;
	unsigned int size;
	union {
		raw_hdlc_proto *raw_hdlc;
		cisco_proto *cisco;
		fr_proto *fr;
		fr_proto_pvc *fr_pvc;
		fr_proto_pvc_info *fr_pvc_info;
		x25_hdlc_proto *x25;

		sync_serial_settings *sync;
		te1_settings *te1;
	} ifs_ifsu;
};
struct ifreq {
	union {
		char ifrn_name[16];
	} ifr_ifrn;

	union {
		struct sockaddr ifru_addr;
		struct sockaddr ifru_dstaddr;
		struct sockaddr ifru_broadaddr;
		struct sockaddr ifru_netmask;
		struct sockaddr ifru_hwaddr;
		short ifru_flags;
		int ifru_ivalue;
		int ifru_mtu;
		struct ifmap ifru_map;
		char ifru_slave[16];
		char ifru_newname[16];
		void *ifru_data;
		struct if_settings ifru_settings;
	} ifr_ifru;
};
struct ifconf {
	int ifc_len;
	union {
		char *ifcu_buf;
		struct ifreq *ifcu_req;
	} ifc_ifcu;
};

struct wait_bit_key {
	void *flags;
	int bit_nr;
	unsigned long timeout;
};

struct wait_bit_queue_entry {
	struct wait_bit_key key;
	struct wait_queue_entry wq_entry;
};

typedef int wait_bit_action_f(struct wait_bit_key *key, int mode);

void __wake_up_bit(struct wait_queue_head *wq_head, void *word, int bit);
int __wait_on_bit(struct wait_queue_head *wq_head,
		  struct wait_bit_queue_entry *wbq_entry,
		  wait_bit_action_f *action, unsigned int mode);
int __wait_on_bit_lock(struct wait_queue_head *wq_head,
		       struct wait_bit_queue_entry *wbq_entry,
		       wait_bit_action_f *action, unsigned int mode);
void wake_up_bit(void *word, int bit);
int out_of_line_wait_on_bit(void *word, int, wait_bit_action_f *action,
			    unsigned int mode);
int out_of_line_wait_on_bit_timeout(void *word, int, wait_bit_action_f *action,
				    unsigned int mode, unsigned long timeout);
int out_of_line_wait_on_bit_lock(void *word, int, wait_bit_action_f *action,
				 unsigned int mode);
struct wait_queue_head *bit_waitqueue(void *word, int bit);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
wait_bit_init(void);

int wake_bit_function(struct wait_queue_entry *wq_entry, unsigned mode,
		      int sync, void *key);
extern int bit_wait(struct wait_bit_key *key, int mode);
extern int bit_wait_io(struct wait_bit_key *key, int mode);
extern int bit_wait_timeout(struct wait_bit_key *key, int mode);
extern int bit_wait_io_timeout(struct wait_bit_key *key, int mode);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
wait_on_bit(unsigned long *word, int bit, unsigned mode)
{
	do {
		might_resched();
	} while (0);
	if (!((__builtin_constant_p(bit) &&
	       __builtin_constant_p((uintptr_t)(word) !=
				    (uintptr_t)((void *)0)) &&
	       (uintptr_t)(word) != (uintptr_t)((void *)0) &&
	       __builtin_constant_p(*(const unsigned long *)(word))) ?
		      generic_test_bit_acquire(bit, word) :
		      _test_bit_acquire(bit, word)))
		return 0;
	return out_of_line_wait_on_bit(word, bit, bit_wait, mode);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
wait_on_bit_io(unsigned long *word, int bit, unsigned mode)
{
	do {
		might_resched();
	} while (0);
	if (!((__builtin_constant_p(bit) &&
	       __builtin_constant_p((uintptr_t)(word) !=
				    (uintptr_t)((void *)0)) &&
	       (uintptr_t)(word) != (uintptr_t)((void *)0) &&
	       __builtin_constant_p(*(const unsigned long *)(word))) ?
		      generic_test_bit_acquire(bit, word) :
		      _test_bit_acquire(bit, word)))
		return 0;
	return out_of_line_wait_on_bit(word, bit, bit_wait_io, mode);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
wait_on_bit_timeout(unsigned long *word, int bit, unsigned mode,
		    unsigned long timeout)
{
	do {
		might_resched();
	} while (0);
	if (!((__builtin_constant_p(bit) &&
	       __builtin_constant_p((uintptr_t)(word) !=
				    (uintptr_t)((void *)0)) &&
	       (uintptr_t)(word) != (uintptr_t)((void *)0) &&
	       __builtin_constant_p(*(const unsigned long *)(word))) ?
		      generic_test_bit_acquire(bit, word) :
		      _test_bit_acquire(bit, word)))
		return 0;
	return out_of_line_wait_on_bit_timeout(word, bit, bit_wait_timeout,
					       mode, timeout);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
wait_on_bit_action(unsigned long *word, int bit, wait_bit_action_f *action,
		   unsigned mode)
{
	do {
		might_resched();
	} while (0);
	if (!((__builtin_constant_p(bit) &&
	       __builtin_constant_p((uintptr_t)(word) !=
				    (uintptr_t)((void *)0)) &&
	       (uintptr_t)(word) != (uintptr_t)((void *)0) &&
	       __builtin_constant_p(*(const unsigned long *)(word))) ?
		      generic_test_bit_acquire(bit, word) :
		      _test_bit_acquire(bit, word)))
		return 0;
	return out_of_line_wait_on_bit(word, bit, action, mode);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
wait_on_bit_lock(unsigned long *word, int bit, unsigned mode)
{
	do {
		might_resched();
	} while (0);
	if (!test_and_set_bit(bit, word))
		return 0;
	return out_of_line_wait_on_bit_lock(word, bit, bit_wait, mode);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
wait_on_bit_lock_io(unsigned long *word, int bit, unsigned mode)
{
	do {
		might_resched();
	} while (0);
	if (!test_and_set_bit(bit, word))
		return 0;
	return out_of_line_wait_on_bit_lock(word, bit, bit_wait_io, mode);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
wait_on_bit_lock_action(unsigned long *word, int bit, wait_bit_action_f *action,
			unsigned mode)
{
	do {
		might_resched();
	} while (0);
	if (!test_and_set_bit(bit, word))
		return 0;
	return out_of_line_wait_on_bit_lock(word, bit, action, mode);
}

extern void init_wait_var_entry(struct wait_bit_queue_entry *wbq_entry,
				void *var, int flags);
extern void wake_up_var(void *var);
extern wait_queue_head_t *__var_waitqueue(void *p);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_and_wake_up_bit(int bit, void *word)
{
	clear_bit_unlock(bit, word);

	do {
		do {
		} while (0);
		do {
		} while (0);
	} while (0);
	wake_up_bit(word, bit);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
old_valid_dev(dev_t dev)
{
	return ((unsigned int)((dev) >> 20)) < 256 &&
	       ((unsigned int)((dev) & ((1U << 20) - 1))) < 256;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u16
old_encode_dev(dev_t dev)
{
	return (((unsigned int)((dev) >> 20)) << 8) |
	       ((unsigned int)((dev) & ((1U << 20) - 1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) dev_t
old_decode_dev(u16 val)
{
	return ((((val >> 8) & 255) << 20) | (val & 255));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u32
new_encode_dev(dev_t dev)
{
	unsigned major = ((unsigned int)((dev) >> 20));
	unsigned minor = ((unsigned int)((dev) & ((1U << 20) - 1)));
	return (minor & 0xff) | (major << 8) | ((minor & ~0xff) << 12);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) dev_t
new_decode_dev(u32 dev)
{
	unsigned major = (dev & 0xfff00) >> 8;
	unsigned minor = (dev & 0xff) | ((dev >> 12) & 0xfff00);
	return (((major) << 20) | (minor));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u64
huge_encode_dev(dev_t dev)
{
	return new_encode_dev(dev);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) dev_t
huge_decode_dev(u64 dev)
{
	return new_decode_dev(dev);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
sysv_valid_dev(dev_t dev)
{
	return ((unsigned int)((dev) >> 20)) < (1 << 14) &&
	       ((unsigned int)((dev) & ((1U << 20) - 1))) < (1 << 18);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) u32
sysv_encode_dev(dev_t dev)
{
	return ((unsigned int)((dev) & ((1U << 20) - 1))) |
	       (((unsigned int)((dev) >> 20)) << 18);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned
sysv_major(u32 dev)
{
	return (dev >> 18) & 0x3fff;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned
sysv_minor(u32 dev)
{
	return dev & 0x3ffff;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
INIT_LIST_HEAD_RCU(struct list_head *list)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_374(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(list->next) == sizeof(char) ||
			       sizeof(list->next) == sizeof(short) ||
			       sizeof(list->next) == sizeof(int) ||
			       sizeof(list->next) == sizeof(long)) ||
			      sizeof(list->next) == sizeof(long long)))
				__compiletime_assert_374();
		} while (0);
		do {
			*(volatile typeof(list->next) *)&(list->next) = (list);
		} while (0);
	} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_375(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(list->prev) == sizeof(char) ||
			       sizeof(list->prev) == sizeof(short) ||
			       sizeof(list->prev) == sizeof(int) ||
			       sizeof(list->prev) == sizeof(long)) ||
			      sizeof(list->prev) == sizeof(long long)))
				__compiletime_assert_375();
		} while (0);
		do {
			*(volatile typeof(list->prev) *)&(list->prev) = (list);
		} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__list_add_rcu(struct list_head *new, struct list_head *prev,
	       struct list_head *next)
{
	if (!__list_add_valid(new, prev, next))
		return;

	new->next = next;
	new->prev = prev;
	do {
		uintptr_t _r_a_p__v = (uintptr_t)(new);
		;
		if (__builtin_constant_p(new) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_376(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(((*((
						       struct list_head *
							       *)(&(prev)->next))))) ==
						       sizeof(char) ||
					       sizeof(((*((
						       struct list_head *
							       *)(&(prev)->next))))) ==
						       sizeof(short) ||
					       sizeof(((*((
						       struct list_head *
							       *)(&(prev)->next))))) ==
						       sizeof(int) ||
					       sizeof(((*((
						       struct list_head *
							       *)(&(prev)->next))))) ==
						       sizeof(long)) ||
					      sizeof(((*((
						      struct list_head *
							      *)(&(prev)->next))))) ==
						      sizeof(long long)))
						__compiletime_assert_376();
				} while (0);
				do {
					*(volatile typeof(((
						*((struct list_head *
							   *)(&(prev)->next)))))
						  *)&(((*((struct list_head *
								   *)(&(prev)->next))))) =
						((typeof((*((
							struct list_head *
								*)(&(prev)->next)))))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_377(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&(*((
							       struct list_head *
								       *)(&(prev)->next)))) ==
							       sizeof(char) ||
						       sizeof(*&(*((
							       struct list_head *
								       *)(&(prev)->next)))) ==
							       sizeof(short) ||
						       sizeof(*&(*((
							       struct list_head *
								       *)(&(prev)->next)))) ==
							       sizeof(int) ||
						       sizeof(*&(*((
							       struct list_head *
								       *)(&(prev)->next)))) ==
							       sizeof(long))))
							__compiletime_assert_377();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_378(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&(*((
								       struct list_head *
									       *)(&(prev)->next)))) ==
								       sizeof(char) ||
							       sizeof(*&(*((
								       struct list_head *
									       *)(&(prev)->next)))) ==
								       sizeof(short) ||
							       sizeof(*&(*((
								       struct list_head *
									       *)(&(prev)->next)))) ==
								       sizeof(int) ||
							       sizeof(*&(*((
								       struct list_head *
									       *)(&(prev)->next)))) ==
								       sizeof(long)) ||
							      sizeof(*&(*((
								      struct list_head *
									      *)(&(prev)->next)))) ==
								      sizeof(long long)))
								__compiletime_assert_378();
						} while (0);
						do {
							*(volatile typeof(*&(*((
								struct list_head *
									*)(&(prev)->next))))
								  *)&(*&(*((struct list_head *
										    *)(&(prev)->next)))) =
								((typeof(*((typeof((*((
									struct list_head *
										*)(&(prev)->next)))))
										   _r_a_p__v))
									  *)((typeof((*((
									struct list_head *
										*)(&(prev)->next)))))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
	next->prev = new;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_add_rcu(struct list_head *new, struct list_head *head)
{
	__list_add_rcu(new, head, head->next);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_add_tail_rcu(struct list_head *new, struct list_head *head)
{
	__list_add_rcu(new, head->prev, head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_del_rcu(struct list_head *entry)
{
	__list_del_entry(entry);
	entry->prev = ((void *)0x122 + (0xdead000000000000UL));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_del_init_rcu(struct hlist_node *n)
{
	if (!hlist_unhashed(n)) {
		__hlist_del(n);
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_379(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(n->pprev) == sizeof(char) ||
				       sizeof(n->pprev) == sizeof(short) ||
				       sizeof(n->pprev) == sizeof(int) ||
				       sizeof(n->pprev) == sizeof(long)) ||
				      sizeof(n->pprev) == sizeof(long long)))
					__compiletime_assert_379();
			} while (0);
			do {
				*(volatile typeof(n->pprev) *)&(n->pprev) =
					(((void *)0));
			} while (0);
		} while (0);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_replace_rcu(struct list_head *old, struct list_head *new)
{
	new->next = old->next;
	new->prev = old->prev;
	do {
		uintptr_t _r_a_p__v = (uintptr_t)(new);
		;
		if (__builtin_constant_p(new) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_380(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(((*((
						       struct list_head *
							       *)(&(new->prev)
									   ->next))))) ==
						       sizeof(char) ||
					       sizeof(((*((
						       struct list_head *
							       *)(&(new->prev)
									   ->next))))) ==
						       sizeof(short) ||
					       sizeof(((*((
						       struct list_head *
							       *)(&(new->prev)
									   ->next))))) ==
						       sizeof(int) ||
					       sizeof(((*((
						       struct list_head *
							       *)(&(new->prev)
									   ->next))))) ==
						       sizeof(long)) ||
					      sizeof(((*((
						      struct list_head *
							      *)(&(new->prev)
									  ->next))))) ==
						      sizeof(long long)))
						__compiletime_assert_380();
				} while (0);
				do {
					*(volatile typeof(((*((
						struct list_head *
							*)(&(new->prev)->next)))))
						  *)&(((*((struct list_head *
								   *)(&(new->prev)
									       ->next))))) =
						((typeof((*((
							struct list_head *
								*)(&(new->prev)
									    ->next)))))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_381(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&(*((
							       struct list_head *
								       *)(&(new->prev)
										   ->next)))) ==
							       sizeof(char) ||
						       sizeof(*&(*((
							       struct list_head *
								       *)(&(new->prev)
										   ->next)))) ==
							       sizeof(short) ||
						       sizeof(*&(*((
							       struct list_head *
								       *)(&(new->prev)
										   ->next)))) ==
							       sizeof(int) ||
						       sizeof(*&(*((
							       struct list_head *
								       *)(&(new->prev)
										   ->next)))) ==
							       sizeof(long))))
							__compiletime_assert_381();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_382(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&(*((
								       struct list_head *
									       *)(&(new->prev)
											   ->next)))) ==
								       sizeof(char) ||
							       sizeof(*&(*((
								       struct list_head *
									       *)(&(new->prev)
											   ->next)))) ==
								       sizeof(short) ||
							       sizeof(*&(*((
								       struct list_head *
									       *)(&(new->prev)
											   ->next)))) ==
								       sizeof(int) ||
							       sizeof(*&(*((
								       struct list_head *
									       *)(&(new->prev)
											   ->next)))) ==
								       sizeof(long)) ||
							      sizeof(*&(*((
								      struct list_head *
									      *)(&(new->prev)
											  ->next)))) ==
								      sizeof(long long)))
								__compiletime_assert_382();
						} while (0);
						do {
							*(volatile typeof(*&(*((
								struct list_head *
									*)(&(new->prev)
										    ->next))))
								  *)&(*&(*((struct list_head *
										    *)(&(new->prev)
												->next)))) =
								((typeof(*((typeof((*((
									struct list_head *
										*)(&(new->prev)
											    ->next)))))
										   _r_a_p__v))
									  *)((typeof((*((
									struct list_head *
										*)(&(new->prev)
											    ->next)))))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
	new->next->prev = new;
	old->prev = ((void *)0x122 + (0xdead000000000000UL));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__list_splice_init_rcu(struct list_head *list, struct list_head *prev,
		       struct list_head *next, void (*sync)(void))
{
	struct list_head *first = list->next;
	struct list_head *last = list->prev;

	INIT_LIST_HEAD_RCU(list);
	sync();
	__kcsan_check_access(&(*first), sizeof(*first), (1 << 0) | (1 << 3));
	__kcsan_check_access(&(*last), sizeof(*last), (1 << 0) | (1 << 3));
	last->next = next;
	do {
		uintptr_t _r_a_p__v = (uintptr_t)(first);
		;
		if (__builtin_constant_p(first) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_383(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(((*((
						       struct list_head *
							       *)(&(prev)->next))))) ==
						       sizeof(char) ||
					       sizeof(((*((
						       struct list_head *
							       *)(&(prev)->next))))) ==
						       sizeof(short) ||
					       sizeof(((*((
						       struct list_head *
							       *)(&(prev)->next))))) ==
						       sizeof(int) ||
					       sizeof(((*((
						       struct list_head *
							       *)(&(prev)->next))))) ==
						       sizeof(long)) ||
					      sizeof(((*((
						      struct list_head *
							      *)(&(prev)->next))))) ==
						      sizeof(long long)))
						__compiletime_assert_383();
				} while (0);
				do {
					*(volatile typeof(((
						*((struct list_head *
							   *)(&(prev)->next)))))
						  *)&(((*((struct list_head *
								   *)(&(prev)->next))))) =
						((typeof((*((
							struct list_head *
								*)(&(prev)->next)))))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_384(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&(*((
							       struct list_head *
								       *)(&(prev)->next)))) ==
							       sizeof(char) ||
						       sizeof(*&(*((
							       struct list_head *
								       *)(&(prev)->next)))) ==
							       sizeof(short) ||
						       sizeof(*&(*((
							       struct list_head *
								       *)(&(prev)->next)))) ==
							       sizeof(int) ||
						       sizeof(*&(*((
							       struct list_head *
								       *)(&(prev)->next)))) ==
							       sizeof(long))))
							__compiletime_assert_384();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_385(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&(*((
								       struct list_head *
									       *)(&(prev)->next)))) ==
								       sizeof(char) ||
							       sizeof(*&(*((
								       struct list_head *
									       *)(&(prev)->next)))) ==
								       sizeof(short) ||
							       sizeof(*&(*((
								       struct list_head *
									       *)(&(prev)->next)))) ==
								       sizeof(int) ||
							       sizeof(*&(*((
								       struct list_head *
									       *)(&(prev)->next)))) ==
								       sizeof(long)) ||
							      sizeof(*&(*((
								      struct list_head *
									      *)(&(prev)->next)))) ==
								      sizeof(long long)))
								__compiletime_assert_385();
						} while (0);
						do {
							*(volatile typeof(*&(*((
								struct list_head *
									*)(&(prev)->next))))
								  *)&(*&(*((struct list_head *
										    *)(&(prev)->next)))) =
								((typeof(*((typeof((*((
									struct list_head *
										*)(&(prev)->next)))))
										   _r_a_p__v))
									  *)((typeof((*((
									struct list_head *
										*)(&(prev)->next)))))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
	first->prev = prev;
	next->prev = last;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_splice_init_rcu(struct list_head *list, struct list_head *head,
		     void (*sync)(void))
{
	if (!list_empty(list))
		__list_splice_init_rcu(list, head, head->next, sync);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
list_splice_tail_init_rcu(struct list_head *list, struct list_head *head,
			  void (*sync)(void))
{
	if (!list_empty(list))
		__list_splice_init_rcu(list, head->prev, head, sync);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_del_rcu(struct hlist_node *n)
{
	__hlist_del(n);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_386(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->pprev) == sizeof(char) ||
			       sizeof(n->pprev) == sizeof(short) ||
			       sizeof(n->pprev) == sizeof(int) ||
			       sizeof(n->pprev) == sizeof(long)) ||
			      sizeof(n->pprev) == sizeof(long long)))
				__compiletime_assert_386();
		} while (0);
		do {
			*(volatile typeof(n->pprev) *)&(n->pprev) =
				(((void *)0x122 + (0xdead000000000000UL)));
		} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_replace_rcu(struct hlist_node *old, struct hlist_node *new)
{
	struct hlist_node *next = old->next;

	new->next = next;
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_387(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(new->pprev) == sizeof(char) ||
			       sizeof(new->pprev) == sizeof(short) ||
			       sizeof(new->pprev) == sizeof(int) ||
			       sizeof(new->pprev) == sizeof(long)) ||
			      sizeof(new->pprev) == sizeof(long long)))
				__compiletime_assert_387();
		} while (0);
		do {
			*(volatile typeof(new->pprev) *)&(new->pprev) =
				(old->pprev);
		} while (0);
	} while (0);
	do {
		uintptr_t _r_a_p__v = (uintptr_t)(new);
		;
		if (__builtin_constant_p(new) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_388(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof((*(struct hlist_node **)new
								->pprev)) ==
						       sizeof(char) ||
					       sizeof((*(struct hlist_node **)new
								->pprev)) ==
						       sizeof(short) ||
					       sizeof((*(struct hlist_node **)new
								->pprev)) ==
						       sizeof(int) ||
					       sizeof((*(struct hlist_node **)new
								->pprev)) ==
						       sizeof(long)) ||
					      sizeof((*(struct hlist_node **)new
							       ->pprev)) ==
						      sizeof(long long)))
						__compiletime_assert_388();
				} while (0);
				do {
					*(volatile typeof((
						*(struct hlist_node **)new
							 ->pprev))
						  *)&((*(struct hlist_node **)new
								->pprev)) =
						((typeof(*(struct hlist_node **)new
								  ->pprev))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_389(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&*(struct hlist_node
									  **)new
									 ->pprev) ==
							       sizeof(char) ||
						       sizeof(*&*(struct hlist_node
									  **)new
									 ->pprev) ==
							       sizeof(short) ||
						       sizeof(*&*(struct hlist_node
									  **)new
									 ->pprev) ==
							       sizeof(int) ||
						       sizeof(*&*(struct hlist_node
									  **)new
									 ->pprev) ==
							       sizeof(long))))
							__compiletime_assert_389();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_390(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&*(struct hlist_node
										  **)new
										 ->pprev) ==
								       sizeof(char) ||
							       sizeof(*&*(struct hlist_node
										  **)new
										 ->pprev) ==
								       sizeof(short) ||
							       sizeof(*&*(struct hlist_node
										  **)new
										 ->pprev) ==
								       sizeof(int) ||
							       sizeof(*&*(struct hlist_node
										  **)new
										 ->pprev) ==
								       sizeof(long)) ||
							      sizeof(*&*(struct hlist_node
										 **)new
										->pprev) ==
								      sizeof(long long)))
								__compiletime_assert_390();
						} while (0);
						do {
							*(volatile typeof(*&*(struct hlist_node
										      **)new
										     ->pprev)
								  *)&(*&*(struct hlist_node
										  **)new
										 ->pprev) =
								((typeof(*((
									typeof(*(struct hlist_node
											 **)new
											->pprev))
										   _r_a_p__v))
									  *)((
									typeof(*(struct hlist_node
											 **)new
											->pprev))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
	if (next)
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_391(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(new->next->pprev) ==
					       sizeof(char) ||
				       sizeof(new->next->pprev) ==
					       sizeof(short) ||
				       sizeof(new->next->pprev) == sizeof(int) ||
				       sizeof(new->next->pprev) ==
					       sizeof(long)) ||
				      sizeof(new->next->pprev) ==
					      sizeof(long long)))
					__compiletime_assert_391();
			} while (0);
			do {
				*(volatile typeof(new->next->pprev) *)&(
					new->next->pprev) = (&new->next);
			} while (0);
		} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_392(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(old->pprev) == sizeof(char) ||
			       sizeof(old->pprev) == sizeof(short) ||
			       sizeof(old->pprev) == sizeof(int) ||
			       sizeof(old->pprev) == sizeof(long)) ||
			      sizeof(old->pprev) == sizeof(long long)))
				__compiletime_assert_392();
		} while (0);
		do {
			*(volatile typeof(old->pprev) *)&(old->pprev) =
				(((void *)0x122 + (0xdead000000000000UL)));
		} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlists_swap_heads_rcu(struct hlist_head *left, struct hlist_head *right)
{
	struct hlist_node *node1 = left->first;
	struct hlist_node *node2 = right->first;

	do {
		uintptr_t _r_a_p__v = (uintptr_t)(node2);
		;
		if (__builtin_constant_p(node2) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_393(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof((left->first)) ==
						       sizeof(char) ||
					       sizeof((left->first)) ==
						       sizeof(short) ||
					       sizeof((left->first)) ==
						       sizeof(int) ||
					       sizeof((left->first)) ==
						       sizeof(long)) ||
					      sizeof((left->first)) ==
						      sizeof(long long)))
						__compiletime_assert_393();
				} while (0);
				do {
					*(volatile typeof((left->first)) *)&(
						(left->first)) =
						((typeof(left->first))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_394(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&left->first) ==
							       sizeof(char) ||
						       sizeof(*&left->first) ==
							       sizeof(short) ||
						       sizeof(*&left->first) ==
							       sizeof(int) ||
						       sizeof(*&left->first) ==
							       sizeof(long))))
							__compiletime_assert_394();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_395(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&left->first) ==
								       sizeof(char) ||
							       sizeof(*&left->first) ==
								       sizeof(short) ||
							       sizeof(*&left->first) ==
								       sizeof(int) ||
							       sizeof(*&left->first) ==
								       sizeof(long)) ||
							      sizeof(*&left->first) ==
								      sizeof(long long)))
								__compiletime_assert_395();
						} while (0);
						do {
							*(volatile typeof(*&left->first)
								  *)&(*&left->first) =
								((typeof(*(
									(typeof(left->first))
										_r_a_p__v))
									  *)((
									typeof(left->first))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
	do {
		uintptr_t _r_a_p__v = (uintptr_t)(node1);
		;
		if (__builtin_constant_p(node1) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_396(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof((right->first)) ==
						       sizeof(char) ||
					       sizeof((right->first)) ==
						       sizeof(short) ||
					       sizeof((right->first)) ==
						       sizeof(int) ||
					       sizeof((right->first)) ==
						       sizeof(long)) ||
					      sizeof((right->first)) ==
						      sizeof(long long)))
						__compiletime_assert_396();
				} while (0);
				do {
					*(volatile typeof((right->first)) *)&(
						(right->first)) =
						((typeof(right->first))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_397(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&right->first) ==
							       sizeof(char) ||
						       sizeof(*&right->first) ==
							       sizeof(short) ||
						       sizeof(*&right->first) ==
							       sizeof(int) ||
						       sizeof(*&right->first) ==
							       sizeof(long))))
							__compiletime_assert_397();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_398(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&right->first) ==
								       sizeof(char) ||
							       sizeof(*&right->first) ==
								       sizeof(short) ||
							       sizeof(*&right->first) ==
								       sizeof(int) ||
							       sizeof(*&right->first) ==
								       sizeof(long)) ||
							      sizeof(*&right->first) ==
								      sizeof(long long)))
								__compiletime_assert_398();
						} while (0);
						do {
							*(volatile typeof(*&right->first)
								  *)&(*&right->first) =
								((typeof(*(
									(typeof(right->first))
										_r_a_p__v))
									  *)((
									typeof(right->first))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_399(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(node2->pprev) == sizeof(char) ||
			       sizeof(node2->pprev) == sizeof(short) ||
			       sizeof(node2->pprev) == sizeof(int) ||
			       sizeof(node2->pprev) == sizeof(long)) ||
			      sizeof(node2->pprev) == sizeof(long long)))
				__compiletime_assert_399();
		} while (0);
		do {
			*(volatile typeof(node2->pprev) *)&(node2->pprev) =
				(&left->first);
		} while (0);
	} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_400(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(node1->pprev) == sizeof(char) ||
			       sizeof(node1->pprev) == sizeof(short) ||
			       sizeof(node1->pprev) == sizeof(int) ||
			       sizeof(node1->pprev) == sizeof(long)) ||
			      sizeof(node1->pprev) == sizeof(long long)))
				__compiletime_assert_400();
		} while (0);
		do {
			*(volatile typeof(node1->pprev) *)&(node1->pprev) =
				(&right->first);
		} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_add_head_rcu(struct hlist_node *n, struct hlist_head *h)
{
	struct hlist_node *first = h->first;

	n->next = first;
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_401(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->pprev) == sizeof(char) ||
			       sizeof(n->pprev) == sizeof(short) ||
			       sizeof(n->pprev) == sizeof(int) ||
			       sizeof(n->pprev) == sizeof(long)) ||
			      sizeof(n->pprev) == sizeof(long long)))
				__compiletime_assert_401();
		} while (0);
		do {
			*(volatile typeof(n->pprev) *)&(n->pprev) = (&h->first);
		} while (0);
	} while (0);
	do {
		uintptr_t _r_a_p__v = (uintptr_t)(n);
		;
		if (__builtin_constant_p(n) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_402(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(((*((
						       struct hlist_node *
							       *)(&(h)->first))))) ==
						       sizeof(char) ||
					       sizeof(((*((
						       struct hlist_node *
							       *)(&(h)->first))))) ==
						       sizeof(short) ||
					       sizeof(((*((
						       struct hlist_node *
							       *)(&(h)->first))))) ==
						       sizeof(int) ||
					       sizeof(((*((
						       struct hlist_node *
							       *)(&(h)->first))))) ==
						       sizeof(long)) ||
					      sizeof(((*((
						      struct hlist_node *
							      *)(&(h)->first))))) ==
						      sizeof(long long)))
						__compiletime_assert_402();
				} while (0);
				do {
					*(volatile typeof((
						(*((struct hlist_node *
							    *)(&(h)->first)))))
						  *)&(((*((struct hlist_node *
								   *)(&(h)->first))))) =
						((typeof((*((
							struct hlist_node *
								*)(&(h)->first)))))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_403(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&(*((
							       struct hlist_node *
								       *)(&(h)->first)))) ==
							       sizeof(char) ||
						       sizeof(*&(*((
							       struct hlist_node *
								       *)(&(h)->first)))) ==
							       sizeof(short) ||
						       sizeof(*&(*((
							       struct hlist_node *
								       *)(&(h)->first)))) ==
							       sizeof(int) ||
						       sizeof(*&(*((
							       struct hlist_node *
								       *)(&(h)->first)))) ==
							       sizeof(long))))
							__compiletime_assert_403();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_404(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&(*((
								       struct hlist_node *
									       *)(&(h)->first)))) ==
								       sizeof(char) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)(&(h)->first)))) ==
								       sizeof(short) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)(&(h)->first)))) ==
								       sizeof(int) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)(&(h)->first)))) ==
								       sizeof(long)) ||
							      sizeof(*&(*((
								      struct hlist_node *
									      *)(&(h)->first)))) ==
								      sizeof(long long)))
								__compiletime_assert_404();
						} while (0);
						do {
							*(volatile typeof(*&(*((
								struct hlist_node *
									*)(&(h)->first))))
								  *)&(*&(*((struct hlist_node *
										    *)(&(h)->first)))) =
								((typeof(*((typeof((*((
									struct hlist_node *
										*)(&(h)->first)))))
										   _r_a_p__v))
									  *)((typeof((*((
									struct hlist_node *
										*)(&(h)->first)))))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
	if (first)
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_405(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(first->pprev) == sizeof(char) ||
				       sizeof(first->pprev) == sizeof(short) ||
				       sizeof(first->pprev) == sizeof(int) ||
				       sizeof(first->pprev) == sizeof(long)) ||
				      sizeof(first->pprev) ==
					      sizeof(long long)))
					__compiletime_assert_405();
			} while (0);
			do {
				*(volatile typeof(first->pprev) *)&(
					first->pprev) = (&n->next);
			} while (0);
		} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_add_tail_rcu(struct hlist_node *n, struct hlist_head *h)
{
	struct hlist_node *i, *last = ((void *)0);

	for (i = h->first; i; i = i->next)
		last = i;

	if (last) {
		n->next = last->next;
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_406(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(n->pprev) == sizeof(char) ||
				       sizeof(n->pprev) == sizeof(short) ||
				       sizeof(n->pprev) == sizeof(int) ||
				       sizeof(n->pprev) == sizeof(long)) ||
				      sizeof(n->pprev) == sizeof(long long)))
					__compiletime_assert_406();
			} while (0);
			do {
				*(volatile typeof(n->pprev) *)&(n->pprev) =
					(&last->next);
			} while (0);
		} while (0);
		do {
			uintptr_t _r_a_p__v = (uintptr_t)(n);
			;
			if (__builtin_constant_p(n) &&
			    (_r_a_p__v) == (uintptr_t)((void *)0))
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_407(void)
							__attribute__((__error__(
								"Unsupported access size for {READ,WRITE}_ONCE().")));
						if (!((sizeof(((*((
							       struct hlist_node *
								       *)(&(last)->next))))) ==
							       sizeof(char) ||
						       sizeof(((*((
							       struct hlist_node *
								       *)(&(last)->next))))) ==
							       sizeof(short) ||
						       sizeof(((*((
							       struct hlist_node *
								       *)(&(last)->next))))) ==
							       sizeof(int) ||
						       sizeof(((*((
							       struct hlist_node *
								       *)(&(last)->next))))) ==
							       sizeof(long)) ||
						      sizeof(((*((
							      struct hlist_node *
								      *)(&(last)->next))))) ==
							      sizeof(long long)))
							__compiletime_assert_407();
					} while (0);
					do {
						*(volatile typeof(((*((
							struct hlist_node *
								*)(&(last)->next)))))
							  *)&(((*((struct hlist_node *
									   *)(&(last)->next))))) =
							((typeof((*((
								struct hlist_node *
									*)(&(last)->next)))))(_r_a_p__v));
					} while (0);
				} while (0);
			else
				do {
					do {
					} while (0);
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_408(
								void)
								__attribute__((__error__(
									"Need native word sized stores/loads for atomicity.")));
							if (!((sizeof(*&(*((
								       struct hlist_node *
									       *)(&(last)->next)))) ==
								       sizeof(char) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)(&(last)->next)))) ==
								       sizeof(short) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)(&(last)->next)))) ==
								       sizeof(int) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)(&(last)->next)))) ==
								       sizeof(long))))
								__compiletime_assert_408();
						} while (0);
						__asm__ __volatile__(
							""
							:
							:
							: "memory");
						do {
							do {
								__attribute__((
									__noreturn__)) extern void
								__compiletime_assert_409(
									void)
									__attribute__((__error__(
										"Unsupported access size for {READ,WRITE}_ONCE().")));
								if (!((sizeof(*&(*((
									       struct hlist_node *
										       *)(&(last)->next)))) ==
									       sizeof(char) ||
								       sizeof(*&(*((
									       struct hlist_node *
										       *)(&(last)->next)))) ==
									       sizeof(short) ||
								       sizeof(*&(*((
									       struct hlist_node *
										       *)(&(last)->next)))) ==
									       sizeof(int) ||
								       sizeof(*&(*((
									       struct hlist_node *
										       *)(&(last)->next)))) ==
									       sizeof(long)) ||
								      sizeof(*&(*((
									      struct hlist_node *
										      *)(&(last)->next)))) ==
									      sizeof(long long)))
									__compiletime_assert_409();
							} while (0);
							do {
								*(volatile typeof(*&(*((
									struct hlist_node *
										*)(&(last)->next))))
									  *)&(*&(*((struct hlist_node *
											    *)(&(last)->next)))) =
									((typeof(*((typeof((*((
										struct hlist_node *
											*)(&(last)->next)))))
											   _r_a_p__v))
										  *)((typeof((*((
										struct hlist_node *
											*)(&(last)->next)))))
											     _r_a_p__v));
							} while (0);
						} while (0);
					} while (0);
				} while (0);
		} while (0);
	} else {
		hlist_add_head_rcu(n, h);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_add_before_rcu(struct hlist_node *n, struct hlist_node *next)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_410(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->pprev) == sizeof(char) ||
			       sizeof(n->pprev) == sizeof(short) ||
			       sizeof(n->pprev) == sizeof(int) ||
			       sizeof(n->pprev) == sizeof(long)) ||
			      sizeof(n->pprev) == sizeof(long long)))
				__compiletime_assert_410();
		} while (0);
		do {
			*(volatile typeof(n->pprev) *)&(n->pprev) =
				(next->pprev);
		} while (0);
	} while (0);
	n->next = next;
	do {
		uintptr_t _r_a_p__v = (uintptr_t)(n);
		;
		if (__builtin_constant_p(n) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_411(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(((*((
						       struct hlist_node *
							       *)((n)->pprev))))) ==
						       sizeof(char) ||
					       sizeof(((*((
						       struct hlist_node *
							       *)((n)->pprev))))) ==
						       sizeof(short) ||
					       sizeof(((*((
						       struct hlist_node *
							       *)((n)->pprev))))) ==
						       sizeof(int) ||
					       sizeof(((*((
						       struct hlist_node *
							       *)((n)->pprev))))) ==
						       sizeof(long)) ||
					      sizeof(((*((
						      struct hlist_node *
							      *)((n)->pprev))))) ==
						      sizeof(long long)))
						__compiletime_assert_411();
				} while (0);
				do {
					*(volatile typeof((
						(*((struct hlist_node *
							    *)((n)->pprev)))))
						  *)&(((*((struct hlist_node *
								   *)((n)->pprev))))) =
						((typeof((*((
							struct hlist_node *
								*)((n)->pprev)))))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_412(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&(*((
							       struct hlist_node *
								       *)((n)->pprev)))) ==
							       sizeof(char) ||
						       sizeof(*&(*((
							       struct hlist_node *
								       *)((n)->pprev)))) ==
							       sizeof(short) ||
						       sizeof(*&(*((
							       struct hlist_node *
								       *)((n)->pprev)))) ==
							       sizeof(int) ||
						       sizeof(*&(*((
							       struct hlist_node *
								       *)((n)->pprev)))) ==
							       sizeof(long))))
							__compiletime_assert_412();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_413(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&(*((
								       struct hlist_node *
									       *)((n)->pprev)))) ==
								       sizeof(char) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)((n)->pprev)))) ==
								       sizeof(short) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)((n)->pprev)))) ==
								       sizeof(int) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)((n)->pprev)))) ==
								       sizeof(long)) ||
							      sizeof(*&(*((
								      struct hlist_node *
									      *)((n)->pprev)))) ==
								      sizeof(long long)))
								__compiletime_assert_413();
						} while (0);
						do {
							*(volatile typeof(*&(*((
								struct hlist_node *
									*)((n)->pprev))))
								  *)&(*&(*((struct hlist_node *
										    *)((n)->pprev)))) =
								((typeof(*((typeof((*((
									struct hlist_node *
										*)((n)->pprev)))))
										   _r_a_p__v))
									  *)((typeof((*((
									struct hlist_node *
										*)((n)->pprev)))))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_414(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(next->pprev) == sizeof(char) ||
			       sizeof(next->pprev) == sizeof(short) ||
			       sizeof(next->pprev) == sizeof(int) ||
			       sizeof(next->pprev) == sizeof(long)) ||
			      sizeof(next->pprev) == sizeof(long long)))
				__compiletime_assert_414();
		} while (0);
		do {
			*(volatile typeof(next->pprev) *)&(next->pprev) =
				(&n->next);
		} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_add_behind_rcu(struct hlist_node *n, struct hlist_node *prev)
{
	n->next = prev->next;
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_415(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(n->pprev) == sizeof(char) ||
			       sizeof(n->pprev) == sizeof(short) ||
			       sizeof(n->pprev) == sizeof(int) ||
			       sizeof(n->pprev) == sizeof(long)) ||
			      sizeof(n->pprev) == sizeof(long long)))
				__compiletime_assert_415();
		} while (0);
		do {
			*(volatile typeof(n->pprev) *)&(n->pprev) =
				(&prev->next);
		} while (0);
	} while (0);
	do {
		uintptr_t _r_a_p__v = (uintptr_t)(n);
		;
		if (__builtin_constant_p(n) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_416(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(((*((
						       struct hlist_node *
							       *)(&(prev)->next))))) ==
						       sizeof(char) ||
					       sizeof(((*((
						       struct hlist_node *
							       *)(&(prev)->next))))) ==
						       sizeof(short) ||
					       sizeof(((*((
						       struct hlist_node *
							       *)(&(prev)->next))))) ==
						       sizeof(int) ||
					       sizeof(((*((
						       struct hlist_node *
							       *)(&(prev)->next))))) ==
						       sizeof(long)) ||
					      sizeof(((*((
						      struct hlist_node *
							      *)(&(prev)->next))))) ==
						      sizeof(long long)))
						__compiletime_assert_416();
				} while (0);
				do {
					*(volatile typeof(((
						*((struct hlist_node *
							   *)(&(prev)->next)))))
						  *)&(((*((struct hlist_node *
								   *)(&(prev)->next))))) =
						((typeof((*((
							struct hlist_node *
								*)(&(prev)->next)))))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_417(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&(*((
							       struct hlist_node *
								       *)(&(prev)->next)))) ==
							       sizeof(char) ||
						       sizeof(*&(*((
							       struct hlist_node *
								       *)(&(prev)->next)))) ==
							       sizeof(short) ||
						       sizeof(*&(*((
							       struct hlist_node *
								       *)(&(prev)->next)))) ==
							       sizeof(int) ||
						       sizeof(*&(*((
							       struct hlist_node *
								       *)(&(prev)->next)))) ==
							       sizeof(long))))
							__compiletime_assert_417();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_418(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&(*((
								       struct hlist_node *
									       *)(&(prev)->next)))) ==
								       sizeof(char) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)(&(prev)->next)))) ==
								       sizeof(short) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)(&(prev)->next)))) ==
								       sizeof(int) ||
							       sizeof(*&(*((
								       struct hlist_node *
									       *)(&(prev)->next)))) ==
								       sizeof(long)) ||
							      sizeof(*&(*((
								      struct hlist_node *
									      *)(&(prev)->next)))) ==
								      sizeof(long long)))
								__compiletime_assert_418();
						} while (0);
						do {
							*(volatile typeof(*&(*((
								struct hlist_node *
									*)(&(prev)->next))))
								  *)&(*&(*((struct hlist_node *
										    *)(&(prev)->next)))) =
								((typeof(*((typeof((*((
									struct hlist_node *
										*)(&(prev)->next)))))
										   _r_a_p__v))
									  *)((typeof((*((
									struct hlist_node *
										*)(&(prev)->next)))))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
	if (n->next)
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_419(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(n->next->pprev) == sizeof(char) ||
				       sizeof(n->next->pprev) == sizeof(short) ||
				       sizeof(n->next->pprev) == sizeof(int) ||
				       sizeof(n->next->pprev) == sizeof(long)) ||
				      sizeof(n->next->pprev) ==
					      sizeof(long long)))
					__compiletime_assert_419();
			} while (0);
			do {
				*(volatile typeof(n->next->pprev) *)&(
					n->next->pprev) = (&n->next);
			} while (0);
		} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
bit_spin_lock(int bitnum, unsigned long *addr)
{
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);

	while (__builtin_expect(!!(test_and_set_bit_lock(bitnum, addr)), 0)) {
		do {
			__asm__ __volatile__("" : : : "memory");
			if (__builtin_expect(!!(__preempt_count_dec_and_test()),
					     0))
				do {
					static void *__attribute__((__used__))
					__attribute__((__section__(
						".discard.addressable")))
					__UNIQUE_ID___addressable___SCK__preempt_schedule420 =
						(void *)(uintptr_t)&__SCK__preempt_schedule;
					;
					asm volatile(
						"call "
						"__SCT__preempt_schedule"
						: "+r"(current_stack_pointer));
				} while (0);
		} while (0);
		do {
			cpu_relax();
		} while ((
			(__builtin_constant_p(bitnum) &&
			 __builtin_constant_p((uintptr_t)(addr) !=
					      (uintptr_t)((void *)0)) &&
			 (uintptr_t)(addr) != (uintptr_t)((void *)0) &&
			 __builtin_constant_p(*(const unsigned long *)(addr))) ?
				const_test_bit(bitnum, addr) :
				_test_bit(bitnum, addr)));
		do {
			__preempt_count_add(1);
			__asm__ __volatile__("" : : : "memory");
		} while (0);
	}

	(void)0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
bit_spin_trylock(int bitnum, unsigned long *addr)
{
	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);

	if (__builtin_expect(!!(test_and_set_bit_lock(bitnum, addr)), 0)) {
		do {
			__asm__ __volatile__("" : : : "memory");
			if (__builtin_expect(!!(__preempt_count_dec_and_test()),
					     0))
				do {
					static void *__attribute__((__used__))
					__attribute__((__section__(
						".discard.addressable")))
					__UNIQUE_ID___addressable___SCK__preempt_schedule421 =
						(void *)(uintptr_t)&__SCK__preempt_schedule;
					;
					asm volatile(
						"call "
						"__SCT__preempt_schedule"
						: "+r"(current_stack_pointer));
				} while (0);
		} while (0);
		return 0;
	}

	(void)0;
	return 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
bit_spin_unlock(int bitnum, unsigned long *addr)
{
	clear_bit_unlock(bitnum, addr);

	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule422 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
	(void)0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__bit_spin_unlock(int bitnum, unsigned long *addr)
{
	__clear_bit_unlock(bitnum, addr);

	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule423 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
	(void)0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
bit_spin_is_locked(int bitnum, unsigned long *addr)
{
	return ((__builtin_constant_p(bitnum) &&
		 __builtin_constant_p((uintptr_t)(addr) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(addr) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(*(const unsigned long *)(addr))) ?
			const_test_bit(bitnum, addr) :
			_test_bit(bitnum, addr));
}
struct hlist_bl_head {
	struct hlist_bl_node *first;
};

struct hlist_bl_node {
	struct hlist_bl_node *next, **pprev;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
INIT_HLIST_BL_NODE(struct hlist_bl_node *h)
{
	h->next = ((void *)0);
	h->pprev = ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
hlist_bl_unhashed(const struct hlist_bl_node *h)
{
	return !h->pprev;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct hlist_bl_node *
hlist_bl_first(struct hlist_bl_head *h)
{
	return (struct hlist_bl_node *)((unsigned long)h->first & ~1UL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_bl_set_first(struct hlist_bl_head *h, struct hlist_bl_node *n)
{
	;

	;
	h->first = (struct hlist_bl_node *)((unsigned long)n | 1UL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
hlist_bl_empty(const struct hlist_bl_head *h)
{
	return !(
		(unsigned long)({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_424(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(h->first) == sizeof(char) ||
				       sizeof(h->first) == sizeof(short) ||
				       sizeof(h->first) == sizeof(int) ||
				       sizeof(h->first) == sizeof(long)) ||
				      sizeof(h->first) == sizeof(long long)))
					__compiletime_assert_424();
			} while (0);
			(*(const volatile typeof(_Generic(
				(h->first),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (h->first)))
				   *)&(h->first));
		}) &
		~1UL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_bl_add_head(struct hlist_bl_node *n, struct hlist_bl_head *h)
{
	struct hlist_bl_node *first = hlist_bl_first(h);

	n->next = first;
	if (first)
		first->pprev = &n->next;
	n->pprev = &h->first;
	hlist_bl_set_first(h, n);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_bl_add_before(struct hlist_bl_node *n, struct hlist_bl_node *next)
{
	struct hlist_bl_node **pprev = next->pprev;

	n->pprev = pprev;
	n->next = next;
	next->pprev = &n->next;

	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_425(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*pprev) == sizeof(char) ||
			       sizeof(*pprev) == sizeof(short) ||
			       sizeof(*pprev) == sizeof(int) ||
			       sizeof(*pprev) == sizeof(long)) ||
			      sizeof(*pprev) == sizeof(long long)))
				__compiletime_assert_425();
		} while (0);
		do {
			*(volatile typeof(*pprev) *)&(*pprev) =
				((struct hlist_bl_node *)((uintptr_t)n |
							  ((uintptr_t)*pprev &
							   1UL)));
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_bl_add_behind(struct hlist_bl_node *n, struct hlist_bl_node *prev)
{
	n->next = prev->next;
	n->pprev = &prev->next;
	prev->next = n;

	if (n->next)
		n->next->pprev = &n->next;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__hlist_bl_del(struct hlist_bl_node *n)
{
	struct hlist_bl_node *next = n->next;
	struct hlist_bl_node **pprev = n->pprev;

	;

	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_426(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*pprev) == sizeof(char) ||
			       sizeof(*pprev) == sizeof(short) ||
			       sizeof(*pprev) == sizeof(int) ||
			       sizeof(*pprev) == sizeof(long)) ||
			      sizeof(*pprev) == sizeof(long long)))
				__compiletime_assert_426();
		} while (0);
		do {
			*(volatile typeof(*pprev) *)&(*pprev) = ((
				struct hlist_bl_node *)((unsigned long)next |
							((unsigned long)*pprev &
							 1UL)));
		} while (0);
	} while (0);

	if (next)
		next->pprev = pprev;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_bl_del(struct hlist_bl_node *n)
{
	__hlist_bl_del(n);
	n->next = ((void *)0x100 + (0xdead000000000000UL));
	n->pprev = ((void *)0x122 + (0xdead000000000000UL));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_bl_del_init(struct hlist_bl_node *n)
{
	if (!hlist_bl_unhashed(n)) {
		__hlist_bl_del(n);
		INIT_HLIST_BL_NODE(n);
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_bl_lock(struct hlist_bl_head *b)
{
	bit_spin_lock(0, (unsigned long *)b);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_bl_unlock(struct hlist_bl_head *b)
{
	__bit_spin_unlock(0, (unsigned long *)b);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
hlist_bl_is_locked(struct hlist_bl_head *b)
{
	return bit_spin_is_locked(0, (unsigned long *)b);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_bl_set_first_rcu(struct hlist_bl_head *h, struct hlist_bl_node *n)
{
	;

	;
	do {
		uintptr_t _r_a_p__v = (uintptr_t)((
			struct hlist_bl_node *)((unsigned long)n | 1UL));
		;
		if (__builtin_constant_p(
			    (struct hlist_bl_node *)((unsigned long)n | 1UL)) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_427(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof((h->first)) ==
						       sizeof(char) ||
					       sizeof((h->first)) ==
						       sizeof(short) ||
					       sizeof((h->first)) ==
						       sizeof(int) ||
					       sizeof((h->first)) ==
						       sizeof(long)) ||
					      sizeof((h->first)) ==
						      sizeof(long long)))
						__compiletime_assert_427();
				} while (0);
				do {
					*(volatile typeof((h->first)) *)&(
						(h->first)) =
						((typeof(h->first))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_428(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&h->first) ==
							       sizeof(char) ||
						       sizeof(*&h->first) ==
							       sizeof(short) ||
						       sizeof(*&h->first) ==
							       sizeof(int) ||
						       sizeof(*&h->first) ==
							       sizeof(long))))
							__compiletime_assert_428();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_429(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&h->first) ==
								       sizeof(char) ||
							       sizeof(*&h->first) ==
								       sizeof(short) ||
							       sizeof(*&h->first) ==
								       sizeof(int) ||
							       sizeof(*&h->first) ==
								       sizeof(long)) ||
							      sizeof(*&h->first) ==
								      sizeof(long long)))
								__compiletime_assert_429();
						} while (0);
						do {
							*(volatile typeof(*&h->first)
								  *)&(*&h->first) =
								((typeof(*(
									(typeof(h->first))
										_r_a_p__v))
									  *)((
									typeof(h->first))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct hlist_bl_node *
hlist_bl_first_rcu(struct hlist_bl_head *h)
{
	return (struct hlist_bl_node
			*)((unsigned long)({
				   typeof(*(h->first)) *__UNIQUE_ID_rcu430 = (typeof(*(
					   h->first)) *)({
					   do {
						   __attribute__((
							   __noreturn__)) extern void
						   __compiletime_assert_431(
							   void)
							   __attribute__((__error__(
								   "Unsupported access size for {READ,WRITE}_ONCE().")));
						   if (!((sizeof((h->first)) ==
								  sizeof(char) ||
							  sizeof((h->first)) ==
								  sizeof(short) ||
							  sizeof((h->first)) ==
								  sizeof(int) ||
							  sizeof((h->first)) ==
								  sizeof(long)) ||
							 sizeof((h->first)) ==
								 sizeof(long long)))
							   __compiletime_assert_431();
					   } while (0);
					   (*(const volatile typeof(_Generic(
						   ((h->first)),
									    char: (char)0,
									    unsigned char: (
										    unsigned char)0,
									    signed char: (
										    signed char)0,
									    unsigned short: (
										    unsigned short)0,
									    signed short: (
										    signed short)0,
									    unsigned int: (
										    unsigned int)0,
									    signed int: (
										    signed int)0,
									    unsigned long: (
										    unsigned long)0,
									    signed long: (
										    signed long)0,
									    unsigned long long: (
										    unsigned long long)0,
									    signed long long: (
										    signed long long)0,
									    default: ((
										    h->first))))
						      *)&((h->first)));
				   });
				   do {
				   } while (0 && (!((hlist_bl_is_locked(h)) ||
						    rcu_read_lock_held())));
				   ;
				   ((typeof(*(h->first)) *)(__UNIQUE_ID_rcu430));
			   }) &
			   ~1UL);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_bl_del_rcu(struct hlist_bl_node *n)
{
	__hlist_bl_del(n);
	n->pprev = ((void *)0x122 + (0xdead000000000000UL));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hlist_bl_add_head_rcu(struct hlist_bl_node *n, struct hlist_bl_head *h)
{
	struct hlist_bl_node *first;

	first = hlist_bl_first(h);

	n->next = first;
	if (first)
		first->pprev = &n->next;
	n->pprev = &h->first;

	hlist_bl_set_first_rcu(h, n);
}

struct lockref {
	union {
		__u64 __attribute__((aligned(8))) lock_count;

		struct {
			spinlock_t lock;
			int count;
		};
	};
};

extern void lockref_get(struct lockref *);
extern int lockref_put_return(struct lockref *);
extern int lockref_get_not_zero(struct lockref *);
extern int lockref_put_not_zero(struct lockref *);
extern int lockref_put_or_lock(struct lockref *);

extern void lockref_mark_dead(struct lockref *);
extern int lockref_get_not_dead(struct lockref *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__lockref_is_dead(const struct lockref *l)
{
	return ((int)l->count < 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
partial_name_hash(unsigned long c, unsigned long prevhash)
{
	return (prevhash + (c << 4) + (c >> 4)) * 11;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
end_name_hash(unsigned long hash)
{
	return hash_64_generic(hash, 32);
}
extern unsigned int __attribute__((__pure__))
full_name_hash(const void *salt, const char *, unsigned int);
extern u64 __attribute__((__pure__)) hashlen_string(const void *salt,
						    const char *name);

struct path;
struct file;
struct vfsmount;
struct qstr {
	union {
		struct {
			u32 hash;
			u32 len;
		};
		u64 hash_len;
	};
	const unsigned char *name;
};

extern const struct qstr empty_name;
extern const struct qstr slash_name;
extern const struct qstr dotdot_name;
struct dentry {
	unsigned int d_flags;
	seqcount_spinlock_t d_seq;
	struct hlist_bl_node d_hash;
	struct dentry *d_parent;
	struct qstr d_name;
	struct inode *d_inode;

	unsigned char d_iname[40];

	const struct dentry_operations *d_op;
	struct super_block *d_sb;
	unsigned long d_time;
	void *d_fsdata;

	struct lockref d_lockref;

	union {
		struct list_head d_lru;
		wait_queue_head_t *d_wait;
	};
	struct hlist_node d_sib;
	struct hlist_head d_children;

	union {
		struct hlist_node d_alias;
		struct hlist_bl_node d_in_lookup_hash;
		struct callback_head d_rcu;
	} d_u;
};

enum dentry_d_lock_class { DENTRY_D_LOCK_NORMAL, DENTRY_D_LOCK_NESTED };

enum d_real_type {
	D_REAL_DATA,
	D_REAL_METADATA,
};

struct dentry_operations {
	int (*d_revalidate)(struct dentry *, unsigned int);
	int (*d_weak_revalidate)(struct dentry *, unsigned int);
	int (*d_hash)(const struct dentry *, struct qstr *);
	int (*d_compare)(const struct dentry *, unsigned int, const char *,
			 const struct qstr *);
	int (*d_delete)(const struct dentry *);
	int (*d_init)(struct dentry *);
	void (*d_release)(struct dentry *);
	void (*d_prune)(struct dentry *);
	void (*d_iput)(struct dentry *, struct inode *);
	char *(*d_dname)(struct dentry *, char *, int);
	struct vfsmount *(*d_automount)(struct path *);
	int (*d_manage)(const struct path *, bool);
	struct dentry *(*d_real)(struct dentry *, enum d_real_type type);
} __attribute__((__aligned__((1 << (6)))));
extern seqlock_t rename_lock;

extern void d_instantiate(struct dentry *, struct inode *);
extern void d_instantiate_new(struct dentry *, struct inode *);
extern void __d_drop(struct dentry *dentry);
extern void d_drop(struct dentry *dentry);
extern void d_delete(struct dentry *);
extern void d_set_d_op(struct dentry *dentry,
		       const struct dentry_operations *op);

extern struct dentry *d_alloc(struct dentry *, const struct qstr *);
extern struct dentry *d_alloc_anon(struct super_block *);
extern struct dentry *d_alloc_parallel(struct dentry *, const struct qstr *,
				       wait_queue_head_t *);
extern struct dentry *d_splice_alias(struct inode *, struct dentry *);
extern struct dentry *d_add_ci(struct dentry *, struct inode *, struct qstr *);
extern bool d_same_name(const struct dentry *dentry,
			const struct dentry *parent, const struct qstr *name);
extern struct dentry *d_exact_alias(struct dentry *, struct inode *);
extern struct dentry *d_find_any_alias(struct inode *inode);
extern struct dentry *d_obtain_alias(struct inode *);
extern struct dentry *d_obtain_root(struct inode *);
extern void shrink_dcache_sb(struct super_block *);
extern void shrink_dcache_parent(struct dentry *);
extern void d_invalidate(struct dentry *);

extern struct dentry *d_make_root(struct inode *);

extern void d_mark_tmpfile(struct file *, struct inode *);
extern void d_tmpfile(struct file *, struct inode *);

extern struct dentry *d_find_alias(struct inode *);
extern void d_prune_aliases(struct inode *);

extern struct dentry *d_find_alias_rcu(struct inode *);

extern int path_has_submounts(const struct path *);

extern void d_rehash(struct dentry *);

extern void d_add(struct dentry *, struct inode *);

extern void d_move(struct dentry *, struct dentry *);
extern void d_exchange(struct dentry *, struct dentry *);
extern struct dentry *d_ancestor(struct dentry *, struct dentry *);

extern struct dentry *d_lookup(const struct dentry *, const struct qstr *);
extern struct dentry *d_hash_and_lookup(struct dentry *, struct qstr *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned
d_count(const struct dentry *dentry)
{
	return dentry->d_lockref.count;
}

ino_t d_parent_ino(struct dentry *dentry);

extern __attribute__((__format__(printf, 3, 4))) char *
dynamic_dname(char *, int, const char *, ...);

extern char *__d_path(const struct path *, const struct path *, char *, int);
extern char *d_absolute_path(const struct path *, char *, int);
extern char *d_path(const struct path *, char *, int);
extern char *dentry_path_raw(const struct dentry *, char *, int);
extern char *dentry_path(const struct dentry *, char *, int);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct dentry *
dget_dlock(struct dentry *dentry)
{
	dentry->d_lockref.count++;
	return dentry;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct dentry *
dget(struct dentry *dentry)
{
	if (dentry)
		lockref_get(&dentry->d_lockref);
	return dentry;
}

extern struct dentry *dget_parent(struct dentry *dentry);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
d_unhashed(const struct dentry *dentry)
{
	return hlist_bl_unhashed(&dentry->d_hash);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
d_unlinked(const struct dentry *dentry)
{
	return d_unhashed(dentry) && !((dentry) == (dentry)->d_parent);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
cant_mount(const struct dentry *dentry)
{
	return (dentry->d_flags & ((((1UL))) << (8)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
dont_mount(struct dentry *dentry)
{
	spin_lock(&dentry->d_lockref.lock);
	dentry->d_flags |= ((((1UL))) << (8));
	spin_unlock(&dentry->d_lockref.lock);
}

extern void __d_lookup_unhash_wake(struct dentry *dentry);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
d_in_lookup(const struct dentry *dentry)
{
	return dentry->d_flags & ((((1UL))) << (28));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
d_lookup_done(struct dentry *dentry)
{
	if (__builtin_expect(!!(d_in_lookup(dentry)), 0))
		__d_lookup_unhash_wake(dentry);
}

extern void dput(struct dentry *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_managed(const struct dentry *dentry)
{
	return dentry->d_flags & (((((1UL))) << (16)) | ((((1UL))) << (17)) |
				  ((((1UL))) << (18)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_mountpoint(const struct dentry *dentry)
{
	return dentry->d_flags & ((((1UL))) << (16));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned
__d_entry_type(const struct dentry *dentry)
{
	return dentry->d_flags & (7 << 20);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_is_miss(const struct dentry *dentry)
{
	return __d_entry_type(dentry) == (0 << 20);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_is_whiteout(const struct dentry *dentry)
{
	return __d_entry_type(dentry) == (1 << 20);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_can_lookup(const struct dentry *dentry)
{
	return __d_entry_type(dentry) == (2 << 20);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_is_autodir(const struct dentry *dentry)
{
	return __d_entry_type(dentry) == (3 << 20);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_is_dir(const struct dentry *dentry)
{
	return d_can_lookup(dentry) || d_is_autodir(dentry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_is_symlink(const struct dentry *dentry)
{
	return __d_entry_type(dentry) == (6 << 20);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_is_reg(const struct dentry *dentry)
{
	return __d_entry_type(dentry) == (4 << 20);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_is_special(const struct dentry *dentry)
{
	return __d_entry_type(dentry) == (5 << 20);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_is_file(const struct dentry *dentry)
{
	return d_is_reg(dentry) || d_is_special(dentry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_is_negative(const struct dentry *dentry)
{
	return d_is_miss(dentry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_flags_negative(unsigned flags)
{
	return (flags & (7 << 20)) == (0 << 20);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_is_positive(const struct dentry *dentry)
{
	return !d_is_negative(dentry);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_really_is_negative(const struct dentry *dentry)
{
	return dentry->d_inode == ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
d_really_is_positive(const struct dentry *dentry)
{
	return dentry->d_inode != ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
simple_positive(const struct dentry *dentry)
{
	return d_really_is_positive(dentry) && !d_unhashed(dentry);
}

extern int sysctl_vfs_cache_pressure;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
vfs_pressure_ratio(unsigned long val)
{
	return ({
		typeof(val) x_ = (val);
		typeof(sysctl_vfs_cache_pressure) n_ =
			(sysctl_vfs_cache_pressure);
		typeof(100) d_ = (100);
		typeof(x_) q = x_ / d_;
		typeof(x_) r = x_ % d_;
		q *n_ + r *n_ / d_;
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct inode *
d_inode(const struct dentry *dentry)
{
	return dentry->d_inode;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct inode *
d_inode_rcu(const struct dentry *dentry)
{
	return ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_432(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(dentry->d_inode) == sizeof(char) ||
			       sizeof(dentry->d_inode) == sizeof(short) ||
			       sizeof(dentry->d_inode) == sizeof(int) ||
			       sizeof(dentry->d_inode) == sizeof(long)) ||
			      sizeof(dentry->d_inode) == sizeof(long long)))
				__compiletime_assert_432();
		} while (0);
		(*(const volatile typeof(_Generic((dentry->d_inode),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (dentry->d_inode)))
			   *)&(dentry->d_inode));
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct inode *
d_backing_inode(const struct dentry *upper)
{
	struct inode *inode = upper->d_inode;

	return inode;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct dentry *
d_real(struct dentry *dentry, enum d_real_type type)
{
	if (__builtin_expect(!!(dentry->d_flags & ((((1UL))) << (26))), 0))
		return dentry->d_op->d_real(dentry, type);
	else
		return dentry;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct inode *
d_real_inode(const struct dentry *dentry)
{
	return d_inode(d_real((struct dentry *)dentry, D_REAL_DATA));
}

struct name_snapshot {
	struct qstr name;
	unsigned char inline_name[40];
};
void take_dentry_name_snapshot(struct name_snapshot *, struct dentry *);
void release_dentry_name_snapshot(struct name_snapshot *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct dentry *
d_first_child(const struct dentry *dentry)
{
	return ({
		typeof(dentry->d_children.first) ____ptr =
			(dentry->d_children.first);
		____ptr ? ({
			void *__mptr = (void *)(____ptr);
			_Static_assert(
				__builtin_types_compatible_p(
					typeof(*(____ptr)),
					typeof(((struct dentry *)0)->d_sib)) ||
					__builtin_types_compatible_p(
						typeof(*(____ptr)),
						typeof(void)),
				"pointer type mismatch in container_of()");
			((struct dentry *)(__mptr -
					   __builtin_offsetof(struct dentry,
							      d_sib)));
		}) :
			  ((void *)0);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct dentry *
d_next_sibling(const struct dentry *dentry)
{
	return ({
		typeof(dentry->d_sib.next) ____ptr = (dentry->d_sib.next);
		____ptr ? ({
			void *__mptr = (void *)(____ptr);
			_Static_assert(
				__builtin_types_compatible_p(
					typeof(*(____ptr)),
					typeof(((struct dentry *)0)->d_sib)) ||
					__builtin_types_compatible_p(
						typeof(*(____ptr)),
						typeof(void)),
				"pointer type mismatch in container_of()");
			((struct dentry *)(__mptr -
					   __builtin_offsetof(struct dentry,
							      d_sib)));
		}) :
			  ((void *)0);
	});
}

struct dentry;
struct vfsmount;

struct path {
	struct vfsmount *mnt;
	struct dentry *dentry;
};

extern void path_get(const struct path *);
extern void path_put(const struct path *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
path_equal(const struct path *path1, const struct path *path2)
{
	return path1->mnt == path2->mnt && path1->dentry == path2->dentry;
}

struct shrinker_info_unit {
	atomic_long_t nr_deferred[64];
	unsigned long
		map[(((64) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];
};

struct shrinker_info {
	struct callback_head rcu;
	int map_nr_max;
	struct shrinker_info_unit *unit[];
};
struct shrink_control {
	gfp_t gfp_mask;

	int nid;

	unsigned long nr_to_scan;

	unsigned long nr_scanned;

	struct mem_cgroup *memcg;
};
struct shrinker {
	unsigned long (*count_objects)(struct shrinker *,
				       struct shrink_control *sc);
	unsigned long (*scan_objects)(struct shrinker *,
				      struct shrink_control *sc);

	long batch;
	int seeks;
	unsigned flags;
	refcount_t refcount;
	struct completion done;
	struct callback_head rcu;

	void *private_data;

	struct list_head list;
	atomic_long_t *nr_deferred;
};
__attribute__((__format__(printf, 2, 3))) struct shrinker *
shrinker_alloc(unsigned int flags, const char *fmt, ...);
void shrinker_register(struct shrinker *shrinker);
void shrinker_free(struct shrinker *shrinker);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
shrinker_try_get(struct shrinker *shrinker)
{
	return refcount_inc_not_zero(&shrinker->refcount);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
shrinker_put(struct shrinker *shrinker)
{
	if (refcount_dec_and_test(&shrinker->refcount))
		complete(&shrinker->done);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__format__(printf, 2, 3))) int
shrinker_debugfs_rename(struct shrinker *shrinker, const char *fmt, ...)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
iret_to_self(void)
{
	unsigned int tmp;

	asm volatile("mov %%ss, %0\n\t"
		     "pushq %q0\n\t"
		     "pushq %%rsp\n\t"
		     "addq $8, (%%rsp)\n\t"
		     "pushfq\n\t"
		     "mov %%cs, %0\n\t"
		     "pushq %q0\n\t"
		     "pushq $1f\n\t"
		     "iretq\n\t"
		     "1:"
		     : "=&r"(tmp), "+r"(current_stack_pointer)
		     :
		     : "cc", "memory");
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sync_core(void)
{
	if ((__builtin_constant_p((
		     __builtin_constant_p((18 * 32 + 14)) &&
				     (((((18 * 32 + 14)) >> 5) == (0) &&
				       (1UL << (((18 * 32 + 14)) & 31) &
					((1 << ((0 * 32 + 0) & 31)) |
					 (1 << ((0 * 32 + 3)) & 31) |
					 (1 << ((0 * 32 + 5) & 31)) |
					 (1 << ((0 * 32 + 6) & 31)) |
					 (1 << ((0 * 32 + 8) & 31)) |
					 (1 << ((0 * 32 + 13)) & 31) |
					 (1 << ((0 * 32 + 24) & 31)) |
					 (1 << ((0 * 32 + 15) & 31)) |
					 (1 << ((0 * 32 + 25) & 31)) |
					 (1 << ((0 * 32 + 26) & 31))))) ||
				      ((((18 * 32 + 14)) >> 5) == (1) &&
				       (1UL << (((18 * 32 + 14)) & 31) &
					((1 << ((1 * 32 + 29) & 31)) | 0))) ||
				      ((((18 * 32 + 14)) >> 5) == (2) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (3) &&
				       (1UL << (((18 * 32 + 14)) & 31) &
					((1 << ((3 * 32 + 20) & 31))))) ||
				      ((((18 * 32 + 14)) >> 5) == (4) &&
				       (1UL << (((18 * 32 + 14)) & 31) & (0))) ||
				      ((((18 * 32 + 14)) >> 5) == (5) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (6) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (7) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (8) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (9) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (10) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (11) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (12) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (13) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (14) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (15) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (16) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (17) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (18) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (19) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (20) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((((18 * 32 + 14)) >> 5) == (21) &&
				       (1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				      ((int)(sizeof(struct {
					      int : (-!!(22 != 22));
				      }))) ||
				      ((int)(sizeof(struct {
					      int : (-!!(22 != 22));
				      })))) ?
			     1 :
			     arch_test_bit(
				     (18 * 32 + 14),
				     (unsigned long
					      *)((&boot_cpu_data)
							 ->x86_capability)))) ?
		     (__builtin_constant_p((18 * 32 + 14)) &&
				      (((((18 * 32 + 14)) >> 5) == (0) &&
					(1UL << (((18 * 32 + 14)) & 31) &
					 ((1 << ((0 * 32 + 0) & 31)) |
					  (1 << ((0 * 32 + 3)) & 31) |
					  (1 << ((0 * 32 + 5) & 31)) |
					  (1 << ((0 * 32 + 6) & 31)) |
					  (1 << ((0 * 32 + 8) & 31)) |
					  (1 << ((0 * 32 + 13)) & 31) |
					  (1 << ((0 * 32 + 24) & 31)) |
					  (1 << ((0 * 32 + 15) & 31)) |
					  (1 << ((0 * 32 + 25) & 31)) |
					  (1 << ((0 * 32 + 26) & 31))))) ||
				       ((((18 * 32 + 14)) >> 5) == (1) &&
					(1UL << (((18 * 32 + 14)) & 31) &
					 ((1 << ((1 * 32 + 29) & 31)) | 0))) ||
				       ((((18 * 32 + 14)) >> 5) == (2) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (3) &&
					(1UL << (((18 * 32 + 14)) & 31) &
					 ((1 << ((3 * 32 + 20) & 31))))) ||
				       ((((18 * 32 + 14)) >> 5) == (4) &&
					(1UL << (((18 * 32 + 14)) & 31) &
					 (0))) ||
				       ((((18 * 32 + 14)) >> 5) == (5) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (6) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (7) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (8) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (9) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (10) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (11) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (12) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (13) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (14) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (15) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (16) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (17) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (18) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (19) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (20) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((((18 * 32 + 14)) >> 5) == (21) &&
					(1UL << (((18 * 32 + 14)) & 31) & 0)) ||
				       ((int)(sizeof(struct {
					       int : (-!!(22 != 22));
				       }))) ||
				       ((int)(sizeof(struct {
					       int : (-!!(22 != 22));
				       })))) ?
			      1 :
			      arch_test_bit(
				      (18 * 32 + 14),
				      (unsigned long
					       *)((&boot_cpu_data)
							  ->x86_capability))) :
		     _static_cpu_has((18 * 32 + 14)))) {
		serialize();
		return;
	}
	iret_to_self();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sync_core_before_usermode(void)
{
	if ((__builtin_constant_p((
		     __builtin_constant_p((7 * 32 + 11)) &&
				     (((((7 * 32 + 11)) >> 5) == (0) &&
				       (1UL << (((7 * 32 + 11)) & 31) &
					((1 << ((0 * 32 + 0) & 31)) |
					 (1 << ((0 * 32 + 3)) & 31) |
					 (1 << ((0 * 32 + 5) & 31)) |
					 (1 << ((0 * 32 + 6) & 31)) |
					 (1 << ((0 * 32 + 8) & 31)) |
					 (1 << ((0 * 32 + 13)) & 31) |
					 (1 << ((0 * 32 + 24) & 31)) |
					 (1 << ((0 * 32 + 15) & 31)) |
					 (1 << ((0 * 32 + 25) & 31)) |
					 (1 << ((0 * 32 + 26) & 31))))) ||
				      ((((7 * 32 + 11)) >> 5) == (1) &&
				       (1UL << (((7 * 32 + 11)) & 31) &
					((1 << ((1 * 32 + 29) & 31)) | 0))) ||
				      ((((7 * 32 + 11)) >> 5) == (2) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (3) &&
				       (1UL << (((7 * 32 + 11)) & 31) &
					((1 << ((3 * 32 + 20) & 31))))) ||
				      ((((7 * 32 + 11)) >> 5) == (4) &&
				       (1UL << (((7 * 32 + 11)) & 31) & (0))) ||
				      ((((7 * 32 + 11)) >> 5) == (5) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (6) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (7) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (8) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (9) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (10) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (11) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (12) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (13) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (14) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (15) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (16) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (17) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (18) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (19) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (20) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((((7 * 32 + 11)) >> 5) == (21) &&
				       (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				      ((int)(sizeof(struct {
					      int : (-!!(22 != 22));
				      }))) ||
				      ((int)(sizeof(struct {
					      int : (-!!(22 != 22));
				      })))) ?
			     1 :
			     arch_test_bit(
				     (7 * 32 + 11),
				     (unsigned long
					      *)((&boot_cpu_data)
							 ->x86_capability)))) ?
		     (__builtin_constant_p((7 * 32 + 11)) &&
				      (((((7 * 32 + 11)) >> 5) == (0) &&
					(1UL << (((7 * 32 + 11)) & 31) &
					 ((1 << ((0 * 32 + 0) & 31)) |
					  (1 << ((0 * 32 + 3)) & 31) |
					  (1 << ((0 * 32 + 5) & 31)) |
					  (1 << ((0 * 32 + 6) & 31)) |
					  (1 << ((0 * 32 + 8) & 31)) |
					  (1 << ((0 * 32 + 13)) & 31) |
					  (1 << ((0 * 32 + 24) & 31)) |
					  (1 << ((0 * 32 + 15) & 31)) |
					  (1 << ((0 * 32 + 25) & 31)) |
					  (1 << ((0 * 32 + 26) & 31))))) ||
				       ((((7 * 32 + 11)) >> 5) == (1) &&
					(1UL << (((7 * 32 + 11)) & 31) &
					 ((1 << ((1 * 32 + 29) & 31)) | 0))) ||
				       ((((7 * 32 + 11)) >> 5) == (2) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (3) &&
					(1UL << (((7 * 32 + 11)) & 31) &
					 ((1 << ((3 * 32 + 20) & 31))))) ||
				       ((((7 * 32 + 11)) >> 5) == (4) &&
					(1UL << (((7 * 32 + 11)) & 31) & (0))) ||
				       ((((7 * 32 + 11)) >> 5) == (5) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (6) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (7) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (8) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (9) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (10) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (11) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (12) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (13) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (14) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (15) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (16) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (17) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (18) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (19) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (20) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (21) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((int)(sizeof(struct {
					       int : (-!!(22 != 22));
				       }))) ||
				       ((int)(sizeof(struct {
					       int : (-!!(22 != 22));
				       })))) ?
			      1 :
			      arch_test_bit(
				      (7 * 32 + 11),
				      (unsigned long
					       *)((&boot_cpu_data)
							  ->x86_capability))) :
		     _static_cpu_has((7 * 32 + 11))))
		return;

	sync_core();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
prepare_sync_core_cmd(struct mm_struct *mm)
{
}
extern void set_dumpable(struct mm_struct *mm, int value);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__get_dumpable(unsigned long mm_flags)
{
	return mm_flags & ((1 << 2) - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_dumpable(struct mm_struct *mm)
{
	return __get_dumpable(mm->flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
mmf_init_flags(unsigned long flags)
{
	if (flags & (1UL << 29))
		flags &= ~((1UL << 28) | (1UL << 29));
	return flags & (((1 << 2) - 1) | (((1 << 9) - 1) << 2) | (1 << 24) |
			(1 << 28) | (1 << 30) | (1 << 31));
}

extern struct mm_struct *mm_alloc(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmgrab(struct mm_struct *mm)
{
	atomic_inc(&mm->mm_count);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
smp_mb__after_mmgrab(void)
{
	do {
		do {
		} while (0);
		do {
		} while (0);
	} while (0);
}

extern void __mmdrop(struct mm_struct *mm);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmdrop(struct mm_struct *mm)
{
	if (__builtin_expect(!!(atomic_dec_and_test(&mm->mm_count)), 0))
		__mmdrop(mm);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmdrop_sched(struct mm_struct *mm)
{
	mmdrop(mm);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmgrab_lazy_tlb(struct mm_struct *mm)
{
	if (1)
		mmgrab(mm);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmdrop_lazy_tlb(struct mm_struct *mm)
{
	if (1) {
		mmdrop(mm);
	} else {
		do {
			do {
			} while (0);
			asm volatile("lock; addl $0,-4(%%"
				     "rsp"
				     ")" ::
					     : "memory", "cc");
		} while (0);
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmdrop_lazy_tlb_sched(struct mm_struct *mm)
{
	if (1)
		mmdrop_sched(mm);
	else
		do {
			do {
			} while (0);
			asm volatile("lock; addl $0,-4(%%"
				     "rsp"
				     ")" ::
					     : "memory", "cc");
		} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmget(struct mm_struct *mm)
{
	atomic_inc(&mm->mm_users);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mmget_not_zero(struct mm_struct *mm)
{
	return atomic_inc_not_zero(&mm->mm_users);
}

extern void mmput(struct mm_struct *);

void mmput_async(struct mm_struct *);

extern struct mm_struct *get_task_mm(struct task_struct *task);

extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);

extern void exit_mm_release(struct task_struct *, struct mm_struct *);

extern void exec_mm_release(struct task_struct *, struct mm_struct *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mm_update_next_owner(struct mm_struct *mm)
{
}
extern void arch_pick_mmap_layout(struct mm_struct *mm,
				  struct rlimit *rlim_stack);

unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr,
				     unsigned long len, unsigned long pgoff,
				     unsigned long flags, vm_flags_t vm_flags);
unsigned long arch_get_unmapped_area_topdown(struct file *filp,
					     unsigned long addr,
					     unsigned long len,
					     unsigned long pgoff,
					     unsigned long flags, vm_flags_t);

unsigned long mm_get_unmapped_area(struct mm_struct *mm, struct file *filp,
				   unsigned long addr, unsigned long len,
				   unsigned long pgoff, unsigned long flags);

unsigned long
mm_get_unmapped_area_vmflags(struct mm_struct *mm, struct file *filp,
			     unsigned long addr, unsigned long len,
			     unsigned long pgoff, unsigned long flags,
			     vm_flags_t vm_flags);

unsigned long generic_get_unmapped_area(struct file *filp, unsigned long addr,
					unsigned long len, unsigned long pgoff,
					unsigned long flags,
					vm_flags_t vm_flags);
unsigned long
generic_get_unmapped_area_topdown(struct file *filp, unsigned long addr,
				  unsigned long len, unsigned long pgoff,
				  unsigned long flags, vm_flags_t vm_flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
in_vfork(struct task_struct *tsk)
{
	bool ret;
	rcu_read_lock();
	ret = tsk->vfork_done &&
	      ({
		      typeof(*(tsk->real_parent)) *__UNIQUE_ID_rcu433 = (typeof(*(
			      tsk->real_parent)) *)({
			      do {
				      __attribute__((__noreturn__)) extern void
				      __compiletime_assert_434(void) __attribute__((__error__(
					      "Unsupported access size for {READ,WRITE}_ONCE().")));
				      if (!((sizeof((tsk->real_parent)) ==
						     sizeof(char) ||
					     sizeof((tsk->real_parent)) ==
						     sizeof(short) ||
					     sizeof((tsk->real_parent)) ==
						     sizeof(int) ||
					     sizeof((tsk->real_parent)) ==
						     sizeof(long)) ||
					    sizeof((tsk->real_parent)) ==
						    sizeof(long long)))
					      __compiletime_assert_434();
			      } while (0);
			      (*(const volatile typeof(_Generic(
				      ((tsk->real_parent)),
							       char: (char)0,
							       unsigned char: (
								       unsigned char)0,
							       signed char: (
								       signed char)0,
							       unsigned short: (
								       unsigned short)0,
							       signed short: (
								       signed short)0,
							       unsigned int: (
								       unsigned int)0,
							       signed int: (
								       signed int)0,
							       unsigned long: (
								       unsigned long)0,
							       signed long: (
								       signed long)0,
							       unsigned long long: (
								       unsigned long long)0,
							       signed long long: (
								       signed long long)0,
							       default: ((
								       tsk->real_parent))))
					 *)&((tsk->real_parent)));
		      });
		      do {
		      } while (0 && (!((0) || rcu_read_lock_held())));
		      ;
		      ((typeof(*(tsk->real_parent)) *)(__UNIQUE_ID_rcu433));
	      })->mm == tsk->mm;
	rcu_read_unlock();

	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) gfp_t
current_gfp_context(gfp_t flags)
{
	unsigned int pflags = ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_435(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(get_current()->flags) == sizeof(char) ||
			       sizeof(get_current()->flags) == sizeof(short) ||
			       sizeof(get_current()->flags) == sizeof(int) ||
			       sizeof(get_current()->flags) == sizeof(long)) ||
			      sizeof(get_current()->flags) ==
				      sizeof(long long)))
				__compiletime_assert_435();
		} while (0);
		(*(const volatile typeof(_Generic((get_current()->flags),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (
							  get_current()->flags)))
			   *)&(get_current()->flags));
	});

	if (__builtin_expect(
		    !!(pflags & (0x00080000 | 0x00040000 | 0x10000000)), 0)) {
		if (pflags & 0x00080000)
			flags &= ~(((gfp_t)((((1UL))) << (___GFP_IO_BIT))) |
				   ((gfp_t)((((1UL))) << (___GFP_FS_BIT))));
		else if (pflags & 0x00040000)
			flags &= ~((gfp_t)((((1UL))) << (___GFP_FS_BIT)));

		if (pflags & 0x10000000)
			flags &= ~((gfp_t)((((1UL))) << (___GFP_MOVABLE_BIT)));
	}
	return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__fs_reclaim_acquire(unsigned long ip)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__fs_reclaim_release(unsigned long ip)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
fs_reclaim_acquire(gfp_t gfp_mask)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
fs_reclaim_release(gfp_t gfp_mask)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
memalloc_retry_wait(gfp_t gfp_flags)
{
	do {
		do {
		} while (0);
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_436(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(get_current()->__state) ==
					       sizeof(char) ||
				       sizeof(get_current()->__state) ==
					       sizeof(short) ||
				       sizeof(get_current()->__state) ==
					       sizeof(int) ||
				       sizeof(get_current()->__state) ==
					       sizeof(long)) ||
				      sizeof(get_current()->__state) ==
					      sizeof(long long)))
					__compiletime_assert_436();
			} while (0);
			do {
				*(volatile typeof(get_current()->__state) *)&(
					get_current()->__state) =
					((0x00000002));
			} while (0);
		} while (0);
	} while (0);
	gfp_flags = current_gfp_context(gfp_flags);
	if (gfpflags_allow_blocking(gfp_flags) &&
	    !(gfp_flags & ((gfp_t)((((1UL))) << (___GFP_NORETRY_BIT)))))

		io_schedule_timeout(1);
	else

		io_schedule_timeout(1000 / 50);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
might_alloc(gfp_t gfp_mask)
{
	fs_reclaim_acquire(gfp_mask);
	fs_reclaim_release(gfp_mask);

	do {
		if (gfpflags_allow_blocking(gfp_mask))
			do {
				might_resched();
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned
memalloc_flags_save(unsigned flags)
{
	unsigned oldflags = ~get_current()->flags & flags;
	get_current()->flags |= flags;
	return oldflags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
memalloc_flags_restore(unsigned flags)
{
	get_current()->flags &= ~flags;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
memalloc_noio_save(void)
{
	return memalloc_flags_save(0x00080000);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
memalloc_noio_restore(unsigned int flags)
{
	memalloc_flags_restore(flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
memalloc_nofs_save(void)
{
	return memalloc_flags_save(0x00040000);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
memalloc_nofs_restore(unsigned int flags)
{
	memalloc_flags_restore(flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
memalloc_noreclaim_save(void)
{
	return memalloc_flags_save(0x00000800);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
memalloc_noreclaim_restore(unsigned int flags)
{
	memalloc_flags_restore(flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
memalloc_pin_save(void)
{
	return memalloc_flags_save(0x10000000);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
memalloc_pin_restore(unsigned int flags)
{
	memalloc_flags_restore(flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct mem_cgroup *
set_active_memcg(struct mem_cgroup *memcg)
{
	return ((void *)0);
}

enum {
	MEMBARRIER_STATE_PRIVATE_EXPEDITED_READY = (1U << 0),
	MEMBARRIER_STATE_PRIVATE_EXPEDITED = (1U << 1),
	MEMBARRIER_STATE_GLOBAL_EXPEDITED_READY = (1U << 2),
	MEMBARRIER_STATE_GLOBAL_EXPEDITED = (1U << 3),
	MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY = (1U << 4),
	MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE = (1U << 5),
	MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY = (1U << 6),
	MEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ = (1U << 7),
};

enum {
	MEMBARRIER_FLAG_SYNC_CORE = (1U << 0),
	MEMBARRIER_FLAG_RSEQ = (1U << 1),
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)
{
	if (get_current()->mm != mm)
		return;
	if (__builtin_expect(!!(!(atomic_read(&mm->membarrier_state) &
				  MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE)),
			     1))

		return;
	sync_core_before_usermode();
}

extern void membarrier_exec_mmap(struct mm_struct *mm);

extern void membarrier_update_current_mm(struct mm_struct *next_mm);

struct list_lru;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_mk_value(unsigned long v)
{
	({
		int __ret_warn_on = !!((long)v < 0);
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) | (((9) << 8));
				({
					asm volatile(
						"437"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"437"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(437));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/xarray.h"),
						  "i"(60), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"438"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"438"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(438));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	return (void *)((v << 1) | 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
xa_to_value(const void *entry)
{
	return (unsigned long)entry >> 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xa_is_value(const void *entry)
{
	return (unsigned long)entry & 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_tag_pointer(void *p, unsigned long tag)
{
	return (void *)((unsigned long)p | tag);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_untag_pointer(void *entry)
{
	return (void *)((unsigned long)entry & ~3UL);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
xa_pointer_tag(void *entry)
{
	return (unsigned long)entry & 3UL;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_mk_internal(unsigned long v)
{
	return (void *)((v << 2) | 2);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
xa_to_internal(const void *entry)
{
	return (unsigned long)entry >> 2;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xa_is_internal(const void *entry)
{
	return ((unsigned long)entry & 3) == 2;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xa_is_zero(const void *entry)
{
	return __builtin_expect(!!(entry == xa_mk_internal(257)), 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xa_is_err(const void *entry)
{
	return __builtin_expect(
		!!(xa_is_internal(entry) && entry >= xa_mk_internal(-4095)), 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
xa_err(void *entry)
{
	if (xa_is_err(entry))
		return (long)entry >> 2;
	return 0;
}
struct xa_limit {
	u32 max;
	u32 min;
};

typedef unsigned xa_mark_t;

enum xa_lock_type {
	XA_LOCK_IRQ = 1,
	XA_LOCK_BH = 2,
};
struct xarray {
	spinlock_t xa_lock;

	gfp_t xa_flags;
	void *xa_head;
};
void *xa_load(struct xarray *, unsigned long index);
void *xa_store(struct xarray *, unsigned long index, void *entry, gfp_t);
void *xa_erase(struct xarray *, unsigned long index);
void *xa_store_range(struct xarray *, unsigned long first, unsigned long last,
		     void *entry, gfp_t);
bool xa_get_mark(struct xarray *, unsigned long index, xa_mark_t);
void xa_set_mark(struct xarray *, unsigned long index, xa_mark_t);
void xa_clear_mark(struct xarray *, unsigned long index, xa_mark_t);
void *xa_find(struct xarray *xa, unsigned long *index, unsigned long max,
	      xa_mark_t) __attribute__((nonnull(2)));
void *xa_find_after(struct xarray *xa, unsigned long *index, unsigned long max,
		    xa_mark_t) __attribute__((nonnull(2)));
unsigned int xa_extract(struct xarray *, void **dst, unsigned long start,
			unsigned long max, unsigned int n, xa_mark_t);
void xa_destroy(struct xarray *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
xa_init_flags(struct xarray *xa, gfp_t flags)
{
	do {
		spinlock_check(&xa->xa_lock);
		*(&xa->xa_lock) =
			(spinlock_t){ { .rlock = {
						.raw_lock = { { .val = { (
									0) } } },
					} } };
	} while (0);
	xa->xa_flags = flags;
	xa->xa_head = ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
xa_init(struct xarray *xa)
{
	xa_init_flags(xa, 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xa_empty(const struct xarray *xa)
{
	return xa->xa_head == ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xa_marked(const struct xarray *xa, xa_mark_t mark)
{
	return xa->xa_flags &
	       ((gfp_t)((1U << ___GFP_LAST_BIT) << (unsigned)(mark)));
}
void *__xa_erase(struct xarray *, unsigned long index);
void *__xa_store(struct xarray *, unsigned long index, void *entry, gfp_t);
void *__xa_cmpxchg(struct xarray *, unsigned long index, void *old, void *entry,
		   gfp_t);
int __attribute__((__warn_unused_result__))
__xa_insert(struct xarray *, unsigned long index, void *entry, gfp_t);
int __attribute__((__warn_unused_result__))
__xa_alloc(struct xarray *, u32 *id, void *entry, struct xa_limit, gfp_t);
int __attribute__((__warn_unused_result__))
__xa_alloc_cyclic(struct xarray *, u32 *id, void *entry, struct xa_limit,
		  u32 *next, gfp_t);
void __xa_set_mark(struct xarray *, unsigned long index, xa_mark_t);
void __xa_clear_mark(struct xarray *, unsigned long index, xa_mark_t);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_store_bh(struct xarray *xa, unsigned long index, void *entry, gfp_t gfp)
{
	void *curr;

	might_alloc(gfp);
	spin_lock_bh(&(xa)->xa_lock);
	curr = __xa_store(xa, index, entry, gfp);
	spin_unlock_bh(&(xa)->xa_lock);

	return curr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_store_irq(struct xarray *xa, unsigned long index, void *entry, gfp_t gfp)
{
	void *curr;

	might_alloc(gfp);
	spin_lock_irq(&(xa)->xa_lock);
	curr = __xa_store(xa, index, entry, gfp);
	spin_unlock_irq(&(xa)->xa_lock);

	return curr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_erase_bh(struct xarray *xa, unsigned long index)
{
	void *entry;

	spin_lock_bh(&(xa)->xa_lock);
	entry = __xa_erase(xa, index);
	spin_unlock_bh(&(xa)->xa_lock);

	return entry;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_erase_irq(struct xarray *xa, unsigned long index)
{
	void *entry;

	spin_lock_irq(&(xa)->xa_lock);
	entry = __xa_erase(xa, index);
	spin_unlock_irq(&(xa)->xa_lock);

	return entry;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_cmpxchg(struct xarray *xa, unsigned long index, void *old, void *entry,
	   gfp_t gfp)
{
	void *curr;

	might_alloc(gfp);
	spin_lock(&(xa)->xa_lock);
	curr = __xa_cmpxchg(xa, index, old, entry, gfp);
	spin_unlock(&(xa)->xa_lock);

	return curr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_cmpxchg_bh(struct xarray *xa, unsigned long index, void *old, void *entry,
	      gfp_t gfp)
{
	void *curr;

	might_alloc(gfp);
	spin_lock_bh(&(xa)->xa_lock);
	curr = __xa_cmpxchg(xa, index, old, entry, gfp);
	spin_unlock_bh(&(xa)->xa_lock);

	return curr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_cmpxchg_irq(struct xarray *xa, unsigned long index, void *old, void *entry,
	       gfp_t gfp)
{
	void *curr;

	might_alloc(gfp);
	spin_lock_irq(&(xa)->xa_lock);
	curr = __xa_cmpxchg(xa, index, old, entry, gfp);
	spin_unlock_irq(&(xa)->xa_lock);

	return curr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	xa_insert(struct xarray *xa, unsigned long index, void *entry,
		  gfp_t gfp)
{
	int err;

	might_alloc(gfp);
	spin_lock(&(xa)->xa_lock);
	err = __xa_insert(xa, index, entry, gfp);
	spin_unlock(&(xa)->xa_lock);

	return err;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	xa_insert_bh(struct xarray *xa, unsigned long index, void *entry,
		     gfp_t gfp)
{
	int err;

	might_alloc(gfp);
	spin_lock_bh(&(xa)->xa_lock);
	err = __xa_insert(xa, index, entry, gfp);
	spin_unlock_bh(&(xa)->xa_lock);

	return err;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	xa_insert_irq(struct xarray *xa, unsigned long index, void *entry,
		      gfp_t gfp)
{
	int err;

	might_alloc(gfp);
	spin_lock_irq(&(xa)->xa_lock);
	err = __xa_insert(xa, index, entry, gfp);
	spin_unlock_irq(&(xa)->xa_lock);

	return err;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) int
xa_alloc(struct xarray *xa, u32 *id, void *entry, struct xa_limit limit,
	 gfp_t gfp)
{
	int err;

	might_alloc(gfp);
	spin_lock(&(xa)->xa_lock);
	err = __xa_alloc(xa, id, entry, limit, gfp);
	spin_unlock(&(xa)->xa_lock);

	return err;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	xa_alloc_bh(struct xarray *xa, u32 *id, void *entry,
		    struct xa_limit limit, gfp_t gfp)
{
	int err;

	might_alloc(gfp);
	spin_lock_bh(&(xa)->xa_lock);
	err = __xa_alloc(xa, id, entry, limit, gfp);
	spin_unlock_bh(&(xa)->xa_lock);

	return err;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	xa_alloc_irq(struct xarray *xa, u32 *id, void *entry,
		     struct xa_limit limit, gfp_t gfp)
{
	int err;

	might_alloc(gfp);
	spin_lock_irq(&(xa)->xa_lock);
	err = __xa_alloc(xa, id, entry, limit, gfp);
	spin_unlock_irq(&(xa)->xa_lock);

	return err;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
xa_alloc_cyclic(struct xarray *xa, u32 *id, void *entry, struct xa_limit limit,
		u32 *next, gfp_t gfp)
{
	int err;

	might_alloc(gfp);
	spin_lock(&(xa)->xa_lock);
	err = __xa_alloc_cyclic(xa, id, entry, limit, next, gfp);
	spin_unlock(&(xa)->xa_lock);

	return err;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
xa_alloc_cyclic_bh(struct xarray *xa, u32 *id, void *entry,
		   struct xa_limit limit, u32 *next, gfp_t gfp)
{
	int err;

	might_alloc(gfp);
	spin_lock_bh(&(xa)->xa_lock);
	err = __xa_alloc_cyclic(xa, id, entry, limit, next, gfp);
	spin_unlock_bh(&(xa)->xa_lock);

	return err;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
xa_alloc_cyclic_irq(struct xarray *xa, u32 *id, void *entry,
		    struct xa_limit limit, u32 *next, gfp_t gfp)
{
	int err;

	might_alloc(gfp);
	spin_lock_irq(&(xa)->xa_lock);
	err = __xa_alloc_cyclic(xa, id, entry, limit, next, gfp);
	spin_unlock_irq(&(xa)->xa_lock);

	return err;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) int
xa_reserve(struct xarray *xa, unsigned long index, gfp_t gfp)
{
	return xa_err(
		xa_cmpxchg(xa, index, ((void *)0), xa_mk_internal(257), gfp));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) int
xa_reserve_bh(struct xarray *xa, unsigned long index, gfp_t gfp)
{
	return xa_err(xa_cmpxchg_bh(xa, index, ((void *)0), xa_mk_internal(257),
				    gfp));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) int
xa_reserve_irq(struct xarray *xa, unsigned long index, gfp_t gfp)
{
	return xa_err(xa_cmpxchg_irq(xa, index, ((void *)0),
				     xa_mk_internal(257), gfp));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
xa_release(struct xarray *xa, unsigned long index)
{
	xa_cmpxchg(xa, index, xa_mk_internal(257), ((void *)0), 0);
}
struct xa_node {
	unsigned char shift;
	unsigned char offset;
	unsigned char count;
	unsigned char nr_values;
	struct xa_node *parent;
	struct xarray *array;
	union {
		struct list_head private_list;
		struct callback_head callback_head;
	};
	void *slots[(1UL << (0 ? 4 : 6))];
	union {
		unsigned long tags[3][(
			(((1UL << (0 ? 4 : 6))) + ((sizeof(long) * 8)) - 1) /
			((sizeof(long) * 8)))];
		unsigned long marks[3][(
			(((1UL << (0 ? 4 : 6))) + ((sizeof(long) * 8)) - 1) /
			((sizeof(long) * 8)))];
	};
};

void xa_dump(const struct xarray *);
void xa_dump_node(const struct xa_node *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_head(const struct xarray *xa)
{
	return ({
		typeof(*(xa->xa_head)) *__UNIQUE_ID_rcu439 = (typeof(*(
			xa->xa_head)) *)({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_440(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof((xa->xa_head)) == sizeof(char) ||
				       sizeof((xa->xa_head)) == sizeof(short) ||
				       sizeof((xa->xa_head)) == sizeof(int) ||
				       sizeof((xa->xa_head)) == sizeof(long)) ||
				      sizeof((xa->xa_head)) ==
					      sizeof(long long)))
					__compiletime_assert_440();
			} while (0);
			(*(const volatile typeof(_Generic(
				((xa->xa_head)),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: ((
								 xa->xa_head))))
				   *)&((xa->xa_head)));
		});
		do {
		} while (0 && (!((lockdep_is_held(&xa->xa_lock)) ||
				 rcu_read_lock_held())));
		;
		((typeof(*(xa->xa_head)) *)(__UNIQUE_ID_rcu439));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_head_locked(const struct xarray *xa)
{
	return ({
		do {
		} while (0 && (!((lockdep_is_held(&xa->xa_lock)))));
		;
		((typeof(*(xa->xa_head)) *)((xa->xa_head)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_entry(const struct xarray *xa, const struct xa_node *node,
	 unsigned int offset)
{
	do {
	} while (0);
	return ({
		typeof(*(node->slots[offset])) *__UNIQUE_ID_rcu441 = (typeof(*(
			node->slots[offset])) *)({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_442(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof((node->slots[offset])) ==
					       sizeof(char) ||
				       sizeof((node->slots[offset])) ==
					       sizeof(short) ||
				       sizeof((node->slots[offset])) ==
					       sizeof(int) ||
				       sizeof((node->slots[offset])) ==
					       sizeof(long)) ||
				      sizeof((node->slots[offset])) ==
					      sizeof(long long)))
					__compiletime_assert_442();
			} while (0);
			(*(const volatile typeof(_Generic(
				((node->slots[offset])),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: ((
								 node->slots
									 [offset]))))
				   *)&((node->slots[offset])));
		});
		do {
		} while (0 && (!((lockdep_is_held(&xa->xa_lock)) ||
				 rcu_read_lock_held())));
		;
		((typeof(*(node->slots[offset])) *)(__UNIQUE_ID_rcu441));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_entry_locked(const struct xarray *xa, const struct xa_node *node,
		unsigned int offset)
{
	do {
	} while (0);
	return ({
		do {
		} while (0 && (!((lockdep_is_held(&xa->xa_lock)))));
		;
		((typeof(*(node->slots[offset])) *)((node->slots[offset])));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct xa_node *
xa_parent(const struct xarray *xa, const struct xa_node *node)
{
	return ({
		typeof(*(node->parent)) *__UNIQUE_ID_rcu443 = (typeof(*(
			node->parent)) *)({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_444(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof((node->parent)) == sizeof(char) ||
				       sizeof((node->parent)) == sizeof(short) ||
				       sizeof((node->parent)) == sizeof(int) ||
				       sizeof((node->parent)) == sizeof(long)) ||
				      sizeof((node->parent)) ==
					      sizeof(long long)))
					__compiletime_assert_444();
			} while (0);
			(*(const volatile typeof(_Generic(
				((node->parent)),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: ((
								 node->parent))))
				   *)&((node->parent)));
		});
		do {
		} while (0 && (!((lockdep_is_held(&xa->xa_lock)) ||
				 rcu_read_lock_held())));
		;
		((typeof(*(node->parent)) *)(__UNIQUE_ID_rcu443));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct xa_node *
xa_parent_locked(const struct xarray *xa, const struct xa_node *node)
{
	return ({
		do {
		} while (0 && (!((lockdep_is_held(&xa->xa_lock)))));
		;
		((typeof(*(node->parent)) *)((node->parent)));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_mk_node(const struct xa_node *node)
{
	return (void *)((unsigned long)node | 2);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct xa_node *
xa_to_node(const void *entry)
{
	return (struct xa_node *)((unsigned long)entry - 2);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xa_is_node(const void *entry)
{
	return xa_is_internal(entry) && (unsigned long)entry > 4096;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xa_mk_sibling(unsigned int offset)
{
	return xa_mk_internal(offset);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
xa_to_sibling(const void *entry)
{
	return xa_to_internal(entry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xa_is_sibling(const void *entry)
{
	return 1 && xa_is_internal(entry) &&
	       (entry < xa_mk_sibling((1UL << (0 ? 4 : 6)) - 1));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xa_is_retry(const void *entry)
{
	return __builtin_expect(!!(entry == xa_mk_internal(256)), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xa_is_advanced(const void *entry)
{
	return xa_is_internal(entry) && (entry <= xa_mk_internal(256));
}
typedef void (*xa_update_node_t)(struct xa_node *node);

void xa_delete_node(struct xa_node *, xa_update_node_t);
struct xa_state {
	struct xarray *xa;
	unsigned long xa_index;
	unsigned char xa_shift;
	unsigned char xa_sibs;
	unsigned char xa_offset;
	unsigned char xa_pad;
	struct xa_node *xa_node;
	struct xa_node *xa_alloc;
	xa_update_node_t xa_update;
	struct list_lru *xa_lru;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
xas_error(const struct xa_state *xas)
{
	return xa_err(xas->xa_node);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
xas_set_err(struct xa_state *xas, long err)
{
	xas->xa_node = ((struct xa_node *)(((unsigned long)err << 2) | 2UL));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xas_invalid(const struct xa_state *xas)
{
	return (unsigned long)xas->xa_node & 3;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xas_valid(const struct xa_state *xas)
{
	return !xas_invalid(xas);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xas_is_node(const struct xa_state *xas)
{
	return xas_valid(xas) && xas->xa_node;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xas_not_node(struct xa_node *node)
{
	return ((unsigned long)node & 3) || !node;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xas_frozen(struct xa_node *node)
{
	return (unsigned long)node & 2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xas_top(struct xa_node *node)
{
	return node <= ((struct xa_node *)3UL);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
xas_reset(struct xa_state *xas)
{
	xas->xa_node = ((struct xa_node *)3UL);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
xas_retry(struct xa_state *xas, const void *entry)
{
	if (xa_is_zero(entry))
		return true;
	if (!xa_is_retry(entry))
		return false;
	xas_reset(xas);
	return true;
}

void *xas_load(struct xa_state *);
void *xas_store(struct xa_state *, void *entry);
void *xas_find(struct xa_state *, unsigned long max);
void *xas_find_conflict(struct xa_state *);

bool xas_get_mark(const struct xa_state *, xa_mark_t);
void xas_set_mark(const struct xa_state *, xa_mark_t);
void xas_clear_mark(const struct xa_state *, xa_mark_t);
void *xas_find_marked(struct xa_state *, unsigned long max, xa_mark_t);
void xas_init_marks(const struct xa_state *);

bool xas_nomem(struct xa_state *, gfp_t);
void xas_destroy(struct xa_state *);
void xas_pause(struct xa_state *);

void xas_create_range(struct xa_state *);

int xa_get_order(struct xarray *, unsigned long index);
int xas_get_order(struct xa_state *xas);
void xas_split(struct xa_state *, void *entry, unsigned int order);
void xas_split_alloc(struct xa_state *, void *entry, unsigned int order, gfp_t);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xas_reload(struct xa_state *xas)
{
	struct xa_node *node = xas->xa_node;
	void *entry;
	char offset;

	if (!node)
		return xa_head(xas->xa);
	if (1) {
		offset = (xas->xa_index >> node->shift) &
			 ((1UL << (0 ? 4 : 6)) - 1);
		entry = xa_entry(xas->xa, node, offset);
		if (!xa_is_sibling(entry))
			return entry;
		offset = xa_to_sibling(entry);
	} else {
		offset = xas->xa_offset;
	}
	return xa_entry(xas->xa, node, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
xas_set(struct xa_state *xas, unsigned long index)
{
	xas->xa_index = index;
	xas->xa_node = ((struct xa_node *)3UL);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
xas_advance(struct xa_state *xas, unsigned long index)
{
	unsigned char shift = xas_is_node(xas) ? xas->xa_node->shift : 0;

	xas->xa_index = index;
	xas->xa_offset = (index >> shift) & ((1UL << (0 ? 4 : 6)) - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
xas_set_order(struct xa_state *xas, unsigned long index, unsigned int order)
{
	xas->xa_index = order < 64 ? (index >> order) << order : 0;
	xas->xa_shift = order - (order % (0 ? 4 : 6));
	xas->xa_sibs = (1 << (order % (0 ? 4 : 6))) - 1;
	xas->xa_node = ((struct xa_node *)3UL);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
xas_set_update(struct xa_state *xas, xa_update_node_t update)
{
	xas->xa_update = update;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
xas_set_lru(struct xa_state *xas, struct list_lru *lru)
{
	xas->xa_lru = lru;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xas_next_entry(struct xa_state *xas, unsigned long max)
{
	struct xa_node *node = xas->xa_node;
	void *entry;

	if (__builtin_expect(!!(xas_not_node(node) || node->shift ||
				xas->xa_offset != (xas->xa_index &
						   ((1UL << (0 ? 4 : 6)) - 1))),
			     0))

		return xas_find(xas, max);

	do {
		if (__builtin_expect(!!(xas->xa_index >= max), 0))
			return xas_find(xas, max);
		if (__builtin_expect(!!(xas->xa_offset ==
					((1UL << (0 ? 4 : 6)) - 1)),
				     0))
			return xas_find(xas, max);
		entry = xa_entry(xas->xa, node, xas->xa_offset + 1);
		if (__builtin_expect(!!(xa_is_internal(entry)), 0))
			return xas_find(xas, max);
		xas->xa_offset++;
		xas->xa_index++;
	} while (!entry);

	return entry;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
xas_find_chunk(struct xa_state *xas, bool advance, xa_mark_t mark)
{
	unsigned long *addr = xas->xa_node->marks[(unsigned)mark];
	unsigned int offset = xas->xa_offset;

	if (advance)
		offset++;
	if ((1UL << (0 ? 4 : 6)) == 64) {
		if (offset < (1UL << (0 ? 4 : 6))) {
			unsigned long data = *addr & (~0UL << offset);
			if (data)
				return (__builtin_constant_p(data) ?
						(unsigned long)__builtin_ctzl(
							data) :
						variable__ffs(data));
		}
		return (1UL << (0 ? 4 : 6));
	}

	return find_next_bit(addr, (1UL << (0 ? 4 : 6)), offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xas_next_marked(struct xa_state *xas, unsigned long max, xa_mark_t mark)
{
	struct xa_node *node = xas->xa_node;
	void *entry;
	unsigned int offset;

	if (__builtin_expect(!!(xas_not_node(node) || node->shift), 0))
		return xas_find_marked(xas, max, mark);
	offset = xas_find_chunk(xas, true, mark);
	xas->xa_offset = offset;
	xas->xa_index = (xas->xa_index & ~((1UL << (0 ? 4 : 6)) - 1)) + offset;
	if (xas->xa_index > max)
		return ((void *)0);
	if (offset == (1UL << (0 ? 4 : 6)))
		return xas_find_marked(xas, max, mark);
	entry = xa_entry(xas->xa, node, offset);
	if (!entry)
		return xas_find_marked(xas, max, mark);
	return entry;
}

enum {
	XA_CHECK_SCHED = 4096,
};
void *__xas_next(struct xa_state *);
void *__xas_prev(struct xa_state *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xas_prev(struct xa_state *xas)
{
	struct xa_node *node = xas->xa_node;

	if (__builtin_expect(!!(xas_not_node(node) || node->shift ||
				xas->xa_offset == 0),
			     0))

		return __xas_prev(xas);

	xas->xa_index--;
	xas->xa_offset--;
	return xa_entry(xas->xa, node, xas->xa_offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
xas_next(struct xa_state *xas)
{
	struct xa_node *node = xas->xa_node;

	if (__builtin_expect(!!(xas_not_node(node) || node->shift ||
				xas->xa_offset == ((1UL << (0 ? 4 : 6)) - 1)),
			     0))

		return __xas_next(xas);

	xas->xa_index++;
	xas->xa_offset++;
	return xa_entry(xas->xa, node, xas->xa_offset);
}

struct mem_cgroup;

enum lru_status {
	LRU_REMOVED,
	LRU_REMOVED_RETRY,

	LRU_ROTATE,
	LRU_SKIP,
	LRU_RETRY,

	LRU_STOP,

};

struct list_lru_one {
	struct list_head list;

	long nr_items;
};

struct list_lru_memcg {
	struct callback_head rcu;

	struct list_lru_one node[];
};

struct list_lru_node {
	spinlock_t lock;

	struct list_lru_one lru;
	long nr_items;
} __attribute__((__aligned__((1 << (6)))));

struct list_lru {
	struct list_lru_node *node;
};

void list_lru_destroy(struct list_lru *lru);
int __list_lru_init(struct list_lru *lru, bool memcg_aware,
		    struct lock_class_key *key, struct shrinker *shrinker);

int memcg_list_lru_alloc(struct mem_cgroup *memcg, struct list_lru *lru,
			 gfp_t gfp);
void memcg_reparent_list_lrus(struct mem_cgroup *memcg,
			      struct mem_cgroup *parent);
bool list_lru_add(struct list_lru *lru, struct list_head *item, int nid,
		  struct mem_cgroup *memcg);
bool list_lru_add_obj(struct list_lru *lru, struct list_head *item);
bool list_lru_del(struct list_lru *lru, struct list_head *item, int nid,
		  struct mem_cgroup *memcg);
bool list_lru_del_obj(struct list_lru *lru, struct list_head *item);
unsigned long list_lru_count_one(struct list_lru *lru, int nid,
				 struct mem_cgroup *memcg);
unsigned long list_lru_count_node(struct list_lru *lru, int nid);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
list_lru_shrink_count(struct list_lru *lru, struct shrink_control *sc)
{
	return list_lru_count_one(lru, sc->nid, sc->memcg);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
list_lru_count(struct list_lru *lru)
{
	long count = 0;
	int nid;

	for (((nid)) = __first_node(&(node_states[N_NORMAL_MEMORY]));
	     ((nid)) < (1 << 6);
	     ((nid)) =
		     __next_node((((nid))), &((node_states[N_NORMAL_MEMORY]))))
		count += list_lru_count_node(lru, nid);

	return count;
}

void list_lru_isolate(struct list_lru_one *list, struct list_head *item);
void list_lru_isolate_move(struct list_lru_one *list, struct list_head *item,
			   struct list_head *head);

typedef enum lru_status (*list_lru_walk_cb)(struct list_head *item,
					    struct list_lru_one *list,
					    spinlock_t *lock, void *cb_arg);
unsigned long list_lru_walk_one(struct list_lru *lru, int nid,
				struct mem_cgroup *memcg,
				list_lru_walk_cb isolate, void *cb_arg,
				unsigned long *nr_to_walk);
unsigned long list_lru_walk_one_irq(struct list_lru *lru, int nid,
				    struct mem_cgroup *memcg,
				    list_lru_walk_cb isolate, void *cb_arg,
				    unsigned long *nr_to_walk);
unsigned long list_lru_walk_node(struct list_lru *lru, int nid,
				 list_lru_walk_cb isolate, void *cb_arg,
				 unsigned long *nr_to_walk);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
list_lru_shrink_walk(struct list_lru *lru, struct shrink_control *sc,
		     list_lru_walk_cb isolate, void *cb_arg)
{
	return list_lru_walk_one(lru, sc->nid, sc->memcg, isolate, cb_arg,
				 &sc->nr_to_scan);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
list_lru_shrink_walk_irq(struct list_lru *lru, struct shrink_control *sc,
			 list_lru_walk_cb isolate, void *cb_arg)
{
	return list_lru_walk_one_irq(lru, sc->nid, sc->memcg, isolate, cb_arg,
				     &sc->nr_to_scan);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
list_lru_walk(struct list_lru *lru, list_lru_walk_cb isolate, void *cb_arg,
	      unsigned long nr_to_walk)
{
	long isolated = 0;
	int nid;

	for (((nid)) = __first_node(&(node_states[N_NORMAL_MEMORY]));
	     ((nid)) < (1 << 6);
	     ((nid)) = __next_node((((nid))),
				   &((node_states[N_NORMAL_MEMORY])))) {
		isolated += list_lru_walk_node(lru, nid, isolate, cb_arg,
					       &nr_to_walk);
		if (nr_to_walk <= 0)
			break;
	}
	return isolated;
}

struct radix_tree_preload {
	local_lock_t lock;
	unsigned nr;

	struct xa_node *nodes;
};
extern __attribute__((
	section(".data..percpu"
		""))) __typeof__(struct radix_tree_preload) radix_tree_preloads;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
radix_tree_is_internal_node(void *ptr)
{
	return ((unsigned long)ptr & 3UL) == 2UL;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
radix_tree_empty(const struct xarray *root)
{
	return root->xa_head == ((void *)0);
}
struct radix_tree_iter {
	unsigned long index;
	unsigned long next_index;
	unsigned long tags;
	struct xa_node *node;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
radix_tree_deref_slot(void **slot)
{
	return ({
		typeof(*(*slot)) *__UNIQUE_ID_rcu445 = (typeof(*(*slot)) *)({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_446(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof((*slot)) == sizeof(char) ||
				       sizeof((*slot)) == sizeof(short) ||
				       sizeof((*slot)) == sizeof(int) ||
				       sizeof((*slot)) == sizeof(long)) ||
				      sizeof((*slot)) == sizeof(long long)))
					__compiletime_assert_446();
			} while (0);
			(*(const volatile typeof(_Generic(
				((*slot)),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: ((*slot))))
				   *)&((*slot)));
		});
		do {
		} while (0 && (!((0) || rcu_read_lock_held())));
		;
		((typeof(*(*slot)) *)(__UNIQUE_ID_rcu445));
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
radix_tree_deref_slot_protected(void **slot, spinlock_t *treelock)
{
	return ({
		do {
		} while (0 && (!((lockdep_is_held(treelock)))));
		;
		((typeof(*(*slot)) *)((*slot)));
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
radix_tree_deref_retry(void *arg)
{
	return __builtin_expect(!!(radix_tree_is_internal_node(arg)), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
radix_tree_exception(void *arg)
{
	return __builtin_expect(!!((unsigned long)arg & 3UL), 0);
}

int radix_tree_insert(struct xarray *, unsigned long index, void *);
void *__radix_tree_lookup(const struct xarray *, unsigned long index,
			  struct xa_node **nodep, void ***slotp);
void *radix_tree_lookup(const struct xarray *, unsigned long);
void **radix_tree_lookup_slot(const struct xarray *, unsigned long index);
void __radix_tree_replace(struct xarray *, struct xa_node *, void **slot,
			  void *entry);
void radix_tree_iter_replace(struct xarray *, const struct radix_tree_iter *,
			     void **slot, void *entry);
void radix_tree_replace_slot(struct xarray *, void **slot, void *entry);
void radix_tree_iter_delete(struct xarray *, struct radix_tree_iter *iter,
			    void **slot);
void *radix_tree_delete_item(struct xarray *, unsigned long, void *);
void *radix_tree_delete(struct xarray *, unsigned long);
unsigned int radix_tree_gang_lookup(const struct xarray *, void **results,
				    unsigned long first_index,
				    unsigned int max_items);
int radix_tree_preload(gfp_t gfp_mask);
int radix_tree_maybe_preload(gfp_t gfp_mask);
void radix_tree_init(void);
void *radix_tree_tag_set(struct xarray *, unsigned long index,
			 unsigned int tag);
void *radix_tree_tag_clear(struct xarray *, unsigned long index,
			   unsigned int tag);
int radix_tree_tag_get(const struct xarray *, unsigned long index,
		       unsigned int tag);
void radix_tree_iter_tag_clear(struct xarray *,
			       const struct radix_tree_iter *iter,
			       unsigned int tag);
unsigned int radix_tree_gang_lookup_tag(const struct xarray *, void **results,
					unsigned long first_index,
					unsigned int max_items,
					unsigned int tag);
unsigned int radix_tree_gang_lookup_tag_slot(const struct xarray *,
					     void ***results,
					     unsigned long first_index,
					     unsigned int max_items,
					     unsigned int tag);
int radix_tree_tagged(const struct xarray *, unsigned int tag);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
radix_tree_preload_end(void)
{
	do {
		local_lock_release(({
			do {
				const void *__vpp_verify =
					(typeof((&radix_tree_preloads.lock) +
						0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			({
				unsigned long tcp_ptr__ = ({
					u64 pfo_val__;
					asm("mov"
					    "q "
					    "%%"
					    "gs"
					    ":"
					    "%"
					    "[var]"
					    ", "
					    "%[val]"
					    : [val] "="
						    "r"(pfo_val__)
					    : [var] "m"((
						    *(typeof(*(&(this_cpu_off)))
							      *)(uintptr_t)(&(
							    this_cpu_off)))));
					(typeof(this_cpu_off))(unsigned long)
						pfo_val__;
				});
				tcp_ptr__ +=
					(unsigned long)(&radix_tree_preloads
								 .lock);
				(typeof(*(
					&radix_tree_preloads.lock)) *)tcp_ptr__;
			});
		}));
		do {
			__asm__ __volatile__("" : : : "memory");
			if (__builtin_expect(!!(__preempt_count_dec_and_test()),
					     0))
				do {
					static void *__attribute__((__used__))
					__attribute__((__section__(
						".discard.addressable")))
					__UNIQUE_ID___addressable___SCK__preempt_schedule447 =
						(void *)(uintptr_t)&__SCK__preempt_schedule;
					;
					asm volatile(
						"call "
						"__SCT__preempt_schedule"
						: "+r"(current_stack_pointer));
				} while (0);
		} while (0);
	} while (0);
}

void **idr_get_free(struct xarray *root, struct radix_tree_iter *iter,
		    gfp_t gfp, unsigned long max);

enum {
	RADIX_TREE_ITER_TAG_MASK = 0x0f,
	RADIX_TREE_ITER_TAGGED = 0x10,
	RADIX_TREE_ITER_CONTIG = 0x20,
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) void **
radix_tree_iter_init(struct radix_tree_iter *iter, unsigned long start)
{
	iter->index = 0;
	iter->next_index = start;
	return ((void *)0);
}
void **radix_tree_next_chunk(const struct xarray *,
			     struct radix_tree_iter *iter, unsigned flags);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void **
radix_tree_iter_lookup(const struct xarray *root, struct radix_tree_iter *iter,
		       unsigned long index)
{
	radix_tree_iter_init(iter, index);
	return radix_tree_next_chunk(root, iter, RADIX_TREE_ITER_CONTIG);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) void **
radix_tree_iter_retry(struct radix_tree_iter *iter)
{
	iter->next_index = iter->index;
	iter->tags = 0;
	return ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
__radix_tree_iter_add(struct radix_tree_iter *iter, unsigned long slots)
{
	return iter->index + slots;
}
void **__attribute__((__warn_unused_result__))
radix_tree_iter_resume(void **slot, struct radix_tree_iter *iter);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) long
radix_tree_chunk_size(struct radix_tree_iter *iter)
{
	return iter->next_index - iter->index;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) void **
radix_tree_next_slot(void **slot, struct radix_tree_iter *iter, unsigned flags)
{
	if (flags & RADIX_TREE_ITER_TAGGED) {
		iter->tags >>= 1;
		if (__builtin_expect(!!(!iter->tags), 0))
			return ((void *)0);
		if (__builtin_expect(!!(iter->tags & 1ul), 1)) {
			iter->index = __radix_tree_iter_add(iter, 1);
			slot++;
			goto found;
		}
		if (!(flags & RADIX_TREE_ITER_CONTIG)) {
			unsigned offset =
				(__builtin_constant_p(iter->tags) ?
					 (unsigned long)__builtin_ctzl(
						 iter->tags) :
					 variable__ffs(iter->tags));

			iter->tags >>= offset++;
			iter->index = __radix_tree_iter_add(iter, offset);
			slot += offset;
			goto found;
		}
	} else {
		long count = radix_tree_chunk_size(iter);

		while (--count > 0) {
			slot++;
			iter->index = __radix_tree_iter_add(iter, 1);

			if (__builtin_expect(!!(*slot), 1))
				goto found;
			if (flags & RADIX_TREE_ITER_CONTIG) {
				iter->next_index = 0;
				break;
			}
		}
	}
	return ((void *)0);

found:
	return slot;
}

struct upid {
	int nr;
	struct pid_namespace *ns;
};

struct pid {
	refcount_t count;
	unsigned int level;
	spinlock_t lock;
	struct dentry *stashed;
	u64 ino;

	struct hlist_head tasks[PIDTYPE_MAX];
	struct hlist_head inodes;

	wait_queue_head_t wait_pidfd;
	struct callback_head rcu;
	struct upid numbers[];
};

extern struct pid init_struct_pid;

struct file;

struct pid *pidfd_pid(const struct file *file);
struct pid *pidfd_get_pid(unsigned int fd, unsigned int *flags);
struct task_struct *pidfd_get_task(int pidfd, unsigned int *flags);
int pidfd_prepare(struct pid *pid, unsigned int flags, struct file **ret);
void do_notify_pidfd(struct task_struct *task);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct pid *
get_pid(struct pid *pid)
{
	if (pid)
		refcount_inc(&pid->count);
	return pid;
}

extern void put_pid(struct pid *pid);
extern struct task_struct *pid_task(struct pid *pid, enum pid_type);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pid_has_task(struct pid *pid, enum pid_type type)
{
	return !hlist_empty(&pid->tasks[type]);
}
extern struct task_struct *get_pid_task(struct pid *pid, enum pid_type);

extern struct pid *get_task_pid(struct task_struct *task, enum pid_type type);

extern void attach_pid(struct task_struct *task, enum pid_type);
extern void detach_pid(struct task_struct *task, enum pid_type);
extern void change_pid(struct task_struct *task, enum pid_type,
		       struct pid *pid);
extern void exchange_tids(struct task_struct *task, struct task_struct *old);
extern void transfer_pid(struct task_struct *old, struct task_struct *new,
			 enum pid_type);

extern int pid_max;
extern int pid_max_min, pid_max_max;
extern struct pid *find_pid_ns(int nr, struct pid_namespace *ns);
extern struct pid *find_vpid(int nr);

extern struct pid *find_get_pid(int nr);
extern struct pid *find_ge_pid(int nr, struct pid_namespace *);

extern struct pid *alloc_pid(struct pid_namespace *ns, pid_t *set_tid,
			     size_t set_tid_size);
extern void free_pid(struct pid *pid);
extern void disable_pid_allocation(struct pid_namespace *ns);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct pid_namespace *
ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = ((void *)0);
	if (pid)
		ns = pid->numbers[pid->level].ns;
	return ns;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_child_reaper(struct pid *pid)
{
	return pid->numbers[pid->level].nr == 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
pid_nr(struct pid *pid)
{
	pid_t nr = 0;
	if (pid)
		nr = pid->numbers[0].nr;
	return nr;
}

pid_t pid_nr_ns(struct pid *pid, struct pid_namespace *ns);
pid_t pid_vnr(struct pid *pid);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct pid *
task_pid(struct task_struct *task)
{
	return task->thread_pid;
}
pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,
		       struct pid_namespace *ns);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_pid_nr(struct task_struct *tsk)
{
	return tsk->pid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_pid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
{
	return __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_pid_vnr(struct task_struct *tsk)
{
	return __task_pid_nr_ns(tsk, PIDTYPE_PID, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_tgid_nr(struct task_struct *tsk)
{
	return tsk->tgid;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pid_alive(const struct task_struct *p)
{
	return p->thread_pid != ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
{
	return __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_pgrp_vnr(struct task_struct *tsk)
{
	return __task_pid_nr_ns(tsk, PIDTYPE_PGID, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_session_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
{
	return __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_session_vnr(struct task_struct *tsk)
{
	return __task_pid_nr_ns(tsk, PIDTYPE_SID, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)
{
	return __task_pid_nr_ns(tsk, PIDTYPE_TGID, ns);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_tgid_vnr(struct task_struct *tsk)
{
	return __task_pid_nr_ns(tsk, PIDTYPE_TGID, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)
{
	pid_t pid = 0;

	rcu_read_lock();
	if (pid_alive(tsk))
		pid = task_tgid_nr_ns(
			({
				typeof(*(tsk->real_parent)) *__UNIQUE_ID_rcu448 = (typeof(*(
					tsk->real_parent)) *)({
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_449(void)
							__attribute__((__error__(
								"Unsupported access size for {READ,WRITE}_ONCE().")));
						if (!((sizeof((tsk->real_parent)) ==
							       sizeof(char) ||
						       sizeof((tsk->real_parent)) ==
							       sizeof(short) ||
						       sizeof((tsk->real_parent)) ==
							       sizeof(int) ||
						       sizeof((tsk->real_parent)) ==
							       sizeof(long)) ||
						      sizeof((tsk->real_parent)) ==
							      sizeof(long long)))
							__compiletime_assert_449();
					} while (0);
					(*(const volatile typeof(_Generic(
						((tsk->real_parent)),
									 char: (char)0,
									 unsigned char: (
										 unsigned char)0,
									 signed char: (
										 signed char)0,
									 unsigned short: (
										 unsigned short)0,
									 signed short: (
										 signed short)0,
									 unsigned int: (
										 unsigned int)0,
									 signed int: (
										 signed int)0,
									 unsigned long: (
										 unsigned long)0,
									 signed long: (
										 signed long)0,
									 unsigned long long: (
										 unsigned long long)0,
									 signed long long: (
										 signed long long)0,
									 default: ((
										 tsk->real_parent))))
						   *)&((tsk->real_parent)));
				});
				do {
				} while (0 && (!((0) || rcu_read_lock_held())));
				;
				((typeof(*(tsk->real_parent))
					  *)(__UNIQUE_ID_rcu448));
			}),
			ns);
	rcu_read_unlock();

	return pid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_ppid_nr(const struct task_struct *tsk)
{
	return task_ppid_nr_ns(tsk, &init_pid_ns);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pid_t
task_pgrp_nr(struct task_struct *tsk)
{
	return task_pgrp_nr_ns(tsk, &init_pid_ns);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
is_global_init(struct task_struct *tsk)
{
	return task_tgid_nr(tsk) == 1;
}

typedef struct __user_cap_header_struct {
	__u32 version;
	int pid;
} *cap_user_header_t;

struct __user_cap_data_struct {
	__u32 effective;
	__u32 permitted;
	__u32 inheritable;
};
typedef struct __user_cap_data_struct *cap_user_data_t;
struct vfs_cap_data {
	__le32 magic_etc;
	struct {
		__le32 permitted;
		__le32 inheritable;
	} data[2];
};

struct vfs_ns_cap_data {
	__le32 magic_etc;
	struct {
		__le32 permitted;
		__le32 inheritable;
	} data[2];
	__le32 rootid;
};

extern int file_caps_enabled;

typedef struct {
	u64 val;
} kernel_cap_t;

struct cpu_vfs_cap_data {
	__u32 magic_etc;
	kuid_t rootid;
	kernel_cap_t permitted;
	kernel_cap_t inheritable;
};

struct file;
struct inode;
struct dentry;
struct task_struct;
struct user_namespace;
struct mnt_idmap;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kernel_cap_t
cap_combine(const kernel_cap_t a, const kernel_cap_t b)
{
	return (kernel_cap_t){ a.val | b.val };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kernel_cap_t
cap_intersect(const kernel_cap_t a, const kernel_cap_t b)
{
	return (kernel_cap_t){ a.val & b.val };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kernel_cap_t
cap_drop(const kernel_cap_t a, const kernel_cap_t drop)
{
	return (kernel_cap_t){ a.val & ~drop.val };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
cap_isclear(const kernel_cap_t a)
{
	return !a.val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
cap_isidentical(const kernel_cap_t a, const kernel_cap_t b)
{
	return a.val == b.val;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
cap_issubset(const kernel_cap_t a, const kernel_cap_t set)
{
	return !(a.val & ~set.val);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kernel_cap_t
cap_drop_fs_set(const kernel_cap_t a)
{
	return cap_drop(a, ((kernel_cap_t){
				   (((((1ULL))) << (0)) | ((((1ULL))) << (27)) |
				    ((((1ULL))) << (1)) | ((((1ULL))) << (2)) |
				    ((((1ULL))) << (3)) | ((((1ULL))) << (4)) |
				    ((((1ULL))) << (32))) |
				   ((((1ULL))) << (9)) }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kernel_cap_t
cap_raise_fs_set(const kernel_cap_t a, const kernel_cap_t permitted)
{
	return cap_combine(
		a, cap_intersect(
			   permitted,
			   ((kernel_cap_t){
				   (((((1ULL))) << (0)) | ((((1ULL))) << (27)) |
				    ((((1ULL))) << (1)) | ((((1ULL))) << (2)) |
				    ((((1ULL))) << (3)) | ((((1ULL))) << (4)) |
				    ((((1ULL))) << (32))) |
				   ((((1ULL))) << (9)) })));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kernel_cap_t
cap_drop_nfsd_set(const kernel_cap_t a)
{
	return cap_drop(a, ((kernel_cap_t){
				   (((((1ULL))) << (0)) | ((((1ULL))) << (27)) |
				    ((((1ULL))) << (1)) | ((((1ULL))) << (2)) |
				    ((((1ULL))) << (3)) | ((((1ULL))) << (4)) |
				    ((((1ULL))) << (32))) |
				   ((((1ULL))) << (24)) }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kernel_cap_t
cap_raise_nfsd_set(const kernel_cap_t a, const kernel_cap_t permitted)
{
	return cap_combine(
		a, cap_intersect(
			   permitted,
			   ((kernel_cap_t){
				   (((((1ULL))) << (0)) | ((((1ULL))) << (27)) |
				    ((((1ULL))) << (1)) | ((((1ULL))) << (2)) |
				    ((((1ULL))) << (3)) | ((((1ULL))) << (4)) |
				    ((((1ULL))) << (32))) |
				   ((((1ULL))) << (24)) })));
}

extern bool has_capability(struct task_struct *t, int cap);
extern bool has_ns_capability(struct task_struct *t, struct user_namespace *ns,
			      int cap);
extern bool has_capability_noaudit(struct task_struct *t, int cap);
extern bool has_ns_capability_noaudit(struct task_struct *t,
				      struct user_namespace *ns, int cap);
extern bool capable(int cap);
extern bool ns_capable(struct user_namespace *ns, int cap);
extern bool ns_capable_noaudit(struct user_namespace *ns, int cap);
extern bool ns_capable_setid(struct user_namespace *ns, int cap);
bool privileged_wrt_inode_uidgid(struct user_namespace *ns,
				 struct mnt_idmap *idmap,
				 const struct inode *inode);
bool capable_wrt_inode_uidgid(struct mnt_idmap *idmap,
			      const struct inode *inode, int cap);
extern bool file_ns_capable(const struct file *file, struct user_namespace *ns,
			    int cap);
extern bool ptracer_capable(struct task_struct *tsk, struct user_namespace *ns);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
perfmon_capable(void)
{
	return capable(38) || capable(21);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
bpf_capable(void)
{
	return capable(39) || capable(21);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
checkpoint_restore_ns_capable(struct user_namespace *ns)
{
	return ns_capable(ns, 40) || ns_capable(ns, 21);
}

int get_vfs_caps_from_disk(struct mnt_idmap *idmap, const struct dentry *dentry,
			   struct cpu_vfs_cap_data *cpu_caps);

int cap_convert_nscap(struct mnt_idmap *idmap, struct dentry *dentry,
		      const void **ivalue, size_t size);
struct semaphore {
	raw_spinlock_t lock;
	unsigned int count;
	struct list_head wait_list;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sema_init(struct semaphore *sem, int val)
{
	static struct lock_class_key __key;
	*sem = (struct semaphore){
		.lock =
			(raw_spinlock_t){
				.raw_lock = { { .val = { (0) } } },
			},
		.count = val,
		.wait_list = { &((*sem).wait_list), &((*sem).wait_list) },
	};
	do {
		(void)("semaphore->lock");
		(void)(&__key);
	} while (0);
}

extern void down(struct semaphore *sem);
extern int __attribute__((__warn_unused_result__))
down_interruptible(struct semaphore *sem);
extern int __attribute__((__warn_unused_result__))
down_killable(struct semaphore *sem);
extern int __attribute__((__warn_unused_result__))
down_trylock(struct semaphore *sem);
extern int __attribute__((__warn_unused_result__))
down_timeout(struct semaphore *sem, long jiffies);
extern void up(struct semaphore *sem);

struct f_owner_ex {
	int type;
	__kernel_pid_t pid;
};
struct flock {
	short l_type;
	short l_whence;
	__kernel_off_t l_start;
	__kernel_off_t l_len;
	__kernel_pid_t l_pid;
};

struct flock64 {
	short l_type;
	short l_whence;
	__kernel_loff_t l_start;
	__kernel_loff_t l_len;
	__kernel_pid_t l_pid;
};
struct open_how {
	__u64 flags;
	__u64 mode;
	__u64 resolve;
};

enum migrate_mode {
	MIGRATE_ASYNC,
	MIGRATE_SYNC_LIGHT,
	MIGRATE_SYNC,
};

enum migrate_reason {
	MR_COMPACTION,
	MR_MEMORY_FAILURE,
	MR_MEMORY_HOTPLUG,
	MR_SYSCALL,
	MR_MEMPOLICY_MBIND,
	MR_NUMA_MISPLACED,
	MR_CONTIG_RANGE,
	MR_LONGTERM_PIN,
	MR_DEMOTION,
	MR_DAMON,
	MR_TYPES
};

struct task_struct;

extern int print_fatal_signals;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
copy_siginfo(kernel_siginfo_t *to, const kernel_siginfo_t *from)
{
	({
		const size_t __fortify_size = (size_t)(sizeof(*to));
		const size_t __p_size = (__builtin_dynamic_object_size(to, 0));
		const size_t __q_size =
			(__builtin_dynamic_object_size(from, 0));
		const size_t __p_size_field =
			(__builtin_dynamic_object_size(to, 1));
		const size_t __q_size_field =
			(__builtin_dynamic_object_size(from, 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"450"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"450"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(450));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"to"
								"\" at "
								"include/linux/signal.h"
								":"
								"18",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"451"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"451"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(451));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("include/linux/signal.h"),
										  "i"(18),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"452"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"452"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(452));
								});
							} while (0);
							({
								asm volatile(
									"453"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"453"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(453));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(to, from, __fortify_size);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_siginfo(kernel_siginfo_t *info)
{
	({
		size_t __fortify_size = (size_t)(sizeof(*info));
		fortify_memset_chk(__fortify_size,
				   __builtin_dynamic_object_size(info, 0),
				   __builtin_dynamic_object_size(info, 1)),
			__builtin_memset(info, 0, __fortify_size);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
copy_siginfo_to_external(siginfo_t *to, const kernel_siginfo_t *from)
{
	({
		const size_t __fortify_size = (size_t)(sizeof(*from));
		const size_t __p_size = (__builtin_dynamic_object_size(to, 0));
		const size_t __q_size =
			(__builtin_dynamic_object_size(from, 0));
		const size_t __p_size_field =
			(__builtin_dynamic_object_size(to, 1));
		const size_t __q_size_field =
			(__builtin_dynamic_object_size(from, 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"454"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"454"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(454));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"to"
								"\" at "
								"include/linux/signal.h"
								":"
								"31",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"455"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"455"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(455));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("include/linux/signal.h"),
										  "i"(31),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"456"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"456"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(456));
								});
							} while (0);
							({
								asm volatile(
									"457"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"457"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(457));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(to, from, __fortify_size);
	});
	({
		size_t __fortify_size =
			(size_t)((sizeof(struct siginfo) -
				  sizeof(struct kernel_siginfo)));
		fortify_memset_chk(
			__fortify_size,
			__builtin_dynamic_object_size(
				((char *)to) + sizeof(struct kernel_siginfo),
				0),
			__builtin_dynamic_object_size(
				((char *)to) + sizeof(struct kernel_siginfo),
				1)),
			__builtin_memset(((char *)to) +
						 sizeof(struct kernel_siginfo),
					 0, __fortify_size);
	});
}

int copy_siginfo_to_user(siginfo_t *to, const kernel_siginfo_t *from);
int copy_siginfo_from_user(kernel_siginfo_t *to, const siginfo_t *from);

enum siginfo_layout {
	SIL_KILL,
	SIL_TIMER,
	SIL_POLL,
	SIL_FAULT,
	SIL_FAULT_TRAPNO,
	SIL_FAULT_MCEERR,
	SIL_FAULT_BNDERR,
	SIL_FAULT_PKUERR,
	SIL_FAULT_PERF_EVENT,
	SIL_CHLD,
	SIL_RT,
	SIL_SYS,
};

enum siginfo_layout siginfo_layout(unsigned sig, int si_code);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sigaddset(sigset_t *set, int _sig)
{
	unsigned long sig = _sig - 1;
	if ((64 / 64) == 1)
		set->sig[0] |= 1UL << sig;
	else
		set->sig[sig / 64] |= 1UL << (sig % 64);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sigdelset(sigset_t *set, int _sig)
{
	unsigned long sig = _sig - 1;
	if ((64 / 64) == 1)
		set->sig[0] &= ~(1UL << sig);
	else
		set->sig[sig / 64] &= ~(1UL << (sig % 64));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sigismember(sigset_t *set, int _sig)
{
	unsigned long sig = _sig - 1;
	if ((64 / 64) == 1)
		return 1 & (set->sig[0] >> sig);
	else
		return 1 & (set->sig[sig / 64] >> (sig % 64));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sigisemptyset(sigset_t *set)
{
	switch ((64 / 64)) {
	case 4:
		return (set->sig[3] | set->sig[2] | set->sig[1] |
			set->sig[0]) == 0;
	case 2:
		return (set->sig[1] | set->sig[0]) == 0;
	case 1:
		return set->sig[0] == 0;
	default:
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_458(void)
				__attribute__((__error__("BUILD_BUG failed")));
			if (!(!(1)))
				__compiletime_assert_458();
		} while (0);
		return 0;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sigequalsets(const sigset_t *set1, const sigset_t *set2)
{
	switch ((64 / 64)) {
	case 4:
		return (set1->sig[3] == set2->sig[3]) &&
		       (set1->sig[2] == set2->sig[2]) &&
		       (set1->sig[1] == set2->sig[1]) &&
		       (set1->sig[0] == set2->sig[0]);
	case 2:
		return (set1->sig[1] == set2->sig[1]) &&
		       (set1->sig[0] == set2->sig[0]);
	case 1:
		return set1->sig[0] == set2->sig[0];
	}
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sigorsets(sigset_t *r, const sigset_t *a, const sigset_t *b)
{
	unsigned long a0, a1, a2, a3, b0, b1, b2, b3;
	switch ((64 / 64)) {
	case 4:
		a3 = a->sig[3];
		a2 = a->sig[2];
		b3 = b->sig[3];
		b2 = b->sig[2];
		r->sig[3] = ((a3) | (b3));
		r->sig[2] = ((a2) | (b2));
		__attribute__((__fallthrough__));
	case 2:
		a1 = a->sig[1];
		b1 = b->sig[1];
		r->sig[1] = ((a1) | (b1));
		__attribute__((__fallthrough__));
	case 1:
		a0 = a->sig[0];
		b0 = b->sig[0];
		r->sig[0] = ((a0) | (b0));
		break;
	default:
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_459(void)
				__attribute__((__error__("BUILD_BUG failed")));
			if (!(!(1)))
				__compiletime_assert_459();
		} while (0);
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sigandsets(sigset_t *r, const sigset_t *a, const sigset_t *b)
{
	unsigned long a0, a1, a2, a3, b0, b1, b2, b3;
	switch ((64 / 64)) {
	case 4:
		a3 = a->sig[3];
		a2 = a->sig[2];
		b3 = b->sig[3];
		b2 = b->sig[2];
		r->sig[3] = ((a3) & (b3));
		r->sig[2] = ((a2) & (b2));
		__attribute__((__fallthrough__));
	case 2:
		a1 = a->sig[1];
		b1 = b->sig[1];
		r->sig[1] = ((a1) & (b1));
		__attribute__((__fallthrough__));
	case 1:
		a0 = a->sig[0];
		b0 = b->sig[0];
		r->sig[0] = ((a0) & (b0));
		break;
	default:
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_460(void)
				__attribute__((__error__("BUILD_BUG failed")));
			if (!(!(1)))
				__compiletime_assert_460();
		} while (0);
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sigandnsets(sigset_t *r, const sigset_t *a, const sigset_t *b)
{
	unsigned long a0, a1, a2, a3, b0, b1, b2, b3;
	switch ((64 / 64)) {
	case 4:
		a3 = a->sig[3];
		a2 = a->sig[2];
		b3 = b->sig[3];
		b2 = b->sig[2];
		r->sig[3] = ((a3) & ~(b3));
		r->sig[2] = ((a2) & ~(b2));
		__attribute__((__fallthrough__));
	case 2:
		a1 = a->sig[1];
		b1 = b->sig[1];
		r->sig[1] = ((a1) & ~(b1));
		__attribute__((__fallthrough__));
	case 1:
		a0 = a->sig[0];
		b0 = b->sig[0];
		r->sig[0] = ((a0) & ~(b0));
		break;
	default:
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_461(void)
				__attribute__((__error__("BUILD_BUG failed")));
			if (!(!(1)))
				__compiletime_assert_461();
		} while (0);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
signotset(sigset_t *set)
{
	switch ((64 / 64)) {
	case 4:
		set->sig[3] = (~(set->sig[3]));
		set->sig[2] = (~(set->sig[2]));
		__attribute__((__fallthrough__));
	case 2:
		set->sig[1] = (~(set->sig[1]));
		__attribute__((__fallthrough__));
	case 1:
		set->sig[0] = (~(set->sig[0]));
		break;
	default:
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_462(void)
				__attribute__((__error__("BUILD_BUG failed")));
			if (!(!(1)))
				__compiletime_assert_462();
		} while (0);
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sigemptyset(sigset_t *set)
{
	switch ((64 / 64)) {
	default:
		({
			size_t __fortify_size = (size_t)(sizeof(sigset_t));
			fortify_memset_chk(
				__fortify_size,
				__builtin_dynamic_object_size(set, 0),
				__builtin_dynamic_object_size(set, 1)),
				__builtin_memset(set, 0, __fortify_size);
		});
		break;
	case 2:
		set->sig[1] = 0;
		__attribute__((__fallthrough__));
	case 1:
		set->sig[0] = 0;
		break;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sigfillset(sigset_t *set)
{
	switch ((64 / 64)) {
	default:
		({
			size_t __fortify_size = (size_t)(sizeof(sigset_t));
			fortify_memset_chk(
				__fortify_size,
				__builtin_dynamic_object_size(set, 0),
				__builtin_dynamic_object_size(set, 1)),
				__builtin_memset(set, -1, __fortify_size);
		});
		break;
	case 2:
		set->sig[1] = -1;
		__attribute__((__fallthrough__));
	case 1:
		set->sig[0] = -1;
		break;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sigaddsetmask(sigset_t *set, unsigned long mask)
{
	set->sig[0] |= mask;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sigdelsetmask(sigset_t *set, unsigned long mask)
{
	set->sig[0] &= ~mask;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sigtestsetmask(sigset_t *set, unsigned long mask)
{
	return (set->sig[0] & mask) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
siginitset(sigset_t *set, unsigned long mask)
{
	set->sig[0] = mask;
	switch ((64 / 64)) {
	default:
		({
			size_t __fortify_size =
				(size_t)(sizeof(long) * ((64 / 64) - 1));
			fortify_memset_chk(
				__fortify_size,
				__builtin_dynamic_object_size(&set->sig[1], 0),
				__builtin_dynamic_object_size(&set->sig[1], 1)),
				__builtin_memset(&set->sig[1], 0,
						 __fortify_size);
		});
		break;
	case 2:
		set->sig[1] = 0;
		break;
	case 1:;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
siginitsetinv(sigset_t *set, unsigned long mask)
{
	set->sig[0] = ~mask;
	switch ((64 / 64)) {
	default:
		({
			size_t __fortify_size =
				(size_t)(sizeof(long) * ((64 / 64) - 1));
			fortify_memset_chk(
				__fortify_size,
				__builtin_dynamic_object_size(&set->sig[1], 0),
				__builtin_dynamic_object_size(&set->sig[1], 1)),
				__builtin_memset(&set->sig[1], -1,
						 __fortify_size);
		});
		break;
	case 2:
		set->sig[1] = -1;
		break;
	case 1:;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
init_sigpending(struct sigpending *sig)
{
	sigemptyset(&sig->signal);
	INIT_LIST_HEAD(&sig->list);
}

extern void flush_sigqueue(struct sigpending *queue);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
valid_signal(unsigned long sig)
{
	return sig <= 64 ? 1 : 0;
}

struct timespec;
struct pt_regs;
enum pid_type;

extern int next_signal(struct sigpending *pending, sigset_t *mask);
extern int do_send_sig_info(int sig, struct kernel_siginfo *info,
			    struct task_struct *p, enum pid_type type);
extern int group_send_sig_info(int sig, struct kernel_siginfo *info,
			       struct task_struct *p, enum pid_type type);
extern int send_signal_locked(int sig, struct kernel_siginfo *info,
			      struct task_struct *p, enum pid_type type);
extern int sigprocmask(int, sigset_t *, sigset_t *);
extern void set_current_blocked(sigset_t *);
extern void __set_current_blocked(const sigset_t *);
extern int show_unhandled_signals;

extern bool get_signal(struct ksignal *ksig);
extern void signal_setup_done(int failed, struct ksignal *ksig, int stepping);
extern void exit_signals(struct task_struct *tsk);
extern void kernel_sigaction(int, __sighandler_t);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
allow_signal(int sig)
{
	kernel_sigaction(sig, ((__sighandler_t)2));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
allow_kernel_signal(int sig)
{
	kernel_sigaction(sig, ((__sighandler_t)3));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
disallow_signal(int sig)
{
	kernel_sigaction(sig, ((__sighandler_t)1));
}

extern struct kmem_cache *sighand_cachep;

extern bool unhandled_signal(struct task_struct *tsk, int sig);
void signals_init(void);

int restore_altstack(const stack_t *);
int __save_altstack(stack_t *, unsigned long);
bool sigaltstack_size_valid(size_t ss_size);

struct seq_file;
extern void render_sigset_t(struct seq_file *, const char *, sigset_t *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
arch_untagged_si_addr(void *addr, unsigned long sig, unsigned long si_code)
{
	return addr;
}

struct task_struct;
extern bool task_set_jobctl_pending(struct task_struct *task,
				    unsigned long mask);
extern void task_clear_jobctl_trapping(struct task_struct *task);
extern void task_clear_jobctl_pending(struct task_struct *task,
				      unsigned long mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
should_fail_usercopy(void)
{
	return false;
}

struct task_struct;
int arch_prctl_spec_ctrl_get(struct task_struct *task, unsigned long which);
int arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,
			     unsigned long ctrl);

void arch_seccomp_spec_mitigate(struct task_struct *task);

extern struct tracepoint __tracepoint_mmap_lock_start_locking;
extern struct tracepoint __tracepoint_mmap_lock_acquire_returned;
extern struct tracepoint __tracepoint_mmap_lock_released;

void __mmap_lock_do_trace_start_locking(struct mm_struct *mm, bool write);
void __mmap_lock_do_trace_acquire_returned(struct mm_struct *mm, bool write,
					   bool success);
void __mmap_lock_do_trace_released(struct mm_struct *mm, bool write);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__mmap_lock_trace_start_locking(struct mm_struct *mm, bool write)
{
	if (static_key_false(&(__tracepoint_mmap_lock_start_locking).key))
		__mmap_lock_do_trace_start_locking(mm, write);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__mmap_lock_trace_acquire_returned(struct mm_struct *mm, bool write,
				   bool success)
{
	if (static_key_false(&(__tracepoint_mmap_lock_acquire_returned).key))
		__mmap_lock_do_trace_acquire_returned(mm, write, success);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__mmap_lock_trace_released(struct mm_struct *mm, bool write)
{
	if (static_key_false(&(__tracepoint_mmap_lock_released).key))
		__mmap_lock_do_trace_released(mm, write);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmap_assert_locked(const struct mm_struct *mm)
{
	rwsem_assert_held(&mm->mmap_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmap_assert_write_locked(const struct mm_struct *mm)
{
	rwsem_assert_held_write(&mm->mmap_lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
vma_end_write_all(struct mm_struct *mm)
{
	mmap_assert_write_locked(mm);

	do {
		do {
		} while (0);
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_463(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*&mm->mm_lock_seq) ==
					       sizeof(char) ||
				       sizeof(*&mm->mm_lock_seq) ==
					       sizeof(short) ||
				       sizeof(*&mm->mm_lock_seq) ==
					       sizeof(int) ||
				       sizeof(*&mm->mm_lock_seq) ==
					       sizeof(long))))
					__compiletime_assert_463();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_464(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*&mm->mm_lock_seq) ==
						       sizeof(char) ||
					       sizeof(*&mm->mm_lock_seq) ==
						       sizeof(short) ||
					       sizeof(*&mm->mm_lock_seq) ==
						       sizeof(int) ||
					       sizeof(*&mm->mm_lock_seq) ==
						       sizeof(long)) ||
					      sizeof(*&mm->mm_lock_seq) ==
						      sizeof(long long)))
						__compiletime_assert_464();
				} while (0);
				do {
					*(volatile typeof(*&mm->mm_lock_seq) *)&(
						*&mm->mm_lock_seq) =
						(mm->mm_lock_seq + 1);
				} while (0);
			} while (0);
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmap_init_lock(struct mm_struct *mm)
{
	do {
		static struct lock_class_key __key;
		__init_rwsem((&mm->mmap_lock), "&mm->mmap_lock", &__key);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmap_write_lock(struct mm_struct *mm)
{
	__mmap_lock_trace_start_locking(mm, true);
	down_write(&mm->mmap_lock);
	__mmap_lock_trace_acquire_returned(mm, true, true);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmap_write_lock_nested(struct mm_struct *mm, int subclass)
{
	__mmap_lock_trace_start_locking(mm, true);
	down_write(&mm->mmap_lock);
	__mmap_lock_trace_acquire_returned(mm, true, true);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mmap_write_lock_killable(struct mm_struct *mm)
{
	int ret;

	__mmap_lock_trace_start_locking(mm, true);
	ret = down_write_killable(&mm->mmap_lock);
	__mmap_lock_trace_acquire_returned(mm, true, ret == 0);
	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmap_write_unlock(struct mm_struct *mm)
{
	__mmap_lock_trace_released(mm, true);
	vma_end_write_all(mm);
	up_write(&mm->mmap_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmap_write_downgrade(struct mm_struct *mm)
{
	__mmap_lock_trace_acquire_returned(mm, false, true);
	vma_end_write_all(mm);
	downgrade_write(&mm->mmap_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmap_read_lock(struct mm_struct *mm)
{
	__mmap_lock_trace_start_locking(mm, false);
	down_read(&mm->mmap_lock);
	__mmap_lock_trace_acquire_returned(mm, false, true);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mmap_read_lock_killable(struct mm_struct *mm)
{
	int ret;

	__mmap_lock_trace_start_locking(mm, false);
	ret = down_read_killable(&mm->mmap_lock);
	__mmap_lock_trace_acquire_returned(mm, false, ret == 0);
	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mmap_read_trylock(struct mm_struct *mm)
{
	bool ret;

	__mmap_lock_trace_start_locking(mm, false);
	ret = down_read_trylock(&mm->mmap_lock) != 0;
	__mmap_lock_trace_acquire_returned(mm, false, ret);
	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmap_read_unlock(struct mm_struct *mm)
{
	__mmap_lock_trace_released(mm, false);
	up_read(&mm->mmap_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmap_read_unlock_non_owner(struct mm_struct *mm)
{
	__mmap_lock_trace_released(mm, false);
	up_read(&mm->mmap_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mmap_lock_is_contended(struct mm_struct *mm)
{
	return rwsem_is_contended(&mm->mmap_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
clac(void)
{
	asm __inline volatile("# ALT: oldinstr\n"
			      "771:\n\t"
			      ""
			      "\n772:\n"
			      "# ALT: padding\n"
			      ".skip -((("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")) > 0) * "
			      "(("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")),0x90\n"
			      "773:\n"
			      ".pushsection .altinstructions,\"a\"\n"
			      " .long 771b - .\n"
			      " .long 774f - .\n"
			      " .4byte "
			      "( 9*32+20)"
			      "\n"
			      " .byte "
			      "773b-771b"
			      "\n"
			      " .byte "
			      "775f-774f"
			      "\n"
			      ".popsection\n"
			      ".pushsection .altinstr_replacement, \"ax\"\n"
			      "# ALT: replacement\n"
			      "774:\n\t"
			      ".byte 0x0f,0x01,0xca"
			      "\n775:\n"
			      ".popsection\n"
			      :
			      :
			      : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
stac(void)
{
	asm __inline volatile("# ALT: oldinstr\n"
			      "771:\n\t"
			      ""
			      "\n772:\n"
			      "# ALT: padding\n"
			      ".skip -((("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")) > 0) * "
			      "(("
			      "775f-774f"
			      ")-("
			      "772b-771b"
			      ")),0x90\n"
			      "773:\n"
			      ".pushsection .altinstructions,\"a\"\n"
			      " .long 771b - .\n"
			      " .long 774f - .\n"
			      " .4byte "
			      "( 9*32+20)"
			      "\n"
			      " .byte "
			      "773b-771b"
			      "\n"
			      " .byte "
			      "775f-774f"
			      "\n"
			      ".popsection\n"
			      ".pushsection .altinstr_replacement, \"ax\"\n"
			      "# ALT: replacement\n"
			      "774:\n\t"
			      ".byte 0x0f,0x01,0xcb"
			      "\n775:\n"
			      ".popsection\n"
			      :
			      :
			      : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
smap_save(void)
{
	unsigned long flags;

	asm volatile("# smap_save\n\t"
		     "# ALT: oldinstr\n"
		     "771:\n\t"
		     ""
		     "\n772:\n"
		     "# ALT: padding\n"
		     ".skip -((("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")) > 0) * "
		     "(("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")),0x90\n"
		     "773:\n"
		     ".pushsection .altinstructions,\"a\"\n"
		     " .long 771b - .\n"
		     " .long 774f - .\n"
		     " .4byte "
		     "( 9*32+20)"
		     "\n"
		     " .byte "
		     "773b-771b"
		     "\n"
		     " .byte "
		     "775f-774f"
		     "\n"
		     ".popsection\n"
		     ".pushsection .altinstr_replacement, \"ax\"\n"
		     "# ALT: replacement\n"
		     "774:\n\t"
		     "pushf; pop %0; "
		     ".byte 0x0f,0x01,0xca"
		     "\n\t"
		     "\n775:\n"
		     ".popsection\n"

		     : "=rm"(flags)
		     :
		     : "memory", "cc");

	return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
smap_restore(unsigned long flags)
{
	asm volatile("# smap_restore\n\t"
		     "# ALT: oldinstr\n"
		     "771:\n\t"
		     ""
		     "\n772:\n"
		     "# ALT: padding\n"
		     ".skip -((("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")) > 0) * "
		     "(("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")),0x90\n"
		     "773:\n"
		     ".pushsection .altinstructions,\"a\"\n"
		     " .long 771b - .\n"
		     " .long 774f - .\n"
		     " .4byte "
		     "( 9*32+20)"
		     "\n"
		     " .byte "
		     "773b-771b"
		     "\n"
		     " .byte "
		     "775f-774f"
		     "\n"
		     ".popsection\n"
		     ".pushsection .altinstr_replacement, \"ax\"\n"
		     "# ALT: replacement\n"
		     "774:\n\t"
		     "push %0; popf\n\t"
		     "\n775:\n"
		     ".popsection\n"

		     :
		     : "g"(flags)
		     : "memory", "cc");
}
struct exception_table_entry {
	int insn, fixup, data;
};
struct pt_regs;
extern int fixup_exception(struct pt_regs *regs, int trapnr,
			   unsigned long error_code, unsigned long fault_addr);
extern int ex_get_fixup_type(unsigned long ip);
extern void early_fixup_exception(struct pt_regs *regs, int trapnr);

extern void __attribute__((__noreturn__))
ex_handler_msr_mce(struct pt_regs *regs, bool wrmsr);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
ex_handler_bpf(const struct exception_table_entry *x, struct pt_regs *regs)
{
	return false;
}

struct interval_tree_node {
	struct rb_node rb;
	unsigned long start;
	unsigned long last;
	unsigned long __subtree_last;
};

extern void interval_tree_insert(struct interval_tree_node *node,
				 struct rb_root_cached *root);

extern void interval_tree_remove(struct interval_tree_node *node,
				 struct rb_root_cached *root);

extern struct interval_tree_node *
interval_tree_iter_first(struct rb_root_cached *root, unsigned long start,
			 unsigned long last);

extern struct interval_tree_node *
interval_tree_iter_next(struct interval_tree_node *node, unsigned long start,
			unsigned long last);
struct interval_tree_span_iter {
	struct interval_tree_node *nodes[2];
	unsigned long first_index;
	unsigned long last_index;

	union {
		unsigned long start_hole;
		unsigned long start_used;
	};
	union {
		unsigned long last_hole;
		unsigned long last_used;
	};
	int is_hole;
};

void interval_tree_span_iter_first(struct interval_tree_span_iter *state,
				   struct rb_root_cached *itree,
				   unsigned long first_index,
				   unsigned long last_index);
void interval_tree_span_iter_advance(struct interval_tree_span_iter *iter,
				     struct rb_root_cached *itree,
				     unsigned long new_index);
void interval_tree_span_iter_next(struct interval_tree_span_iter *state);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
interval_tree_span_iter_done(struct interval_tree_span_iter *state)
{
	return state->is_hole == -1;
}

struct mmu_notifier_subscriptions;
struct mmu_notifier;
struct mmu_notifier_range;
struct mmu_interval_notifier;
enum mmu_notifier_event {
	MMU_NOTIFY_UNMAP = 0,
	MMU_NOTIFY_CLEAR,
	MMU_NOTIFY_PROTECTION_VMA,
	MMU_NOTIFY_PROTECTION_PAGE,
	MMU_NOTIFY_SOFT_DIRTY,
	MMU_NOTIFY_RELEASE,
	MMU_NOTIFY_MIGRATE,
	MMU_NOTIFY_EXCLUSIVE,
};

struct mmu_notifier_ops {
	void (*release)(struct mmu_notifier *subscription,
			struct mm_struct *mm);
	int (*clear_flush_young)(struct mmu_notifier *subscription,
				 struct mm_struct *mm, unsigned long start,
				 unsigned long end);

	int (*clear_young)(struct mmu_notifier *subscription,
			   struct mm_struct *mm, unsigned long start,
			   unsigned long end);

	int (*test_young)(struct mmu_notifier *subscription,
			  struct mm_struct *mm, unsigned long address);
	int (*invalidate_range_start)(struct mmu_notifier *subscription,
				      const struct mmu_notifier_range *range);
	void (*invalidate_range_end)(struct mmu_notifier *subscription,
				     const struct mmu_notifier_range *range);
	void (*arch_invalidate_secondary_tlbs)(
		struct mmu_notifier *subscription, struct mm_struct *mm,
		unsigned long start, unsigned long end);
	struct mmu_notifier *(*alloc_notifier)(struct mm_struct *mm);
	void (*free_notifier)(struct mmu_notifier *subscription);
};
struct mmu_notifier {
	struct hlist_node hlist;
	const struct mmu_notifier_ops *ops;
	struct mm_struct *mm;
	struct callback_head rcu;
	unsigned int users;
};

struct mmu_interval_notifier_ops {
	bool (*invalidate)(struct mmu_interval_notifier *interval_sub,
			   const struct mmu_notifier_range *range,
			   unsigned long cur_seq);
};

struct mmu_interval_notifier {
	struct interval_tree_node interval_tree;
	const struct mmu_interval_notifier_ops *ops;
	struct mm_struct *mm;
	struct hlist_node deferred_item;
	unsigned long invalidate_seq;
};

struct mmu_notifier_range {
	struct mm_struct *mm;
	unsigned long start;
	unsigned long end;
	unsigned flags;
	enum mmu_notifier_event event;
	void *owner;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mm_has_notifiers(struct mm_struct *mm)
{
	return __builtin_expect(!!(mm->notifier_subscriptions), 0);
}

struct mmu_notifier *mmu_notifier_get_locked(const struct mmu_notifier_ops *ops,
					     struct mm_struct *mm);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct mmu_notifier *
mmu_notifier_get(const struct mmu_notifier_ops *ops, struct mm_struct *mm)
{
	struct mmu_notifier *ret;

	mmap_write_lock(mm);
	ret = mmu_notifier_get_locked(ops, mm);
	mmap_write_unlock(mm);
	return ret;
}
void mmu_notifier_put(struct mmu_notifier *subscription);
void mmu_notifier_synchronize(void);

extern int mmu_notifier_register(struct mmu_notifier *subscription,
				 struct mm_struct *mm);
extern int __mmu_notifier_register(struct mmu_notifier *subscription,
				   struct mm_struct *mm);
extern void mmu_notifier_unregister(struct mmu_notifier *subscription,
				    struct mm_struct *mm);

unsigned long
mmu_interval_read_begin(struct mmu_interval_notifier *interval_sub);
int mmu_interval_notifier_insert(struct mmu_interval_notifier *interval_sub,
				 struct mm_struct *mm, unsigned long start,
				 unsigned long length,
				 const struct mmu_interval_notifier_ops *ops);
int mmu_interval_notifier_insert_locked(
	struct mmu_interval_notifier *interval_sub, struct mm_struct *mm,
	unsigned long start, unsigned long length,
	const struct mmu_interval_notifier_ops *ops);
void mmu_interval_notifier_remove(struct mmu_interval_notifier *interval_sub);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmu_interval_set_seq(struct mmu_interval_notifier *interval_sub,
		     unsigned long cur_seq)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_465(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(interval_sub->invalidate_seq) ==
				       sizeof(char) ||
			       sizeof(interval_sub->invalidate_seq) ==
				       sizeof(short) ||
			       sizeof(interval_sub->invalidate_seq) ==
				       sizeof(int) ||
			       sizeof(interval_sub->invalidate_seq) ==
				       sizeof(long)) ||
			      sizeof(interval_sub->invalidate_seq) ==
				      sizeof(long long)))
				__compiletime_assert_465();
		} while (0);
		do {
			*(volatile typeof(interval_sub->invalidate_seq) *)&(
				interval_sub->invalidate_seq) = (cur_seq);
		} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mmu_interval_read_retry(struct mmu_interval_notifier *interval_sub,
			unsigned long seq)
{
	return interval_sub->invalidate_seq != seq;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mmu_interval_check_retry(struct mmu_interval_notifier *interval_sub,
			 unsigned long seq)
{
	return ({
		       do {
			       __attribute__((__noreturn__)) extern void
			       __compiletime_assert_466(void) __attribute__((__error__(
				       "Unsupported access size for {READ,WRITE}_ONCE().")));
			       if (!((sizeof(interval_sub->invalidate_seq) ==
					      sizeof(char) ||
				      sizeof(interval_sub->invalidate_seq) ==
					      sizeof(short) ||
				      sizeof(interval_sub->invalidate_seq) ==
					      sizeof(int) ||
				      sizeof(interval_sub->invalidate_seq) ==
					      sizeof(long)) ||
				     sizeof(interval_sub->invalidate_seq) ==
					     sizeof(long long)))
				       __compiletime_assert_466();
		       } while (0);
		       (*(const volatile typeof(_Generic(
			       (interval_sub->invalidate_seq),
							char: (char)0,
							unsigned char: (
								unsigned char)0,
							signed char: (
								signed char)0,
							unsigned short: (
								unsigned short)0,
							signed short: (
								signed short)0,
							unsigned int: (
								unsigned int)0,
							signed int: (
								signed int)0,
							unsigned long: (
								unsigned long)0,
							signed long: (
								signed long)0,
							unsigned long long: (
								unsigned long long)0,
							signed long long: (
								signed long long)0,
							default: (
								interval_sub
									->invalidate_seq)))
				  *)&(interval_sub->invalidate_seq));
	       }) != seq;
}

extern void __mmu_notifier_subscriptions_destroy(struct mm_struct *mm);
extern void __mmu_notifier_release(struct mm_struct *mm);
extern int __mmu_notifier_clear_flush_young(struct mm_struct *mm,
					    unsigned long start,
					    unsigned long end);
extern int __mmu_notifier_clear_young(struct mm_struct *mm, unsigned long start,
				      unsigned long end);
extern int __mmu_notifier_test_young(struct mm_struct *mm,
				     unsigned long address);
extern int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *r);
extern void __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *r);
extern void __mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,
							  unsigned long start,
							  unsigned long end);
extern bool
mmu_notifier_range_update_to_read_only(const struct mmu_notifier_range *range);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mmu_notifier_range_blockable(const struct mmu_notifier_range *range)
{
	return (range->flags & (1 << 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmu_notifier_release(struct mm_struct *mm)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_release(mm);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mmu_notifier_clear_flush_young(struct mm_struct *mm, unsigned long start,
			       unsigned long end)
{
	if (mm_has_notifiers(mm))
		return __mmu_notifier_clear_flush_young(mm, start, end);
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mmu_notifier_clear_young(struct mm_struct *mm, unsigned long start,
			 unsigned long end)
{
	if (mm_has_notifiers(mm))
		return __mmu_notifier_clear_young(mm, start, end);
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mmu_notifier_test_young(struct mm_struct *mm, unsigned long address)
{
	if (mm_has_notifiers(mm))
		return __mmu_notifier_test_young(mm, address);
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)
{
	do {
		might_resched();
	} while (0);

	do {
	} while (0);
	if (mm_has_notifiers(range->mm)) {
		range->flags |= (1 << 0);
		__mmu_notifier_invalidate_range_start(range);
	}
	do {
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int __attribute__((
	__warn_unused_result__))
mmu_notifier_invalidate_range_start_nonblock(struct mmu_notifier_range *range)
{
	int ret = 0;

	do {
	} while (0);
	if (mm_has_notifiers(range->mm)) {
		range->flags &= ~(1 << 0);
		ret = __mmu_notifier_invalidate_range_start(range);
	}
	do {
	} while (0);
	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range)
{
	if (mmu_notifier_range_blockable(range))
		do {
			might_resched();
		} while (0);

	if (mm_has_notifiers(range->mm))
		__mmu_notifier_invalidate_range_end(range);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,
					    unsigned long start,
					    unsigned long end)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_arch_invalidate_secondary_tlbs(mm, start, end);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmu_notifier_subscriptions_init(struct mm_struct *mm)
{
	mm->notifier_subscriptions = ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmu_notifier_subscriptions_destroy(struct mm_struct *mm)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_subscriptions_destroy(mm);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmu_notifier_range_init(struct mmu_notifier_range *range,
			enum mmu_notifier_event event, unsigned flags,
			struct mm_struct *mm, unsigned long start,
			unsigned long end)
{
	range->event = event;
	range->mm = mm;
	range->start = start;
	range->end = end;
	range->flags = flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mmu_notifier_range_init_owner(struct mmu_notifier_range *range,
			      enum mmu_notifier_event event, unsigned int flags,
			      struct mm_struct *mm, unsigned long start,
			      unsigned long end, void *owner)
{
	mmu_notifier_range_init(range, event, flags, mm, start, end);
	range->owner = owner;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__invpcid(unsigned long pcid, unsigned long addr, unsigned long type)
{
	struct {
		u64 d[2];
	} desc = { { pcid, addr } };

	asm volatile(
		"invpcid %[desc], %[type]" ::[desc] "m"(desc), [type] "r"(type)
		: "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
invpcid_flush_one(unsigned long pcid, unsigned long addr)
{
	__invpcid(pcid, addr, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
invpcid_flush_single_context(unsigned long pcid)
{
	__invpcid(pcid, 0, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
invpcid_flush_all(void)
{
	__invpcid(0, 0, 2);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
invpcid_flush_all_nonglobals(void)
{
	__invpcid(0, 0, 3);
}

extern void pti_init(void);
extern void pti_check_boottime_disable(void);
extern void pti_finalize(void);

extern u32 init_pkru_value;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__pkru_allows_read(u32 pkru, u16 pkey)
{
	int pkru_pkey_bits = pkey * 2;
	return !(pkru & (0x1u << pkru_pkey_bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__pkru_allows_write(u32 pkru, u16 pkey)
{
	int pkru_pkey_bits = pkey * 2;

	return !(pkru & ((0x1u | 0x2u) << pkru_pkey_bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
read_pkru(void)
{
	if ((__builtin_constant_p((16 * 32 + 4)) &&
			     (((((16 * 32 + 4)) >> 5) == (0) &&
			       (1UL << (((16 * 32 + 4)) & 31) &
				((1 << ((0 * 32 + 1) & 31))))) ||
			      ((((16 * 32 + 4)) >> 5) == (1) &&
			       (1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			      ((((16 * 32 + 4)) >> 5) == (2) &&
			       (1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			      ((((16 * 32 + 4)) >> 5) == (3) &&
			       (1UL << (((16 * 32 + 4)) & 31) &
				((1 << ((3 * 32 + 2) & 31)) |
				 (1 << ((3 * 32 + 3) & 31)) |
				 (1 << ((3 * 32 + 1) & 31))))) ||
			      ((((16 * 32 + 4)) >> 5) == (4) &&
			       (1UL << (((16 * 32 + 4)) & 31) & (0))) ||
			      ((((16 * 32 + 4)) >> 5) == (5) &&
			       (1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			      ((((16 * 32 + 4)) >> 5) == (6) &&
			       (1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			      ((((16 * 32 + 4)) >> 5) == (7) &&
			       (1UL << (((16 * 32 + 4)) & 31) & (0))) ||
			      ((((16 * 32 + 4)) >> 5) == (8) &&
			       (1UL << (((16 * 32 + 4)) & 31) &
				((1 << ((8 * 32 + 16) & 31)) |
				 (1 << ((8 * 32 + 22) & 31))))) ||
			      ((((16 * 32 + 4)) >> 5) == (9) &&
			       (1UL << (((16 * 32 + 4)) & 31) &
				((1 << ((9 * 32 + 2) & 31))))) ||
			      ((((16 * 32 + 4)) >> 5) == (10) &&
			       (1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			      ((((16 * 32 + 4)) >> 5) == (11) &&
			       (1UL << (((16 * 32 + 4)) & 31) &
				(0 | 0 | 0 | 0 |
				 (1 << ((11 * 32 + 23) & 31))))) ||
			      ((((16 * 32 + 4)) >> 5) == (12) &&
			       (1UL << (((16 * 32 + 4)) & 31) &
				((1 << ((12 * 32 + 17) & 31)) |
				 (1 << ((12 * 32 + 26) & 31))))) ||
			      ((((16 * 32 + 4)) >> 5) == (13) &&
			       (1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			      ((((16 * 32 + 4)) >> 5) == (14) &&
			       (1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			      ((((16 * 32 + 4)) >> 5) == (15) &&
			       (1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			      ((((16 * 32 + 4)) >> 5) == (16) &&
			       (1UL << (((16 * 32 + 4)) & 31) &
				(0 | 0 | 0 | 0 |
				 (1 << ((16 * 32 + 29) & 31))))) ||
			      ((((16 * 32 + 4)) >> 5) == (17) &&
			       (1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			      ((((16 * 32 + 4)) >> 5) == (18) &&
			       (1UL << (((16 * 32 + 4)) & 31) & (0))) ||
			      ((((16 * 32 + 4)) >> 5) == (19) &&
			       (1UL << (((16 * 32 + 4)) & 31) &
				((1 << ((19 * 32 + 4) & 31))))) ||
			      ((((16 * 32 + 4)) >> 5) == (20) &&
			       (1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			      ((((16 * 32 + 4)) >> 5) == (21) &&
			       (1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			      ((int)(sizeof(
				      struct { int : (-!!(22 != 22)); }))) ||
			      ((int)(sizeof(
				      struct { int : (-!!(22 != 22)); })))) ?
		     0 :
		     (__builtin_constant_p((
			      __builtin_constant_p((
				      16 * 32 +
				      4)) && (((((16 * 32 + 4)) >> 5) == (0) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						((1 << ((0 * 32 + 0) & 31)) |
						 (1 << ((0 * 32 + 3)) & 31) |
						 (1 << ((0 * 32 + 5) & 31)) |
						 (1 << ((0 * 32 + 6) & 31)) |
						 (1 << ((0 * 32 + 8) & 31)) |
						 (1 << ((0 * 32 + 13)) & 31) |
						 (1 << ((0 * 32 + 24) & 31)) |
						 (1 << ((0 * 32 + 15) & 31)) |
						 (1 << ((0 * 32 + 25) & 31)) |
						 (1 << ((0 * 32 + 26) & 31))))) ||
					      ((((16 * 32 + 4)) >> 5) == (1) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						((1 << ((1 * 32 + 29) & 31)) |
						 0))) ||
					      ((((16 * 32 + 4)) >> 5) == (2) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (3) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						((1 << ((3 * 32 + 20) & 31))))) ||
					      ((((16 * 32 + 4)) >> 5) == (4) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						(0))) ||
					      ((((16 * 32 + 4)) >> 5) == (5) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (6) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (7) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (8) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (9) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (10) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (11) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (12) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (13) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (14) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (15) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (16) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (17) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (18) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (19) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (20) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((((16 * 32 + 4)) >> 5) == (21) &&
					       (1UL << (((16 * 32 + 4)) & 31) &
						0)) ||
					      ((int)(sizeof(struct {
						      int : (-!!(22 != 22));
					      }))) ||
					      ((int)(sizeof(struct {
						      int : (-!!(22 != 22));
					      })))) ?
				      1 :
				      arch_test_bit(
					      (16 * 32 + 4),
					      (unsigned long
						       *)((&boot_cpu_data)
								  ->x86_capability)))) ?
			      (__builtin_constant_p((
				       16 * 32 +
				       4)) && (((((16 * 32 + 4)) >> 5) == (0) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 ((1 << ((0 * 32 + 0) & 31)) |
						  (1 << ((0 * 32 + 3)) & 31) |
						  (1 << ((0 * 32 + 5) & 31)) |
						  (1 << ((0 * 32 + 6) & 31)) |
						  (1 << ((0 * 32 + 8) & 31)) |
						  (1 << ((0 * 32 + 13)) & 31) |
						  (1 << ((0 * 32 + 24) & 31)) |
						  (1 << ((0 * 32 + 15) & 31)) |
						  (1 << ((0 * 32 + 25) & 31)) |
						  (1 << ((0 * 32 + 26) &
							 31))))) ||
					       ((((16 * 32 + 4)) >> 5) == (1) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 ((1 << ((1 * 32 + 29) & 31)) |
						  0))) ||
					       ((((16 * 32 + 4)) >> 5) == (2) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (3) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 ((1 << ((3 * 32 + 20) &
							 31))))) ||
					       ((((16 * 32 + 4)) >> 5) == (4) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 (0))) ||
					       ((((16 * 32 + 4)) >> 5) == (5) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (6) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (7) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (8) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (9) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (10) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (11) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (12) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (13) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (14) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (15) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (16) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (17) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (18) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (19) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (20) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (21) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       }))) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       })))) ?
				       1 :
				       arch_test_bit(
					       (16 * 32 + 4),
					       (unsigned long
							*)((&boot_cpu_data)
								   ->x86_capability))) :
			      _static_cpu_has((16 * 32 + 4)))))
		return rdpkru();
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
write_pkru(u32 pkru)
{
	if (!(__builtin_constant_p((16 * 32 + 4)) &&
			      (((((16 * 32 + 4)) >> 5) == (0) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((0 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (1) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (2) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (3) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((3 * 32 + 2) & 31)) |
				  (1 << ((3 * 32 + 3) & 31)) |
				  (1 << ((3 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (4) &&
				(1UL << (((16 * 32 + 4)) & 31) & (0))) ||
			       ((((16 * 32 + 4)) >> 5) == (5) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (6) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (7) &&
				(1UL << (((16 * 32 + 4)) & 31) & (0))) ||
			       ((((16 * 32 + 4)) >> 5) == (8) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((8 * 32 + 16) & 31)) |
				  (1 << ((8 * 32 + 22) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (9) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((9 * 32 + 2) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (10) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (11) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((11 * 32 + 23) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (12) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((12 * 32 + 17) & 31)) |
				  (1 << ((12 * 32 + 26) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (13) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (14) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (15) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (16) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((16 * 32 + 29) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (17) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (18) &&
				(1UL << (((16 * 32 + 4)) & 31) & (0))) ||
			       ((((16 * 32 + 4)) >> 5) == (19) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((19 * 32 + 4) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (20) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (21) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); }))) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); })))) ?
		      0 :
		      (__builtin_constant_p((
			       __builtin_constant_p((
				       16 * 32 +
				       4)) && (((((16 * 32 + 4)) >> 5) == (0) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 ((1 << ((0 * 32 + 0) & 31)) |
						  (1 << ((0 * 32 + 3)) & 31) |
						  (1 << ((0 * 32 + 5) & 31)) |
						  (1 << ((0 * 32 + 6) & 31)) |
						  (1 << ((0 * 32 + 8) & 31)) |
						  (1 << ((0 * 32 + 13)) & 31) |
						  (1 << ((0 * 32 + 24) & 31)) |
						  (1 << ((0 * 32 + 15) & 31)) |
						  (1 << ((0 * 32 + 25) & 31)) |
						  (1 << ((0 * 32 + 26) &
							 31))))) ||
					       ((((16 * 32 + 4)) >> 5) == (1) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 ((1 << ((1 * 32 + 29) & 31)) |
						  0))) ||
					       ((((16 * 32 + 4)) >> 5) == (2) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (3) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 ((1 << ((3 * 32 + 20) &
							 31))))) ||
					       ((((16 * 32 + 4)) >> 5) == (4) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 (0))) ||
					       ((((16 * 32 + 4)) >> 5) == (5) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (6) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (7) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (8) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (9) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (10) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (11) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (12) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (13) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (14) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (15) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (16) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (17) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (18) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (19) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (20) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (21) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       }))) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       })))) ?
				       1 :
				       arch_test_bit(
					       (16 * 32 + 4),
					       (unsigned long
							*)((&boot_cpu_data)
								   ->x86_capability)))) ?
			       (__builtin_constant_p((
					16 * 32 +
					4)) && (((((16 * 32 + 4)) >> 5) == (0) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  ((1 << ((0 * 32 + 0) & 31)) |
						   (1 << ((0 * 32 + 3)) & 31) |
						   (1 << ((0 * 32 + 5) & 31)) |
						   (1 << ((0 * 32 + 6) & 31)) |
						   (1 << ((0 * 32 + 8) & 31)) |
						   (1 << ((0 * 32 + 13)) & 31) |
						   (1 << ((0 * 32 + 24) & 31)) |
						   (1 << ((0 * 32 + 15) & 31)) |
						   (1 << ((0 * 32 + 25) & 31)) |
						   (1 << ((0 * 32 + 26) &
							  31))))) ||
						((((16 * 32 + 4)) >> 5) == (1) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  ((1 << ((1 * 32 + 29) & 31)) |
						   0))) ||
						((((16 * 32 + 4)) >> 5) == (2) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) == (3) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  ((1 << ((3 * 32 + 20) &
							  31))))) ||
						((((16 * 32 + 4)) >> 5) == (4) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  (0))) ||
						((((16 * 32 + 4)) >> 5) == (5) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) == (6) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) == (7) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) == (8) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) == (9) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (10) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (11) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (12) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (13) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (14) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (15) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (16) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (17) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (18) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (19) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (20) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (21) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						}))) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						})))) ?
					1 :
					arch_test_bit(
						(16 * 32 + 4),
						(unsigned long
							 *)((&boot_cpu_data)
								    ->x86_capability))) :
			       _static_cpu_has((16 * 32 + 4)))))
		return;

	if (pkru != rdpkru())
		wrpkru(pkru);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
pkru_write_default(void)
{
	if (!(__builtin_constant_p((16 * 32 + 4)) &&
			      (((((16 * 32 + 4)) >> 5) == (0) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((0 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (1) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (2) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (3) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((3 * 32 + 2) & 31)) |
				  (1 << ((3 * 32 + 3) & 31)) |
				  (1 << ((3 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (4) &&
				(1UL << (((16 * 32 + 4)) & 31) & (0))) ||
			       ((((16 * 32 + 4)) >> 5) == (5) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (6) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (7) &&
				(1UL << (((16 * 32 + 4)) & 31) & (0))) ||
			       ((((16 * 32 + 4)) >> 5) == (8) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((8 * 32 + 16) & 31)) |
				  (1 << ((8 * 32 + 22) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (9) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((9 * 32 + 2) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (10) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (11) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((11 * 32 + 23) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (12) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((12 * 32 + 17) & 31)) |
				  (1 << ((12 * 32 + 26) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (13) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (14) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (15) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (16) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((16 * 32 + 29) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (17) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (18) &&
				(1UL << (((16 * 32 + 4)) & 31) & (0))) ||
			       ((((16 * 32 + 4)) >> 5) == (19) &&
				(1UL << (((16 * 32 + 4)) & 31) &
				 ((1 << ((19 * 32 + 4) & 31))))) ||
			       ((((16 * 32 + 4)) >> 5) == (20) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((((16 * 32 + 4)) >> 5) == (21) &&
				(1UL << (((16 * 32 + 4)) & 31) & 0)) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); }))) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); })))) ?
		      0 :
		      (__builtin_constant_p((
			       __builtin_constant_p((
				       16 * 32 +
				       4)) && (((((16 * 32 + 4)) >> 5) == (0) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 ((1 << ((0 * 32 + 0) & 31)) |
						  (1 << ((0 * 32 + 3)) & 31) |
						  (1 << ((0 * 32 + 5) & 31)) |
						  (1 << ((0 * 32 + 6) & 31)) |
						  (1 << ((0 * 32 + 8) & 31)) |
						  (1 << ((0 * 32 + 13)) & 31) |
						  (1 << ((0 * 32 + 24) & 31)) |
						  (1 << ((0 * 32 + 15) & 31)) |
						  (1 << ((0 * 32 + 25) & 31)) |
						  (1 << ((0 * 32 + 26) &
							 31))))) ||
					       ((((16 * 32 + 4)) >> 5) == (1) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 ((1 << ((1 * 32 + 29) & 31)) |
						  0))) ||
					       ((((16 * 32 + 4)) >> 5) == (2) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (3) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 ((1 << ((3 * 32 + 20) &
							 31))))) ||
					       ((((16 * 32 + 4)) >> 5) == (4) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 (0))) ||
					       ((((16 * 32 + 4)) >> 5) == (5) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (6) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (7) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (8) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (9) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (10) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (11) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (12) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (13) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (14) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (15) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (16) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (17) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (18) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (19) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (20) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((((16 * 32 + 4)) >> 5) == (21) &&
						(1UL << (((16 * 32 + 4)) & 31) &
						 0)) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       }))) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       })))) ?
				       1 :
				       arch_test_bit(
					       (16 * 32 + 4),
					       (unsigned long
							*)((&boot_cpu_data)
								   ->x86_capability)))) ?
			       (__builtin_constant_p((
					16 * 32 +
					4)) && (((((16 * 32 + 4)) >> 5) == (0) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  ((1 << ((0 * 32 + 0) & 31)) |
						   (1 << ((0 * 32 + 3)) & 31) |
						   (1 << ((0 * 32 + 5) & 31)) |
						   (1 << ((0 * 32 + 6) & 31)) |
						   (1 << ((0 * 32 + 8) & 31)) |
						   (1 << ((0 * 32 + 13)) & 31) |
						   (1 << ((0 * 32 + 24) & 31)) |
						   (1 << ((0 * 32 + 15) & 31)) |
						   (1 << ((0 * 32 + 25) & 31)) |
						   (1 << ((0 * 32 + 26) &
							  31))))) ||
						((((16 * 32 + 4)) >> 5) == (1) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  ((1 << ((1 * 32 + 29) & 31)) |
						   0))) ||
						((((16 * 32 + 4)) >> 5) == (2) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) == (3) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  ((1 << ((3 * 32 + 20) &
							  31))))) ||
						((((16 * 32 + 4)) >> 5) == (4) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  (0))) ||
						((((16 * 32 + 4)) >> 5) == (5) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) == (6) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) == (7) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) == (8) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) == (9) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (10) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (11) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (12) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (13) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (14) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (15) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (16) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (17) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (18) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (19) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (20) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((((16 * 32 + 4)) >> 5) ==
							 (21) &&
						 (1UL << (((16 * 32 + 4)) & 31) &
						  0)) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						}))) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						})))) ?
					1 :
					arch_test_bit(
						(16 * 32 + 4),
						(unsigned long
							 *)((&boot_cpu_data)
								    ->x86_capability))) :
			       _static_cpu_has((16 * 32 + 4)))))
		return;

	wrpkru(({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_467(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(init_pkru_value) == sizeof(char) ||
			       sizeof(init_pkru_value) == sizeof(short) ||
			       sizeof(init_pkru_value) == sizeof(int) ||
			       sizeof(init_pkru_value) == sizeof(long)) ||
			      sizeof(init_pkru_value) == sizeof(long long)))
				__compiletime_assert_467();
		} while (0);
		(*(const volatile typeof(_Generic((init_pkru_value),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (init_pkru_value)))
			   *)&(init_pkru_value));
	}));
}
extern void kernel_fpu_begin_mask(unsigned int kfpu_mask);
extern void kernel_fpu_end(void);
extern bool irq_fpu_usable(void);
extern void fpregs_mark_activate(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kernel_fpu_begin(void)
{
	kernel_fpu_begin_mask((((1UL)) << (1)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
fpregs_lock(void)
{
	if (!0)
		local_bh_disable();
	else
		do {
			__preempt_count_add(1);
			__asm__ __volatile__("" : : : "memory");
		} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
fpregs_unlock(void)
{
	if (!0)
		local_bh_enable();
	else
		do {
			__asm__ __volatile__("" : : : "memory");
			if (__builtin_expect(!!(__preempt_count_dec_and_test()),
					     0))
				do {
					static void *__attribute__((__used__))
					__attribute__((__section__(
						".discard.addressable")))
					__UNIQUE_ID___addressable___SCK__preempt_schedule468 =
						(void *)(uintptr_t)&__SCK__preempt_schedule;
					;
					asm volatile(
						"call "
						"__SCT__preempt_schedule"
						: "+r"(current_stack_pointer));
				} while (0);
		} while (0);
}
void fpregs_lock_and_load(void);

extern void fpregs_assert_state_consistent(void);

extern void switch_fpu_return(void);
extern int cpu_has_xfeatures(u64 xfeatures_mask, const char **feature_name);

extern int fpu__exception_code(struct fpu *fpu, int trap_nr);
extern void fpu_sync_fpstate(struct fpu *fpu);
extern void fpu_reset_from_exception_fixup(void);

extern void fpu__init_cpu(void);
extern void fpu__init_system(void);
extern void fpu__init_check_bugs(void);
extern void fpu__resume_cpu(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
fpstate_init_soft(struct swregs_state *soft)
{
}

extern __attribute__((
	section(".data..percpu"
		""))) __typeof__(struct fpu *) fpu_fpregs_owner_ctx;

extern void fpstate_free(struct fpu *fpu);

extern void fpstate_clear_xstate_component(struct fpstate *fps,
					   unsigned int xfeature);

extern u64 xstate_get_guest_group_perm(void);

extern void *get_xsave_addr(struct xregs_state *xsave, int xfeature_nr);

extern bool fpu_alloc_guest_fpstate(struct fpu_guest *gfpu);
extern void fpu_free_guest_fpstate(struct fpu_guest *gfpu);
extern int fpu_swap_kvm_fpstate(struct fpu_guest *gfpu, bool enter_guest);
extern int fpu_enable_guest_xfd_features(struct fpu_guest *guest_fpu,
					 u64 xfeatures);

extern void fpu_update_guest_xfd(struct fpu_guest *guest_fpu, u64 xfd);
extern void fpu_sync_guest_vmexit_xfd_state(void);

extern void fpu_copy_guest_fpstate_to_uabi(struct fpu_guest *gfpu, void *buf,
					   unsigned int size, u64 xfeatures,
					   u32 pkru);
extern int fpu_copy_uabi_to_guest_fpstate(struct fpu_guest *gfpu,
					  const void *buf, u64 xcr0,
					  u32 *vpkru);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
fpstate_set_confidential(struct fpu_guest *gfpu)
{
	gfpu->fpstate->is_confidential = true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
fpstate_is_confidential(struct fpu_guest *gfpu)
{
	return gfpu->fpstate->is_confidential;
}

extern long fpu_xstate_prctl(int option, unsigned long arg2);

extern void fpu_idle_fpregs(void);

enum cc_vendor {
	CC_VENDOR_NONE,
	CC_VENDOR_AMD,
	CC_VENDOR_INTEL,
};
static const u64 cc_mask = 0;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
cc_mkenc(u64 val)
{
	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
cc_mkdec(u64 val)
{
	return val;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cc_random_init(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
pte_uffd_wp(pte_t pte)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
pmd_uffd_wp(pmd_t pmd)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) pte_t
pte_mkuffd_wp(pte_t pte)
{
	return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) pmd_t
pmd_mkuffd_wp(pmd_t pmd)
{
	return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) pte_t
pte_clear_uffd_wp(pte_t pte)
{
	return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) pmd_t
pmd_clear_uffd_wp(pmd_t pmd)
{
	return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) pte_t
pte_swp_mkuffd_wp(pte_t pte)
{
	return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) int
pte_swp_uffd_wp(pte_t pte)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) pte_t
pte_swp_clear_uffd_wp(pte_t pte)
{
	return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_swp_mkuffd_wp(pmd_t pmd)
{
	return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pmd_swp_uffd_wp(pmd_t pmd)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_swp_clear_uffd_wp(pmd_t pmd)
{
	return pmd;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
page_table_check_alloc(struct page *page, unsigned int order)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
page_table_check_free(struct page *page, unsigned int order)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
page_table_check_pte_clear(struct mm_struct *mm, pte_t pte)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
page_table_check_pmd_clear(struct mm_struct *mm, pmd_t pmd)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
page_table_check_pud_clear(struct mm_struct *mm, pud_t pud)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
page_table_check_ptes_set(struct mm_struct *mm, pte_t *ptep, pte_t pte,
			  unsigned int nr)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
page_table_check_pmd_set(struct mm_struct *mm, pmd_t *pmdp, pmd_t pmd)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
page_table_check_pud_set(struct mm_struct *mm, pud_t *pudp, pud_t pud)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
page_table_check_pte_clear_range(struct mm_struct *mm, unsigned long addr,
				 pmd_t pmd)
{
}

extern pgd_t early_top_pgt[512];
bool __attribute__((__section__(".init.text"))) __attribute__((__cold__))
__early_make_pgtable(unsigned long address, pmdval_t pmd);

struct seq_file;
void ptdump_walk_pgd_level(struct seq_file *m, struct mm_struct *mm);
void ptdump_walk_pgd_level_debugfs(struct seq_file *m, struct mm_struct *mm,
				   bool user);
bool ptdump_walk_pgd_level_checkwx(void);

void ptdump_walk_user_pgd_level_checkwx(void);
extern unsigned long empty_zero_page[((1UL) << 12) / sizeof(unsigned long)];

extern spinlock_t pgd_lock;
extern struct list_head pgd_list;

extern struct mm_struct *pgd_page_get_mm(struct page *page);

extern pmdval_t early_pmd_flags;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_set_flags(pmd_t pmd, pmdval_t set)
{
	pmdval_t v = native_pmd_val(pmd);

	return native_make_pmd(v | set);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_clear_flags(pmd_t pmd, pmdval_t clear)
{
	pmdval_t v = native_pmd_val(pmd);

	return native_make_pmd(v & ~clear);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_set_flags(pud_t pud, pudval_t set)
{
	pudval_t v = native_pud_val(pud);

	return native_make_pud(v | set);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_clear_flags(pud_t pud, pudval_t clear)
{
	pudval_t v = native_pud_val(pud);

	return native_make_pud(v & ~clear);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pte_dirty(pte_t pte)
{
	return pte_flags(pte) &
	       ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 58));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pte_shstk(pte_t pte)
{
	return (__builtin_constant_p((16 * 32 + 7)) &&
				(((((16 * 32 + 7)) >> 5) == (0) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((0 * 32 + 1) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (1) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (2) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (3) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((3 * 32 + 2) & 31)) |
				    (1 << ((3 * 32 + 3) & 31)) |
				    (1 << ((3 * 32 + 1) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (4) &&
				  (1UL << (((16 * 32 + 7)) & 31) & (0))) ||
				 ((((16 * 32 + 7)) >> 5) == (5) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (6) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (7) &&
				  (1UL << (((16 * 32 + 7)) & 31) & (0))) ||
				 ((((16 * 32 + 7)) >> 5) == (8) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((8 * 32 + 16) & 31)) |
				    (1 << ((8 * 32 + 22) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (9) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((9 * 32 + 2) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (10) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (11) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   (0 | 0 | 0 | 0 |
				    (1 << ((11 * 32 + 23) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (12) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((12 * 32 + 17) & 31)) |
				    (1 << ((12 * 32 + 26) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (13) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (14) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (15) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (16) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   (0 | 0 | 0 | 0 |
				    (1 << ((16 * 32 + 29) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (17) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (18) &&
				  (1UL << (((16 * 32 + 7)) & 31) & (0))) ||
				 ((((16 * 32 + 7)) >> 5) == (19) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((19 * 32 + 4) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (20) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (21) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((int)(sizeof(
					 struct { int : (-!!(22 != 22)); }))) ||
				 ((int)(sizeof(
					 struct { int : (-!!(22 != 22)); })))) ?
			0 :
			(__builtin_constant_p((
				 __builtin_constant_p((
					 16 * 32 +
					 7)) && (((((16 * 32 + 7)) >> 5) ==
							  (0) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   ((1 << ((0 * 32 + 0) & 31)) |
						    (1 << ((0 * 32 + 3)) & 31) |
						    (1 << ((0 * 32 + 5) & 31)) |
						    (1 << ((0 * 32 + 6) & 31)) |
						    (1 << ((0 * 32 + 8) & 31)) |
						    (1 << ((0 * 32 + 13)) & 31) |
						    (1 << ((0 * 32 + 24) & 31)) |
						    (1 << ((0 * 32 + 15) & 31)) |
						    (1 << ((0 * 32 + 25) & 31)) |
						    (1 << ((0 * 32 + 26) &
							   31))))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (1) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   ((1 << ((1 * 32 + 29) & 31)) |
						    0))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (2) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (3) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   ((1 << ((3 * 32 + 20) &
							   31))))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (4) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   (0))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (5) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (6) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (7) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (8) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (9) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (10) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (11) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (12) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (13) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (14) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (15) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (16) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (17) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (18) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (19) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (20) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (21) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 }))) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 })))) ?
					 1 :
					 arch_test_bit(
						 (16 * 32 + 7),
						 (unsigned long
							  *)((&boot_cpu_data)
								     ->x86_capability)))) ?
				 (__builtin_constant_p((
					  16 * 32 +
					  7)) && (((((16 * 32 + 7)) >> 5) ==
							   (0) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    ((1 << ((0 * 32 + 0) & 31)) |
						     (1 << ((0 * 32 + 3)) & 31) |
						     (1 << ((0 * 32 + 5) & 31)) |
						     (1 << ((0 * 32 + 6) & 31)) |
						     (1 << ((0 * 32 + 8) & 31)) |
						     (1 << ((0 * 32 + 13)) & 31) |
						     (1
						      << ((0 * 32 + 24) & 31)) |
						     (1
						      << ((0 * 32 + 15) & 31)) |
						     (1
						      << ((0 * 32 + 25) & 31)) |
						     (1 << ((0 * 32 + 26) &
							    31))))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (1) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    ((1
						      << ((1 * 32 + 29) & 31)) |
						     0))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (2) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (3) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    ((1 << ((3 * 32 + 20) &
							    31))))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (4) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    (0))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (5) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (6) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (7) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (8) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (9) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (10) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (11) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (12) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (13) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (14) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (15) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (16) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (17) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (18) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (19) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (20) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (21) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((int)(sizeof(struct {
							  int : (-!!(22 != 22));
						  }))) ||
						  ((int)(sizeof(struct {
							  int : (-!!(22 != 22));
						  })))) ?
					  1 :
					  arch_test_bit(
						  (16 * 32 + 7),
						  (unsigned long
							   *)((&boot_cpu_data)
								      ->x86_capability))) :
				 _static_cpu_has((16 * 32 + 7)))) &&
	       (pte_flags(pte) &
		((((pteval_t)(1)) << 1) | (((pteval_t)(1)) << 6))) ==
		       (((pteval_t)(1)) << 6);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_young(pte_t pte)
{
	return pte_flags(pte) & (((pteval_t)(1)) << 5);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pte_decrypted(pte_t pte)
{
	return cc_mkdec(native_pte_val(pte)) == native_pte_val(pte);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pmd_dirty(pmd_t pmd)
{
	return pmd_flags(pmd) &
	       ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 58));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pmd_shstk(pmd_t pmd)
{
	return (__builtin_constant_p((16 * 32 + 7)) &&
				(((((16 * 32 + 7)) >> 5) == (0) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((0 * 32 + 1) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (1) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (2) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (3) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((3 * 32 + 2) & 31)) |
				    (1 << ((3 * 32 + 3) & 31)) |
				    (1 << ((3 * 32 + 1) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (4) &&
				  (1UL << (((16 * 32 + 7)) & 31) & (0))) ||
				 ((((16 * 32 + 7)) >> 5) == (5) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (6) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (7) &&
				  (1UL << (((16 * 32 + 7)) & 31) & (0))) ||
				 ((((16 * 32 + 7)) >> 5) == (8) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((8 * 32 + 16) & 31)) |
				    (1 << ((8 * 32 + 22) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (9) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((9 * 32 + 2) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (10) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (11) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   (0 | 0 | 0 | 0 |
				    (1 << ((11 * 32 + 23) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (12) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((12 * 32 + 17) & 31)) |
				    (1 << ((12 * 32 + 26) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (13) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (14) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (15) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (16) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   (0 | 0 | 0 | 0 |
				    (1 << ((16 * 32 + 29) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (17) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (18) &&
				  (1UL << (((16 * 32 + 7)) & 31) & (0))) ||
				 ((((16 * 32 + 7)) >> 5) == (19) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((19 * 32 + 4) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (20) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (21) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((int)(sizeof(
					 struct { int : (-!!(22 != 22)); }))) ||
				 ((int)(sizeof(
					 struct { int : (-!!(22 != 22)); })))) ?
			0 :
			(__builtin_constant_p((
				 __builtin_constant_p((
					 16 * 32 +
					 7)) && (((((16 * 32 + 7)) >> 5) ==
							  (0) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   ((1 << ((0 * 32 + 0) & 31)) |
						    (1 << ((0 * 32 + 3)) & 31) |
						    (1 << ((0 * 32 + 5) & 31)) |
						    (1 << ((0 * 32 + 6) & 31)) |
						    (1 << ((0 * 32 + 8) & 31)) |
						    (1 << ((0 * 32 + 13)) & 31) |
						    (1 << ((0 * 32 + 24) & 31)) |
						    (1 << ((0 * 32 + 15) & 31)) |
						    (1 << ((0 * 32 + 25) & 31)) |
						    (1 << ((0 * 32 + 26) &
							   31))))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (1) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   ((1 << ((1 * 32 + 29) & 31)) |
						    0))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (2) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (3) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   ((1 << ((3 * 32 + 20) &
							   31))))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (4) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   (0))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (5) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (6) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (7) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (8) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (9) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (10) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (11) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (12) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (13) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (14) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (15) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (16) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (17) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (18) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (19) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (20) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (21) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 }))) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 })))) ?
					 1 :
					 arch_test_bit(
						 (16 * 32 + 7),
						 (unsigned long
							  *)((&boot_cpu_data)
								     ->x86_capability)))) ?
				 (__builtin_constant_p((
					  16 * 32 +
					  7)) && (((((16 * 32 + 7)) >> 5) ==
							   (0) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    ((1 << ((0 * 32 + 0) & 31)) |
						     (1 << ((0 * 32 + 3)) & 31) |
						     (1 << ((0 * 32 + 5) & 31)) |
						     (1 << ((0 * 32 + 6) & 31)) |
						     (1 << ((0 * 32 + 8) & 31)) |
						     (1 << ((0 * 32 + 13)) & 31) |
						     (1
						      << ((0 * 32 + 24) & 31)) |
						     (1
						      << ((0 * 32 + 15) & 31)) |
						     (1
						      << ((0 * 32 + 25) & 31)) |
						     (1 << ((0 * 32 + 26) &
							    31))))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (1) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    ((1
						      << ((1 * 32 + 29) & 31)) |
						     0))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (2) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (3) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    ((1 << ((3 * 32 + 20) &
							    31))))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (4) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    (0))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (5) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (6) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (7) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (8) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (9) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (10) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (11) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (12) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (13) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (14) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (15) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (16) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (17) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (18) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (19) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (20) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (21) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((int)(sizeof(struct {
							  int : (-!!(22 != 22));
						  }))) ||
						  ((int)(sizeof(struct {
							  int : (-!!(22 != 22));
						  })))) ?
					  1 :
					  arch_test_bit(
						  (16 * 32 + 7),
						  (unsigned long
							   *)((&boot_cpu_data)
								      ->x86_capability))) :
				 _static_cpu_has((16 * 32 + 7)))) &&
	       (pmd_flags(pmd) &
		((((pteval_t)(1)) << 1) | (((pteval_t)(1)) << 6) |
		 (((pteval_t)(1)) << 7))) ==
		       ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 7));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pmd_young(pmd_t pmd)
{
	return pmd_flags(pmd) & (((pteval_t)(1)) << 5);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pud_dirty(pud_t pud)
{
	return pud_flags(pud) &
	       ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 58));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pud_young(pud_t pud)
{
	return pud_flags(pud) & (((pteval_t)(1)) << 5);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pud_shstk(pud_t pud)
{
	return (__builtin_constant_p((16 * 32 + 7)) &&
				(((((16 * 32 + 7)) >> 5) == (0) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((0 * 32 + 1) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (1) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (2) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (3) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((3 * 32 + 2) & 31)) |
				    (1 << ((3 * 32 + 3) & 31)) |
				    (1 << ((3 * 32 + 1) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (4) &&
				  (1UL << (((16 * 32 + 7)) & 31) & (0))) ||
				 ((((16 * 32 + 7)) >> 5) == (5) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (6) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (7) &&
				  (1UL << (((16 * 32 + 7)) & 31) & (0))) ||
				 ((((16 * 32 + 7)) >> 5) == (8) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((8 * 32 + 16) & 31)) |
				    (1 << ((8 * 32 + 22) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (9) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((9 * 32 + 2) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (10) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (11) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   (0 | 0 | 0 | 0 |
				    (1 << ((11 * 32 + 23) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (12) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((12 * 32 + 17) & 31)) |
				    (1 << ((12 * 32 + 26) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (13) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (14) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (15) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (16) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   (0 | 0 | 0 | 0 |
				    (1 << ((16 * 32 + 29) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (17) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (18) &&
				  (1UL << (((16 * 32 + 7)) & 31) & (0))) ||
				 ((((16 * 32 + 7)) >> 5) == (19) &&
				  (1UL << (((16 * 32 + 7)) & 31) &
				   ((1 << ((19 * 32 + 4) & 31))))) ||
				 ((((16 * 32 + 7)) >> 5) == (20) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((((16 * 32 + 7)) >> 5) == (21) &&
				  (1UL << (((16 * 32 + 7)) & 31) & 0)) ||
				 ((int)(sizeof(
					 struct { int : (-!!(22 != 22)); }))) ||
				 ((int)(sizeof(
					 struct { int : (-!!(22 != 22)); })))) ?
			0 :
			(__builtin_constant_p((
				 __builtin_constant_p((
					 16 * 32 +
					 7)) && (((((16 * 32 + 7)) >> 5) ==
							  (0) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   ((1 << ((0 * 32 + 0) & 31)) |
						    (1 << ((0 * 32 + 3)) & 31) |
						    (1 << ((0 * 32 + 5) & 31)) |
						    (1 << ((0 * 32 + 6) & 31)) |
						    (1 << ((0 * 32 + 8) & 31)) |
						    (1 << ((0 * 32 + 13)) & 31) |
						    (1 << ((0 * 32 + 24) & 31)) |
						    (1 << ((0 * 32 + 15) & 31)) |
						    (1 << ((0 * 32 + 25) & 31)) |
						    (1 << ((0 * 32 + 26) &
							   31))))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (1) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   ((1 << ((1 * 32 + 29) & 31)) |
						    0))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (2) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (3) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   ((1 << ((3 * 32 + 20) &
							   31))))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (4) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   (0))) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (5) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (6) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (7) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (8) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (9) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (10) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (11) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (12) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (13) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (14) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (15) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (16) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (17) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (18) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (19) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (20) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((((16 * 32 + 7)) >> 5) ==
							  (21) &&
						  (1UL << (((16 * 32 + 7)) & 31) &
						   0)) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 }))) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 })))) ?
					 1 :
					 arch_test_bit(
						 (16 * 32 + 7),
						 (unsigned long
							  *)((&boot_cpu_data)
								     ->x86_capability)))) ?
				 (__builtin_constant_p((
					  16 * 32 +
					  7)) && (((((16 * 32 + 7)) >> 5) ==
							   (0) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    ((1 << ((0 * 32 + 0) & 31)) |
						     (1 << ((0 * 32 + 3)) & 31) |
						     (1 << ((0 * 32 + 5) & 31)) |
						     (1 << ((0 * 32 + 6) & 31)) |
						     (1 << ((0 * 32 + 8) & 31)) |
						     (1 << ((0 * 32 + 13)) & 31) |
						     (1
						      << ((0 * 32 + 24) & 31)) |
						     (1
						      << ((0 * 32 + 15) & 31)) |
						     (1
						      << ((0 * 32 + 25) & 31)) |
						     (1 << ((0 * 32 + 26) &
							    31))))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (1) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    ((1
						      << ((1 * 32 + 29) & 31)) |
						     0))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (2) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (3) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    ((1 << ((3 * 32 + 20) &
							    31))))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (4) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    (0))) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (5) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (6) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (7) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (8) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (9) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (10) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (11) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (12) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (13) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (14) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (15) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (16) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (17) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (18) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (19) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (20) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 7)) >> 5) ==
							   (21) &&
						   (1UL << (((16 * 32 + 7)) &
							    31) &
						    0)) ||
						  ((int)(sizeof(struct {
							  int : (-!!(22 != 22));
						  }))) ||
						  ((int)(sizeof(struct {
							  int : (-!!(22 != 22));
						  })))) ?
					  1 :
					  arch_test_bit(
						  (16 * 32 + 7),
						  (unsigned long
							   *)((&boot_cpu_data)
								      ->x86_capability))) :
				 _static_cpu_has((16 * 32 + 7)))) &&
	       (pud_flags(pud) &
		((((pteval_t)(1)) << 1) | (((pteval_t)(1)) << 6) |
		 (((pteval_t)(1)) << 7))) ==
		       ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 7));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_write(pte_t pte)
{
	return (pte_flags(pte) & (((pteval_t)(1)) << 1)) || pte_shstk(pte);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pmd_write(pmd_t pmd)
{
	return (pmd_flags(pmd) & (((pteval_t)(1)) << 1)) || pmd_shstk(pmd);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pud_write(pud_t pud)
{
	return pud_flags(pud) & (((pteval_t)(1)) << 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_huge(pte_t pte)
{
	return pte_flags(pte) & (((pteval_t)(1)) << 7);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_global(pte_t pte)
{
	return pte_flags(pte) & (((pteval_t)(1)) << 8);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_exec(pte_t pte)
{
	return !(pte_flags(pte) & (((pteval_t)(1)) << 63));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_special(pte_t pte)
{
	return pte_flags(pte) & (((pteval_t)(1)) << 9);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
protnone_mask(u64 val);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
pte_pfn(pte_t pte)
{
	phys_addr_t pfn = native_pte_val(pte);
	pfn ^= protnone_mask(pfn);
	return (pfn & ((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
				  ((phys_addr_t)((1ULL << 52) - 1))))) >>
	       12;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
pmd_pfn(pmd_t pmd)
{
	phys_addr_t pfn = native_pmd_val(pmd);
	pfn ^= protnone_mask(pfn);
	return (pfn & pmd_pfn_mask(pmd)) >> 12;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
pud_pfn(pud_t pud)
{
	phys_addr_t pfn = native_pud_val(pud);
	pfn ^= protnone_mask(pfn);
	return (pfn & pud_pfn_mask(pud)) >> 12;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
p4d_pfn(p4d_t p4d)
{
	return (native_p4d_val(p4d) & p4d_pfn_mask(p4d)) >> 12;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
pgd_pfn(pgd_t pgd)
{
	return (native_pgd_val(pgd) &
		((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			    ((phys_addr_t)((1ULL << 52) - 1))))) >>
	       12;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
p4d_leaf(p4d_t p4d)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pmd_leaf(pmd_t pte)
{
	return pmd_flags(pte) & (((pteval_t)(1)) << 7);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_clear_flags(pte_t pte, pteval_t clear)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v & ~clear);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgprotval_t
mksaveddirty_shift(pgprotval_t v)
{
	pgprotval_t cond = (~v >> 1) & 1;

	v |= ((v >> 6) & cond) << 58;
	v &= ~(cond << 6);

	return v;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgprotval_t
clear_saveddirty_shift(pgprotval_t v)
{
	pgprotval_t cond = (v >> 1) & 1;

	v |= ((v >> 58) & cond) << 6;
	v &= ~(cond << 58);

	return v;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mksaveddirty(pte_t pte)
{
	pteval_t v = native_pte_val(pte);

	v = mksaveddirty_shift(v);
	return native_make_pte(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_clear_saveddirty(pte_t pte)
{
	pteval_t v = native_pte_val(pte);

	v = clear_saveddirty_shift(v);
	return native_make_pte(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_wrprotect(pte_t pte)
{
	pte = pte_clear_flags(pte, (((pteval_t)(1)) << 1));

	return pte_mksaveddirty(pte);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mkclean(pte_t pte)
{
	return pte_clear_flags(pte, ((((pteval_t)(1)) << 6) |
				     (((pteval_t)(1)) << 58)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mkold(pte_t pte)
{
	return pte_clear_flags(pte, (((pteval_t)(1)) << 5));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mkexec(pte_t pte)
{
	return pte_clear_flags(pte, (((pteval_t)(1)) << 63));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mkdirty(pte_t pte)
{
	pte = pte_set_flags(pte, (((pteval_t)(1)) << 6) | (((pteval_t)(0))));

	return pte_mksaveddirty(pte);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mkwrite_shstk(pte_t pte)
{
	pte = pte_clear_flags(pte, (((pteval_t)(1)) << 1));

	return pte_set_flags(pte, (((pteval_t)(1)) << 6));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mkyoung(pte_t pte)
{
	return pte_set_flags(pte, (((pteval_t)(1)) << 5));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mkwrite_novma(pte_t pte)
{
	return pte_set_flags(pte, (((pteval_t)(1)) << 1));
}

struct vm_area_struct;
pte_t pte_mkwrite(pte_t pte, struct vm_area_struct *vma);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mkhuge(pte_t pte)
{
	return pte_set_flags(pte, (((pteval_t)(1)) << 7));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_clrhuge(pte_t pte)
{
	return pte_clear_flags(pte, (((pteval_t)(1)) << 7));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mkglobal(pte_t pte)
{
	return pte_set_flags(pte, (((pteval_t)(1)) << 8));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_clrglobal(pte_t pte)
{
	return pte_clear_flags(pte, (((pteval_t)(1)) << 8));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mkspecial(pte_t pte)
{
	return pte_set_flags(pte, (((pteval_t)(1)) << 9));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mkdevmap(pte_t pte)
{
	return pte_set_flags(pte, (((pteval_t)(1)) << 9) | (((u64)(1)) << 57));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_mksaveddirty(pmd_t pmd)
{
	pmdval_t v = native_pmd_val(pmd);

	v = mksaveddirty_shift(v);
	return native_make_pmd(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_clear_saveddirty(pmd_t pmd)
{
	pmdval_t v = native_pmd_val(pmd);

	v = clear_saveddirty_shift(v);
	return native_make_pmd(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_wrprotect(pmd_t pmd)
{
	pmd = pmd_clear_flags(pmd, (((pteval_t)(1)) << 1));

	return pmd_mksaveddirty(pmd);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_mkold(pmd_t pmd)
{
	return pmd_clear_flags(pmd, (((pteval_t)(1)) << 5));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_mkclean(pmd_t pmd)
{
	return pmd_clear_flags(pmd, ((((pteval_t)(1)) << 6) |
				     (((pteval_t)(1)) << 58)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_mkdirty(pmd_t pmd)
{
	pmd = pmd_set_flags(pmd, (((pteval_t)(1)) << 6) | (((pteval_t)(0))));

	return pmd_mksaveddirty(pmd);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_mkwrite_shstk(pmd_t pmd)
{
	pmd = pmd_clear_flags(pmd, (((pteval_t)(1)) << 1));

	return pmd_set_flags(pmd, (((pteval_t)(1)) << 6));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_mkdevmap(pmd_t pmd)
{
	return pmd_set_flags(pmd, (((u64)(1)) << 57));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_mkhuge(pmd_t pmd)
{
	return pmd_set_flags(pmd, (((pteval_t)(1)) << 7));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_mkyoung(pmd_t pmd)
{
	return pmd_set_flags(pmd, (((pteval_t)(1)) << 5));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_mkwrite_novma(pmd_t pmd)
{
	return pmd_set_flags(pmd, (((pteval_t)(1)) << 1));
}

pmd_t pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_mksaveddirty(pud_t pud)
{
	pudval_t v = native_pud_val(pud);

	v = mksaveddirty_shift(v);
	return native_make_pud(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_clear_saveddirty(pud_t pud)
{
	pudval_t v = native_pud_val(pud);

	v = clear_saveddirty_shift(v);
	return native_make_pud(v);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_mkold(pud_t pud)
{
	return pud_clear_flags(pud, (((pteval_t)(1)) << 5));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_mkclean(pud_t pud)
{
	return pud_clear_flags(pud, ((((pteval_t)(1)) << 6) |
				     (((pteval_t)(1)) << 58)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_wrprotect(pud_t pud)
{
	pud = pud_clear_flags(pud, (((pteval_t)(1)) << 1));

	return pud_mksaveddirty(pud);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_mkdirty(pud_t pud)
{
	pud = pud_set_flags(pud, (((pteval_t)(1)) << 6) | (((pteval_t)(0))));

	return pud_mksaveddirty(pud);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_mkdevmap(pud_t pud)
{
	return pud_set_flags(pud, (((u64)(1)) << 57));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_mkhuge(pud_t pud)
{
	return pud_set_flags(pud, (((pteval_t)(1)) << 7));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_mkyoung(pud_t pud)
{
	return pud_set_flags(pud, (((pteval_t)(1)) << 5));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_mkwrite(pud_t pud)
{
	pud = pud_set_flags(pud, (((pteval_t)(1)) << 1));

	return pud_clear_saveddirty(pud);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_soft_dirty(pte_t pte)
{
	return pte_flags(pte) & (((pteval_t)(0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pmd_soft_dirty(pmd_t pmd)
{
	return pmd_flags(pmd) & (((pteval_t)(0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pud_soft_dirty(pud_t pud)
{
	return pud_flags(pud) & (((pteval_t)(0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_mksoft_dirty(pte_t pte)
{
	return pte_set_flags(pte, (((pteval_t)(0))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_mksoft_dirty(pmd_t pmd)
{
	return pmd_set_flags(pmd, (((pteval_t)(0))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_mksoft_dirty(pud_t pud)
{
	return pud_set_flags(pud, (((pteval_t)(0))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_clear_soft_dirty(pte_t pte)
{
	return pte_clear_flags(pte, (((pteval_t)(0))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_clear_soft_dirty(pmd_t pmd)
{
	return pmd_clear_flags(pmd, (((pteval_t)(0))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_clear_soft_dirty(pud_t pud)
{
	return pud_clear_flags(pud, (((pteval_t)(0))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgprotval_t
massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = ((pgprot).pgprot);

	if (protval & (((pteval_t)(1)) << 0))
		protval &= __supported_pte_mask;

	return protval;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgprotval_t
check_pgprot(pgprot_t pgprot)
{
	pgprotval_t massaged_val = massage_pgprot(pgprot);
	return massaged_val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	phys_addr_t pfn = (phys_addr_t)page_nr << 12;
	pfn ^= protnone_mask(((pgprot).pgprot));
	pfn &= ((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			   ((phys_addr_t)((1ULL << 52) - 1))));
	return native_make_pte(pfn | check_pgprot(pgprot));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pfn_pmd(unsigned long page_nr, pgprot_t pgprot)
{
	phys_addr_t pfn = (phys_addr_t)page_nr << 12;
	pfn ^= protnone_mask(((pgprot).pgprot));
	pfn &= (((signed long)(~(((1UL) << 21) - 1))) &
		((phys_addr_t)((1ULL << 52) - 1)));
	return native_make_pmd(pfn | check_pgprot(pgprot));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pfn_pud(unsigned long page_nr, pgprot_t pgprot)
{
	phys_addr_t pfn = (phys_addr_t)page_nr << 12;
	pfn ^= protnone_mask(((pgprot).pgprot));
	pfn &= (((signed long)(~(((1UL) << 30) - 1))) &
		((phys_addr_t)((1ULL << 52) - 1)));
	return native_make_pud(pfn | check_pgprot(pgprot));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_mkinvalid(pmd_t pmd)
{
	return pfn_pmd(
		pmd_pfn(pmd),
		((pgprot_t){ (pmd_flags(pmd) & ~((((pteval_t)(1)) << 0) |
						 (((pteval_t)(1)) << 8))) }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_mkinvalid(pud_t pud)
{
	return pfn_pud(
		pud_pfn(pud),
		((pgprot_t){ (pud_flags(pud) & ~((((pteval_t)(1)) << 0) |
						 (((pteval_t)(1)) << 8))) }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
flip_protnone_guard(u64 oldval, u64 val, u64 mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_modify(pte_t pte, pgprot_t newprot)
{
	pteval_t val = native_pte_val(pte), oldval = val;
	pte_t pte_result;

	val &= ((((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			     ((phys_addr_t)((1ULL << 52) - 1)))) |
		 (((pteval_t)(1)) << 4) | (((pteval_t)(1)) << 3) |
		 (((pteval_t)(1)) << 9) | (((pteval_t)(1)) << 5) |
		 ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 58)) |
		 (((pteval_t)(0))) | (((u64)(1)) << 57) |
		 (((pteval_t)(cc_mask))) | (((pteval_t)(0)))) |
		(((pteval_t)(1)) << 7));
	val |= check_pgprot(newprot) &
	       ~((((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			      ((phys_addr_t)((1ULL << 52) - 1)))) |
		  (((pteval_t)(1)) << 4) | (((pteval_t)(1)) << 3) |
		  (((pteval_t)(1)) << 9) | (((pteval_t)(1)) << 5) |
		  ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 58)) |
		  (((pteval_t)(0))) | (((u64)(1)) << 57) |
		  (((pteval_t)(cc_mask))) | (((pteval_t)(0)))) |
		 (((pteval_t)(1)) << 7));
	val = flip_protnone_guard(
		oldval, val,
		((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			    ((phys_addr_t)((1ULL << 52) - 1)))));

	pte_result = native_make_pte(val);
	if (oldval & (((pteval_t)(1)) << 1))
		pte_result = pte_mksaveddirty(pte_result);
	else
		pte_result = pte_clear_saveddirty(pte_result);

	return pte_result;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmd_modify(pmd_t pmd, pgprot_t newprot)
{
	pmdval_t val = native_pmd_val(pmd), oldval = val;
	pmd_t pmd_result;

	val &= (((((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			      ((phys_addr_t)((1ULL << 52) - 1)))) |
		  (((pteval_t)(1)) << 4) | (((pteval_t)(1)) << 3) |
		  (((pteval_t)(1)) << 9) | (((pteval_t)(1)) << 5) |
		  ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 58)) |
		  (((pteval_t)(0))) | (((u64)(1)) << 57) |
		  (((pteval_t)(cc_mask))) | (((pteval_t)(0)))) |
		 (((pteval_t)(1)) << 7) | (((pteval_t)(1)) << 12)) &
		~(((pteval_t)(1)) << 6));
	val |= check_pgprot(newprot) &
	       ~((((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			      ((phys_addr_t)((1ULL << 52) - 1)))) |
		  (((pteval_t)(1)) << 4) | (((pteval_t)(1)) << 3) |
		  (((pteval_t)(1)) << 9) | (((pteval_t)(1)) << 5) |
		  ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 58)) |
		  (((pteval_t)(0))) | (((u64)(1)) << 57) |
		  (((pteval_t)(cc_mask))) | (((pteval_t)(0)))) |
		 (((pteval_t)(1)) << 7) | (((pteval_t)(1)) << 12));
	val = flip_protnone_guard(oldval, val,
				  (((signed long)(~(((1UL) << 21) - 1))) &
				   ((phys_addr_t)((1ULL << 52) - 1))));

	pmd_result = native_make_pmd(val);

	if (oldval & (((pteval_t)(1)) << 1))
		pmd_result = pmd_mksaveddirty(pmd_result);
	else
		pmd_result = pmd_clear_saveddirty(pmd_result);

	return pmd_result;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pud_modify(pud_t pud, pgprot_t newprot)
{
	pudval_t val = native_pud_val(pud), oldval = val;
	pud_t pud_result;

	val &= ((((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			     ((phys_addr_t)((1ULL << 52) - 1)))) |
		 (((pteval_t)(1)) << 4) | (((pteval_t)(1)) << 3) |
		 (((pteval_t)(1)) << 9) | (((pteval_t)(1)) << 5) |
		 ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 58)) |
		 (((pteval_t)(0))) | (((u64)(1)) << 57) |
		 (((pteval_t)(cc_mask))) | (((pteval_t)(0)))) |
		(((pteval_t)(1)) << 7) | (((pteval_t)(1)) << 12));
	val |= check_pgprot(newprot) &
	       ~((((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			      ((phys_addr_t)((1ULL << 52) - 1)))) |
		  (((pteval_t)(1)) << 4) | (((pteval_t)(1)) << 3) |
		  (((pteval_t)(1)) << 9) | (((pteval_t)(1)) << 5) |
		  ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 58)) |
		  (((pteval_t)(0))) | (((u64)(1)) << 57) |
		  (((pteval_t)(cc_mask))) | (((pteval_t)(0)))) |
		 (((pteval_t)(1)) << 7) | (((pteval_t)(1)) << 12));
	val = flip_protnone_guard(oldval, val,
				  (((signed long)(~(((1UL) << 30) - 1))) &
				   ((phys_addr_t)((1ULL << 52) - 1))));

	pud_result = native_make_pud(val);

	if (oldval & (((pteval_t)(1)) << 1))
		pud_result = pud_mksaveddirty(pud_result);
	else
		pud_result = pud_clear_saveddirty(pud_result);

	return pud_result;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgprot_t
pgprot_modify(pgprot_t oldprot, pgprot_t newprot)
{
	pgprotval_t preservebits =
		((oldprot).pgprot) &
		((((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			      ((phys_addr_t)((1ULL << 52) - 1)))) |
		  (((pteval_t)(1)) << 4) | (((pteval_t)(1)) << 3) |
		  (((pteval_t)(1)) << 9) | (((pteval_t)(1)) << 5) |
		  ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 58)) |
		  (((pteval_t)(0))) | (((u64)(1)) << 57) |
		  (((pteval_t)(cc_mask))) | (((pteval_t)(0)))) |
		 (((pteval_t)(1)) << 7));
	pgprotval_t addbits =
		((newprot).pgprot) &
		~((((pteval_t)(((signed long)(~(((1UL) << 12) - 1))) &
			       ((phys_addr_t)((1ULL << 52) - 1)))) |
		   (((pteval_t)(1)) << 4) | (((pteval_t)(1)) << 3) |
		   (((pteval_t)(1)) << 9) | (((pteval_t)(1)) << 5) |
		   ((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 58)) |
		   (((pteval_t)(0))) | (((u64)(1)) << 57) |
		   (((pteval_t)(cc_mask))) | (((pteval_t)(0)))) |
		  (((pteval_t)(1)) << 7));
	return ((pgprot_t){ (preservebits | addbits) });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
is_new_memtype_allowed(u64 paddr, unsigned long size, enum page_cache_mode pcm,
		       enum page_cache_mode new_pcm)
{
	if (x86_platform.is_untracked_pat_range(paddr, paddr + size))
		return 1;
	if ((pcm == _PAGE_CACHE_MODE_UC_MINUS &&
	     new_pcm == _PAGE_CACHE_MODE_WB) ||
	    (pcm == _PAGE_CACHE_MODE_WC && new_pcm == _PAGE_CACHE_MODE_WB) ||
	    (pcm == _PAGE_CACHE_MODE_WT && new_pcm == _PAGE_CACHE_MODE_WB) ||
	    (pcm == _PAGE_CACHE_MODE_WT && new_pcm == _PAGE_CACHE_MODE_WC)) {
		return 0;
	}

	return 1;
}

pmd_t *populate_extra_pmd(unsigned long vaddr);
pte_t *populate_extra_pte(unsigned long vaddr);

pgd_t __pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgd_t
pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd)
{
	if (!(__builtin_constant_p((
		      __builtin_constant_p((7 * 32 + 11)) &&
				      (((((7 * 32 + 11)) >> 5) == (0) &&
					(1UL << (((7 * 32 + 11)) & 31) &
					 ((1 << ((0 * 32 + 0) & 31)) |
					  (1 << ((0 * 32 + 3)) & 31) |
					  (1 << ((0 * 32 + 5) & 31)) |
					  (1 << ((0 * 32 + 6) & 31)) |
					  (1 << ((0 * 32 + 8) & 31)) |
					  (1 << ((0 * 32 + 13)) & 31) |
					  (1 << ((0 * 32 + 24) & 31)) |
					  (1 << ((0 * 32 + 15) & 31)) |
					  (1 << ((0 * 32 + 25) & 31)) |
					  (1 << ((0 * 32 + 26) & 31))))) ||
				       ((((7 * 32 + 11)) >> 5) == (1) &&
					(1UL << (((7 * 32 + 11)) & 31) &
					 ((1 << ((1 * 32 + 29) & 31)) | 0))) ||
				       ((((7 * 32 + 11)) >> 5) == (2) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (3) &&
					(1UL << (((7 * 32 + 11)) & 31) &
					 ((1 << ((3 * 32 + 20) & 31))))) ||
				       ((((7 * 32 + 11)) >> 5) == (4) &&
					(1UL << (((7 * 32 + 11)) & 31) & (0))) ||
				       ((((7 * 32 + 11)) >> 5) == (5) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (6) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (7) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (8) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (9) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (10) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (11) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (12) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (13) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (14) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (15) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (16) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (17) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (18) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (19) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (20) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (21) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((int)(sizeof(struct {
					       int : (-!!(22 != 22));
				       }))) ||
				       ((int)(sizeof(struct {
					       int : (-!!(22 != 22));
				       })))) ?
			      1 :
			      arch_test_bit(
				      (7 * 32 + 11),
				      (unsigned long
					       *)((&boot_cpu_data)
							  ->x86_capability)))) ?
		      (__builtin_constant_p((7 * 32 + 11)) &&
				       (((((7 * 32 + 11)) >> 5) == (0) &&
					 (1UL << (((7 * 32 + 11)) & 31) &
					  ((1 << ((0 * 32 + 0) & 31)) |
					   (1 << ((0 * 32 + 3)) & 31) |
					   (1 << ((0 * 32 + 5) & 31)) |
					   (1 << ((0 * 32 + 6) & 31)) |
					   (1 << ((0 * 32 + 8) & 31)) |
					   (1 << ((0 * 32 + 13)) & 31) |
					   (1 << ((0 * 32 + 24) & 31)) |
					   (1 << ((0 * 32 + 15) & 31)) |
					   (1 << ((0 * 32 + 25) & 31)) |
					   (1 << ((0 * 32 + 26) & 31))))) ||
					((((7 * 32 + 11)) >> 5) == (1) &&
					 (1UL << (((7 * 32 + 11)) & 31) &
					  ((1 << ((1 * 32 + 29) & 31)) | 0))) ||
					((((7 * 32 + 11)) >> 5) == (2) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (3) &&
					 (1UL << (((7 * 32 + 11)) & 31) &
					  ((1 << ((3 * 32 + 20) & 31))))) ||
					((((7 * 32 + 11)) >> 5) == (4) &&
					 (1UL << (((7 * 32 + 11)) & 31) &
					  (0))) ||
					((((7 * 32 + 11)) >> 5) == (5) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (6) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (7) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (8) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (9) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (10) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (11) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (12) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (13) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (14) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (15) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (16) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (17) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (18) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (19) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (20) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (21) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((int)(sizeof(struct {
						int : (-!!(22 != 22));
					}))) ||
					((int)(sizeof(struct {
						int : (-!!(22 != 22));
					})))) ?
			       1 :
			       arch_test_bit(
				       (7 * 32 + 11),
				       (unsigned long
						*)((&boot_cpu_data)
							   ->x86_capability))) :
		      _static_cpu_has((7 * 32 + 11))))
		return pgd;
	return __pti_set_user_pgtbl(pgdp, pgd);
}

enum vsyscall_num {
	__NR_vgettimeofday,
	__NR_vtime,
	__NR_vgetcpu,
};
enum fixed_addresses {

	VSYSCALL_PAGE = (((((((-10UL << 20) + ((1UL) << 12)) - 1) |
			    ((__typeof__((-10UL << 20) +
					 ((1UL) << 12)))((1 << 21) - 1))) +
			   1) -
			  ((1UL) << 12)) -
			 (-10UL << 20)) >>
			12,

	FIX_DBGP_BASE,
	FIX_EARLYCON_MEM_BASE,

	FIX_OHCI1394_BASE,

	FIX_APIC_BASE,

	FIX_IO_APIC_BASE_0,
	FIX_IO_APIC_BASE_END = FIX_IO_APIC_BASE_0 + 128 - 1,
	__end_of_permanent_fixed_addresses,
	FIX_BTMAP_END = (__end_of_permanent_fixed_addresses ^
			 (__end_of_permanent_fixed_addresses + (64 * 8) - 1)) &
					-512 ?
				__end_of_permanent_fixed_addresses + (64 * 8) -
					(__end_of_permanent_fixed_addresses &
					 ((64 * 8) - 1)) :
				__end_of_permanent_fixed_addresses,
	FIX_BTMAP_BEGIN = FIX_BTMAP_END + (64 * 8) - 1,

	__end_of_fixed_addresses
};

extern void reserve_top_address(unsigned long reserve);

extern int fixmaps_set;

extern pte_t *pkmap_page_table;

void __native_set_fixmap(enum fixed_addresses idx, pte_t pte);
void native_set_fixmap(unsigned idx, phys_addr_t phys, pgprot_t flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__set_fixmap(enum fixed_addresses idx, phys_addr_t phys, pgprot_t flags)
{
	native_set_fixmap(idx, phys, flags);
}
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) *
	early_memremap_encrypted(resource_size_t phys_addr, unsigned long size);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) *
	early_memremap_encrypted_wp(resource_size_t phys_addr,
				    unsigned long size);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) *
	early_memremap_decrypted(resource_size_t phys_addr, unsigned long size);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) *
	early_memremap_decrypted_wp(resource_size_t phys_addr,
				    unsigned long size);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
fix_to_virt(const unsigned int idx)
{
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_469(void) __attribute__((
			__error__("BUILD_BUG_ON failed: "
				  "idx >= __end_of_fixed_addresses")));
		if (!(!(idx >= __end_of_fixed_addresses)))
			__compiletime_assert_469();
	} while (0);
	return (((((((-10UL << 20) + ((1UL) << 12)) - 1) |
		   ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1 << 21) -
								1))) +
		  1) -
		 ((1UL) << 12)) -
		((idx) << 12));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
virt_to_fix(const unsigned long vaddr)
{
	do {
		if (__builtin_expect(
			    !!(vaddr >=
				       ((((((-10UL << 20) + ((1UL) << 12)) - 1) |
					  ((__typeof__((-10UL << 20) +
						       ((1UL) << 12)))((1 << 21) -
								       1))) +
					 1) -
					((1UL) << 12)) ||
			       vaddr < (((((((-10UL << 20) + ((1UL) << 12)) - 1) |
					   ((__typeof__((-10UL << 20) +
							((1UL)
							 << 12)))((1 << 21) -
								  1))) +
					  1) -
					 ((1UL) << 12)) -
					(__end_of_permanent_fixed_addresses
					 << 12))),
			    0))
			do {
				({
					asm volatile(
						"470"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"470"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(470));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						""
						:
						: "i"("include/asm-generic/fixmap.h"),
						  "i"(38), "i"(0),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				__builtin_unreachable();
			} while (0);
	} while (0);
	return ((((((((-10UL << 20) + ((1UL) << 12)) - 1) |
		    ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1 << 21) -
								 1))) +
		   1) -
		  ((1UL) << 12)) -
		 ((vaddr) & (~(((1UL) << 12) - 1)))) >>
		12);
}

void __early_set_fixmap(enum fixed_addresses idx, phys_addr_t phys,
			pgprot_t flags);

extern p4d_t level4_kernel_pgt[512];
extern p4d_t level4_ident_pgt[512];
extern pud_t level3_kernel_pgt[512];
extern pud_t level3_ident_pgt[512];
extern pmd_t level2_kernel_pgt[512];
extern pmd_t level2_fixmap_pgt[512];
extern pmd_t level2_ident_pgt[512];
extern pte_t level1_fixmap_pgt[512 * 2];
extern pgd_t init_top_pgt[];

extern void paging_init(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sync_initial_page_table(void)
{
}
struct mm_struct;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mm_p4d_folded(struct mm_struct *mm)
{
	return !(
		__builtin_constant_p((16 * 32 + 16)) &&
				(((((16 * 32 + 16)) >> 5) == (0) &&
				  (1UL << (((16 * 32 + 16)) & 31) &
				   ((1 << ((0 * 32 + 1) & 31))))) ||
				 ((((16 * 32 + 16)) >> 5) == (1) &&
				  (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				 ((((16 * 32 + 16)) >> 5) == (2) &&
				  (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				 ((((16 * 32 + 16)) >> 5) == (3) &&
				  (1UL << (((16 * 32 + 16)) & 31) &
				   ((1 << ((3 * 32 + 2) & 31)) |
				    (1 << ((3 * 32 + 3) & 31)) |
				    (1 << ((3 * 32 + 1) & 31))))) ||
				 ((((16 * 32 + 16)) >> 5) == (4) &&
				  (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
				 ((((16 * 32 + 16)) >> 5) == (5) &&
				  (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				 ((((16 * 32 + 16)) >> 5) == (6) &&
				  (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				 ((((16 * 32 + 16)) >> 5) == (7) &&
				  (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
				 ((((16 * 32 + 16)) >> 5) == (8) &&
				  (1UL << (((16 * 32 + 16)) & 31) &
				   ((1 << ((8 * 32 + 16) & 31)) |
				    (1 << ((8 * 32 + 22) & 31))))) ||
				 ((((16 * 32 + 16)) >> 5) == (9) &&
				  (1UL << (((16 * 32 + 16)) & 31) &
				   ((1 << ((9 * 32 + 2) & 31))))) ||
				 ((((16 * 32 + 16)) >> 5) == (10) &&
				  (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				 ((((16 * 32 + 16)) >> 5) == (11) &&
				  (1UL << (((16 * 32 + 16)) & 31) &
				   (0 | 0 | 0 | 0 |
				    (1 << ((11 * 32 + 23) & 31))))) ||
				 ((((16 * 32 + 16)) >> 5) == (12) &&
				  (1UL << (((16 * 32 + 16)) & 31) &
				   ((1 << ((12 * 32 + 17) & 31)) |
				    (1 << ((12 * 32 + 26) & 31))))) ||
				 ((((16 * 32 + 16)) >> 5) == (13) &&
				  (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				 ((((16 * 32 + 16)) >> 5) == (14) &&
				  (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				 ((((16 * 32 + 16)) >> 5) == (15) &&
				  (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				 ((((16 * 32 + 16)) >> 5) == (16) &&
				  (1UL << (((16 * 32 + 16)) & 31) &
				   (0 | 0 | 0 | 0 |
				    (1 << ((16 * 32 + 29) & 31))))) ||
				 ((((16 * 32 + 16)) >> 5) == (17) &&
				  (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				 ((((16 * 32 + 16)) >> 5) == (18) &&
				  (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
				 ((((16 * 32 + 16)) >> 5) == (19) &&
				  (1UL << (((16 * 32 + 16)) & 31) &
				   ((1 << ((19 * 32 + 4) & 31))))) ||
				 ((((16 * 32 + 16)) >> 5) == (20) &&
				  (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				 ((((16 * 32 + 16)) >> 5) == (21) &&
				  (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
				 ((int)(sizeof(
					 struct { int : (-!!(22 != 22)); }))) ||
				 ((int)(sizeof(
					 struct { int : (-!!(22 != 22)); })))) ?
			0 :
			(__builtin_constant_p((
				 __builtin_constant_p((16 * 32 + 16)) &&
						 (((((16 * 32 + 16)) >> 5) ==
							   (0) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    ((1 << ((0 * 32 + 0) & 31)) |
						     (1 << ((0 * 32 + 3)) & 31) |
						     (1 << ((0 * 32 + 5) & 31)) |
						     (1 << ((0 * 32 + 6) & 31)) |
						     (1 << ((0 * 32 + 8) & 31)) |
						     (1 << ((0 * 32 + 13)) & 31) |
						     (1 << ((0 * 32 + 24) & 31)) |
						     (1 << ((0 * 32 + 15) & 31)) |
						     (1 << ((0 * 32 + 25) & 31)) |
						     (1 << ((0 * 32 + 26) &
							    31))))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (1) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    ((1 << ((1 * 32 + 29) & 31)) |
						     0))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (2) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (3) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    ((1 << ((3 * 32 + 20) &
							    31))))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (4) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    (0))) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (5) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (6) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (7) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (8) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (9) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (10) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (11) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (12) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (13) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (14) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (15) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (16) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (17) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (18) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (19) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (20) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((((16 * 32 + 16)) >> 5) ==
							   (21) &&
						   (1UL << (((16 * 32 + 16)) &
							    31) &
						    0)) ||
						  ((int)(sizeof(struct {
							  int : (-!!(22 != 22));
						  }))) ||
						  ((int)(sizeof(struct {
							  int : (-!!(22 != 22));
						  })))) ?
					 1 :
					 arch_test_bit(
						 (16 * 32 + 16),
						 (unsigned long
							  *)((&boot_cpu_data)
								     ->x86_capability)))) ?
				 (__builtin_constant_p((16 * 32 + 16)) &&
						  (((((16 * 32 + 16)) >> 5) ==
							    (0) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     ((1
						       << ((0 * 32 + 0) & 31)) |
						      (1 << ((0 * 32 + 3)) & 31) |
						      (1
						       << ((0 * 32 + 5) & 31)) |
						      (1
						       << ((0 * 32 + 6) & 31)) |
						      (1
						       << ((0 * 32 + 8) & 31)) |
						      (1 << ((0 * 32 + 13)) &
						       31) |
						      (1 << ((0 * 32 + 24) &
							     31)) |
						      (1 << ((0 * 32 + 15) &
							     31)) |
						      (1 << ((0 * 32 + 25) &
							     31)) |
						      (1 << ((0 * 32 + 26) &
							     31))))) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (1) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     ((1 << ((1 * 32 + 29) &
							     31)) |
						      0))) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (2) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (3) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     ((1 << ((3 * 32 + 20) &
							     31))))) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (4) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     (0))) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (5) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (6) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (7) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (8) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (9) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (10) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (11) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (12) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (13) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (14) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (15) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (16) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (17) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (18) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (19) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (20) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((((16 * 32 + 16)) >> 5) ==
							    (21) &&
						    (1UL << (((16 * 32 + 16)) &
							     31) &
						     0)) ||
						   ((int)(sizeof(struct {
							   int : (-!!(22 !=
								      22));
						   }))) ||
						   ((int)(sizeof(struct {
							   int : (-!!(22 !=
								      22));
						   })))) ?
					  1 :
					  arch_test_bit(
						  (16 * 32 + 16),
						  (unsigned long
							   *)((&boot_cpu_data)
								      ->x86_capability))) :
				 _static_cpu_has((16 * 32 + 16))));
}

void set_pte_vaddr_p4d(p4d_t *p4d_page, unsigned long vaddr, pte_t new_pte);
void set_pte_vaddr_pud(pud_t *pud_page, unsigned long vaddr, pte_t new_pte);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_set_pte(pte_t *ptep, pte_t pte)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_471(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*ptep) == sizeof(char) ||
			       sizeof(*ptep) == sizeof(short) ||
			       sizeof(*ptep) == sizeof(int) ||
			       sizeof(*ptep) == sizeof(long)) ||
			      sizeof(*ptep) == sizeof(long long)))
				__compiletime_assert_471();
		} while (0);
		do {
			*(volatile typeof(*ptep) *)&(*ptep) = (pte);
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_pte_clear(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
{
	native_set_pte(ptep, native_make_pte(0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_set_pte_atomic(pte_t *ptep, pte_t pte)
{
	native_set_pte(ptep, pte);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_set_pmd(pmd_t *pmdp, pmd_t pmd)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_472(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*pmdp) == sizeof(char) ||
			       sizeof(*pmdp) == sizeof(short) ||
			       sizeof(*pmdp) == sizeof(int) ||
			       sizeof(*pmdp) == sizeof(long)) ||
			      sizeof(*pmdp) == sizeof(long long)))
				__compiletime_assert_472();
		} while (0);
		do {
			*(volatile typeof(*pmdp) *)&(*pmdp) = (pmd);
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_pmd_clear(pmd_t *pmd)
{
	native_set_pmd(pmd, native_make_pmd(0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
native_ptep_get_and_clear(pte_t *xp)
{
	return native_make_pte(({
		typeof(&xp->pte) __ai_ptr = (&xp->pte);
		do {
		} while (0);
		instrument_atomic_read_write(__ai_ptr, sizeof(*__ai_ptr));
		({
			__typeof__(*((__ai_ptr))) __ret = ((0));
			switch (sizeof(*((__ai_ptr)))) {
			case 1:
				asm volatile(""
					     "xchg"
					     "b %b0, %1\n"
					     : "+q"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 2:
				asm volatile(""
					     "xchg"
					     "w %w0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 4:
				asm volatile(""
					     "xchg"
					     "l %0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 8:
				asm volatile(""
					     "xchg"
					     "q %q0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			default:
				__xchg_wrong_size();
			}
			__ret;
		});
	}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
native_pmdp_get_and_clear(pmd_t *xp)
{
	return native_make_pmd(({
		typeof(&xp->pmd) __ai_ptr = (&xp->pmd);
		do {
		} while (0);
		instrument_atomic_read_write(__ai_ptr, sizeof(*__ai_ptr));
		({
			__typeof__(*((__ai_ptr))) __ret = ((0));
			switch (sizeof(*((__ai_ptr)))) {
			case 1:
				asm volatile(""
					     "xchg"
					     "b %b0, %1\n"
					     : "+q"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 2:
				asm volatile(""
					     "xchg"
					     "w %w0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 4:
				asm volatile(""
					     "xchg"
					     "l %0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 8:
				asm volatile(""
					     "xchg"
					     "q %q0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			default:
				__xchg_wrong_size();
			}
			__ret;
		});
	}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_set_pud(pud_t *pudp, pud_t pud)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_473(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*pudp) == sizeof(char) ||
			       sizeof(*pudp) == sizeof(short) ||
			       sizeof(*pudp) == sizeof(int) ||
			       sizeof(*pudp) == sizeof(long)) ||
			      sizeof(*pudp) == sizeof(long long)))
				__compiletime_assert_473();
		} while (0);
		do {
			*(volatile typeof(*pudp) *)&(*pudp) = (pud);
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_pud_clear(pud_t *pud)
{
	native_set_pud(pud, native_make_pud(0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
native_pudp_get_and_clear(pud_t *xp)
{
	return native_make_pud(({
		typeof(&xp->pud) __ai_ptr = (&xp->pud);
		do {
		} while (0);
		instrument_atomic_read_write(__ai_ptr, sizeof(*__ai_ptr));
		({
			__typeof__(*((__ai_ptr))) __ret = ((0));
			switch (sizeof(*((__ai_ptr)))) {
			case 1:
				asm volatile(""
					     "xchg"
					     "b %b0, %1\n"
					     : "+q"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 2:
				asm volatile(""
					     "xchg"
					     "w %w0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 4:
				asm volatile(""
					     "xchg"
					     "l %0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			case 8:
				asm volatile(""
					     "xchg"
					     "q %q0, %1\n"
					     : "+r"(__ret), "+m"(*((__ai_ptr)))
					     :
					     : "memory", "cc");
				break;
			default:
				__xchg_wrong_size();
			}
			__ret;
		});
	}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_set_p4d(p4d_t *p4dp, p4d_t p4d)
{
	pgd_t pgd;

	if ((__builtin_constant_p((16 * 32 + 16)) &&
			     (((((16 * 32 + 16)) >> 5) == (0) &&
			       (1UL << (((16 * 32 + 16)) & 31) &
				((1 << ((0 * 32 + 1) & 31))))) ||
			      ((((16 * 32 + 16)) >> 5) == (1) &&
			       (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			      ((((16 * 32 + 16)) >> 5) == (2) &&
			       (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			      ((((16 * 32 + 16)) >> 5) == (3) &&
			       (1UL << (((16 * 32 + 16)) & 31) &
				((1 << ((3 * 32 + 2) & 31)) |
				 (1 << ((3 * 32 + 3) & 31)) |
				 (1 << ((3 * 32 + 1) & 31))))) ||
			      ((((16 * 32 + 16)) >> 5) == (4) &&
			       (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			      ((((16 * 32 + 16)) >> 5) == (5) &&
			       (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			      ((((16 * 32 + 16)) >> 5) == (6) &&
			       (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			      ((((16 * 32 + 16)) >> 5) == (7) &&
			       (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			      ((((16 * 32 + 16)) >> 5) == (8) &&
			       (1UL << (((16 * 32 + 16)) & 31) &
				((1 << ((8 * 32 + 16) & 31)) |
				 (1 << ((8 * 32 + 22) & 31))))) ||
			      ((((16 * 32 + 16)) >> 5) == (9) &&
			       (1UL << (((16 * 32 + 16)) & 31) &
				((1 << ((9 * 32 + 2) & 31))))) ||
			      ((((16 * 32 + 16)) >> 5) == (10) &&
			       (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			      ((((16 * 32 + 16)) >> 5) == (11) &&
			       (1UL << (((16 * 32 + 16)) & 31) &
				(0 | 0 | 0 | 0 |
				 (1 << ((11 * 32 + 23) & 31))))) ||
			      ((((16 * 32 + 16)) >> 5) == (12) &&
			       (1UL << (((16 * 32 + 16)) & 31) &
				((1 << ((12 * 32 + 17) & 31)) |
				 (1 << ((12 * 32 + 26) & 31))))) ||
			      ((((16 * 32 + 16)) >> 5) == (13) &&
			       (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			      ((((16 * 32 + 16)) >> 5) == (14) &&
			       (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			      ((((16 * 32 + 16)) >> 5) == (15) &&
			       (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			      ((((16 * 32 + 16)) >> 5) == (16) &&
			       (1UL << (((16 * 32 + 16)) & 31) &
				(0 | 0 | 0 | 0 |
				 (1 << ((16 * 32 + 29) & 31))))) ||
			      ((((16 * 32 + 16)) >> 5) == (17) &&
			       (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			      ((((16 * 32 + 16)) >> 5) == (18) &&
			       (1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			      ((((16 * 32 + 16)) >> 5) == (19) &&
			       (1UL << (((16 * 32 + 16)) & 31) &
				((1 << ((19 * 32 + 4) & 31))))) ||
			      ((((16 * 32 + 16)) >> 5) == (20) &&
			       (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			      ((((16 * 32 + 16)) >> 5) == (21) &&
			       (1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			      ((int)(sizeof(
				      struct { int : (-!!(22 != 22)); }))) ||
			      ((int)(sizeof(
				      struct { int : (-!!(22 != 22)); })))) ?
		     0 :
		     (__builtin_constant_p((
			      __builtin_constant_p((16 * 32 + 16)) &&
					      (((((16 * 32 + 16)) >> 5) == (0) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 ((1 << ((0 * 32 + 0) & 31)) |
						  (1 << ((0 * 32 + 3)) & 31) |
						  (1 << ((0 * 32 + 5) & 31)) |
						  (1 << ((0 * 32 + 6) & 31)) |
						  (1 << ((0 * 32 + 8) & 31)) |
						  (1 << ((0 * 32 + 13)) & 31) |
						  (1 << ((0 * 32 + 24) & 31)) |
						  (1 << ((0 * 32 + 15) & 31)) |
						  (1 << ((0 * 32 + 25) & 31)) |
						  (1 << ((0 * 32 + 26) &
							 31))))) ||
					       ((((16 * 32 + 16)) >> 5) == (1) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 ((1 << ((1 * 32 + 29) & 31)) |
						  0))) ||
					       ((((16 * 32 + 16)) >> 5) == (2) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) == (3) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 ((1 << ((3 * 32 + 20) &
							 31))))) ||
					       ((((16 * 32 + 16)) >> 5) == (4) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 (0))) ||
					       ((((16 * 32 + 16)) >> 5) == (5) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) == (6) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) == (7) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) == (8) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) == (9) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(10) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(11) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(12) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(13) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(14) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(15) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(16) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(17) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(18) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(19) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(20) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((((16 * 32 + 16)) >> 5) ==
							(21) &&
						(1UL << (((16 * 32 + 16)) & 31) &
						 0)) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       }))) ||
					       ((int)(sizeof(struct {
						       int : (-!!(22 != 22));
					       })))) ?
				      1 :
				      arch_test_bit(
					      (16 * 32 + 16),
					      (unsigned long
						       *)((&boot_cpu_data)
								  ->x86_capability)))) ?
			      (__builtin_constant_p((16 * 32 + 16)) &&
					       (((((16 * 32 + 16)) >> 5) ==
							 (0) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((0 * 32 + 0) & 31)) |
						   (1 << ((0 * 32 + 3)) & 31) |
						   (1 << ((0 * 32 + 5) & 31)) |
						   (1 << ((0 * 32 + 6) & 31)) |
						   (1 << ((0 * 32 + 8) & 31)) |
						   (1 << ((0 * 32 + 13)) & 31) |
						   (1 << ((0 * 32 + 24) & 31)) |
						   (1 << ((0 * 32 + 15) & 31)) |
						   (1 << ((0 * 32 + 25) & 31)) |
						   (1 << ((0 * 32 + 26) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (1) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((1 * 32 + 29) & 31)) |
						   0))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (2) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (3) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((3 * 32 + 20) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (4) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  (0))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (5) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (6) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (7) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (8) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (9) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (10) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (11) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (12) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (13) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (14) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (15) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (16) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (17) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (18) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (19) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (20) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (21) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						}))) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						})))) ?
				       1 :
				       arch_test_bit(
					       (16 * 32 + 16),
					       (unsigned long
							*)((&boot_cpu_data)
								   ->x86_capability))) :
			      _static_cpu_has((16 * 32 + 16)))) ||
	    !1) {
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_474(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*p4dp) == sizeof(char) ||
				       sizeof(*p4dp) == sizeof(short) ||
				       sizeof(*p4dp) == sizeof(int) ||
				       sizeof(*p4dp) == sizeof(long)) ||
				      sizeof(*p4dp) == sizeof(long long)))
					__compiletime_assert_474();
			} while (0);
			do {
				*(volatile typeof(*p4dp) *)&(*p4dp) = (p4d);
			} while (0);
		} while (0);
		return;
	}

	pgd = native_make_pgd(native_p4d_val(p4d));
	pgd = pti_set_user_pgtbl((pgd_t *)p4dp, pgd);
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_475(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*p4dp) == sizeof(char) ||
			       sizeof(*p4dp) == sizeof(short) ||
			       sizeof(*p4dp) == sizeof(int) ||
			       sizeof(*p4dp) == sizeof(long)) ||
			      sizeof(*p4dp) == sizeof(long long)))
				__compiletime_assert_475();
		} while (0);
		do {
			*(volatile typeof(*p4dp) *)&(*p4dp) =
				(native_make_p4d(native_pgd_val(pgd)));
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_p4d_clear(p4d_t *p4d)
{
	native_set_p4d(p4d, native_make_p4d(0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_set_pgd(pgd_t *pgdp, pgd_t pgd)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_476(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*pgdp) == sizeof(char) ||
			       sizeof(*pgdp) == sizeof(short) ||
			       sizeof(*pgdp) == sizeof(int) ||
			       sizeof(*pgdp) == sizeof(long)) ||
			      sizeof(*pgdp) == sizeof(long long)))
				__compiletime_assert_476();
		} while (0);
		do {
			*(volatile typeof(*pgdp) *)&(*pgdp) =
				(pti_set_user_pgtbl(pgdp, pgd));
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
native_pgd_clear(pgd_t *pgd)
{
	native_set_pgd(pgd, native_make_pgd(0));
}
extern void cleanup_highmap(void);
extern void init_extra_mapping_uc(unsigned long phys, unsigned long size);
extern void init_extra_mapping_wb(unsigned long phys, unsigned long size);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
gup_fast_permitted(unsigned long start, unsigned long end)
{
	if (end >>
	    ((__builtin_constant_p((16 * 32 + 16)) &&
			      (((((16 * 32 + 16)) >> 5) == (0) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((0 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (1) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (2) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (3) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((3 * 32 + 2) & 31)) |
				  (1 << ((3 * 32 + 3) & 31)) |
				  (1 << ((3 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (4) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (5) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (6) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (7) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (8) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((8 * 32 + 16) & 31)) |
				  (1 << ((8 * 32 + 22) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (9) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((9 * 32 + 2) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (10) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (11) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((11 * 32 + 23) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (12) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((12 * 32 + 17) & 31)) |
				  (1 << ((12 * 32 + 26) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (13) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (14) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (15) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (16) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((16 * 32 + 29) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (17) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (18) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (19) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((19 * 32 + 4) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (20) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (21) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); }))) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); })))) ?
		      0 :
		      (__builtin_constant_p((
			       __builtin_constant_p((16 * 32 + 16)) &&
					       (((((16 * 32 + 16)) >> 5) == (0) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((0 * 32 + 0) & 31)) |
						   (1 << ((0 * 32 + 3)) & 31) |
						   (1 << ((0 * 32 + 5) & 31)) |
						   (1 << ((0 * 32 + 6) & 31)) |
						   (1 << ((0 * 32 + 8) & 31)) |
						   (1 << ((0 * 32 + 13)) & 31) |
						   (1 << ((0 * 32 + 24) & 31)) |
						   (1 << ((0 * 32 + 15) & 31)) |
						   (1 << ((0 * 32 + 25) & 31)) |
						   (1 << ((0 * 32 + 26) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) == (1) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((1 * 32 + 29) & 31)) |
						   0))) ||
						((((16 * 32 + 16)) >> 5) == (2) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) == (3) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((3 * 32 + 20) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) == (4) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  (0))) ||
						((((16 * 32 + 16)) >> 5) == (5) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) == (6) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) == (7) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) == (8) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) == (9) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (10) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (11) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (12) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (13) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (14) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (15) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (16) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (17) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (18) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (19) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (20) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (21) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						}))) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						})))) ?
				       1 :
				       arch_test_bit(
					       (16 * 32 + 16),
					       (unsigned long
							*)((&boot_cpu_data)
								   ->x86_capability)))) ?
			       (__builtin_constant_p((16 * 32 + 16)) &&
						(((((16 * 32 + 16)) >> 5) ==
							  (0) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((0 * 32 + 0) & 31)) |
						    (1 << ((0 * 32 + 3)) & 31) |
						    (1 << ((0 * 32 + 5) & 31)) |
						    (1 << ((0 * 32 + 6) & 31)) |
						    (1 << ((0 * 32 + 8) & 31)) |
						    (1 << ((0 * 32 + 13)) & 31) |
						    (1 << ((0 * 32 + 24) & 31)) |
						    (1 << ((0 * 32 + 15) & 31)) |
						    (1 << ((0 * 32 + 25) & 31)) |
						    (1 << ((0 * 32 + 26) &
							   31))))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (1) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((1 * 32 + 29) & 31)) |
						    0))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (2) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (3) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((3 * 32 + 20) &
							   31))))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (4) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   (0))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (5) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (6) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (7) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (8) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (9) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (10) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (11) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (12) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (13) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (14) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (15) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (16) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (17) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (18) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (19) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (20) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (21) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 }))) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 })))) ?
					1 :
					arch_test_bit(
						(16 * 32 + 16),
						(unsigned long
							 *)((&boot_cpu_data)
								    ->x86_capability))) :
			       _static_cpu_has((16 * 32 + 16)))) ?
		     56 :
		     47))
		return false;
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__pte_needs_invert(u64 val)
{
	return val && !(val & (((pteval_t)(1)) << 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
protnone_mask(u64 val)
{
	return __pte_needs_invert(val) ? ~0ull : 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
flip_protnone_guard(u64 oldval, u64 val, u64 mask)
{
	if (__pte_needs_invert(oldval) != __pte_needs_invert(val))
		val = (val & ~mask) | (~val & mask);
	return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_none(pte_t pte)
{
	return !(pte.pte &
		 ~(((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 5))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_same(pte_t a, pte_t b)
{
	return a.pte == b.pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_advance_pfn(pte_t pte, unsigned long nr)
{
	if (__pte_needs_invert(native_pte_val(pte)))
		return native_make_pte(native_pte_val(pte) - (nr << 12));
	return native_make_pte(native_pte_val(pte) + (nr << 12));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_present(pte_t a)
{
	return pte_flags(a) & ((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 8));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_devmap(pte_t a)
{
	return (pte_flags(a) & (((u64)(1)) << 57)) == (((u64)(1)) << 57);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pte_accessible(struct mm_struct *mm, pte_t a)
{
	if (pte_flags(a) & (((pteval_t)(1)) << 0))
		return true;

	if ((pte_flags(a) & (((pteval_t)(1)) << 8)) &&
	    atomic_read(&mm->tlb_flush_pending))
		return true;

	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pmd_present(pmd_t pmd)
{
	return pmd_flags(pmd) &
	       ((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 8) |
		(((pteval_t)(1)) << 7));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pmd_none(pmd_t pmd)
{
	unsigned long val = native_pmd_val(pmd);
	return (val & ~((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 5))) == 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)((void *)((unsigned long)(native_pmd_val(pmd) &
							pmd_pfn_mask(pmd)) +
					((unsigned long)page_offset_base)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pmd_bad(pmd_t pmd)
{
	return (pmd_flags(pmd) &
		~((((pteval_t)(1)) << 2) | (((pteval_t)(1)) << 5))) !=
	       (((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 1) | 0 |
		 (((pteval_t)(1)) << 5) | 0 | (((pteval_t)(1)) << 6) | 0 | 0 |
		 (((pteval_t)(0ULL)))) &
		~(((pteval_t)(1)) << 5));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
pages_to_mb(unsigned long npg)
{
	return npg >> (20 - 12);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pud_none(pud_t pud)
{
	return (native_pud_val(pud) &
		~(((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 5)))) == 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pud_present(pud_t pud)
{
	return pud_flags(pud) & (((pteval_t)(1)) << 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t *
pud_pgtable(pud_t pud)
{
	return (pmd_t *)((void *)((unsigned long)(native_pud_val(pud) &
						  pud_pfn_mask(pud)) +
				  ((unsigned long)page_offset_base)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pud_leaf(pud_t pud)
{
	return native_pud_val(pud) & (((pteval_t)(1)) << 7);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pud_bad(pud_t pud)
{
	return (pud_flags(pud) &
		~(((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 1) | 0 |
		   (((pteval_t)(1)) << 5) | 0 | (((pteval_t)(1)) << 6) | 0 | 0 |
		   (((pteval_t)(0ULL)))) |
		  (((pteval_t)(1)) << 2))) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
p4d_none(p4d_t p4d)
{
	return (native_p4d_val(p4d) &
		~(((((pteval_t)(1)) << 6) | (((pteval_t)(1)) << 5)))) == 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
p4d_present(p4d_t p4d)
{
	return p4d_flags(p4d) & (((pteval_t)(1)) << 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t *
p4d_pgtable(p4d_t p4d)
{
	return (pud_t *)((void *)((unsigned long)(native_p4d_val(p4d) &
						  p4d_pfn_mask(p4d)) +
				  ((unsigned long)page_offset_base)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
p4d_bad(p4d_t p4d)
{
	unsigned long ignore_flags =
		((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 1) | 0 |
		 (((pteval_t)(1)) << 5) | 0 | (((pteval_t)(1)) << 6) | 0 | 0 |
		 (((pteval_t)(0ULL)))) |
		(((pteval_t)(1)) << 2);

	if (1)
		ignore_flags |= (((pteval_t)(1)) << 63);

	return (p4d_flags(p4d) & ~ignore_flags) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
p4d_index(unsigned long address)
{
	return (address >> 39) & (ptrs_per_p4d - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pgd_present(pgd_t pgd)
{
	if (!(__builtin_constant_p((16 * 32 + 16)) &&
			      (((((16 * 32 + 16)) >> 5) == (0) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((0 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (1) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (2) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (3) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((3 * 32 + 2) & 31)) |
				  (1 << ((3 * 32 + 3) & 31)) |
				  (1 << ((3 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (4) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (5) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (6) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (7) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (8) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((8 * 32 + 16) & 31)) |
				  (1 << ((8 * 32 + 22) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (9) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((9 * 32 + 2) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (10) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (11) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((11 * 32 + 23) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (12) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((12 * 32 + 17) & 31)) |
				  (1 << ((12 * 32 + 26) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (13) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (14) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (15) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (16) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((16 * 32 + 29) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (17) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (18) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (19) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((19 * 32 + 4) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (20) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (21) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); }))) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); })))) ?
		      0 :
		      (__builtin_constant_p((
			       __builtin_constant_p((16 * 32 + 16)) &&
					       (((((16 * 32 + 16)) >> 5) ==
							 (0) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((0 * 32 + 0) & 31)) |
						   (1 << ((0 * 32 + 3)) & 31) |
						   (1 << ((0 * 32 + 5) & 31)) |
						   (1 << ((0 * 32 + 6) & 31)) |
						   (1 << ((0 * 32 + 8) & 31)) |
						   (1 << ((0 * 32 + 13)) & 31) |
						   (1 << ((0 * 32 + 24) & 31)) |
						   (1 << ((0 * 32 + 15) & 31)) |
						   (1 << ((0 * 32 + 25) & 31)) |
						   (1 << ((0 * 32 + 26) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (1) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((1 * 32 + 29) & 31)) |
						   0))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (2) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (3) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((3 * 32 + 20) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (4) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  (0))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (5) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (6) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (7) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (8) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (9) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (10) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (11) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (12) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (13) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (14) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (15) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (16) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (17) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (18) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (19) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (20) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (21) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						}))) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						})))) ?
				       1 :
				       arch_test_bit(
					       (16 * 32 + 16),
					       (unsigned long
							*)((&boot_cpu_data)
								   ->x86_capability)))) ?
			       (__builtin_constant_p((16 * 32 + 16)) &&
						(((((16 * 32 + 16)) >> 5) ==
							  (0) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((0 * 32 + 0) & 31)) |
						    (1 << ((0 * 32 + 3)) & 31) |
						    (1 << ((0 * 32 + 5) & 31)) |
						    (1 << ((0 * 32 + 6) & 31)) |
						    (1 << ((0 * 32 + 8) & 31)) |
						    (1 << ((0 * 32 + 13)) & 31) |
						    (1 << ((0 * 32 + 24) & 31)) |
						    (1 << ((0 * 32 + 15) & 31)) |
						    (1 << ((0 * 32 + 25) & 31)) |
						    (1 << ((0 * 32 + 26) &
							   31))))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (1) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((1 * 32 + 29) & 31)) |
						    0))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (2) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (3) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((3 * 32 + 20) &
							   31))))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (4) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   (0))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (5) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (6) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (7) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (8) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (9) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (10) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (11) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (12) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (13) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (14) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (15) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (16) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (17) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (18) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (19) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (20) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (21) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 }))) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 })))) ?
					1 :
					arch_test_bit(
						(16 * 32 + 16),
						(unsigned long
							 *)((&boot_cpu_data)
								    ->x86_capability))) :
			       _static_cpu_has((16 * 32 + 16)))))
		return 1;
	return pgd_flags(pgd) & (((pteval_t)(1)) << 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)((
		void *)((unsigned long)((unsigned long)native_pgd_val(pgd) &
					((pteval_t)(((signed long)(~(
							    ((1UL) << 12) - 1))) &
						    ((phys_addr_t)((1ULL << 52) -
								   1))))) +
			((unsigned long)page_offset_base)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) p4d_t *
p4d_offset(pgd_t *pgd, unsigned long address)
{
	if (!(__builtin_constant_p((16 * 32 + 16)) &&
			      (((((16 * 32 + 16)) >> 5) == (0) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((0 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (1) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (2) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (3) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((3 * 32 + 2) & 31)) |
				  (1 << ((3 * 32 + 3) & 31)) |
				  (1 << ((3 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (4) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (5) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (6) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (7) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (8) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((8 * 32 + 16) & 31)) |
				  (1 << ((8 * 32 + 22) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (9) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((9 * 32 + 2) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (10) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (11) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((11 * 32 + 23) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (12) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((12 * 32 + 17) & 31)) |
				  (1 << ((12 * 32 + 26) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (13) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (14) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (15) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (16) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((16 * 32 + 29) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (17) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (18) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (19) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((19 * 32 + 4) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (20) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (21) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); }))) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); })))) ?
		      0 :
		      (__builtin_constant_p((
			       __builtin_constant_p((16 * 32 + 16)) &&
					       (((((16 * 32 + 16)) >> 5) ==
							 (0) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((0 * 32 + 0) & 31)) |
						   (1 << ((0 * 32 + 3)) & 31) |
						   (1 << ((0 * 32 + 5) & 31)) |
						   (1 << ((0 * 32 + 6) & 31)) |
						   (1 << ((0 * 32 + 8) & 31)) |
						   (1 << ((0 * 32 + 13)) & 31) |
						   (1 << ((0 * 32 + 24) & 31)) |
						   (1 << ((0 * 32 + 15) & 31)) |
						   (1 << ((0 * 32 + 25) & 31)) |
						   (1 << ((0 * 32 + 26) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (1) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((1 * 32 + 29) & 31)) |
						   0))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (2) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (3) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((3 * 32 + 20) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (4) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  (0))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (5) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (6) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (7) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (8) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (9) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (10) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (11) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (12) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (13) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (14) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (15) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (16) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (17) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (18) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (19) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (20) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (21) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						}))) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						})))) ?
				       1 :
				       arch_test_bit(
					       (16 * 32 + 16),
					       (unsigned long
							*)((&boot_cpu_data)
								   ->x86_capability)))) ?
			       (__builtin_constant_p((16 * 32 + 16)) &&
						(((((16 * 32 + 16)) >> 5) ==
							  (0) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((0 * 32 + 0) & 31)) |
						    (1 << ((0 * 32 + 3)) & 31) |
						    (1 << ((0 * 32 + 5) & 31)) |
						    (1 << ((0 * 32 + 6) & 31)) |
						    (1 << ((0 * 32 + 8) & 31)) |
						    (1 << ((0 * 32 + 13)) & 31) |
						    (1 << ((0 * 32 + 24) & 31)) |
						    (1 << ((0 * 32 + 15) & 31)) |
						    (1 << ((0 * 32 + 25) & 31)) |
						    (1 << ((0 * 32 + 26) &
							   31))))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (1) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((1 * 32 + 29) & 31)) |
						    0))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (2) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (3) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((3 * 32 + 20) &
							   31))))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (4) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   (0))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (5) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (6) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (7) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (8) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (9) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (10) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (11) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (12) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (13) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (14) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (15) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (16) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (17) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (18) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (19) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (20) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (21) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 }))) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 })))) ?
					1 :
					arch_test_bit(
						(16 * 32 + 16),
						(unsigned long
							 *)((&boot_cpu_data)
								    ->x86_capability))) :
			       _static_cpu_has((16 * 32 + 16)))))
		return (p4d_t *)pgd;
	return (p4d_t *)pgd_page_vaddr(*pgd) + p4d_index(address);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pgd_bad(pgd_t pgd)
{
	unsigned long ignore_flags = (((pteval_t)(1)) << 2);

	if (!(__builtin_constant_p((16 * 32 + 16)) &&
			      (((((16 * 32 + 16)) >> 5) == (0) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((0 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (1) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (2) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (3) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((3 * 32 + 2) & 31)) |
				  (1 << ((3 * 32 + 3) & 31)) |
				  (1 << ((3 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (4) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (5) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (6) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (7) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (8) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((8 * 32 + 16) & 31)) |
				  (1 << ((8 * 32 + 22) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (9) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((9 * 32 + 2) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (10) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (11) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((11 * 32 + 23) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (12) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((12 * 32 + 17) & 31)) |
				  (1 << ((12 * 32 + 26) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (13) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (14) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (15) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (16) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((16 * 32 + 29) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (17) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (18) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (19) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((19 * 32 + 4) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (20) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (21) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); }))) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); })))) ?
		      0 :
		      (__builtin_constant_p((
			       __builtin_constant_p((16 * 32 + 16)) &&
					       (((((16 * 32 + 16)) >> 5) ==
							 (0) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((0 * 32 + 0) & 31)) |
						   (1 << ((0 * 32 + 3)) & 31) |
						   (1 << ((0 * 32 + 5) & 31)) |
						   (1 << ((0 * 32 + 6) & 31)) |
						   (1 << ((0 * 32 + 8) & 31)) |
						   (1 << ((0 * 32 + 13)) & 31) |
						   (1 << ((0 * 32 + 24) & 31)) |
						   (1 << ((0 * 32 + 15) & 31)) |
						   (1 << ((0 * 32 + 25) & 31)) |
						   (1 << ((0 * 32 + 26) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (1) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((1 * 32 + 29) & 31)) |
						   0))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (2) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (3) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((3 * 32 + 20) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (4) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  (0))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (5) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (6) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (7) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (8) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (9) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (10) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (11) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (12) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (13) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (14) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (15) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (16) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (17) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (18) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (19) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (20) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (21) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						}))) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						})))) ?
				       1 :
				       arch_test_bit(
					       (16 * 32 + 16),
					       (unsigned long
							*)((&boot_cpu_data)
								   ->x86_capability)))) ?
			       (__builtin_constant_p((16 * 32 + 16)) &&
						(((((16 * 32 + 16)) >> 5) ==
							  (0) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((0 * 32 + 0) & 31)) |
						    (1 << ((0 * 32 + 3)) & 31) |
						    (1 << ((0 * 32 + 5) & 31)) |
						    (1 << ((0 * 32 + 6) & 31)) |
						    (1 << ((0 * 32 + 8) & 31)) |
						    (1 << ((0 * 32 + 13)) & 31) |
						    (1 << ((0 * 32 + 24) & 31)) |
						    (1 << ((0 * 32 + 15) & 31)) |
						    (1 << ((0 * 32 + 25) & 31)) |
						    (1 << ((0 * 32 + 26) &
							   31))))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (1) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((1 * 32 + 29) & 31)) |
						    0))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (2) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (3) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((3 * 32 + 20) &
							   31))))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (4) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   (0))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (5) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (6) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (7) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (8) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (9) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (10) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (11) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (12) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (13) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (14) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (15) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (16) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (17) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (18) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (19) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (20) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (21) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 }))) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 })))) ?
					1 :
					arch_test_bit(
						(16 * 32 + 16),
						(unsigned long
							 *)((&boot_cpu_data)
								    ->x86_capability))) :
			       _static_cpu_has((16 * 32 + 16)))))
		return 0;

	if (1)
		ignore_flags |= (((pteval_t)(1)) << 63);

	return (pgd_flags(pgd) & ~ignore_flags) !=
	       ((((pteval_t)(1)) << 0) | (((pteval_t)(1)) << 1) | 0 |
		(((pteval_t)(1)) << 5) | 0 | (((pteval_t)(1)) << 6) | 0 | 0 |
		(((pteval_t)(0ULL))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pgd_none(pgd_t pgd)
{
	if (!(__builtin_constant_p((16 * 32 + 16)) &&
			      (((((16 * 32 + 16)) >> 5) == (0) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((0 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (1) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (2) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (3) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((3 * 32 + 2) & 31)) |
				  (1 << ((3 * 32 + 3) & 31)) |
				  (1 << ((3 * 32 + 1) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (4) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (5) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (6) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (7) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (8) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((8 * 32 + 16) & 31)) |
				  (1 << ((8 * 32 + 22) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (9) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((9 * 32 + 2) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (10) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (11) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((11 * 32 + 23) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (12) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((12 * 32 + 17) & 31)) |
				  (1 << ((12 * 32 + 26) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (13) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (14) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (15) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (16) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 (0 | 0 | 0 | 0 |
				  (1 << ((16 * 32 + 29) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (17) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (18) &&
				(1UL << (((16 * 32 + 16)) & 31) & (0))) ||
			       ((((16 * 32 + 16)) >> 5) == (19) &&
				(1UL << (((16 * 32 + 16)) & 31) &
				 ((1 << ((19 * 32 + 4) & 31))))) ||
			       ((((16 * 32 + 16)) >> 5) == (20) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((((16 * 32 + 16)) >> 5) == (21) &&
				(1UL << (((16 * 32 + 16)) & 31) & 0)) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); }))) ||
			       ((int)(sizeof(
				       struct { int : (-!!(22 != 22)); })))) ?
		      0 :
		      (__builtin_constant_p((
			       __builtin_constant_p((16 * 32 + 16)) &&
					       (((((16 * 32 + 16)) >> 5) ==
							 (0) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((0 * 32 + 0) & 31)) |
						   (1 << ((0 * 32 + 3)) & 31) |
						   (1 << ((0 * 32 + 5) & 31)) |
						   (1 << ((0 * 32 + 6) & 31)) |
						   (1 << ((0 * 32 + 8) & 31)) |
						   (1 << ((0 * 32 + 13)) & 31) |
						   (1 << ((0 * 32 + 24) & 31)) |
						   (1 << ((0 * 32 + 15) & 31)) |
						   (1 << ((0 * 32 + 25) & 31)) |
						   (1 << ((0 * 32 + 26) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (1) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((1 * 32 + 29) & 31)) |
						   0))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (2) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (3) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  ((1 << ((3 * 32 + 20) &
							  31))))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (4) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  (0))) ||
						((((16 * 32 + 16)) >> 5) ==
							 (5) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (6) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (7) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (8) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (9) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (10) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (11) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (12) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (13) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (14) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (15) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (16) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (17) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (18) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (19) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (20) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((((16 * 32 + 16)) >> 5) ==
							 (21) &&
						 (1UL << (((16 * 32 + 16)) & 31) &
						  0)) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						}))) ||
						((int)(sizeof(struct {
							int : (-!!(22 != 22));
						})))) ?
				       1 :
				       arch_test_bit(
					       (16 * 32 + 16),
					       (unsigned long
							*)((&boot_cpu_data)
								   ->x86_capability)))) ?
			       (__builtin_constant_p((16 * 32 + 16)) &&
						(((((16 * 32 + 16)) >> 5) ==
							  (0) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((0 * 32 + 0) & 31)) |
						    (1 << ((0 * 32 + 3)) & 31) |
						    (1 << ((0 * 32 + 5) & 31)) |
						    (1 << ((0 * 32 + 6) & 31)) |
						    (1 << ((0 * 32 + 8) & 31)) |
						    (1 << ((0 * 32 + 13)) & 31) |
						    (1 << ((0 * 32 + 24) & 31)) |
						    (1 << ((0 * 32 + 15) & 31)) |
						    (1 << ((0 * 32 + 25) & 31)) |
						    (1 << ((0 * 32 + 26) &
							   31))))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (1) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((1 * 32 + 29) & 31)) |
						    0))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (2) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (3) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   ((1 << ((3 * 32 + 20) &
							   31))))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (4) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   (0))) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (5) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (6) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (7) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (8) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (9) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (10) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (11) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (12) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (13) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (14) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (15) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (16) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (17) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (18) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (19) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (20) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((((16 * 32 + 16)) >> 5) ==
							  (21) &&
						  (1UL << (((16 * 32 + 16)) &
							   31) &
						   0)) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 }))) ||
						 ((int)(sizeof(struct {
							 int : (-!!(22 != 22));
						 })))) ?
					1 :
					arch_test_bit(
						(16 * 32 + 16),
						(unsigned long
							 *)((&boot_cpu_data)
								    ->x86_capability))) :
			       _static_cpu_has((16 * 32 + 16)))))
		return 0;

	return !native_pgd_val(pgd);
}
extern int direct_gbpages;
void init_mem_mapping(void);
void early_alloc_pgt_buf(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
poking_init(void);
unsigned long init_memory_mapping(unsigned long start, unsigned long end,
				  pgprot_t prot);

extern pgd_t trampoline_pgd_entry;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
native_local_ptep_get_and_clear(pte_t *ptep)
{
	pte_t res = *ptep;

	native_pte_clear(((void *)0), 0, ptep);
	return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
native_local_pmdp_get_and_clear(pmd_t *pmdp)
{
	pmd_t res = *pmdp;

	native_pmd_clear(pmdp);
	return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
native_local_pudp_get_and_clear(pud_t *pudp)
{
	pud_t res = *pudp;

	native_pud_clear(pudp);
	return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_pmd_at(struct mm_struct *mm, unsigned long addr, pmd_t *pmdp, pmd_t pmd)
{
	page_table_check_pmd_set(mm, pmdp, pmd);
	native_set_pmd(pmdp, pmd);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_pud_at(struct mm_struct *mm, unsigned long addr, pud_t *pudp, pud_t pud)
{
	page_table_check_pud_set(mm, pudp, pud);
	native_set_pud(pudp, pud);
}
struct vm_area_struct;

extern int ptep_set_access_flags(struct vm_area_struct *vma,
				 unsigned long address, pte_t *ptep,
				 pte_t entry, int dirty);

extern int ptep_test_and_clear_young(struct vm_area_struct *vma,
				     unsigned long addr, pte_t *ptep);

extern int ptep_clear_flush_young(struct vm_area_struct *vma,
				  unsigned long address, pte_t *ptep);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
ptep_get_and_clear(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
{
	pte_t pte = native_ptep_get_and_clear(ptep);
	page_table_check_pte_clear(mm, pte);
	return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
ptep_get_and_clear_full(struct mm_struct *mm, unsigned long addr, pte_t *ptep,
			int full)
{
	pte_t pte;
	if (full) {
		pte = native_local_ptep_get_and_clear(ptep);
		page_table_check_pte_clear(mm, pte);
	} else {
		pte = ptep_get_and_clear(mm, addr, ptep);
	}
	return pte;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ptep_set_wrprotect(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
{
	pte_t old_pte, new_pte;

	old_pte = ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_477(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*ptep) == sizeof(char) ||
			       sizeof(*ptep) == sizeof(short) ||
			       sizeof(*ptep) == sizeof(int) ||
			       sizeof(*ptep) == sizeof(long)) ||
			      sizeof(*ptep) == sizeof(long long)))
				__compiletime_assert_477();
		} while (0);
		(*(const volatile typeof(_Generic((*ptep),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (*ptep))) *)&(*ptep));
	});
	do {
		new_pte = pte_wrprotect(old_pte);
	} while (!({
		typeof((long *)&ptep->pte) __ai_ptr = ((long *)&ptep->pte);
		typeof((long *)&old_pte) __ai_oldp = ((long *)&old_pte);
		do {
		} while (0);
		instrument_atomic_read_write(__ai_ptr, sizeof(*__ai_ptr));
		instrument_read_write(__ai_oldp, sizeof(*__ai_oldp));
		({
			bool success;
			__typeof__(((__ai_ptr))) _old =
				(__typeof__(((__ai_ptr))))(((__ai_oldp)));
			__typeof__(*(((__ai_ptr)))) __old = *_old;
			__typeof__(*(((__ai_ptr)))) __new =
				(((*(long *)&new_pte)));
			switch ((sizeof(*(__ai_ptr)))) {
			case 1: {
				volatile u8 *__ptr =
					(volatile u8 *)(((__ai_ptr)));
				asm volatile(
					".pushsection .smp_locks,\"a\"\n"
					".balign 4\n"
					".long 671f - .\n"
					".popsection\n"
					"671:"
					"\n\tlock; "
					"cmpxchgb %[new], %[ptr]"
					"\n\t/* output condition code "
					"z"
					"*/\n"
					: "=@cc"
					  "z"(success),
					  [ptr] "+m"(*__ptr), [old] "+a"(__old)
					: [new] "q"(__new)
					: "memory");
				break;
			}
			case 2: {
				volatile u16 *__ptr =
					(volatile u16 *)(((__ai_ptr)));
				asm volatile(
					".pushsection .smp_locks,\"a\"\n"
					".balign 4\n"
					".long 671f - .\n"
					".popsection\n"
					"671:"
					"\n\tlock; "
					"cmpxchgw %[new], %[ptr]"
					"\n\t/* output condition code "
					"z"
					"*/\n"
					: "=@cc"
					  "z"(success),
					  [ptr] "+m"(*__ptr), [old] "+a"(__old)
					: [new] "r"(__new)
					: "memory");
				break;
			}
			case 4: {
				volatile u32 *__ptr =
					(volatile u32 *)(((__ai_ptr)));
				asm volatile(
					".pushsection .smp_locks,\"a\"\n"
					".balign 4\n"
					".long 671f - .\n"
					".popsection\n"
					"671:"
					"\n\tlock; "
					"cmpxchgl %[new], %[ptr]"
					"\n\t/* output condition code "
					"z"
					"*/\n"
					: "=@cc"
					  "z"(success),
					  [ptr] "+m"(*__ptr), [old] "+a"(__old)
					: [new] "r"(__new)
					: "memory");
				break;
			}
			case 8: {
				volatile u64 *__ptr =
					(volatile u64 *)(((__ai_ptr)));
				asm volatile(
					".pushsection .smp_locks,\"a\"\n"
					".balign 4\n"
					".long 671f - .\n"
					".popsection\n"
					"671:"
					"\n\tlock; "
					"cmpxchgq %[new], %[ptr]"
					"\n\t/* output condition code "
					"z"
					"*/\n"
					: "=@cc"
					  "z"(success),
					  [ptr] "+m"(*__ptr), [old] "+a"(__old)
					: [new] "r"(__new)
					: "memory");
				break;
			}
			default:
				__cmpxchg_wrong_size();
			}
			if (__builtin_expect(!!(!success), 0))
				*_old = __old;
			__builtin_expect(!!(success), 1);
		});
	}));
}

extern int pmdp_set_access_flags(struct vm_area_struct *vma,
				 unsigned long address, pmd_t *pmdp,
				 pmd_t entry, int dirty);
extern int pudp_set_access_flags(struct vm_area_struct *vma,
				 unsigned long address, pud_t *pudp,
				 pud_t entry, int dirty);

extern int pmdp_test_and_clear_young(struct vm_area_struct *vma,
				     unsigned long addr, pmd_t *pmdp);
extern int pudp_test_and_clear_young(struct vm_area_struct *vma,
				     unsigned long addr, pud_t *pudp);

extern int pmdp_clear_flush_young(struct vm_area_struct *vma,
				  unsigned long address, pmd_t *pmdp);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr, pmd_t *pmdp)
{
	pmd_t pmd = native_pmdp_get_and_clear(pmdp);

	page_table_check_pmd_clear(mm, pmd);

	return pmd;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pudp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr, pud_t *pudp)
{
	pud_t pud = native_pudp_get_and_clear(pudp);

	page_table_check_pud_clear(mm, pud);

	return pud;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
pmdp_set_wrprotect(struct mm_struct *mm, unsigned long addr, pmd_t *pmdp)
{
	pmd_t old_pmd, new_pmd;

	old_pmd = ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_478(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(*pmdp) == sizeof(char) ||
			       sizeof(*pmdp) == sizeof(short) ||
			       sizeof(*pmdp) == sizeof(int) ||
			       sizeof(*pmdp) == sizeof(long)) ||
			      sizeof(*pmdp) == sizeof(long long)))
				__compiletime_assert_478();
		} while (0);
		(*(const volatile typeof(_Generic((*pmdp),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (*pmdp))) *)&(*pmdp));
	});
	do {
		new_pmd = pmd_wrprotect(old_pmd);
	} while (!({
		typeof((long *)pmdp) __ai_ptr = ((long *)pmdp);
		typeof((long *)&old_pmd) __ai_oldp = ((long *)&old_pmd);
		do {
		} while (0);
		instrument_atomic_read_write(__ai_ptr, sizeof(*__ai_ptr));
		instrument_read_write(__ai_oldp, sizeof(*__ai_oldp));
		({
			bool success;
			__typeof__(((__ai_ptr))) _old =
				(__typeof__(((__ai_ptr))))(((__ai_oldp)));
			__typeof__(*(((__ai_ptr)))) __old = *_old;
			__typeof__(*(((__ai_ptr)))) __new =
				(((*(long *)&new_pmd)));
			switch ((sizeof(*(__ai_ptr)))) {
			case 1: {
				volatile u8 *__ptr =
					(volatile u8 *)(((__ai_ptr)));
				asm volatile(
					".pushsection .smp_locks,\"a\"\n"
					".balign 4\n"
					".long 671f - .\n"
					".popsection\n"
					"671:"
					"\n\tlock; "
					"cmpxchgb %[new], %[ptr]"
					"\n\t/* output condition code "
					"z"
					"*/\n"
					: "=@cc"
					  "z"(success),
					  [ptr] "+m"(*__ptr), [old] "+a"(__old)
					: [new] "q"(__new)
					: "memory");
				break;
			}
			case 2: {
				volatile u16 *__ptr =
					(volatile u16 *)(((__ai_ptr)));
				asm volatile(
					".pushsection .smp_locks,\"a\"\n"
					".balign 4\n"
					".long 671f - .\n"
					".popsection\n"
					"671:"
					"\n\tlock; "
					"cmpxchgw %[new], %[ptr]"
					"\n\t/* output condition code "
					"z"
					"*/\n"
					: "=@cc"
					  "z"(success),
					  [ptr] "+m"(*__ptr), [old] "+a"(__old)
					: [new] "r"(__new)
					: "memory");
				break;
			}
			case 4: {
				volatile u32 *__ptr =
					(volatile u32 *)(((__ai_ptr)));
				asm volatile(
					".pushsection .smp_locks,\"a\"\n"
					".balign 4\n"
					".long 671f - .\n"
					".popsection\n"
					"671:"
					"\n\tlock; "
					"cmpxchgl %[new], %[ptr]"
					"\n\t/* output condition code "
					"z"
					"*/\n"
					: "=@cc"
					  "z"(success),
					  [ptr] "+m"(*__ptr), [old] "+a"(__old)
					: [new] "r"(__new)
					: "memory");
				break;
			}
			case 8: {
				volatile u64 *__ptr =
					(volatile u64 *)(((__ai_ptr)));
				asm volatile(
					".pushsection .smp_locks,\"a\"\n"
					".balign 4\n"
					".long 671f - .\n"
					".popsection\n"
					"671:"
					"\n\tlock; "
					"cmpxchgq %[new], %[ptr]"
					"\n\t/* output condition code "
					"z"
					"*/\n"
					: "=@cc"
					  "z"(success),
					  [ptr] "+m"(*__ptr), [old] "+a"(__old)
					: [new] "r"(__new)
					: "memory");
				break;
			}
			default:
				__cmpxchg_wrong_size();
			}
			if (__builtin_expect(!!(!success), 0))
				*_old = __old;
			__builtin_expect(!!(success), 1);
		});
	}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pmd_t
pmdp_establish(struct vm_area_struct *vma, unsigned long address, pmd_t *pmdp,
	       pmd_t pmd)
{
	page_table_check_pmd_set(vma->vm_mm, pmdp, pmd);
	if (1) {
		return ({
			typeof(pmdp) __ai_ptr = (pmdp);
			do {
			} while (0);
			instrument_atomic_read_write(__ai_ptr,
						     sizeof(*__ai_ptr));
			({
				__typeof__(*((__ai_ptr))) __ret = ((pmd));
				switch (sizeof(*((__ai_ptr)))) {
				case 1:
					asm volatile(""
						     "xchg"
						     "b %b0, %1\n"
						     : "+q"(__ret),
						       "+m"(*((__ai_ptr)))
						     :
						     : "memory", "cc");
					break;
				case 2:
					asm volatile(""
						     "xchg"
						     "w %w0, %1\n"
						     : "+r"(__ret),
						       "+m"(*((__ai_ptr)))
						     :
						     : "memory", "cc");
					break;
				case 4:
					asm volatile(""
						     "xchg"
						     "l %0, %1\n"
						     : "+r"(__ret),
						       "+m"(*((__ai_ptr)))
						     :
						     : "memory", "cc");
					break;
				case 8:
					asm volatile(""
						     "xchg"
						     "q %q0, %1\n"
						     : "+r"(__ret),
						       "+m"(*((__ai_ptr)))
						     :
						     : "memory", "cc");
					break;
				default:
					__xchg_wrong_size();
				}
				__ret;
			});
		});
	} else {
		pmd_t old = *pmdp;
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_479(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*pmdp) == sizeof(char) ||
				       sizeof(*pmdp) == sizeof(short) ||
				       sizeof(*pmdp) == sizeof(int) ||
				       sizeof(*pmdp) == sizeof(long)) ||
				      sizeof(*pmdp) == sizeof(long long)))
					__compiletime_assert_479();
			} while (0);
			do {
				*(volatile typeof(*pmdp) *)&(*pmdp) = (pmd);
			} while (0);
		} while (0);
		return old;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pud_t
pudp_establish(struct vm_area_struct *vma, unsigned long address, pud_t *pudp,
	       pud_t pud)
{
	page_table_check_pud_set(vma->vm_mm, pudp, pud);
	if (1) {
		return ({
			typeof(pudp) __ai_ptr = (pudp);
			do {
			} while (0);
			instrument_atomic_read_write(__ai_ptr,
						     sizeof(*__ai_ptr));
			({
				__typeof__(*((__ai_ptr))) __ret = ((pud));
				switch (sizeof(*((__ai_ptr)))) {
				case 1:
					asm volatile(""
						     "xchg"
						     "b %b0, %1\n"
						     : "+q"(__ret),
						       "+m"(*((__ai_ptr)))
						     :
						     : "memory", "cc");
					break;
				case 2:
					asm volatile(""
						     "xchg"
						     "w %w0, %1\n"
						     : "+r"(__ret),
						       "+m"(*((__ai_ptr)))
						     :
						     : "memory", "cc");
					break;
				case 4:
					asm volatile(""
						     "xchg"
						     "l %0, %1\n"
						     : "+r"(__ret),
						       "+m"(*((__ai_ptr)))
						     :
						     : "memory", "cc");
					break;
				case 8:
					asm volatile(""
						     "xchg"
						     "q %q0, %1\n"
						     : "+r"(__ret),
						       "+m"(*((__ai_ptr)))
						     :
						     : "memory", "cc");
					break;
				default:
					__xchg_wrong_size();
				}
				__ret;
			});
		});
	} else {
		pud_t old = *pudp;
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_480(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*pudp) == sizeof(char) ||
				       sizeof(*pudp) == sizeof(short) ||
				       sizeof(*pudp) == sizeof(int) ||
				       sizeof(*pudp) == sizeof(long)) ||
				      sizeof(*pudp) == sizeof(long long)))
					__compiletime_assert_480();
			} while (0);
			do {
				*(volatile typeof(*pudp) *)&(*pudp) = (pud);
			} while (0);
		} while (0);
		return old;
	}
}

extern pmd_t pmdp_invalidate_ad(struct vm_area_struct *vma,
				unsigned long address, pmd_t *pmdp);

pud_t pudp_invalidate(struct vm_area_struct *vma, unsigned long address,
		      pud_t *pudp);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pgdp_maps_userspace(void *__ptr)
{
	unsigned long ptr = (unsigned long)__ptr;

	return (((ptr & ~(~(((1UL) << 12) - 1))) / sizeof(pgd_t)) <
		((((1UL) << 12) / 2) / sizeof(pgd_t)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pgd_leaf(pgd_t pgd)
{
	return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
ptr_set_bit(void *ptr, int bit)
{
	unsigned long __ptr = (unsigned long)ptr;

	__ptr |= ((((1UL))) << (bit));
	return (void *)__ptr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
ptr_clear_bit(void *ptr, int bit)
{
	unsigned long __ptr = (unsigned long)ptr;

	__ptr &= ~((((1UL))) << (bit));
	return (void *)__ptr;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgd_t *
kernel_to_user_pgdp(pgd_t *pgdp)
{
	return ptr_set_bit(pgdp, 12);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pgd_t *
user_to_kernel_pgdp(pgd_t *pgdp)
{
	return ptr_clear_bit(pgdp, 12);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) p4d_t *
kernel_to_user_p4dp(p4d_t *p4dp)
{
	return ptr_set_bit(p4dp, 12);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) p4d_t *
user_to_kernel_p4dp(p4d_t *p4dp)
{
	return ptr_clear_bit(p4dp, 12);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clone_pgd_range(pgd_t *dst, pgd_t *src, int count)
{
	({
		const size_t __fortify_size = (size_t)(count * sizeof(pgd_t));
		const size_t __p_size = (__builtin_dynamic_object_size(dst, 0));
		const size_t __q_size = (__builtin_dynamic_object_size(src, 0));
		const size_t __p_size_field =
			(__builtin_dynamic_object_size(dst, 1));
		const size_t __q_size_field =
			(__builtin_dynamic_object_size(src, 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"481"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"481"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(481));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"dst"
								"\" at "
								"arch/x86/include/asm/pgtable.h"
								":"
								"1539",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"482"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"482"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(482));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("arch/x86/include/asm/pgtable.h"),
										  "i"(1539),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"483"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"483"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(483));
								});
							} while (0);
							({
								asm volatile(
									"484"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"484"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(484));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(dst, src, __fortify_size);
	});

	if (!(__builtin_constant_p((
		      __builtin_constant_p((7 * 32 + 11)) &&
				      (((((7 * 32 + 11)) >> 5) == (0) &&
					(1UL << (((7 * 32 + 11)) & 31) &
					 ((1 << ((0 * 32 + 0) & 31)) |
					  (1 << ((0 * 32 + 3)) & 31) |
					  (1 << ((0 * 32 + 5) & 31)) |
					  (1 << ((0 * 32 + 6) & 31)) |
					  (1 << ((0 * 32 + 8) & 31)) |
					  (1 << ((0 * 32 + 13)) & 31) |
					  (1 << ((0 * 32 + 24) & 31)) |
					  (1 << ((0 * 32 + 15) & 31)) |
					  (1 << ((0 * 32 + 25) & 31)) |
					  (1 << ((0 * 32 + 26) & 31))))) ||
				       ((((7 * 32 + 11)) >> 5) == (1) &&
					(1UL << (((7 * 32 + 11)) & 31) &
					 ((1 << ((1 * 32 + 29) & 31)) | 0))) ||
				       ((((7 * 32 + 11)) >> 5) == (2) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (3) &&
					(1UL << (((7 * 32 + 11)) & 31) &
					 ((1 << ((3 * 32 + 20) & 31))))) ||
				       ((((7 * 32 + 11)) >> 5) == (4) &&
					(1UL << (((7 * 32 + 11)) & 31) & (0))) ||
				       ((((7 * 32 + 11)) >> 5) == (5) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (6) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (7) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (8) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (9) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (10) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (11) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (12) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (13) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (14) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (15) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (16) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (17) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (18) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (19) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (20) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((((7 * 32 + 11)) >> 5) == (21) &&
					(1UL << (((7 * 32 + 11)) & 31) & 0)) ||
				       ((int)(sizeof(struct {
					       int : (-!!(22 != 22));
				       }))) ||
				       ((int)(sizeof(struct {
					       int : (-!!(22 != 22));
				       })))) ?
			      1 :
			      arch_test_bit(
				      (7 * 32 + 11),
				      (unsigned long
					       *)((&boot_cpu_data)
							  ->x86_capability)))) ?
		      (__builtin_constant_p((7 * 32 + 11)) &&
				       (((((7 * 32 + 11)) >> 5) == (0) &&
					 (1UL << (((7 * 32 + 11)) & 31) &
					  ((1 << ((0 * 32 + 0) & 31)) |
					   (1 << ((0 * 32 + 3)) & 31) |
					   (1 << ((0 * 32 + 5) & 31)) |
					   (1 << ((0 * 32 + 6) & 31)) |
					   (1 << ((0 * 32 + 8) & 31)) |
					   (1 << ((0 * 32 + 13)) & 31) |
					   (1 << ((0 * 32 + 24) & 31)) |
					   (1 << ((0 * 32 + 15) & 31)) |
					   (1 << ((0 * 32 + 25) & 31)) |
					   (1 << ((0 * 32 + 26) & 31))))) ||
					((((7 * 32 + 11)) >> 5) == (1) &&
					 (1UL << (((7 * 32 + 11)) & 31) &
					  ((1 << ((1 * 32 + 29) & 31)) | 0))) ||
					((((7 * 32 + 11)) >> 5) == (2) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (3) &&
					 (1UL << (((7 * 32 + 11)) & 31) &
					  ((1 << ((3 * 32 + 20) & 31))))) ||
					((((7 * 32 + 11)) >> 5) == (4) &&
					 (1UL << (((7 * 32 + 11)) & 31) &
					  (0))) ||
					((((7 * 32 + 11)) >> 5) == (5) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (6) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (7) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (8) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (9) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (10) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (11) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (12) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (13) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (14) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (15) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (16) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (17) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (18) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (19) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (20) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((((7 * 32 + 11)) >> 5) == (21) &&
					 (1UL << (((7 * 32 + 11)) & 31) & 0)) ||
					((int)(sizeof(struct {
						int : (-!!(22 != 22));
					}))) ||
					((int)(sizeof(struct {
						int : (-!!(22 != 22));
					})))) ?
			       1 :
			       arch_test_bit(
				       (7 * 32 + 11),
				       (unsigned long
						*)((&boot_cpu_data)
							   ->x86_capability))) :
		      _static_cpu_has((7 * 32 + 11))))
		return;

	({
		const size_t __fortify_size = (size_t)(count * sizeof(pgd_t));
		const size_t __p_size = (__builtin_dynamic_object_size(
			kernel_to_user_pgdp(dst), 0));
		const size_t __q_size = (__builtin_dynamic_object_size(
			kernel_to_user_pgdp(src), 0));
		const size_t __p_size_field = (__builtin_dynamic_object_size(
			kernel_to_user_pgdp(dst), 1));
		const size_t __q_size_field = (__builtin_dynamic_object_size(
			kernel_to_user_pgdp(src), 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"485"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"485"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(485));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"kernel_to_user_pgdp(dst)"
								"\" at "
								"arch/x86/include/asm/pgtable.h"
								":"
								"1545",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"486"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"486"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(486));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("arch/x86/include/asm/pgtable.h"),
										  "i"(1545),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"487"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"487"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(487));
								});
							} while (0);
							({
								asm volatile(
									"488"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"488"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(488));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(kernel_to_user_pgdp(dst),
				 kernel_to_user_pgdp(src), __fortify_size);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
page_level_shift(enum pg_level level)
{
	return (12 - (__builtin_constant_p(512) ?
			      ((512) < 2 ? 0 : 63 - __builtin_clzll(512)) :
		      (sizeof(512) <= 4) ? __ilog2_u32(512) :
					   __ilog2_u64(512))) +
	       level * (__builtin_constant_p(512) ?
				((512) < 2 ? 0 : 63 - __builtin_clzll(512)) :
			(sizeof(512) <= 4) ? __ilog2_u32(512) :
					     __ilog2_u64(512));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
page_level_size(enum pg_level level)
{
	return 1UL << page_level_shift(level);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
page_level_mask(enum pg_level level)
{
	return ~(page_level_size(level) - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
update_mmu_cache(struct vm_area_struct *vma, unsigned long addr, pte_t *ptep)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
update_mmu_cache_range(struct vm_fault *vmf, struct vm_area_struct *vma,
		       unsigned long addr, pte_t *ptep, unsigned int nr)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
update_mmu_cache_pmd(struct vm_area_struct *vma, unsigned long addr, pmd_t *pmd)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
update_mmu_cache_pud(struct vm_area_struct *vma, unsigned long addr, pud_t *pud)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_swp_mkexclusive(pte_t pte)
{
	return pte_set_flags(pte, (((pteval_t)(1)) << 3));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_swp_exclusive(pte_t pte)
{
	return pte_flags(pte) & (((pteval_t)(1)) << 3);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_swp_clear_exclusive(pte_t pte)
{
	return pte_clear_flags(pte, (((pteval_t)(1)) << 3));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_swp_mksoft_dirty(pte_t pte)
{
	return pte_set_flags(pte, (((pteval_t)(0))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
pte_swp_soft_dirty(pte_t pte)
{
	return pte_flags(pte) & (((pteval_t)(0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) pte_t
pte_swp_clear_soft_dirty(pte_t pte)
{
	return pte_clear_flags(pte, (((pteval_t)(0))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u16
pte_flags_pkey(unsigned long pte_flags)
{
	return (pte_flags &
		((((pteval_t)(1)) << 59) | (((pteval_t)(1)) << 60) |
		 (((pteval_t)(1)) << 61) | (((pteval_t)(1)) << 62))) >>
	       59;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__pkru_allows_pkey(u16 pkey, bool write)
{
	u32 pkru = read_pkru();

	if (!__pkru_allows_read(pkru, pkey))
		return false;
	if (write && !__pkru_allows_write(pkru, pkey))
		return false;

	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__pte_access_permitted(unsigned long pteval, bool write)
{
	unsigned long need_pte_bits = (((pteval_t)(1)) << 0) |
				      (((pteval_t)(1)) << 2);

	if (write)
		need_pte_bits |= (((pteval_t)(1)) << 1);

	if ((pteval & need_pte_bits) != need_pte_bits)
		return 0;

	return __pkru_allows_pkey(pte_flags_pkey(pteval), write);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pte_access_permitted(pte_t pte, bool write)
{
	return __pte_access_permitted(native_pte_val(pte), write);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pmd_access_permitted(pmd_t pmd, bool write)
{
	return __pte_access_permitted(native_pmd_val(pmd), write);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pud_access_permitted(pud_t pud, bool write)
{
	return __pte_access_permitted(native_pud_val(pud), write);
}

extern bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
arch_has_pfn_modify_check(void)
{
	return (__builtin_constant_p((((22 * 32 + (18))))) &&
				(((((((22 * 32 + (18))))) >> 5) == (0) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) &
				   ((1 << ((0 * 32 + 0) & 31)) |
				    (1 << ((0 * 32 + 3)) & 31) |
				    (1 << ((0 * 32 + 5) & 31)) |
				    (1 << ((0 * 32 + 6) & 31)) |
				    (1 << ((0 * 32 + 8) & 31)) |
				    (1 << ((0 * 32 + 13)) & 31) |
				    (1 << ((0 * 32 + 24) & 31)) |
				    (1 << ((0 * 32 + 15) & 31)) |
				    (1 << ((0 * 32 + 25) & 31)) |
				    (1 << ((0 * 32 + 26) & 31))))) ||
				 ((((((22 * 32 + (18))))) >> 5) == (1) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) &
				   ((1 << ((1 * 32 + 29) & 31)) | 0))) ||
				 ((((((22 * 32 + (18))))) >> 5) == (2) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (3) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) &
				   ((1 << ((3 * 32 + 20) & 31))))) ||
				 ((((((22 * 32 + (18))))) >> 5) == (4) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) &
				   (0))) ||
				 ((((((22 * 32 + (18))))) >> 5) == (5) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (6) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (7) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (8) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (9) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (10) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (11) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (12) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (13) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (14) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (15) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (16) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (17) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (18) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (19) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (20) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((((((22 * 32 + (18))))) >> 5) == (21) &&
				  (1UL << (((((22 * 32 + (18))))) & 31) & 0)) ||
				 ((int)(sizeof(
					 struct { int : (-!!(22 != 22)); }))) ||
				 ((int)(sizeof(
					 struct { int : (-!!(22 != 22)); })))) ?
			1 :
			arch_test_bit(
				(((22 * 32 + (18)))),
				(unsigned long *)((&boot_cpu_data)
							  ->x86_capability)));
}

void arch_check_zapped_pte(struct vm_area_struct *vma, pte_t pte);

void arch_check_zapped_pmd(struct vm_area_struct *vma, pmd_t pmd);

void arch_check_zapped_pud(struct vm_area_struct *vma, pud_t pud);

extern __attribute__((section(".data..percpu"
			      ""))) __typeof__(u64) tlbstate_untag_mask;

void __flush_tlb_all(void);

void cr4_update_irqsoff(unsigned long set, unsigned long clear);
unsigned long cr4_read_shadow(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cr4_set_bits_irqsoff(unsigned long mask)
{
	cr4_update_irqsoff(mask, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cr4_clear_bits_irqsoff(unsigned long mask)
{
	cr4_update_irqsoff(0, mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cr4_set_bits(unsigned long mask)
{
	unsigned long flags;

	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			flags = arch_local_irq_save();
		} while (0);
	} while (0);
	cr4_set_bits_irqsoff(mask);
	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			do {
			} while (0);
			arch_local_irq_restore(flags);
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cr4_clear_bits(unsigned long mask)
{
	unsigned long flags;

	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			flags = arch_local_irq_save();
		} while (0);
	} while (0);
	cr4_clear_bits_irqsoff(mask);
	do {
		do {
			({
				unsigned long __dummy;
				typeof(flags) __dummy2;
				(void)(&__dummy == &__dummy2);
				1;
			});
			do {
			} while (0);
			arch_local_irq_restore(flags);
		} while (0);
	} while (0);
}
struct tlb_context {
	u64 ctx_id;
	u64 tlb_gen;
};

struct tlb_state {
	struct mm_struct *loaded_mm;

	union {
		struct mm_struct *last_user_mm;
		unsigned long last_user_mm_spec;
	};

	u16 loaded_mm_asid;
	u16 next_asid;
	bool invalidate_other;
	unsigned short user_pcid_flush_mask;

	unsigned long cr4;
	struct tlb_context ctxs[6];
};
extern __attribute__((
	section(".data..percpu"
		"..shared_aligned"))) __typeof__(struct tlb_state) cpu_tlbstate
	__attribute__((__aligned__((1 << (6)))));

struct tlb_state_shared {
	bool is_lazy;
};
extern __attribute__((
	section(".data..percpu"
		"..shared_aligned"))) __typeof__(struct tlb_state_shared)
	cpu_tlbstate_shared __attribute__((__aligned__((1 << (6)))));

bool nmi_uaccess_okay(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cr4_init_shadow(void)
{
	do {
		do {
			const void *__vpp_verify =
				(typeof((&(cpu_tlbstate.cr4)) + 0))((void *)0);
			(void)__vpp_verify;
		} while (0);
		switch (sizeof(cpu_tlbstate.cr4)) {
		case 1:
			do {
				u8 pto_val__ =
					((u8)(((unsigned long)__read_cr4()) &
					      0xff));
				if (0) {
					typeof(cpu_tlbstate.cr4) pto_tmp__;
					pto_tmp__ = (__read_cr4());
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"b "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((
						*(typeof(*(&(cpu_tlbstate.cr4)))
							  *)(uintptr_t)(&(
							cpu_tlbstate.cr4))))
					: [val] "qi"(pto_val__));
			} while (0);
			break;
		case 2:
			do {
				u16 pto_val__ =
					((u16)(((unsigned long)__read_cr4()) &
					       0xffff));
				if (0) {
					typeof(cpu_tlbstate.cr4) pto_tmp__;
					pto_tmp__ = (__read_cr4());
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"w "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((
						*(typeof(*(&(cpu_tlbstate.cr4)))
							  *)(uintptr_t)(&(
							cpu_tlbstate.cr4))))
					: [val] "ri"(pto_val__));
			} while (0);
			break;
		case 4:
			do {
				u32 pto_val__ =
					((u32)(((unsigned long)__read_cr4()) &
					       0xffffffff));
				if (0) {
					typeof(cpu_tlbstate.cr4) pto_tmp__;
					pto_tmp__ = (__read_cr4());
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"l "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((
						*(typeof(*(&(cpu_tlbstate.cr4)))
							  *)(uintptr_t)(&(
							cpu_tlbstate.cr4))))
					: [val] "ri"(pto_val__));
			} while (0);
			break;
		case 8:
			do {
				u64 pto_val__ = ((u64)(__read_cr4()));
				if (0) {
					typeof(cpu_tlbstate.cr4) pto_tmp__;
					pto_tmp__ = (__read_cr4());
					(void)pto_tmp__;
				}
				asm volatile(
					"mov"
					"q "
					"%[val]"
					", "
					"%%"
					"gs"
					":"
					"%"
					"[var]"
					: [var] "=m"((
						*(typeof(*(&(cpu_tlbstate.cr4)))
							  *)(uintptr_t)(&(
							cpu_tlbstate.cr4))))
					: [val] "re"(pto_val__));
			} while (0);
			break;
		default:
			__bad_size_call_parameter();
			break;
		}
	} while (0);
}

extern unsigned long mmu_cr4_features;
extern u32 *trampoline_cr4_features;

extern void initialize_tlbstate_and_flush(void);
struct flush_tlb_info {
	struct mm_struct *mm;
	unsigned long start;
	unsigned long end;
	u64 new_tlb_gen;
	unsigned int initiating_cpu;
	u8 stride_shift;
	u8 freed_tables;
};

void flush_tlb_local(void);
void flush_tlb_one_user(unsigned long addr);
void flush_tlb_one_kernel(unsigned long addr);
void flush_tlb_multi(const struct cpumask *cpumask,
		     const struct flush_tlb_info *info);
extern void flush_tlb_all(void);
extern void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
			       unsigned long end, unsigned int stride_shift,
			       bool freed_tables);
extern void flush_tlb_kernel_range(unsigned long start, unsigned long end);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
flush_tlb_page(struct vm_area_struct *vma, unsigned long a)
{
	flush_tlb_mm_range(vma->vm_mm, a, a + ((1UL) << 12), 12, false);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
arch_tlbbatch_should_defer(struct mm_struct *mm)
{
	bool should_defer = false;

	if (cpumask_any_but(
		    mm_cpumask(mm), ({
			    do {
				    __preempt_count_add(1);
				    __asm__ __volatile__("" : : : "memory");
			    } while (0);
			    ({
				    __this_cpu_preempt_check("read");
				    ({
					    typeof(pcpu_hot.cpu_number)
						    pscr_ret__;
					    do {
						    const void *__vpp_verify =
							    (typeof((&(pcpu_hot.cpu_number)) +
								    0))((
								    void *)0);
						    (void)__vpp_verify;
					    } while (0);
					    switch (sizeof(
						    pcpu_hot.cpu_number)) {
					    case 1:
						    pscr_ret__ = ({
							    u8 pfo_val__;
							    asm("mov"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								", "
								"%[val]"
								: [val] "="
									"q"(pfo_val__)
								: [var] "m"((*(
									typeof(*(&(
										pcpu_hot.cpu_number)))
										*)(uintptr_t)(&(
									pcpu_hot.cpu_number)))));
							    (typeof(pcpu_hot.cpu_number))(unsigned long)
								    pfo_val__;
						    });
						    break;
					    case 2:
						    pscr_ret__ = ({
							    u16 pfo_val__;
							    asm("mov"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								", "
								"%[val]"
								: [val] "="
									"r"(pfo_val__)
								: [var] "m"((*(
									typeof(*(&(
										pcpu_hot.cpu_number)))
										*)(uintptr_t)(&(
									pcpu_hot.cpu_number)))));
							    (typeof(pcpu_hot.cpu_number))(unsigned long)
								    pfo_val__;
						    });
						    break;
					    case 4:
						    pscr_ret__ = ({
							    u32 pfo_val__;
							    asm("mov"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								", "
								"%[val]"
								: [val] "="
									"r"(pfo_val__)
								: [var] "m"((*(
									typeof(*(&(
										pcpu_hot.cpu_number)))
										*)(uintptr_t)(&(
									pcpu_hot.cpu_number)))));
							    (typeof(pcpu_hot.cpu_number))(unsigned long)
								    pfo_val__;
						    });
						    break;
					    case 8:
						    pscr_ret__ = ({
							    u64 pfo_val__;
							    asm("mov"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								", "
								"%[val]"
								: [val] "="
									"r"(pfo_val__)
								: [var] "m"((*(
									typeof(*(&(
										pcpu_hot.cpu_number)))
										*)(uintptr_t)(&(
									pcpu_hot.cpu_number)))));
							    (typeof(pcpu_hot.cpu_number))(unsigned long)
								    pfo_val__;
						    });
						    break;
					    default:
						    __bad_size_call_parameter();
						    break;
					    }
					    pscr_ret__;
				    });
			    });
		    })) < nr_cpu_ids)
		should_defer = true;
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule489 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);

	return should_defer;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
inc_mm_tlb_gen(struct mm_struct *mm)
{
	return atomic64_inc_return(&mm->context.tlb_gen);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
arch_tlbbatch_add_pending(struct arch_tlbflush_unmap_batch *batch,
			  struct mm_struct *mm, unsigned long uaddr)
{
	inc_mm_tlb_gen(mm);
	cpumask_or(&batch->cpumask, &batch->cpumask, mm_cpumask(mm));
	mmu_notifier_arch_invalidate_secondary_tlbs(mm, 0, -1UL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
arch_flush_tlb_batched_pending(struct mm_struct *mm)
{
	flush_tlb_mm_range(mm, 0UL, -1UL, 0UL, true);
}

extern void arch_tlbbatch_flush(struct arch_tlbflush_unmap_batch *batch);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pte_flags_need_flush(unsigned long oldflags, unsigned long newflags,
		     bool ignore_access)
{
	const pteval_t flush_on_clear = (((pteval_t)(1)) << 6) |
					(((pteval_t)(1)) << 0) |
					(((pteval_t)(1)) << 5);
	const pteval_t software_flags =
		(((pteval_t)(1)) << 9) | (((pteval_t)(1)) << 10) |
		(((pteval_t)(1)) << 11) | (((pteval_t)(1)) << 57) |
		(((pteval_t)(1)) << 58);
	const pteval_t flush_on_change =
		(((pteval_t)(1)) << 1) | (((pteval_t)(1)) << 2) |
		(((pteval_t)(1)) << 3) | (((pteval_t)(1)) << 4) |
		(((pteval_t)(1)) << 7) | (((pteval_t)(1)) << 8) |
		(((pteval_t)(1)) << 7) | (((pteval_t)(1)) << 12) |
		(((pteval_t)(1)) << 59) | (((pteval_t)(1)) << 60) |
		(((pteval_t)(1)) << 61) | (((pteval_t)(1)) << 62) |
		(((pteval_t)(1)) << 63);
	unsigned long diff = oldflags ^ newflags;

	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_490(void) __attribute__((
			__error__("BUILD_BUG_ON failed: "
				  "flush_on_clear & software_flags")));
		if (!(!(flush_on_clear & software_flags)))
			__compiletime_assert_490();
	} while (0);
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_491(void) __attribute__((
			__error__("BUILD_BUG_ON failed: "
				  "flush_on_clear & flush_on_change")));
		if (!(!(flush_on_clear & flush_on_change)))
			__compiletime_assert_491();
	} while (0);
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_492(void) __attribute__((
			__error__("BUILD_BUG_ON failed: "
				  "flush_on_change & software_flags")));
		if (!(!(flush_on_change & software_flags)))
			__compiletime_assert_492();
	} while (0);

	diff &= ~software_flags;

	if (ignore_access)
		diff &= ~(((pteval_t)(1)) << 5);

	if (diff & oldflags & flush_on_clear)
		return true;

	if (diff & flush_on_change)
		return true;

	if (0 &&
	    (diff & ~(flush_on_clear | software_flags | flush_on_change))) {
		((void)(sizeof((long)(1))));
		return true;
	}

	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pte_needs_flush(pte_t oldpte, pte_t newpte)
{
	if (!(pte_flags(oldpte) & (((pteval_t)(1)) << 0)))
		return false;

	if (pte_pfn(oldpte) != pte_pfn(newpte))
		return true;

	return pte_flags_need_flush(pte_flags(oldpte), pte_flags(newpte), true);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
huge_pmd_needs_flush(pmd_t oldpmd, pmd_t newpmd)
{
	if (!(pmd_flags(oldpmd) & (((pteval_t)(1)) << 0)))
		return false;

	if (pmd_pfn(oldpmd) != pmd_pfn(newpmd))
		return true;

	return pte_flags_need_flush(pmd_flags(oldpmd), pmd_flags(newpmd),
				    false);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
tlbstate_lam_cr3_mask(void)
{
	return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cpu_tlbstate_update_lam(unsigned long lam, u64 untag_mask)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__native_tlb_flush_global(unsigned long cr4)
{
	native_write_cr4(cr4 ^ (((1UL)) << (7)));
	native_write_cr4(cr4);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__runtime_fixup_ptr(void *where, unsigned long val)
{
	*(unsigned long *)where = val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__runtime_fixup_shift(void *where, unsigned long val)
{
	*(unsigned char *)where = val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
runtime_const_fixup(void (*fn)(void *, unsigned long), unsigned long val,
		    s32 *start, s32 *end)
{
	while (start < end) {
		fn(*start + (void *)start, val);
		start++;
	}
}

extern unsigned long USER_PTR_MAX;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
mask_user_address(const void *ptr)
{
	unsigned long mask;
	asm("cmp %1,%0\n\t"
	    "sbb %0,%0"
	    : "=r"(mask)
	    : "r"(ptr), "0"(({
		      typeof(USER_PTR_MAX) __ret;
		      asm __inline("mov %1,%0\n1:\n"
				   ".pushsection runtime_ptr_"
				   "USER_PTR_MAX"
				   ",\"a\"\n\t"
				   ".long 1b - %c2 - .\n"
				   ".popsection"
				   : "=r"(__ret)
				   : "i"((unsigned long)0x0123456789abcdefull),
				     "i"(sizeof(long)));
		      __ret;
	      })));
	return (void *)(mask | (unsigned long)ptr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__access_ok(const void *ptr, unsigned long size)
{
	if (__builtin_constant_p(size <= ((1UL) << 12)) &&
	    size <= ((1UL) << 12)) {
		return ((unsigned long)(ptr) <= ({
				typeof(USER_PTR_MAX) __ret;
				asm __inline(
					"mov %1,%0\n1:\n"
					".pushsection runtime_ptr_"
					"USER_PTR_MAX"
					",\"a\"\n\t"
					".long 1b - %c2 - .\n"
					".popsection"
					: "=r"(__ret)
					: "i"((unsigned long)0x0123456789abcdefull),
					  "i"(sizeof(long)));
				__ret;
			}));
	} else {
		unsigned long sum = size + (unsigned long)ptr;

		return ((unsigned long)(sum) <= ({
				typeof(USER_PTR_MAX) __ret;
				asm __inline(
					"mov %1,%0\n1:\n"
					".pushsection runtime_ptr_"
					"USER_PTR_MAX"
					",\"a\"\n\t"
					".long 1b - %c2 - .\n"
					".popsection"
					: "=r"(__ret)
					: "i"((unsigned long)0x0123456789abcdefull),
					  "i"(sizeof(long)));
				__ret;
			})) &&
		       sum >= (unsigned long)ptr;
	}
}

__attribute__((__warn_unused_result__)) unsigned long
rep_movs_alternative(void *to, const void *from, unsigned len);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) unsigned long
copy_user_generic(void *to, const void *from, unsigned long len)
{
	stac();

	asm volatile("1:\n\t"
		     "# ALT: oldinstr\n"
		     "771:\n\t"
		     "rep movsb"
		     "\n772:\n"
		     "# ALT: padding\n"
		     ".skip -((("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")) > 0) * "
		     "(("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")),0x90\n"
		     "773:\n"
		     ".pushsection .altinstructions,\"a\"\n"
		     " .long 771b - .\n"
		     " .long 774f - .\n"
		     " .4byte "
		     "(((1 << 0) << 16) | ((18*32+ 4)))"
		     "\n"
		     " .byte "
		     "773b-771b"
		     "\n"
		     " .byte "
		     "775f-774f"
		     "\n"
		     ".popsection\n"
		     ".pushsection .altinstr_replacement, \"ax\"\n"
		     "# ALT: replacement\n"
		     "774:\n\t"
		     "call rep_movs_alternative"
		     "\n775:\n"
		     ".popsection\n"

		     "2:\n"
		     " .pushsection \"__ex_table\",\"a\"\n"
		     " .balign 4\n"
		     " .long ("
		     "1b"
		     ") - .\n"
		     " .long ("
		     "2b"
		     ") - .\n"
		     " .long "
		     "3"
		     " \n"
		     " .popsection\n"
		     : "+c"(len), "+D"(to), "+S"(from),
		       "+r"(current_stack_pointer)
		     :
		     : "memory", "rax");
	clac();
	return len;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) unsigned long
raw_copy_from_user(void *dst, const void *src, unsigned long size)
{
	return copy_user_generic(dst, (void *)src, size);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) unsigned long
raw_copy_to_user(void *dst, const void *src, unsigned long size)
{
	return copy_user_generic((void *)dst, src, size);
}

extern long __copy_user_nocache(void *dst, const void *src, unsigned size);
extern long __copy_user_flushcache(void *dst, const void *src, unsigned size);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__copy_from_user_inatomic_nocache(void *dst, const void *src, unsigned size)
{
	long ret;
	kasan_check_write(dst, size);
	stac();
	ret = __copy_user_nocache(dst, src, size);
	clac();
	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__copy_from_user_flushcache(void *dst, const void *src, unsigned size)
{
	kasan_check_write(dst, size);
	return __copy_user_flushcache(dst, src, size);
}

__attribute__((__warn_unused_result__)) unsigned long
rep_stos_alternative(void *addr, unsigned long len);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) unsigned long
__clear_user(void *addr, unsigned long size)
{
	might_fault();
	stac();

	asm volatile("1:\n\t"
		     "# ALT: oldinstr\n"
		     "771:\n\t"
		     "rep stosb"
		     "\n772:\n"
		     "# ALT: padding\n"
		     ".skip -((("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")) > 0) * "
		     "(("
		     "775f-774f"
		     ")-("
		     "772b-771b"
		     ")),0x90\n"
		     "773:\n"
		     ".pushsection .altinstructions,\"a\"\n"
		     " .long 771b - .\n"
		     " .long 774f - .\n"
		     " .4byte "
		     "(((1 << 0) << 16) | ((12*32+11)))"
		     "\n"
		     " .byte "
		     "773b-771b"
		     "\n"
		     " .byte "
		     "775f-774f"
		     "\n"
		     ".popsection\n"
		     ".pushsection .altinstr_replacement, \"ax\"\n"
		     "# ALT: replacement\n"
		     "774:\n\t"
		     "call rep_stos_alternative"
		     "\n775:\n"
		     ".popsection\n"

		     "2:\n"
		     " .pushsection \"__ex_table\",\"a\"\n"
		     " .balign 4\n"
		     " .long ("
		     "1b"
		     ") - .\n"
		     " .long ("
		     "2b"
		     ") - .\n"
		     " .long "
		     "3"
		     " \n"
		     " .popsection\n"
		     : "+c"(size), "+D"(addr), "+r"(current_stack_pointer)
		     : "a"(0));

	clac();

	return size;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
clear_user(void *to, unsigned long n)
{
	if (__access_ok(to, n))
		return __clear_user(to, n);
	return n;
}

extern int __get_user_1(void);
extern int __get_user_2(void);
extern int __get_user_4(void);
extern int __get_user_8(void);
extern int __get_user_nocheck_1(void);
extern int __get_user_nocheck_2(void);
extern int __get_user_nocheck_4(void);
extern int __get_user_nocheck_8(void);
extern int __get_user_bad(void);
extern void __put_user_bad(void);

extern void __put_user_1(void);
extern void __put_user_2(void);
extern void __put_user_4(void);
extern void __put_user_8(void);
extern void __put_user_nocheck_1(void);
extern void __put_user_nocheck_2(void);
extern void __put_user_nocheck_4(void);
extern void __put_user_nocheck_8(void);
struct __large_struct {
	unsigned long buf[100];
};
extern unsigned long copy_from_user_nmi(void *to, const void *from,
					unsigned long n);
extern __attribute__((__warn_unused_result__)) long
strncpy_from_user(char *dst, const char *src, long count);

extern __attribute__((__warn_unused_result__)) long
strnlen_user(const char *str, long n);

unsigned long __attribute__((__warn_unused_result__))
copy_mc_to_kernel(void *to, const void *from, unsigned len);

unsigned long __attribute__((__warn_unused_result__))
copy_mc_to_user(void *to, const void *from, unsigned len);
static __attribute__((__warn_unused_result__)) inline
	__attribute__((__gnu_inline__)) __attribute__((__unused__))
	__attribute__((no_instrument_function))
	__attribute__((__always_inline__)) bool
	user_access_begin(const void *ptr, size_t len)
{
	if (__builtin_expect(
		    !!(!__builtin_expect(!!(__access_ok(ptr, len)), 1)), 0))
		return 0;
	({
		stac();
		asm __inline volatile(
			"# ALT: oldinstr\n"
			"771:\n\t"
			""
			"\n772:\n"
			"# ALT: padding\n"
			".skip -((("
			"775f-774f"
			")-("
			"772b-771b"
			")) > 0) * "
			"(("
			"775f-774f"
			")-("
			"772b-771b"
			")),0x90\n"
			"773:\n"
			".pushsection .altinstructions,\"a\"\n"
			" .long 771b - .\n"
			" .long 774f - .\n"
			" .4byte "
			"(20*32+ 2)"
			"\n"
			" .byte "
			"773b-771b"
			"\n"
			" .byte "
			"775f-774f"
			"\n"
			".popsection\n"
			".pushsection .altinstr_replacement, \"ax\"\n"
			"# ALT: replacement\n"
			"774:\n\t"
			"lfence"
			"\n775:\n"
			".popsection\n"
			:
			:
			: "memory");
	});
	return 1;
}
extern void __try_cmpxchg_user_wrong_size(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) unsigned long
__copy_from_user_inatomic(void *to, const void *from, unsigned long n)
{
	unsigned long res;

	instrument_copy_from_user_before(to, from, n);
	check_object_size(to, n, false);
	res = raw_copy_from_user(to, from, n);
	instrument_copy_from_user_after(to, from, n, res);
	return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) unsigned long
__copy_from_user(void *to, const void *from, unsigned long n)
{
	unsigned long res;

	might_fault();
	instrument_copy_from_user_before(to, from, n);
	if (should_fail_usercopy())
		return n;
	check_object_size(to, n, false);
	res = raw_copy_from_user(to, from, n);
	instrument_copy_from_user_after(to, from, n, res);
	return res;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) unsigned long
__copy_to_user_inatomic(void *to, const void *from, unsigned long n)
{
	if (should_fail_usercopy())
		return n;
	instrument_copy_to_user(to, from, n);
	check_object_size(from, n, true);
	return raw_copy_to_user(to, from, n);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) unsigned long
__copy_to_user(void *to, const void *from, unsigned long n)
{
	might_fault();
	if (should_fail_usercopy())
		return n;
	instrument_copy_to_user(to, from, n);
	check_object_size(from, n, true);
	return raw_copy_to_user(to, from, n);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) unsigned long
_inline_copy_from_user(void *to, const void *from, unsigned long n)
{
	unsigned long res = n;
	might_fault();
	if (should_fail_usercopy())
		goto fail;
	if (1)
		from = mask_user_address(from);
	else {
		if (!__builtin_expect(!!(__access_ok(from, n)), 1))
			goto fail;

		asm __inline volatile(
			"# ALT: oldinstr\n"
			"771:\n\t"
			""
			"\n772:\n"
			"# ALT: padding\n"
			".skip -((("
			"775f-774f"
			")-("
			"772b-771b"
			")) > 0) * "
			"(("
			"775f-774f"
			")-("
			"772b-771b"
			")),0x90\n"
			"773:\n"
			".pushsection .altinstructions,\"a\"\n"
			" .long 771b - .\n"
			" .long 774f - .\n"
			" .4byte "
			"(20*32+ 2)"
			"\n"
			" .byte "
			"773b-771b"
			"\n"
			" .byte "
			"775f-774f"
			"\n"
			".popsection\n"
			".pushsection .altinstr_replacement, \"ax\"\n"
			"# ALT: replacement\n"
			"774:\n\t"
			"lfence"
			"\n775:\n"
			".popsection\n"
			:
			:
			: "memory");
	}
	instrument_copy_from_user_before(to, from, n);
	res = raw_copy_from_user(to, from, n);
	instrument_copy_from_user_after(to, from, n, res);
	if (__builtin_expect(!!(!res), 1))
		return 0;
fail:
	({
		size_t __fortify_size = (size_t)(res);
		fortify_memset_chk(
			__fortify_size,
			__builtin_dynamic_object_size(to + (n - res), 0),
			__builtin_dynamic_object_size(to + (n - res), 1)),
			__builtin_memset(to + (n - res), 0, __fortify_size);
	});
	return res;
}
extern __attribute__((__warn_unused_result__)) unsigned long
_copy_from_user(void *, const void *, unsigned long);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__warn_unused_result__)) unsigned long
_inline_copy_to_user(void *to, const void *from, unsigned long n)
{
	might_fault();
	if (should_fail_usercopy())
		return n;
	if (__builtin_expect(!!(__access_ok(to, n)), 1)) {
		instrument_copy_to_user(to, from, n);
		n = raw_copy_to_user(to, from, n);
	}
	return n;
}
extern __attribute__((__warn_unused_result__)) unsigned long
_copy_to_user(void *, const void *, unsigned long);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
	__attribute__((__warn_unused_result__))
	copy_from_user(void *to, const void *from, unsigned long n)
{
	if (!check_copy_size(to, n, false))
		return n;

	return _copy_from_user(to, from, n);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
	__attribute__((__warn_unused_result__))
	copy_to_user(void *to, const void *from, unsigned long n)
{
	if (!check_copy_size(from, n, true))
		return n;

	return _copy_to_user(to, from, n);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
pagefault_disabled_inc(void)
{
	get_current()->pagefault_disabled++;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
pagefault_disabled_dec(void)
{
	get_current()->pagefault_disabled--;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
pagefault_disable(void)
{
	pagefault_disabled_inc();

	__asm__ __volatile__("" : : : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
pagefault_enable(void)
{
	__asm__ __volatile__("" : : : "memory");
	pagefault_disabled_dec();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
pagefault_disabled(void)
{
	return get_current()->pagefault_disabled != 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) size_t
probe_subpage_writeable(char *uaddr, size_t size)
{
	return 0;
}
extern __attribute__((__warn_unused_result__)) int
check_zeroed_user(const void *from, size_t size);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__))
__attribute__((__warn_unused_result__)) int
copy_struct_from_user(void *dst, size_t ksize, const void *src, size_t usize)
{
	size_t size = ({
		__auto_type __UNIQUE_ID_x_493 = (ksize);
		__auto_type __UNIQUE_ID_y_494 = (usize);
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_495(void) __attribute__((
				__error__("min"
					  "("
					  "ksize"
					  ", "
					  "usize"
					  ") signedness error")));
			if (!(!(!(((((typeof(__UNIQUE_ID_x_493))(-1)) <
				    (typeof(__UNIQUE_ID_x_493))1) ?
					   (2 + (__builtin_constant_p(
							 (long)(ksize) >= 0) &&
						 ((long)(ksize) >= 0))) :
					   (1 + 2 * (sizeof(__UNIQUE_ID_x_493) <
						     4))) &
				  ((((typeof(__UNIQUE_ID_y_494))(-1)) <
				    (typeof(__UNIQUE_ID_y_494))1) ?
					   (2 + (__builtin_constant_p(
							 (long)(usize) >= 0) &&
						 ((long)(usize) >= 0))) :
					   (1 + 2 * (sizeof(__UNIQUE_ID_y_494) <
						     4)))))))
				__compiletime_assert_495();
		} while (0);
		((__UNIQUE_ID_x_493) < (__UNIQUE_ID_y_494) ?
			 (__UNIQUE_ID_x_493) :
			 (__UNIQUE_ID_y_494));
	});
	size_t rest =
		({
			__auto_type __UNIQUE_ID_x_496 = (ksize);
			__auto_type __UNIQUE_ID_y_497 = (usize);
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_498(void) __attribute__((
					__error__("max"
						  "("
						  "ksize"
						  ", "
						  "usize"
						  ") signedness error")));
				if (!(!(!(((((typeof(__UNIQUE_ID_x_496))(-1)) <
					    (typeof(__UNIQUE_ID_x_496))1) ?
						   (2 +
						    (__builtin_constant_p(
							     (long)(ksize) >=
							     0) &&
						     ((long)(ksize) >= 0))) :
						   (1 +
						    2 * (sizeof(__UNIQUE_ID_x_496) <
							 4))) &
					  ((((typeof(__UNIQUE_ID_y_497))(-1)) <
					    (typeof(__UNIQUE_ID_y_497))1) ?
						   (2 +
						    (__builtin_constant_p(
							     (long)(usize) >=
							     0) &&
						     ((long)(usize) >= 0))) :
						   (1 +
						    2 * (sizeof(__UNIQUE_ID_y_497) <
							 4)))))))
					__compiletime_assert_498();
			} while (0);
			((__UNIQUE_ID_x_496) > (__UNIQUE_ID_y_497) ?
				 (__UNIQUE_ID_x_496) :
				 (__UNIQUE_ID_y_497));
		}) -
		size;

	if (({
		    int __ret_warn_on =
			    !!(ksize > __builtin_object_size(dst, 1));
		    if (__builtin_expect(!!(__ret_warn_on), 0))
			    do {
				    __auto_type __flags =
					    (1 << 0) | ((1 << 1) | ((9) << 8));
				    ({
					    asm volatile(
						    "499"
						    ": nop\n\t"
						    ".pushsection .discard.instr_begin\n\t"
						    ".long "
						    "499"
						    "b - .\n\t"
						    ".popsection\n\t"
						    :
						    : "i"(499));
				    });
				    do {
					    asm __inline volatile(
						    "1:\t"
						    ".byte 0x0f, 0x0b"
						    "\n"
						    ".pushsection __bug_table,\"aw\"\n"
						    "2:\t"
						    ".long "
						    "1b"
						    " - ."
						    "\t# bug_entry::bug_addr\n"
						    "\t"
						    ".long "
						    "%c0"
						    " - ."
						    "\t# bug_entry::file\n"
						    "\t.word %c1"
						    "\t# bug_entry::line\n"
						    "\t.word %c2"
						    "\t# bug_entry::flags\n"
						    "\t.org 2b+%c3\n"
						    ".popsection\n"
						    "998:\n\t"
						    ".pushsection .discard.reachable\n\t"
						    ".long 998b\n\t"
						    ".popsection\n\t"
						    :
						    : "i"("include/linux/uaccess.h"),
						      "i"(389), "i"(__flags),
						      "i"(sizeof(
							      struct bug_entry)));
				    } while (0);
				    ({
					    asm volatile(
						    "500"
						    ": nop\n\t"
						    ".pushsection .discard.instr_end\n\t"
						    ".long "
						    "500"
						    "b - .\n\t"
						    ".popsection\n\t"
						    :
						    : "i"(500));
				    });
			    } while (0);
		    __builtin_expect(!!(__ret_warn_on), 0);
	    }))
		return -7;

	if (usize < ksize) {
		({
			size_t __fortify_size = (size_t)(rest);
			fortify_memset_chk(
				__fortify_size,
				__builtin_dynamic_object_size(dst + size, 0),
				__builtin_dynamic_object_size(dst + size, 1)),
				__builtin_memset(dst + size, 0, __fortify_size);
		});
	} else if (usize > ksize) {
		int ret = check_zeroed_user(src + size, rest);
		if (ret <= 0)
			return ret ?: -7;
	}

	if (copy_from_user(dst, src, size))
		return -14;
	return 0;
}

bool copy_from_kernel_nofault_allowed(const void *unsafe_src, size_t size);

long copy_from_kernel_nofault(void *dst, const void *src, size_t size);
long __attribute__((no_instrument_function))
copy_to_kernel_nofault(void *dst, const void *src, size_t size);

long copy_from_user_nofault(void *dst, const void *src, size_t size);
long __attribute__((no_instrument_function))
copy_to_user_nofault(void *dst, const void *src, size_t size);

long strncpy_from_kernel_nofault(char *dst, const void *unsafe_addr,
				 long count);

long strncpy_from_user_nofault(char *dst, const void *unsafe_addr, long count);
long strnlen_user_nofault(const void *unsafe_addr, long count);

struct task_struct;
struct rusage;
union thread_union;
struct css_set;

struct kernel_clone_args {
	u64 flags;
	int *pidfd;
	int *child_tid;
	int *parent_tid;
	const char *name;
	int exit_signal;
	u32 kthread : 1;
	u32 io_thread : 1;
	u32 user_worker : 1;
	u32 no_files : 1;
	unsigned long stack;
	unsigned long stack_size;
	unsigned long tls;
	pid_t *set_tid;

	size_t set_tid_size;
	int cgroup;
	int idle;
	int (*fn)(void *);
	void *fn_arg;
	struct cgroup *cgrp;
	struct css_set *cset;
};

extern rwlock_t tasklist_lock;
extern spinlock_t mmlist_lock;

extern union thread_union init_thread_union;
extern struct task_struct init_task;

extern int lockdep_tasklist_lock_is_held(void);

extern void schedule_tail(struct task_struct *prev);
extern void init_idle(struct task_struct *idle, int cpu);

extern int sched_fork(unsigned long clone_flags, struct task_struct *p);
extern int sched_cgroup_fork(struct task_struct *p,
			     struct kernel_clone_args *kargs);
extern void sched_cancel_fork(struct task_struct *p);
extern void sched_post_fork(struct task_struct *p);
extern void sched_dead(struct task_struct *p);

void __attribute__((__noreturn__)) do_task_dead(void);
void __attribute__((__noreturn__)) make_task_dead(int signr);

extern void mm_cache_init(void);
extern void proc_caches_init(void);

extern void fork_init(void);

extern void release_task(struct task_struct *p);

extern int copy_thread(struct task_struct *, const struct kernel_clone_args *);

extern void flush_thread(void);

extern void exit_thread(struct task_struct *tsk);

extern __attribute__((__noreturn__)) void do_group_exit(int);

extern void exit_files(struct task_struct *);
extern void exit_itimers(struct task_struct *);

extern pid_t kernel_clone(struct kernel_clone_args *kargs);
struct task_struct *copy_process(struct pid *pid, int trace, int node,
				 struct kernel_clone_args *args);
struct task_struct *create_io_thread(int (*fn)(void *), void *arg, int node);
struct task_struct *fork_idle(int);
extern pid_t kernel_thread(int (*fn)(void *), void *arg, const char *name,
			   unsigned long flags);
extern pid_t user_mode_thread(int (*fn)(void *), void *arg,
			      unsigned long flags);
extern long kernel_wait4(pid_t, int *, int, struct rusage *);
int kernel_wait(pid_t pid, int *stat);

extern void free_task(struct task_struct *tsk);

extern void sched_exec(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct task_struct *
get_task_struct(struct task_struct *t)
{
	refcount_inc(&t->usage);
	return t;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct task_struct *
tryget_task_struct(struct task_struct *t)
{
	return refcount_inc_not_zero(&t->usage) ? t : ((void *)0);
}

extern void __put_task_struct(struct task_struct *t);
extern void __put_task_struct_rcu_cb(struct callback_head *rhp);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
put_task_struct(struct task_struct *t)
{
	if (!refcount_dec_and_test(&t->usage))
		return;

	if (!0 || (preempt_count() == 0 && !({
			   unsigned long _flags;
			   do {
				   ({
					   unsigned long __dummy;
					   typeof(_flags) __dummy2;
					   (void)(&__dummy == &__dummy2);
					   1;
				   });
				   _flags = arch_local_save_flags();
			   } while (0);
			   ({
				   ({
					   unsigned long __dummy;
					   typeof(_flags) __dummy2;
					   (void)(&__dummy == &__dummy2);
					   1;
				   });
				   arch_irqs_disabled_flags(_flags);
			   });
		   }))) {
		static struct lockdep_map
			__attribute__((__unused__)) put_task_map = {};

		do {
		} while (0);
		__put_task_struct(t);
		do {
		} while (0);
		return;
	}
	call_rcu(&t->rcu, __put_task_struct_rcu_cb);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__free_put_task(void *p)
{
	struct task_struct *_T = *(struct task_struct **)p;
	if (_T)
		put_task_struct(_T);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
put_task_struct_many(struct task_struct *t, int nr)
{
	if (refcount_sub_and_test(nr, &t->usage))
		__put_task_struct(t);
}

void put_task_struct_rcu_user(struct task_struct *task);

void release_thread(struct task_struct *dead_task);

extern int arch_task_struct_size
	__attribute__((__section__(".data..read_mostly")));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct vm_struct *
task_stack_vm_area(const struct task_struct *t)
{
	return t->stack_vm_area;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_lock(struct task_struct *p)
{
	spin_lock(&p->alloc_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
task_unlock(struct task_struct *p)
{
	spin_unlock(&p->alloc_lock);
}

typedef struct task_struct *class_task_lock_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
class_task_lock_destructor(struct task_struct **p)
{
	struct task_struct *_T = *p;
	if (_T) {
		task_unlock(_T);
	};
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct task_struct *
class_task_lock_constructor(struct task_struct *_T)
{
	struct task_struct *t = ({
		task_lock(_T);
		_T;
	});
	return t;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
class_task_lock_lock_ptr(class_task_lock_t *_T)
{
	return *_T;
}
struct assoc_array {
	struct assoc_array_ptr *root;
	unsigned long nr_leaves_on_tree;
};

struct assoc_array_ops {
	unsigned long (*get_key_chunk)(const void *index_key, int level);

	unsigned long (*get_object_key_chunk)(const void *object, int level);

	bool (*compare_object)(const void *object, const void *index_key);

	int (*diff_objects)(const void *object, const void *index_key);

	void (*free_object)(void *object);
};

struct assoc_array_edit;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
assoc_array_init(struct assoc_array *array)
{
	array->root = ((void *)0);
	array->nr_leaves_on_tree = 0;
}

extern int assoc_array_iterate(const struct assoc_array *array,
			       int (*iterator)(const void *object,
					       void *iterator_data),
			       void *iterator_data);
extern void *assoc_array_find(const struct assoc_array *array,
			      const struct assoc_array_ops *ops,
			      const void *index_key);
extern void assoc_array_destroy(struct assoc_array *array,
				const struct assoc_array_ops *ops);
extern struct assoc_array_edit *
assoc_array_insert(struct assoc_array *array, const struct assoc_array_ops *ops,
		   const void *index_key, void *object);
extern void assoc_array_insert_set_object(struct assoc_array_edit *edit,
					  void *object);
extern struct assoc_array_edit *
assoc_array_delete(struct assoc_array *array, const struct assoc_array_ops *ops,
		   const void *index_key);
extern struct assoc_array_edit *
assoc_array_clear(struct assoc_array *array, const struct assoc_array_ops *ops);
extern void assoc_array_apply_edit(struct assoc_array_edit *edit);
extern void assoc_array_cancel_edit(struct assoc_array_edit *edit);
extern int assoc_array_gc(struct assoc_array *array,
			  const struct assoc_array_ops *ops,
			  bool (*iterator)(void *object, void *iterator_data),
			  void *iterator_data);

typedef int32_t key_serial_t;

typedef uint32_t key_perm_t;

struct key;
struct net;
enum key_need_perm {
	KEY_NEED_UNSPECIFIED,
	KEY_NEED_VIEW,
	KEY_NEED_READ,
	KEY_NEED_WRITE,
	KEY_NEED_SEARCH,
	KEY_NEED_LINK,
	KEY_NEED_SETATTR,
	KEY_NEED_UNLINK,
	KEY_SYSADMIN_OVERRIDE,
	KEY_AUTHTOKEN_OVERRIDE,
	KEY_DEFER_PERM_CHECK,
};

enum key_lookup_flag {
	KEY_LOOKUP_CREATE = 0x01,
	KEY_LOOKUP_PARTIAL = 0x02,
	KEY_LOOKUP_ALL = (KEY_LOOKUP_CREATE | KEY_LOOKUP_PARTIAL),
};

struct seq_file;
struct user_struct;
struct signal_struct;
struct cred;

struct key_type;
struct key_owner;
struct key_tag;
struct keyring_list;
struct keyring_name;

struct key_tag {
	struct callback_head rcu;
	refcount_t usage;
	bool removed;
};

struct keyring_index_key {
	unsigned long hash;
	union {
		struct {
			u16 desc_len;
			char desc[sizeof(long) - 2];
		};
		unsigned long x;
	};
	struct key_type *type;
	struct key_tag *domain_tag;
	const char *description;
};

union key_payload {
	void *rcu_data0;
	void *data[4];
};
typedef struct __key_reference_with_attributes *key_ref_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) key_ref_t
make_key_ref(const struct key *key, bool possession)
{
	return (key_ref_t)((unsigned long)key | possession);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct key *
key_ref_to_ptr(const key_ref_t key_ref)
{
	return (struct key *)((unsigned long)key_ref & ~1UL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_key_possessed(const key_ref_t key_ref)
{
	return (unsigned long)key_ref & 1UL;
}

typedef int (*key_restrict_link_func_t)(struct key *dest_keyring,
					const struct key_type *type,
					const union key_payload *payload,
					struct key *restriction_key);

struct key_restriction {
	key_restrict_link_func_t check;
	struct key *key;
	struct key_type *keytype;
};

enum key_state {
	KEY_IS_UNINSTANTIATED,
	KEY_IS_POSITIVE,
};
struct key {
	refcount_t usage;
	key_serial_t serial;
	union {
		struct list_head graveyard_link;
		struct rb_node serial_node;
	};

	struct rw_semaphore sem;
	struct key_user *user;
	void *security;
	union {
		time64_t expiry;
		time64_t revoked_at;
	};
	time64_t last_used_at;
	kuid_t uid;
	kgid_t gid;
	key_perm_t perm;
	unsigned short quotalen;
	unsigned short datalen;

	short state;

	unsigned long flags;
	union {
		struct keyring_index_key index_key;
		struct {
			unsigned long hash;
			unsigned long len_desc;
			struct key_type *type;
			struct key_tag *domain_tag;
			char *description;
		};
	};

	union {
		union key_payload payload;
		struct {
			struct list_head name_link;
			struct assoc_array keys;
		};
	};
	struct key_restriction *restrict_link;
};

extern struct key *key_alloc(struct key_type *type, const char *desc,
			     kuid_t uid, kgid_t gid, const struct cred *cred,
			     key_perm_t perm, unsigned long flags,
			     struct key_restriction *restrict_link);
extern void key_revoke(struct key *key);
extern void key_invalidate(struct key *key);
extern void key_put(struct key *key);
extern bool key_put_tag(struct key_tag *tag);
extern void key_remove_domain(struct key_tag *domain_tag);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct key *
__key_get(struct key *key)
{
	refcount_inc(&key->usage);
	return key;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct key *
key_get(struct key *key)
{
	return key ? __key_get(key) : key;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
key_ref_put(key_ref_t key_ref)
{
	key_put(key_ref_to_ptr(key_ref));
}

extern struct key *request_key_tag(struct key_type *type,
				   const char *description,
				   struct key_tag *domain_tag,
				   const char *callout_info);

extern struct key *request_key_rcu(struct key_type *type,
				   const char *description,
				   struct key_tag *domain_tag);

extern struct key *request_key_with_auxdata(struct key_type *type,
					    const char *description,
					    struct key_tag *domain_tag,
					    const void *callout_info,
					    size_t callout_len, void *aux);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct key *
request_key(struct key_type *type, const char *description,
	    const char *callout_info)
{
	return request_key_tag(type, description, ((void *)0), callout_info);
}
extern int wait_for_key_construction(struct key *key, bool intr);

extern int key_validate(const struct key *key);

extern key_ref_t key_create(key_ref_t keyring, const char *type,
			    const char *description, const void *payload,
			    size_t plen, key_perm_t perm, unsigned long flags);

extern key_ref_t key_create_or_update(key_ref_t keyring, const char *type,
				      const char *description,
				      const void *payload, size_t plen,
				      key_perm_t perm, unsigned long flags);

extern int key_update(key_ref_t key, const void *payload, size_t plen);

extern int key_link(struct key *keyring, struct key *key);

extern int key_move(struct key *key, struct key *from_keyring,
		    struct key *to_keyring, unsigned int flags);

extern int key_unlink(struct key *keyring, struct key *key);

extern struct key *keyring_alloc(const char *description, kuid_t uid,
				 kgid_t gid, const struct cred *cred,
				 key_perm_t perm, unsigned long flags,
				 struct key_restriction *restrict_link,
				 struct key *dest);

extern int restrict_link_reject(struct key *keyring,
				const struct key_type *type,
				const union key_payload *payload,
				struct key *restriction_key);

extern int keyring_clear(struct key *keyring);

extern key_ref_t keyring_search(key_ref_t keyring, struct key_type *type,
				const char *description, bool recurse);

extern int keyring_restrict(key_ref_t keyring, const char *type,
			    const char *restriction);

extern struct key *key_lookup(key_serial_t id);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) key_serial_t
key_serial(const struct key *key)
{
	return key ? key->serial : 0;
}

extern void key_set_timeout(struct key *, unsigned);

extern key_ref_t lookup_user_key(key_serial_t id, unsigned long flags,
				 enum key_need_perm need_perm);
extern void key_free_user_ns(struct user_namespace *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) short
key_read_state(const struct key *key)
{
	return ({
		typeof(*&key->state) ___p1 = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_501(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*&key->state) == sizeof(char) ||
				       sizeof(*&key->state) == sizeof(short) ||
				       sizeof(*&key->state) == sizeof(int) ||
				       sizeof(*&key->state) == sizeof(long)) ||
				      sizeof(*&key->state) ==
					      sizeof(long long)))
					__compiletime_assert_501();
			} while (0);
			(*(const volatile typeof(_Generic(
				(*&key->state),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 *&key->state)))
				   *)&(*&key->state));
		});
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_502(void) __attribute__((__error__(
				"Need native word sized stores/loads for atomicity.")));
			if (!((sizeof(*&key->state) == sizeof(char) ||
			       sizeof(*&key->state) == sizeof(short) ||
			       sizeof(*&key->state) == sizeof(int) ||
			       sizeof(*&key->state) == sizeof(long))))
				__compiletime_assert_502();
		} while (0);
		__asm__ __volatile__("" : : : "memory");
		___p1;
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
key_is_positive(const struct key *key)
{
	return key_read_state(key) == KEY_IS_POSITIVE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
key_is_negative(const struct key *key)
{
	return key_read_state(key) < 0;
}
extern int install_thread_keyring_to_cred(struct cred *cred);
extern void key_fsuid_changed(struct cred *new_cred);
extern void key_fsgid_changed(struct cred *new_cred);
extern void key_init(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ratelimit_state_init(struct ratelimit_state *rs, int interval, int burst)
{
	({
		size_t __fortify_size = (size_t)(sizeof(*rs));
		fortify_memset_chk(__fortify_size,
				   __builtin_dynamic_object_size(rs, 0),
				   __builtin_dynamic_object_size(rs, 1)),
			__builtin_memset(rs, 0, __fortify_size);
	});

	do {
		*(&rs->lock) = (raw_spinlock_t){
			.raw_lock = { { .val = { (0) } } },
		};
	} while (0);
	rs->interval = interval;
	rs->burst = burst;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ratelimit_default_init(struct ratelimit_state *rs)
{
	return ratelimit_state_init(rs, (5 * 1000), 10);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ratelimit_state_exit(struct ratelimit_state *rs)
{
	if (!(rs->flags & ((((1UL))) << (0))))
		return;

	if (rs->missed) {
		({
			do {
			} while (0);
			_printk("\001"
				"4"
				"%s: %d output lines suppressed due to ratelimiting\n",
				get_current()->comm, rs->missed);
		});

		rs->missed = 0;
	}
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ratelimit_set_flags(struct ratelimit_state *rs, unsigned long flags)
{
	rs->flags = flags;
}

extern struct ratelimit_state printk_ratelimit_state;

struct user_struct {
	refcount_t __count;

	struct percpu_counter epoll_watches;

	unsigned long unix_inflight;
	atomic_long_t pipe_bufs;

	struct hlist_node uidhash_node;
	kuid_t uid;

	atomic_long_t locked_vm;

	struct ratelimit_state ratelimit;
};

extern int uids_sysfs_init(void);

extern struct user_struct *find_user(kuid_t);

extern struct user_struct root_user;

extern struct user_struct *alloc_uid(kuid_t);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct user_struct *
get_uid(struct user_struct *u)
{
	refcount_inc(&u->__count);
	return u;
}
extern void free_uid(struct user_struct *);

struct cred;
struct inode;

struct group_info {
	refcount_t usage;
	int ngroups;
	kgid_t gid[];
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct group_info *
get_group_info(struct group_info *gi)
{
	refcount_inc(&gi->usage);
	return gi;
}
extern struct group_info *groups_alloc(int);
extern void groups_free(struct group_info *);

extern int in_group_p(kgid_t);
extern int in_egroup_p(kgid_t);
extern int groups_search(const struct group_info *, kgid_t);

extern int set_current_groups(struct group_info *);
extern void set_groups(struct cred *, struct group_info *);
extern bool may_setgroups(void);
extern void groups_sort(struct group_info *);
struct cred {
	atomic_long_t usage;
	kuid_t uid;
	kgid_t gid;
	kuid_t suid;
	kgid_t sgid;
	kuid_t euid;
	kgid_t egid;
	kuid_t fsuid;
	kgid_t fsgid;
	unsigned securebits;
	kernel_cap_t cap_inheritable;
	kernel_cap_t cap_permitted;
	kernel_cap_t cap_effective;
	kernel_cap_t cap_bset;
	kernel_cap_t cap_ambient;

	unsigned char jit_keyring;

	struct key *session_keyring;
	struct key *process_keyring;
	struct key *thread_keyring;
	struct key *request_key_auth;

	void *security;

	struct user_struct *user;
	struct user_namespace *user_ns;
	struct ucounts *ucounts;
	struct group_info *group_info;

	union {
		int non_rcu;
		struct callback_head rcu;
	};
};

extern void __put_cred(struct cred *);
extern void exit_creds(struct task_struct *);
extern int copy_creds(struct task_struct *, unsigned long);
extern const struct cred *get_task_cred(struct task_struct *);
extern struct cred *cred_alloc_blank(void);
extern struct cred *prepare_creds(void);
extern struct cred *prepare_exec_creds(void);
extern int commit_creds(struct cred *);
extern void abort_creds(struct cred *);
extern const struct cred *override_creds(const struct cred *);
extern void revert_creds(const struct cred *);
extern struct cred *prepare_kernel_cred(struct task_struct *);
extern int set_security_override(struct cred *, u32);
extern int set_security_override_from_ctx(struct cred *, const char *);
extern int set_create_files_as(struct cred *, struct inode *);
extern int cred_fscmp(const struct cred *, const struct cred *);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
cred_init(void);
extern int set_cred_ucounts(struct cred *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
cap_ambient_invariant_ok(const struct cred *cred)
{
	return cap_issubset(cred->cap_ambient,
			    cap_intersect(cred->cap_permitted,
					  cred->cap_inheritable));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct cred *
get_new_cred_many(struct cred *cred, int nr)
{
	atomic_long_add(nr, &cred->usage);
	return cred;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct cred *
get_new_cred(struct cred *cred)
{
	return get_new_cred_many(cred, 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const struct cred *
get_cred_many(const struct cred *cred, int nr)
{
	struct cred *nonconst_cred = (struct cred *)cred;
	if (!cred)
		return cred;
	nonconst_cred->non_rcu = 0;
	return get_new_cred_many(nonconst_cred, nr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const struct cred *
get_cred(const struct cred *cred)
{
	return get_cred_many(cred, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const struct cred *
get_cred_rcu(const struct cred *cred)
{
	struct cred *nonconst_cred = (struct cred *)cred;
	if (!cred)
		return ((void *)0);
	if (!atomic_long_inc_not_zero(&nonconst_cred->usage))
		return ((void *)0);
	nonconst_cred->non_rcu = 0;
	return cred;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
put_cred_many(const struct cred *_cred, int nr)
{
	struct cred *cred = (struct cred *)_cred;

	if (cred) {
		if (atomic_long_sub_and_test(nr, &cred->usage))
			__put_cred(cred);
	}
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
put_cred(const struct cred *cred)
{
	put_cred_many(cred, 1);
}
extern struct user_namespace init_user_ns;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct user_namespace *
current_user_ns(void)
{
	return &init_user_ns;
}

extern bool timerqueue_add(struct timerqueue_head *head,
			   struct timerqueue_node *node);
extern bool timerqueue_del(struct timerqueue_head *head,
			   struct timerqueue_node *node);
extern struct timerqueue_node *
timerqueue_iterate_next(struct timerqueue_node *node);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timerqueue_node *
timerqueue_getnext(struct timerqueue_head *head)
{
	struct rb_node *leftmost = (&head->rb_root)->rb_leftmost;

	return ({
		typeof(leftmost) ____ptr = (leftmost);
		____ptr ? ({
			void *__mptr = (void *)(____ptr);
			_Static_assert(
				__builtin_types_compatible_p(
					typeof(*(____ptr)),
					typeof(((struct timerqueue_node *)0)
						       ->node)) ||
					__builtin_types_compatible_p(
						typeof(*(____ptr)),
						typeof(void)),
				"pointer type mismatch in container_of()");
			((struct timerqueue_node
				  *)(__mptr -
				     __builtin_offsetof(struct timerqueue_node,
							node)));
		}) :
			  ((void *)0);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
timerqueue_init(struct timerqueue_node *node)
{
	((&node->node)->__rb_parent_color = (unsigned long)(&node->node));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
timerqueue_node_queued(struct timerqueue_node *node)
{
	return !((&node->node)->__rb_parent_color ==
		 (unsigned long)(&node->node));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
timerqueue_init_head(struct timerqueue_head *head)
{
	head->rb_root = (struct rb_root_cached){ {
							 ((void *)0),
						 },
						 ((void *)0) };
}
struct hrtimer_clock_base {
	struct hrtimer_cpu_base *cpu_base;
	unsigned int index;
	clockid_t clockid;
	seqcount_raw_spinlock_t seq;
	struct hrtimer *running;
	struct timerqueue_head active;
	ktime_t (*get_time)(void);
	ktime_t offset;
} __attribute__((__aligned__((1 << (6)))));

enum hrtimer_base_type {
	HRTIMER_BASE_MONOTONIC,
	HRTIMER_BASE_REALTIME,
	HRTIMER_BASE_BOOTTIME,
	HRTIMER_BASE_TAI,
	HRTIMER_BASE_MONOTONIC_SOFT,
	HRTIMER_BASE_REALTIME_SOFT,
	HRTIMER_BASE_BOOTTIME_SOFT,
	HRTIMER_BASE_TAI_SOFT,
	HRTIMER_MAX_CLOCK_BASES,
};
struct hrtimer_cpu_base {
	raw_spinlock_t lock;
	unsigned int cpu;
	unsigned int active_bases;
	unsigned int clock_was_set_seq;
	unsigned int hres_active : 1, in_hrtirq : 1, hang_detected : 1,
		softirq_activated : 1, online : 1;

	unsigned int nr_events;
	unsigned short nr_retries;
	unsigned short nr_hangs;
	unsigned int max_hang_time;

	ktime_t expires_next;
	struct hrtimer *next_timer;
	ktime_t softirq_expires_next;
	struct hrtimer *softirq_next_timer;
	struct hrtimer_clock_base clock_base[HRTIMER_MAX_CLOCK_BASES];
} __attribute__((__aligned__((1 << (6)))));
enum hrtimer_mode {
	HRTIMER_MODE_ABS = 0x00,
	HRTIMER_MODE_REL = 0x01,
	HRTIMER_MODE_PINNED = 0x02,
	HRTIMER_MODE_SOFT = 0x04,
	HRTIMER_MODE_HARD = 0x08,

	HRTIMER_MODE_ABS_PINNED = HRTIMER_MODE_ABS | HRTIMER_MODE_PINNED,
	HRTIMER_MODE_REL_PINNED = HRTIMER_MODE_REL | HRTIMER_MODE_PINNED,

	HRTIMER_MODE_ABS_SOFT = HRTIMER_MODE_ABS | HRTIMER_MODE_SOFT,
	HRTIMER_MODE_REL_SOFT = HRTIMER_MODE_REL | HRTIMER_MODE_SOFT,

	HRTIMER_MODE_ABS_PINNED_SOFT = HRTIMER_MODE_ABS_PINNED |
				       HRTIMER_MODE_SOFT,
	HRTIMER_MODE_REL_PINNED_SOFT = HRTIMER_MODE_REL_PINNED |
				       HRTIMER_MODE_SOFT,

	HRTIMER_MODE_ABS_HARD = HRTIMER_MODE_ABS | HRTIMER_MODE_HARD,
	HRTIMER_MODE_REL_HARD = HRTIMER_MODE_REL | HRTIMER_MODE_HARD,

	HRTIMER_MODE_ABS_PINNED_HARD = HRTIMER_MODE_ABS_PINNED |
				       HRTIMER_MODE_HARD,
	HRTIMER_MODE_REL_PINNED_HARD = HRTIMER_MODE_REL_PINNED |
				       HRTIMER_MODE_HARD,
};
struct hrtimer_sleeper {
	struct hrtimer timer;
	struct task_struct *task;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_set_expires(struct hrtimer *timer, ktime_t time)
{
	timer->node.expires = time;
	timer->_softexpires = time;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_set_expires_range(struct hrtimer *timer, ktime_t time, ktime_t delta)
{
	timer->_softexpires = time;
	timer->node.expires = ktime_add_safe(time, delta);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_set_expires_range_ns(struct hrtimer *timer, ktime_t time, u64 delta)
{
	timer->_softexpires = time;
	timer->node.expires = ktime_add_safe(time, ns_to_ktime(delta));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_set_expires_tv64(struct hrtimer *timer, s64 tv64)
{
	timer->node.expires = tv64;
	timer->_softexpires = tv64;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_add_expires(struct hrtimer *timer, ktime_t time)
{
	timer->node.expires = ktime_add_safe(timer->node.expires, time);
	timer->_softexpires = ktime_add_safe(timer->_softexpires, time);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_add_expires_ns(struct hrtimer *timer, u64 ns)
{
	timer->node.expires = ((timer->node.expires) + (ns));
	timer->_softexpires = ((timer->_softexpires) + (ns));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
hrtimer_get_expires(const struct hrtimer *timer)
{
	return timer->node.expires;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
hrtimer_get_softexpires(const struct hrtimer *timer)
{
	return timer->_softexpires;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
hrtimer_get_expires_tv64(const struct hrtimer *timer)
{
	return timer->node.expires;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
hrtimer_get_softexpires_tv64(const struct hrtimer *timer)
{
	return timer->_softexpires;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) s64
hrtimer_get_expires_ns(const struct hrtimer *timer)
{
	return ktime_to_ns(timer->node.expires);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
hrtimer_expires_remaining(const struct hrtimer *timer)
{
	return ((timer->node.expires) - (timer->base->get_time()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
hrtimer_cb_get_time(struct hrtimer *timer)
{
	return timer->base->get_time();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
hrtimer_is_hres_active(struct hrtimer *timer)
{
	return 1 ? timer->base->cpu_base->hres_active : 0;
}

struct clock_event_device;

extern void hrtimer_interrupt(struct clock_event_device *dev);

extern unsigned int hrtimer_resolution;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
__hrtimer_expires_remaining_adjusted(const struct hrtimer *timer, ktime_t now)
{
	ktime_t rem = ((timer->node.expires) - (now));

	if (0 && timer->is_rel)
		rem -= hrtimer_resolution;
	return rem;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
hrtimer_expires_remaining_adjusted(const struct hrtimer *timer)
{
	return __hrtimer_expires_remaining_adjusted(timer,
						    timer->base->get_time());
}

extern void timerfd_clock_was_set(void);
extern void timerfd_resume(void);

extern __attribute__((
	section(".data..percpu"
		""))) __typeof__(struct tick_device) tick_cpu_device;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_cancel_wait_running(struct hrtimer *timer)
{
	cpu_relax();
}

extern void hrtimer_init(struct hrtimer *timer, clockid_t which_clock,
			 enum hrtimer_mode mode);
extern void hrtimer_init_sleeper(struct hrtimer_sleeper *sl, clockid_t clock_id,
				 enum hrtimer_mode mode);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_init_on_stack(struct hrtimer *timer, clockid_t which_clock,
		      enum hrtimer_mode mode)
{
	hrtimer_init(timer, which_clock, mode);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_init_sleeper_on_stack(struct hrtimer_sleeper *sl, clockid_t clock_id,
			      enum hrtimer_mode mode)
{
	hrtimer_init_sleeper(sl, clock_id, mode);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
destroy_hrtimer_on_stack(struct hrtimer *timer)
{
}

extern void hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
				   u64 range_ns, const enum hrtimer_mode mode);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_start(struct hrtimer *timer, ktime_t tim, const enum hrtimer_mode mode)
{
	hrtimer_start_range_ns(timer, tim, 0, mode);
}

extern int hrtimer_cancel(struct hrtimer *timer);
extern int hrtimer_try_to_cancel(struct hrtimer *timer);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_start_expires(struct hrtimer *timer, enum hrtimer_mode mode)
{
	u64 delta;
	ktime_t soft, hard;
	soft = hrtimer_get_softexpires(timer);
	hard = hrtimer_get_expires(timer);
	delta = ktime_to_ns(((hard) - (soft)));
	hrtimer_start_range_ns(timer, soft, delta, mode);
}

void hrtimer_sleeper_start_expires(struct hrtimer_sleeper *sl,
				   enum hrtimer_mode mode);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
hrtimer_restart(struct hrtimer *timer)
{
	hrtimer_start_expires(timer, HRTIMER_MODE_ABS);
}

extern ktime_t __hrtimer_get_remaining(const struct hrtimer *timer,
				       bool adjust);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ktime_t
hrtimer_get_remaining(const struct hrtimer *timer)
{
	return __hrtimer_get_remaining(timer, false);
}

extern u64 hrtimer_get_next_event(void);
extern u64 hrtimer_next_event_without(const struct hrtimer *exclude);

extern bool hrtimer_active(const struct hrtimer *timer);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
hrtimer_is_queued(struct hrtimer *timer)
{
	return !!(
		({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_503(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(timer->state) == sizeof(char) ||
				       sizeof(timer->state) == sizeof(short) ||
				       sizeof(timer->state) == sizeof(int) ||
				       sizeof(timer->state) == sizeof(long)) ||
				      sizeof(timer->state) ==
					      sizeof(long long)))
					__compiletime_assert_503();
			} while (0);
			(*(const volatile typeof(_Generic(
				(timer->state),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 timer->state)))
				   *)&(timer->state));
		}) &
		0x01);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
hrtimer_callback_running(struct hrtimer *timer)
{
	return timer->base->running == timer;
}

extern u64 hrtimer_forward(struct hrtimer *timer, ktime_t now,
			   ktime_t interval);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
hrtimer_forward_now(struct hrtimer *timer, ktime_t interval)
{
	return hrtimer_forward(timer, timer->base->get_time(), interval);
}

extern int nanosleep_copyout(struct restart_block *, struct timespec64 *);
extern long hrtimer_nanosleep(ktime_t rqtp, const enum hrtimer_mode mode,
			      const clockid_t clockid);

extern int schedule_hrtimeout_range(ktime_t *expires, u64 delta,
				    const enum hrtimer_mode mode);
extern int schedule_hrtimeout_range_clock(ktime_t *expires, u64 delta,
					  const enum hrtimer_mode mode,
					  clockid_t clock_id);
extern int schedule_hrtimeout(ktime_t *expires, const enum hrtimer_mode mode);

extern void hrtimer_run_queues(void);

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
hrtimers_init(void);

extern void sysrq_timer_list_show(void);

int hrtimers_prepare_cpu(unsigned int cpu);

int hrtimers_cpu_dying(unsigned int cpu);

struct rtc_device;

enum alarmtimer_type {
	ALARM_REALTIME,
	ALARM_BOOTTIME,

	ALARM_NUMTYPE,

	ALARM_REALTIME_FREEZER,
	ALARM_BOOTTIME_FREEZER,
};

enum alarmtimer_restart {
	ALARMTIMER_NORESTART,
	ALARMTIMER_RESTART,
};
struct alarm {
	struct timerqueue_node node;
	struct hrtimer timer;
	enum alarmtimer_restart (*function)(struct alarm *, ktime_t now);
	enum alarmtimer_type type;
	int state;
	void *data;
};

void alarm_init(struct alarm *alarm, enum alarmtimer_type type,
		enum alarmtimer_restart (*function)(struct alarm *, ktime_t));
void alarm_start(struct alarm *alarm, ktime_t start);
void alarm_start_relative(struct alarm *alarm, ktime_t start);
void alarm_restart(struct alarm *alarm);
int alarm_try_to_cancel(struct alarm *alarm);
int alarm_cancel(struct alarm *alarm);

u64 alarm_forward(struct alarm *alarm, ktime_t now, ktime_t interval);
u64 alarm_forward_now(struct alarm *alarm, ktime_t interval);
ktime_t alarm_expires_remaining(const struct alarm *alarm);

struct rtc_device *alarmtimer_get_rtcdev(void);

struct kernel_siginfo;
struct task_struct;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) clockid_t
make_process_cpuclock(const unsigned int pid, const clockid_t clock)
{
	return ((~pid) << 3) | clock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) clockid_t
make_thread_cpuclock(const unsigned int tid, const clockid_t clock)
{
	return make_process_cpuclock(tid, clock | 4);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) clockid_t
fd_to_clockid(const int fd)
{
	return make_process_cpuclock((unsigned int)fd, 3);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
clockid_to_fd(const clockid_t clk)
{
	return ~(clk >> 3);
}
struct cpu_timer {
	struct timerqueue_node node;
	struct timerqueue_head *head;
	struct pid *pid;
	struct list_head elist;
	int firing;
	struct task_struct *handling;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
cpu_timer_enqueue(struct timerqueue_head *head, struct cpu_timer *ctmr)
{
	ctmr->head = head;
	return timerqueue_add(head, &ctmr->node);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
cpu_timer_queued(struct cpu_timer *ctmr)
{
	return !!ctmr->head;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
cpu_timer_dequeue(struct cpu_timer *ctmr)
{
	if (cpu_timer_queued(ctmr)) {
		timerqueue_del(ctmr->head, &ctmr->node);
		ctmr->head = ((void *)0);
		return true;
	}
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u64
cpu_timer_getexpires(struct cpu_timer *ctmr)
{
	return ctmr->node.expires;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
cpu_timer_setexpires(struct cpu_timer *ctmr, u64 exp)
{
	ctmr->node.expires = exp;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
posix_cputimers_init(struct posix_cputimers *pct)
{
	({
		size_t __fortify_size = (size_t)(sizeof(*pct));
		fortify_memset_chk(__fortify_size,
				   __builtin_dynamic_object_size(pct, 0),
				   __builtin_dynamic_object_size(pct, 1)),
			__builtin_memset(pct, 0, __fortify_size);
	});
	pct->bases[0].nextevt = ((u64)~0ULL);
	pct->bases[1].nextevt = ((u64)~0ULL);
	pct->bases[2].nextevt = ((u64)~0ULL);
}

void posix_cputimers_group_init(struct posix_cputimers *pct, u64 cpu_limit);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
posix_cputimers_rt_watchdog(struct posix_cputimers *pct, u64 runtime)
{
	pct->bases[2].nextevt = runtime;
}
void clear_posix_cputimers_work(struct task_struct *p);
void posix_cputimers_init_work(void);
struct k_itimer {
	struct hlist_node list;
	struct hlist_node t_hash;
	spinlock_t it_lock;
	const struct k_clock *kclock;
	clockid_t it_clock;
	timer_t it_id;
	int it_active;
	s64 it_overrun;
	s64 it_overrun_last;
	int it_requeue_pending;
	int it_sigev_notify;
	ktime_t it_interval;
	struct signal_struct *it_signal;
	union {
		struct pid *it_pid;
		struct task_struct *it_process;
	};
	struct sigqueue *sigq;
	union {
		struct {
			struct hrtimer timer;
		} real;
		struct cpu_timer cpu;
		struct {
			struct alarm alarmtimer;
		} alarm;
	} it;
	struct callback_head rcu;
};

void run_posix_cpu_timers(void);
void posix_cpu_timers_exit(struct task_struct *task);
void posix_cpu_timers_exit_group(struct task_struct *task);
void set_process_cpu_timer(struct task_struct *task, unsigned int clock_idx,
			   u64 *newval, u64 *oldval);

int update_rlimit_cpu(struct task_struct *task, unsigned long rlim_new);

void posixtimer_rearm(struct kernel_siginfo *info);

struct sighand_struct {
	spinlock_t siglock;
	refcount_t count;
	wait_queue_head_t signalfd_wqh;
	struct k_sigaction action[64];
};

struct pacct_struct {
	int ac_flag;
	long ac_exitcode;
	unsigned long ac_mem;
	u64 ac_utime, ac_stime;
	unsigned long ac_minflt, ac_majflt;
};

struct cpu_itimer {
	u64 expires;
	u64 incr;
};

struct task_cputime_atomic {
	atomic64_t utime;
	atomic64_t stime;
	atomic64_t sum_exec_runtime;
};
struct thread_group_cputimer {
	struct task_cputime_atomic cputime_atomic;
};

struct multiprocess_signals {
	sigset_t signal;
	struct hlist_node node;
};

struct core_thread {
	struct task_struct *task;
	struct core_thread *next;
};

struct core_state {
	atomic_t nr_threads;
	struct core_thread dumper;
	struct completion startup;
};
struct signal_struct {
	refcount_t sigcnt;
	atomic_t live;
	int nr_threads;
	int quick_threads;
	struct list_head thread_head;

	wait_queue_head_t wait_chldexit;

	struct task_struct *curr_target;

	struct sigpending shared_pending;

	struct hlist_head multiprocess;

	int group_exit_code;

	int notify_count;
	struct task_struct *group_exec_task;

	int group_stop_count;
	unsigned int flags;

	struct core_state *core_state;
	unsigned int is_child_subreaper : 1;
	unsigned int has_child_subreaper : 1;

	unsigned int next_posix_timer_id;
	struct hlist_head posix_timers;

	struct hrtimer real_timer;
	ktime_t it_real_incr;

	struct cpu_itimer it[2];

	struct thread_group_cputimer cputimer;

	struct posix_cputimers posix_cputimers;

	struct pid *pids[PIDTYPE_MAX];

	struct pid *tty_old_pgrp;

	int leader;

	struct tty_struct *tty;
	seqlock_t stats_lock;
	u64 utime, stime, cutime, cstime;
	u64 gtime;
	u64 cgtime;
	struct prev_cputime prev_cputime;
	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
	unsigned long inblock, oublock, cinblock, coublock;
	unsigned long maxrss, cmaxrss;
	struct task_io_accounting ioac;

	unsigned long long sum_sched_runtime;
	struct rlimit rlim[16];

	struct pacct_struct pacct;

	struct taskstats *stats;

	unsigned audit_tty;
	struct tty_audit_buf *tty_audit_buf;

	bool oom_flag_origin;
	short oom_score_adj;
	short oom_score_adj_min;

	struct mm_struct *oom_mm;

	struct mutex cred_guard_mutex;

	struct rw_semaphore exec_update_lock;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
signal_set_stop_flags(struct signal_struct *sig, unsigned int flags)
{
	({
		int __ret_warn_on = !!(sig->flags & 0x00000004);
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) | (((9) << 8));
				({
					asm volatile(
						"504"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"504"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(504));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/sched/signal.h"),
						  "i"(272), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"505"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"505"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(505));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	sig->flags = (sig->flags &
		      ~((0x00000010 | 0x00000020) | 0x00000001 | 0x00000002)) |
		     flags;
}

extern void flush_signals(struct task_struct *);
extern void ignore_signals(struct task_struct *);
extern void flush_signal_handlers(struct task_struct *, int force_default);
extern int dequeue_signal(sigset_t *mask, kernel_siginfo_t *info,
			  enum pid_type *type);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kernel_dequeue_signal(void)
{
	struct task_struct *task = get_current();
	kernel_siginfo_t __info;
	enum pid_type __type;
	int ret;

	spin_lock_irq(&task->sighand->siglock);
	ret = dequeue_signal(&task->blocked, &__info, &__type);
	spin_unlock_irq(&task->sighand->siglock);

	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kernel_signal_stop(void)
{
	spin_lock_irq(&get_current()->sighand->siglock);
	if (get_current()->jobctl & (1UL << 16)) {
		get_current()->jobctl |= (1UL << 26);
		do {
			unsigned long flags;
			do {
				({
					unsigned long __dummy;
					typeof(flags) __dummy2;
					(void)(&__dummy == &__dummy2);
					1;
				});
				flags = _raw_spin_lock_irqsave(
					&get_current()->pi_lock);
			} while (0);
			do {
			} while (0);
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_506(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(get_current()->__state) ==
						       sizeof(char) ||
					       sizeof(get_current()->__state) ==
						       sizeof(short) ||
					       sizeof(get_current()->__state) ==
						       sizeof(int) ||
					       sizeof(get_current()->__state) ==
						       sizeof(long)) ||
					      sizeof(get_current()->__state) ==
						      sizeof(long long)))
						__compiletime_assert_506();
				} while (0);
				do {
					*(volatile typeof(get_current()->__state)
						  *)&(get_current()->__state) =
						(((0x00000100 | 0x00000004)));
				} while (0);
			} while (0);
			do {
				({
					unsigned long __dummy;
					typeof(flags) __dummy2;
					(void)(&__dummy == &__dummy2);
					1;
				});
				_raw_spin_unlock_irqrestore(
					&get_current()->pi_lock, flags);
			} while (0);
		} while (0);
	}
	spin_unlock_irq(&get_current()->sighand->siglock);

	schedule();
}

int force_sig_fault_to_task(int sig, int code, void *addr,
			    struct task_struct *t);
int force_sig_fault(int sig, int code, void *addr);
int send_sig_fault(int sig, int code, void *addr, struct task_struct *t);

int force_sig_mceerr(int code, void *, short);
int send_sig_mceerr(int code, void *, short, struct task_struct *);

int force_sig_bnderr(void *addr, void *lower, void *upper);
int force_sig_pkuerr(void *addr, u32 pkey);
int send_sig_perf(void *addr, u32 type, u64 sig_data);

int force_sig_ptrace_errno_trap(int errno, void *addr);
int force_sig_fault_trapno(int sig, int code, void *addr, int trapno);
int send_sig_fault_trapno(int sig, int code, void *addr, int trapno,
			  struct task_struct *t);
int force_sig_seccomp(int syscall, int reason, bool force_coredump);

extern int send_sig_info(int, struct kernel_siginfo *, struct task_struct *);
extern void force_sigsegv(int sig);
extern int force_sig_info(struct kernel_siginfo *);
extern int __kill_pgrp_info(int sig, struct kernel_siginfo *info,
			    struct pid *pgrp);
extern int kill_pid_info(int sig, struct kernel_siginfo *info, struct pid *pid);
extern int kill_pid_usb_asyncio(int sig, int errno, sigval_t addr, struct pid *,
				const struct cred *);
extern int kill_pgrp(struct pid *pid, int sig, int priv);
extern int kill_pid(struct pid *pid, int sig, int priv);
extern __attribute__((__warn_unused_result__)) bool
do_notify_parent(struct task_struct *, int);
extern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);
extern void force_sig(int);
extern void force_fatal_sig(int);
extern void force_exit_sig(int);
extern int send_sig(int, struct task_struct *, int);
extern int zap_other_threads(struct task_struct *p);
extern struct sigqueue *sigqueue_alloc(void);
extern void sigqueue_free(struct sigqueue *);
extern int send_sigqueue(struct sigqueue *, struct pid *, enum pid_type);
extern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_notify_signal(void)
{
	clear_ti_thread_flag(((struct thread_info *)get_current()), 17);
	do {
		do {
		} while (0);
		do {
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__set_notify_signal(struct task_struct *task)
{
	return !test_and_set_tsk_thread_flag(task, 17) &&
	       !wake_up_state(task, 0x00000001);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_notify_signal(struct task_struct *task)
{
	if (__set_notify_signal(task))
		kick_process(task);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
restart_syscall(void)
{
	set_tsk_thread_flag(get_current(), 2);
	return -513;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
task_sigpending(struct task_struct *p)
{
	return __builtin_expect(!!(test_tsk_thread_flag(p, 2)), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
signal_pending(struct task_struct *p)
{
	if (__builtin_expect(!!(test_tsk_thread_flag(p, 17)), 0))
		return 1;
	return task_sigpending(p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__fatal_signal_pending(struct task_struct *p)
{
	return __builtin_expect(!!(sigismember(&p->pending.signal, 9)), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
fatal_signal_pending(struct task_struct *p)
{
	return task_sigpending(p) && __fatal_signal_pending(p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
signal_pending_state(unsigned int state, struct task_struct *p)
{
	if (!(state & (0x00000001 | 0x00000100)))
		return 0;
	if (!signal_pending(p))
		return 0;

	return (state & 0x00000001) || __fatal_signal_pending(p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
fault_signal_pending(vm_fault_t fault_flags, struct pt_regs *regs)
{
	return __builtin_expect(
		!!((fault_flags & VM_FAULT_RETRY) &&
		   (fatal_signal_pending(get_current()) ||
		    (user_mode(regs) && signal_pending(get_current())))),
		0);
}

extern void recalc_sigpending(void);
extern void calculate_sigpending(void);

extern void signal_wake_up_state(struct task_struct *t, unsigned int state);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
signal_wake_up(struct task_struct *t, bool fatal)
{
	unsigned int state = 0;
	if (fatal && !(t->jobctl & (1UL << 24))) {
		t->jobctl &= ~((1UL << 26) | (1UL << 27));
		state = 0x00000100 | 0x00000008;
	}
	signal_wake_up_state(t, state);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ptrace_signal_wake_up(struct task_struct *t, bool resume)
{
	unsigned int state = 0;
	if (resume) {
		t->jobctl &= ~(1UL << 27);
		state = 0x00000008;
	}
	signal_wake_up_state(t, state);
}

void task_join_group_stop(struct task_struct *task);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_restore_sigmask(void)
{
	get_current()->restore_sigmask = true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_tsk_restore_sigmask(struct task_struct *task)
{
	task->restore_sigmask = false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_restore_sigmask(void)
{
	get_current()->restore_sigmask = false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
test_restore_sigmask(void)
{
	return get_current()->restore_sigmask;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
test_tsk_restore_sigmask(struct task_struct *task)
{
	return task->restore_sigmask;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
test_and_clear_restore_sigmask(void)
{
	if (!get_current()->restore_sigmask)
		return false;
	get_current()->restore_sigmask = false;
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
restore_saved_sigmask(void)
{
	if (test_and_clear_restore_sigmask())
		__set_current_blocked(&get_current()->saved_sigmask);
}

extern int set_user_sigmask(const sigset_t *umask, size_t sigsetsize);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
restore_saved_sigmask_unless(bool interrupted)
{
	if (interrupted)
		({
			int __ret_warn_on = !!(!signal_pending(get_current()));
			if (__builtin_expect(!!(__ret_warn_on), 0))
				do {
					__auto_type __flags = (1 << 0) |
							      (((9) << 8));
					({
						asm volatile(
							"507"
							": nop\n\t"
							".pushsection .discard.instr_begin\n\t"
							".long "
							"507"
							"b - .\n\t"
							".popsection\n\t"
							:
							: "i"(507));
					});
					do {
						asm __inline volatile(
							"1:\t"
							".byte 0x0f, 0x0b"
							"\n"
							".pushsection __bug_table,\"aw\"\n"
							"2:\t"
							".long "
							"1b"
							" - ."
							"\t# bug_entry::bug_addr\n"
							"\t"
							".long "
							"%c0"
							" - ."
							"\t# bug_entry::file\n"
							"\t.word %c1"
							"\t# bug_entry::line\n"
							"\t.word %c2"
							"\t# bug_entry::flags\n"
							"\t.org 2b+%c3\n"
							".popsection\n"
							"998:\n\t"
							".pushsection .discard.reachable\n\t"
							".long 998b\n\t"
							".popsection\n\t"
							:
							: "i"("include/linux/sched/signal.h"),
							  "i"(548),
							  "i"(__flags),
							  "i"(sizeof(
								  struct bug_entry)));
					} while (0);
					({
						asm volatile(
							"508"
							": nop\n\t"
							".pushsection .discard.instr_end\n\t"
							".long "
							"508"
							"b - .\n\t"
							".popsection\n\t"
							:
							: "i"(508));
					});
				} while (0);
			__builtin_expect(!!(__ret_warn_on), 0);
		});
	else
		restore_saved_sigmask();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) sigset_t *
sigmask_to_save(void)
{
	sigset_t *res = &get_current()->blocked;
	if (__builtin_expect(!!(test_restore_sigmask()), 0))
		res = &get_current()->saved_sigmask;
	return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kill_cad_pid(int sig, int priv)
{
	return kill_pid(cad_pid, sig, priv);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__on_sig_stack(unsigned long sp)
{
	return sp > get_current()->sas_ss_sp &&
	       sp - get_current()->sas_ss_sp <= get_current()->sas_ss_size;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
on_sig_stack(unsigned long sp)
{
	if (get_current()->sas_ss_flags & (1U << 31))
		return 0;

	return __on_sig_stack(sp);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sas_ss_flags(unsigned long sp)
{
	if (!get_current()->sas_ss_size)
		return 2;

	return on_sig_stack(sp) ? 1 : 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sas_ss_reset(struct task_struct *p)
{
	p->sas_ss_sp = 0;
	p->sas_ss_size = 0;
	p->sas_ss_flags = 2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
sigsp(unsigned long sp, struct ksignal *ksig)
{
	if (__builtin_expect(!!((ksig->ka.sa.sa_flags & 0x08000000)), 0) &&
	    !sas_ss_flags(sp))

		return get_current()->sas_ss_sp + get_current()->sas_ss_size;

	return sp;
}

extern void __cleanup_sighand(struct sighand_struct *);
extern void flush_itimer_signals(void);
extern bool current_is_single_threaded(void);
typedef int (*proc_visitor)(struct task_struct *p, void *data);
void walk_process_tree(struct task_struct *top, proc_visitor, void *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct pid *
task_pid_type(struct task_struct *task, enum pid_type type)
{
	struct pid *pid;
	if (type == PIDTYPE_PID)
		pid = task_pid(task);
	else
		pid = task->signal->pids[type];
	return pid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct pid *
task_tgid(struct task_struct *task)
{
	return task->signal->pids[PIDTYPE_TGID];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct pid *
task_pgrp(struct task_struct *task)
{
	return task->signal->pids[PIDTYPE_PGID];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct pid *
task_session(struct task_struct *task)
{
	return task->signal->pids[PIDTYPE_SID];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_nr_threads(struct task_struct *task)
{
	return task->signal->nr_threads;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
thread_group_leader(struct task_struct *p)
{
	return p->exit_signal >= 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
same_thread_group(struct task_struct *p1, struct task_struct *p2)
{
	return p1->signal == p2->signal;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct task_struct *
__next_thread(struct task_struct *p)
{
	return ({
		struct list_head *__head = (&p->signal->thread_head);
		struct list_head *__ptr = (&p->thread_node);
		struct list_head *__next = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_509(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(__ptr->next) == sizeof(char) ||
				       sizeof(__ptr->next) == sizeof(short) ||
				       sizeof(__ptr->next) == sizeof(int) ||
				       sizeof(__ptr->next) == sizeof(long)) ||
				      sizeof(__ptr->next) == sizeof(long long)))
					__compiletime_assert_509();
			} while (0);
			(*(const volatile typeof(_Generic(
				(__ptr->next),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (__ptr->next)))
				   *)&(__ptr->next));
		});
		__builtin_expect(!!(__next != __head), 1) ? ({
			void *__mptr = (void *)(({
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_510(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(__next) == sizeof(char) ||
					       sizeof(__next) == sizeof(short) ||
					       sizeof(__next) == sizeof(int) ||
					       sizeof(__next) == sizeof(long)) ||
					      sizeof(__next) ==
						      sizeof(long long)))
						__compiletime_assert_510();
				} while (0);
				(*(const volatile typeof(_Generic(
					(__next),
								 char: (char)0,
								 unsigned char: (
									 unsigned char)0,
								 signed char: (
									 signed char)0,
								 unsigned short: (
									 unsigned short)0,
								 signed short: (
									 signed short)0,
								 unsigned int: (
									 unsigned int)0,
								 signed int: (
									 signed int)0,
								 unsigned long: (
									 unsigned long)0,
								 signed long: (
									 signed long)0,
								 unsigned long long: (
									 unsigned long long)0,
								 signed long long: (
									 signed long long)0,
								 default: (
									 __next)))
					   *)&(__next));
			}));
			_Static_assert(
				__builtin_types_compatible_p(
					typeof(*(({
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_510(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(__next) ==
								       sizeof(char) ||
							       sizeof(__next) ==
								       sizeof(short) ||
							       sizeof(__next) ==
								       sizeof(int) ||
							       sizeof(__next) ==
								       sizeof(long)) ||
							      sizeof(__next) ==
								      sizeof(long long)))
								__compiletime_assert_510();
						} while (0);
						(*(const volatile typeof(_Generic(
							(__next),
										 char: (char)0,
										 unsigned char: (
											 unsigned char)0,
										 signed char: (
											 signed char)0,
										 unsigned short: (
											 unsigned short)0,
										 signed short: (
											 signed short)0,
										 unsigned int: (
											 unsigned int)0,
										 signed int: (
											 signed int)0,
										 unsigned long: (
											 unsigned long)0,
										 signed long: (
											 signed long)0,
										 unsigned long long: (
											 unsigned long long)0,
										 signed long long: (
											 signed long long)0,
										 default: (
											 __next)))
							   *)&(__next));
					}))),
					typeof(((struct task_struct *)0)
						       ->thread_node)) ||
					__builtin_types_compatible_p(
						typeof(*(({
							do {
								__attribute__((
									__noreturn__)) extern void
								__compiletime_assert_510(
									void)
									__attribute__((__error__(
										"Unsupported access size for {READ,WRITE}_ONCE().")));
								if (!((sizeof(__next) ==
									       sizeof(char) ||
								       sizeof(__next) ==
									       sizeof(short) ||
								       sizeof(__next) ==
									       sizeof(int) ||
								       sizeof(__next) ==
									       sizeof(long)) ||
								      sizeof(__next) ==
									      sizeof(long long)))
									__compiletime_assert_510();
							} while (0);
							(*(const volatile typeof(_Generic(
								(__next),
											 char: (char)0,
											 unsigned char: (
												 unsigned char)0,
											 signed char: (
												 signed char)0,
											 unsigned short: (
												 unsigned short)0,
											 signed short: (
												 signed short)0,
											 unsigned int: (
												 unsigned int)0,
											 signed int: (
												 signed int)0,
											 unsigned long: (
												 unsigned long)0,
											 signed long: (
												 signed long)0,
											 unsigned long long: (
												 unsigned long long)0,
											 signed long long: (
												 signed long long)0,
											 default: (
												 __next)))
								   *)&(__next));
						}))),
						typeof(void)),
				"pointer type mismatch in container_of()");
			((struct task_struct *)(__mptr -
						__builtin_offsetof(
							struct task_struct,
							thread_node)));
		}) :
							    ((void *)0);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct task_struct *
next_thread(struct task_struct *p)
{
	return __next_thread(p) ?: p->group_leader;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
thread_group_empty(struct task_struct *p)
{
	return thread_group_leader(p) &&
	       list_is_last(&p->thread_node, &p->signal->thread_head);
}

extern struct sighand_struct *__lock_task_sighand(struct task_struct *task,
						  unsigned long *flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct sighand_struct *
lock_task_sighand(struct task_struct *task, unsigned long *flags)
{
	struct sighand_struct *ret;

	ret = __lock_task_sighand(task, flags);
	(void)(ret);
	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
unlock_task_sighand(struct task_struct *task, unsigned long *flags)
{
	spin_unlock_irqrestore(&task->sighand->siglock, *flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_assert_task_sighand_held(struct task_struct *task)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
task_rlimit(const struct task_struct *task, unsigned int limit)
{
	return ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_511(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(task->signal->rlim[limit].rlim_cur) ==
				       sizeof(char) ||
			       sizeof(task->signal->rlim[limit].rlim_cur) ==
				       sizeof(short) ||
			       sizeof(task->signal->rlim[limit].rlim_cur) ==
				       sizeof(int) ||
			       sizeof(task->signal->rlim[limit].rlim_cur) ==
				       sizeof(long)) ||
			      sizeof(task->signal->rlim[limit].rlim_cur) ==
				      sizeof(long long)))
				__compiletime_assert_511();
		} while (0);
		(*(const volatile typeof(_Generic((task->signal->rlim[limit]
							   .rlim_cur),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (task->signal
								   ->rlim[limit]
								   .rlim_cur)))
			   *)&(task->signal->rlim[limit].rlim_cur));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
task_rlimit_max(const struct task_struct *task, unsigned int limit)
{
	return ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_512(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(task->signal->rlim[limit].rlim_max) ==
				       sizeof(char) ||
			       sizeof(task->signal->rlim[limit].rlim_max) ==
				       sizeof(short) ||
			       sizeof(task->signal->rlim[limit].rlim_max) ==
				       sizeof(int) ||
			       sizeof(task->signal->rlim[limit].rlim_max) ==
				       sizeof(long)) ||
			      sizeof(task->signal->rlim[limit].rlim_max) ==
				      sizeof(long long)))
				__compiletime_assert_512();
		} while (0);
		(*(const volatile typeof(_Generic((task->signal->rlim[limit]
							   .rlim_max),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (task->signal
								   ->rlim[limit]
								   .rlim_max)))
			   *)&(task->signal->rlim[limit].rlim_max));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
rlimit(unsigned int limit)
{
	return task_rlimit(get_current(), limit);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
rlimit_max(unsigned int limit)
{
	return task_rlimit_max(get_current(), limit);
}
struct rcuwait {
	struct task_struct *task;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
rcuwait_init(struct rcuwait *w)
{
	w->task = ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
rcuwait_active(struct rcuwait *w)
{
	return !!({
		typeof(*(
			w->task)) *__UNIQUE_ID_rcu513 = (typeof(*(w->task)) *)({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_514(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof((w->task)) == sizeof(char) ||
				       sizeof((w->task)) == sizeof(short) ||
				       sizeof((w->task)) == sizeof(int) ||
				       sizeof((w->task)) == sizeof(long)) ||
				      sizeof((w->task)) == sizeof(long long)))
					__compiletime_assert_514();
			} while (0);
			(*(const volatile typeof(_Generic(
				((w->task)),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: ((w->task))))
				   *)&((w->task)));
		});
		;
		((typeof(*(w->task)) *)(__UNIQUE_ID_rcu513));
	});
}

extern int rcuwait_wake_up(struct rcuwait *w);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
prepare_to_rcuwait(struct rcuwait *w)
{
	do {
		uintptr_t _r_a_p__v = (uintptr_t)(get_current());
		;
		if (__builtin_constant_p(get_current()) &&
		    (_r_a_p__v) == (uintptr_t)((void *)0))
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_515(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof((w->task)) ==
						       sizeof(char) ||
					       sizeof((w->task)) ==
						       sizeof(short) ||
					       sizeof((w->task)) ==
						       sizeof(int) ||
					       sizeof((w->task)) ==
						       sizeof(long)) ||
					      sizeof((w->task)) ==
						      sizeof(long long)))
						__compiletime_assert_515();
				} while (0);
				do {
					*(volatile typeof((w->task)) *)&(
						(w->task)) =
						((typeof(w->task))(_r_a_p__v));
				} while (0);
			} while (0);
		else
			do {
				do {
				} while (0);
				do {
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_516(void)
							__attribute__((__error__(
								"Need native word sized stores/loads for atomicity.")));
						if (!((sizeof(*&w->task) ==
							       sizeof(char) ||
						       sizeof(*&w->task) ==
							       sizeof(short) ||
						       sizeof(*&w->task) ==
							       sizeof(int) ||
						       sizeof(*&w->task) ==
							       sizeof(long))))
							__compiletime_assert_516();
					} while (0);
					__asm__ __volatile__("" : : : "memory");
					do {
						do {
							__attribute__((
								__noreturn__)) extern void
							__compiletime_assert_517(
								void)
								__attribute__((__error__(
									"Unsupported access size for {READ,WRITE}_ONCE().")));
							if (!((sizeof(*&w->task) ==
								       sizeof(char) ||
							       sizeof(*&w->task) ==
								       sizeof(short) ||
							       sizeof(*&w->task) ==
								       sizeof(int) ||
							       sizeof(*&w->task) ==
								       sizeof(long)) ||
							      sizeof(*&w->task) ==
								      sizeof(long long)))
								__compiletime_assert_517();
						} while (0);
						do {
							*(volatile typeof(*&w->task)
								  *)&(*&w->task) =
								((typeof(*(
									(typeof(w->task))
										_r_a_p__v))
									  *)((
									typeof(w->task))
										     _r_a_p__v));
						} while (0);
					} while (0);
				} while (0);
			} while (0);
	} while (0);
}

extern void finish_rcuwait(struct rcuwait *w);

struct rcu_sync {
	int gp_state;
	int gp_count;
	wait_queue_head_t gp_wait;

	struct callback_head cb_head;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
rcu_sync_is_idle(struct rcu_sync *rsp)
{
	do {
	} while (0 && (!rcu_read_lock_any_held()));

	return !({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_518(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(rsp->gp_state) == sizeof(char) ||
			       sizeof(rsp->gp_state) == sizeof(short) ||
			       sizeof(rsp->gp_state) == sizeof(int) ||
			       sizeof(rsp->gp_state) == sizeof(long)) ||
			      sizeof(rsp->gp_state) == sizeof(long long)))
				__compiletime_assert_518();
		} while (0);
		(*(const volatile typeof(_Generic((rsp->gp_state),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (rsp->gp_state)))
			   *)&(rsp->gp_state));
	});
}

extern void rcu_sync_init(struct rcu_sync *);
extern void rcu_sync_enter(struct rcu_sync *);
extern void rcu_sync_exit(struct rcu_sync *);
extern void rcu_sync_dtor(struct rcu_sync *);

struct percpu_rw_semaphore {
	struct rcu_sync rss;
	unsigned int *read_count;
	struct rcuwait writer;
	wait_queue_head_t waiters;
	atomic_t block;
};
extern bool __percpu_down_read(struct percpu_rw_semaphore *, bool);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_down_read(struct percpu_rw_semaphore *sem)
{
	do {
		might_resched();
	} while (0);

	do {
	} while (0);

	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);
	if (__builtin_expect(!!(rcu_sync_is_idle(&sem->rss)), 1))
		do {
			do {
				const void *__vpp_verify =
					(typeof((&(*sem->read_count)) +
						0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			switch (sizeof(*sem->read_count)) {
			case 1:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u8 pto_val__ = ((
								u8)(((unsigned long)1) &
								    0xff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"b "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "qi"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 2:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u16 pto_val__ = ((
								u16)(((unsigned long)1) &
								     0xffff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"w "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 4:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u32 pto_val__ = ((
								u32)(((unsigned long)1) &
								     0xffffffff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"l "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 8:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u64 pto_val__ =
								((u64)(1));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"q "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "re"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			default:
				__bad_size_call_parameter();
				break;
			}
		} while (0);
	else
		__percpu_down_read(sem, false);

	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule519 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
percpu_down_read_trylock(struct percpu_rw_semaphore *sem)
{
	bool ret = true;

	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);

	if (__builtin_expect(!!(rcu_sync_is_idle(&sem->rss)), 1))
		do {
			do {
				const void *__vpp_verify =
					(typeof((&(*sem->read_count)) +
						0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			switch (sizeof(*sem->read_count)) {
			case 1:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u8 pto_val__ = ((
								u8)(((unsigned long)1) &
								    0xff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"b "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "qi"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 2:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u16 pto_val__ = ((
								u16)(((unsigned long)1) &
								     0xffff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"w "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 4:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u32 pto_val__ = ((
								u32)(((unsigned long)1) &
								     0xffffffff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"l "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 8:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(1) &&
						 ((1) == 1 || (1) == -1)) ?
							(int)(1) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (1);
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u64 pto_val__ =
								((u64)(1));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (1);
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"q "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "re"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			default:
				__bad_size_call_parameter();
				break;
			}
		} while (0);
	else
		ret = __percpu_down_read(sem, true);
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule520 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);

	if (ret)
		do {
		} while (0);

	return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_up_read(struct percpu_rw_semaphore *sem)
{
	do {
	} while (0);

	do {
		__preempt_count_add(1);
		__asm__ __volatile__("" : : : "memory");
	} while (0);

	if (__builtin_expect(!!(rcu_sync_is_idle(&sem->rss)), 1)) {
		do {
			do {
				const void *__vpp_verify =
					(typeof((&(*sem->read_count)) +
						0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			switch (sizeof(*sem->read_count)) {
			case 1:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*sem->read_count))(1)) &&
						 ((-(typeof(*sem->read_count))(1)) ==
							  1 ||
						  (-(typeof(*sem->read_count))(1)) ==
							  -1)) ?
							(int)(-(typeof(*sem->read_count))(1)) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*sem->read_count))(1));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u8 pto_val__ = ((
								u8)(((unsigned long)-(
									    typeof(*sem->read_count))(1)) &
								    0xff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*sem->read_count))(1));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"b "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "qi"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 2:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*sem->read_count))(1)) &&
						 ((-(typeof(*sem->read_count))(1)) ==
							  1 ||
						  (-(typeof(*sem->read_count))(1)) ==
							  -1)) ?
							(int)(-(typeof(*sem->read_count))(1)) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*sem->read_count))(1));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u16 pto_val__ = ((
								u16)(((unsigned long)-(
									     typeof(*sem->read_count))(1)) &
								     0xffff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*sem->read_count))(1));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"w "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 4:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*sem->read_count))(1)) &&
						 ((-(typeof(*sem->read_count))(1)) ==
							  1 ||
						  (-(typeof(*sem->read_count))(1)) ==
							  -1)) ?
							(int)(-(typeof(*sem->read_count))(1)) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*sem->read_count))(1));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u32 pto_val__ = ((
								u32)(((unsigned long)-(
									     typeof(*sem->read_count))(1)) &
								     0xffffffff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*sem->read_count))(1));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"l "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 8:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*sem->read_count))(1)) &&
						 ((-(typeof(*sem->read_count))(1)) ==
							  1 ||
						  (-(typeof(*sem->read_count))(1)) ==
							  -1)) ?
							(int)(-(typeof(*sem->read_count))(1)) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*sem->read_count))(1));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u64 pto_val__ = ((
								u64)(-(
								typeof(*sem->read_count))(1)));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*sem->read_count))(1));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"q "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "re"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			default:
				__bad_size_call_parameter();
				break;
			}
		} while (0);
	} else {
		do {
			do {
			} while (0);
			asm volatile("lock; addl $0,-4(%%"
				     "rsp"
				     ")" ::
					     : "memory", "cc");
		} while (0);

		do {
			do {
				const void *__vpp_verify =
					(typeof((&(*sem->read_count)) +
						0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			switch (sizeof(*sem->read_count)) {
			case 1:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*sem->read_count))(1)) &&
						 ((-(typeof(*sem->read_count))(1)) ==
							  1 ||
						  (-(typeof(*sem->read_count))(1)) ==
							  -1)) ?
							(int)(-(typeof(*sem->read_count))(1)) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*sem->read_count))(1));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"b "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u8 pto_val__ = ((
								u8)(((unsigned long)-(
									    typeof(*sem->read_count))(1)) &
								    0xff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*sem->read_count))(1));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"b "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "qi"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 2:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*sem->read_count))(1)) &&
						 ((-(typeof(*sem->read_count))(1)) ==
							  1 ||
						  (-(typeof(*sem->read_count))(1)) ==
							  -1)) ?
							(int)(-(typeof(*sem->read_count))(1)) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*sem->read_count))(1));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"w "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u16 pto_val__ = ((
								u16)(((unsigned long)-(
									     typeof(*sem->read_count))(1)) &
								     0xffff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*sem->read_count))(1));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"w "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 4:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*sem->read_count))(1)) &&
						 ((-(typeof(*sem->read_count))(1)) ==
							  1 ||
						  (-(typeof(*sem->read_count))(1)) ==
							  -1)) ?
							(int)(-(typeof(*sem->read_count))(1)) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*sem->read_count))(1));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"l "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u32 pto_val__ = ((
								u32)(((unsigned long)-(
									     typeof(*sem->read_count))(1)) &
								     0xffffffff));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*sem->read_count))(1));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"l "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "ri"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			case 8:
				do {
					const int pao_ID__ =
						(__builtin_constant_p(-(
							 typeof(*sem->read_count))(1)) &&
						 ((-(typeof(*sem->read_count))(1)) ==
							  1 ||
						  (-(typeof(*sem->read_count))(1)) ==
							  -1)) ?
							(int)(-(typeof(*sem->read_count))(1)) :
							0;
					if (0) {
						typeof((*sem->read_count))
							pao_tmp__;
						pao_tmp__ = (-(
							typeof(*sem->read_count))(1));
						(void)pao_tmp__;
					}
					if (pao_ID__ == 1)
						({
							asm volatile(
								"inc"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else if (pao_ID__ == -1)
						({
							asm volatile(
								"dec"
								"q "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count))))));
						});
					else
						do {
							u64 pto_val__ = ((
								u64)(-(
								typeof(*sem->read_count))(1)));
							if (0) {
								typeof((*sem->read_count))
									pto_tmp__;
								pto_tmp__ = (-(
									typeof(*sem->read_count))(1));
								(void)pto_tmp__;
							}
							asm volatile(
								"add"
								"q "
								"%[val]"
								", "
								"%%"
								"gs"
								":"
								"%"
								"[var]"
								: [var] "+m"((*(
									typeof(*(&((
										*sem->read_count))))
										*)(uintptr_t)(&(
									(*sem->read_count)))))
								: [val] "re"(
									pto_val__));
						} while (0);
				} while (0);
				break;
			default:
				__bad_size_call_parameter();
				break;
			}
		} while (0);
		rcuwait_wake_up(&sem->writer);
	}
	do {
		__asm__ __volatile__("" : : : "memory");
		if (__builtin_expect(!!(__preempt_count_dec_and_test()), 0))
			do {
				static void *__attribute__((__used__))
				__attribute__((
					__section__(".discard.addressable")))
				__UNIQUE_ID___addressable___SCK__preempt_schedule521 =
					(void *)(uintptr_t)&__SCK__preempt_schedule;
				;
				asm volatile("call "
					     "__SCT__preempt_schedule"
					     : "+r"(current_stack_pointer));
			} while (0);
	} while (0);
}

extern bool percpu_is_read_locked(struct percpu_rw_semaphore *);
extern void percpu_down_write(struct percpu_rw_semaphore *);
extern void percpu_up_write(struct percpu_rw_semaphore *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
percpu_is_write_locked(struct percpu_rw_semaphore *sem)
{
	return atomic_read(&sem->block);
}

extern int __percpu_init_rwsem(struct percpu_rw_semaphore *, const char *,
			       struct lock_class_key *);

extern void percpu_free_rwsem(struct percpu_rw_semaphore *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_rwsem_release(struct percpu_rw_semaphore *sem, unsigned long ip)
{
	do {
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
percpu_rwsem_acquire(struct percpu_rw_semaphore *sem, bool read,
		     unsigned long ip)
{
	do {
	} while (0);
}

struct delayed_call {
	void (*fn)(void *);
	void *arg;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
set_delayed_call(struct delayed_call *call, void (*fn)(void *), void *arg)
{
	call->fn = fn;
	call->arg = arg;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
do_delayed_call(struct delayed_call *call)
{
	if (call->fn)
		call->fn(call->arg);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
clear_delayed_call(struct delayed_call *call)
{
	call->fn = ((void *)0);
}
typedef struct {
	__u8 b[16];
} guid_t;

typedef struct {
	__u8 b[16];
} uuid_t;
extern const guid_t guid_null;
extern const uuid_t uuid_null;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
guid_equal(const guid_t *u1, const guid_t *u2)
{
	return memcmp(u1, u2, sizeof(guid_t)) == 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
guid_copy(guid_t *dst, const guid_t *src)
{
	({
		const size_t __fortify_size = (size_t)(sizeof(guid_t));
		const size_t __p_size = (__builtin_dynamic_object_size(dst, 0));
		const size_t __q_size = (__builtin_dynamic_object_size(src, 0));
		const size_t __p_size_field =
			(__builtin_dynamic_object_size(dst, 1));
		const size_t __q_size_field =
			(__builtin_dynamic_object_size(src, 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"522"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"522"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(522));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"dst"
								"\" at "
								"include/linux/uuid.h"
								":"
								"53",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"523"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"523"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(523));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("include/linux/uuid.h"),
										  "i"(53),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"524"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"524"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(524));
								});
							} while (0);
							({
								asm volatile(
									"525"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"525"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(525));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(dst, src, __fortify_size);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
import_guid(guid_t *dst, const __u8 *src)
{
	({
		const size_t __fortify_size = (size_t)(sizeof(guid_t));
		const size_t __p_size = (__builtin_dynamic_object_size(dst, 0));
		const size_t __q_size = (__builtin_dynamic_object_size(src, 0));
		const size_t __p_size_field =
			(__builtin_dynamic_object_size(dst, 1));
		const size_t __q_size_field =
			(__builtin_dynamic_object_size(src, 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"526"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"526"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(526));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"dst"
								"\" at "
								"include/linux/uuid.h"
								":"
								"58",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"527"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"527"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(527));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("include/linux/uuid.h"),
										  "i"(58),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"528"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"528"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(528));
								});
							} while (0);
							({
								asm volatile(
									"529"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"529"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(529));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(dst, src, __fortify_size);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
export_guid(__u8 *dst, const guid_t *src)
{
	({
		const size_t __fortify_size = (size_t)(sizeof(guid_t));
		const size_t __p_size = (__builtin_dynamic_object_size(dst, 0));
		const size_t __q_size = (__builtin_dynamic_object_size(src, 0));
		const size_t __p_size_field =
			(__builtin_dynamic_object_size(dst, 1));
		const size_t __q_size_field =
			(__builtin_dynamic_object_size(src, 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"530"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"530"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(530));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"dst"
								"\" at "
								"include/linux/uuid.h"
								":"
								"63",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"531"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"531"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(531));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("include/linux/uuid.h"),
										  "i"(63),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"532"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"532"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(532));
								});
							} while (0);
							({
								asm volatile(
									"533"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"533"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(533));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(dst, src, __fortify_size);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
guid_is_null(const guid_t *guid)
{
	return guid_equal(guid, &guid_null);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
uuid_equal(const uuid_t *u1, const uuid_t *u2)
{
	return memcmp(u1, u2, sizeof(uuid_t)) == 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
uuid_copy(uuid_t *dst, const uuid_t *src)
{
	({
		const size_t __fortify_size = (size_t)(sizeof(uuid_t));
		const size_t __p_size = (__builtin_dynamic_object_size(dst, 0));
		const size_t __q_size = (__builtin_dynamic_object_size(src, 0));
		const size_t __p_size_field =
			(__builtin_dynamic_object_size(dst, 1));
		const size_t __q_size_field =
			(__builtin_dynamic_object_size(src, 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"534"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"534"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(534));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"dst"
								"\" at "
								"include/linux/uuid.h"
								":"
								"78",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"535"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"535"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(535));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("include/linux/uuid.h"),
										  "i"(78),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"536"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"536"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(536));
								});
							} while (0);
							({
								asm volatile(
									"537"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"537"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(537));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(dst, src, __fortify_size);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
import_uuid(uuid_t *dst, const __u8 *src)
{
	({
		const size_t __fortify_size = (size_t)(sizeof(uuid_t));
		const size_t __p_size = (__builtin_dynamic_object_size(dst, 0));
		const size_t __q_size = (__builtin_dynamic_object_size(src, 0));
		const size_t __p_size_field =
			(__builtin_dynamic_object_size(dst, 1));
		const size_t __q_size_field =
			(__builtin_dynamic_object_size(src, 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"538"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"538"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(538));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"dst"
								"\" at "
								"include/linux/uuid.h"
								":"
								"83",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"539"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"539"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(539));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("include/linux/uuid.h"),
										  "i"(83),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"540"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"540"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(540));
								});
							} while (0);
							({
								asm volatile(
									"541"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"541"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(541));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(dst, src, __fortify_size);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
export_uuid(__u8 *dst, const uuid_t *src)
{
	({
		const size_t __fortify_size = (size_t)(sizeof(uuid_t));
		const size_t __p_size = (__builtin_dynamic_object_size(dst, 0));
		const size_t __q_size = (__builtin_dynamic_object_size(src, 0));
		const size_t __p_size_field =
			(__builtin_dynamic_object_size(dst, 1));
		const size_t __q_size_field =
			(__builtin_dynamic_object_size(src, 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"542"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"542"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(542));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"dst"
								"\" at "
								"include/linux/uuid.h"
								":"
								"88",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"543"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"543"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(543));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("include/linux/uuid.h"),
										  "i"(88),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"544"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"544"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(544));
								});
							} while (0);
							({
								asm volatile(
									"545"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"545"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(545));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(dst, src, __fortify_size);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
uuid_is_null(const uuid_t *uuid)
{
	return uuid_equal(uuid, &uuid_null);
}

void generate_random_uuid(unsigned char uuid[16]);
void generate_random_guid(unsigned char guid[16]);

extern void guid_gen(guid_t *u);
extern void uuid_gen(uuid_t *u);

bool __attribute__((__warn_unused_result__)) uuid_is_valid(const char *uuid);

extern const u8 guid_index[16];
extern const u8 uuid_index[16];

int guid_parse(const char *uuid, guid_t *u);
int uuid_parse(const char *uuid, uuid_t *u);

typedef u32 errseq_t;

errseq_t errseq_set(errseq_t *eseq, int err);
errseq_t errseq_sample(errseq_t *eseq);
int errseq_check(errseq_t *eseq, errseq_t since);
int errseq_check_and_advance(errseq_t *eseq, errseq_t *since);

struct task_struct;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
rt_prio(int prio)
{
	return __builtin_expect(!!(prio < 100 && prio >= 0), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
rt_or_dl_prio(int prio)
{
	return __builtin_expect(!!(prio < 100), 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
rt_task(struct task_struct *p)
{
	return rt_prio(p->prio);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
rt_or_dl_task(struct task_struct *p)
{
	return rt_or_dl_prio(p->prio);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
rt_or_dl_task_policy(struct task_struct *tsk)
{
	int policy = tsk->policy;

	if (policy == 1 || policy == 2)
		return true;
	if (policy == 6)
		return true;
	return false;
}

extern void rt_mutex_pre_schedule(void);
extern void rt_mutex_schedule(void);
extern void rt_mutex_post_schedule(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct task_struct *
rt_mutex_get_top_task(struct task_struct *p)
{
	return p->pi_top_task;
}
extern void rt_mutex_setprio(struct task_struct *p,
			     struct task_struct *pi_task);
extern void rt_mutex_adjust_pi(struct task_struct *p);
extern void normalize_rt_tasks(void);

enum {
	ICQ_EXITED = 1 << 2,
	ICQ_DESTROYED = 1 << 3,
};
struct io_cq {
	struct request_queue *q;
	struct io_context *ioc;

	union {
		struct list_head q_node;
		struct kmem_cache *__rcu_icq_cache;
	};
	union {
		struct hlist_node ioc_node;
		struct callback_head __rcu_head;
	};

	unsigned int flags;
};

struct io_context {
	atomic_long_t refcount;
	atomic_t active_ref;

	unsigned short ioprio;
};

struct task_struct;

void put_io_context(struct io_context *ioc);
void exit_io_context(struct task_struct *task);
int __copy_io(unsigned long clone_flags, struct task_struct *tsk);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
copy_io(unsigned long clone_flags, struct task_struct *tsk)
{
	if (!get_current()->io_context)
		return 0;
	return __copy_io(clone_flags, tsk);
}

enum {
	IOPRIO_CLASS_NONE = 0,
	IOPRIO_CLASS_RT = 1,
	IOPRIO_CLASS_BE = 2,
	IOPRIO_CLASS_IDLE = 3,

	IOPRIO_CLASS_INVALID = 7,
};
enum {
	IOPRIO_WHO_PROCESS = 1,
	IOPRIO_WHO_PGRP,
	IOPRIO_WHO_USER,
};
enum {

	IOPRIO_HINT_NONE = 0,
	IOPRIO_HINT_DEV_DURATION_LIMIT_1 = 1,
	IOPRIO_HINT_DEV_DURATION_LIMIT_2 = 2,
	IOPRIO_HINT_DEV_DURATION_LIMIT_3 = 3,
	IOPRIO_HINT_DEV_DURATION_LIMIT_4 = 4,
	IOPRIO_HINT_DEV_DURATION_LIMIT_5 = 5,
	IOPRIO_HINT_DEV_DURATION_LIMIT_6 = 6,
	IOPRIO_HINT_DEV_DURATION_LIMIT_7 = 7,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) __u16
ioprio_value(int prioclass, int priolevel, int priohint)
{
	if (((prioclass) < 0 || (prioclass) >= (8)) ||
	    ((priolevel) < 0 || (priolevel) >= ((1 << 3))) ||
	    ((priohint) < 0 || (priohint) >= ((1 << 10))))
		return IOPRIO_CLASS_INVALID << 13;

	return (prioclass << 13) | (priohint << 3) | priolevel;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
ioprio_valid(unsigned short ioprio)
{
	unsigned short class = (((ioprio) >> 13) & (8 - 1));

	return class > IOPRIO_CLASS_NONE && class <= IOPRIO_CLASS_IDLE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
task_nice_ioprio(struct task_struct *task)
{
	return (task_nice(task) + 20) / 5;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
task_nice_ioclass(struct task_struct *task)
{
	if (task->policy == 5)
		return IOPRIO_CLASS_IDLE;
	else if (rt_or_dl_task_policy(task))
		return IOPRIO_CLASS_RT;
	else
		return IOPRIO_CLASS_BE;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__get_task_ioprio(struct task_struct *p)
{
	struct io_context *ioc = p->io_context;
	int prio;

	if (!ioc)
		return ioprio_value(IOPRIO_CLASS_NONE, 0, IOPRIO_HINT_NONE);

	if (p != get_current())
		do {
			(void)(&p->alloc_lock);
		} while (0);

	prio = ioc->ioprio;
	if ((((prio) >> 13) & (8 - 1)) == IOPRIO_CLASS_NONE)
		prio = ioprio_value(task_nice_ioclass(p), task_nice_ioprio(p),
				    IOPRIO_HINT_NONE);

	return prio;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_current_ioprio(void)
{
	return __get_task_ioprio(get_current());
}

extern int set_task_ioprio(struct task_struct *task, int ioprio);

extern int ioprio_check_cap(int ioprio);
extern unsigned char fs_ftype_to_dtype(unsigned int filetype);
extern unsigned char fs_umode_to_ftype(umode_t mode);
extern unsigned char fs_umode_to_dtype(umode_t mode);

struct super_block;
struct dentry;
struct user_namespace;
struct mnt_idmap;
struct file_system_type;
struct fs_context;
struct file;
struct path;
struct vfsmount {
	struct dentry *mnt_root;
	struct super_block *mnt_sb;
	int mnt_flags;
	struct mnt_idmap *mnt_idmap;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct mnt_idmap *
mnt_idmap(const struct vfsmount *mnt)
{
	return ({
		typeof(*&mnt->mnt_idmap) ___p1 = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_546(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*&mnt->mnt_idmap) ==
					       sizeof(char) ||
				       sizeof(*&mnt->mnt_idmap) ==
					       sizeof(short) ||
				       sizeof(*&mnt->mnt_idmap) == sizeof(int) ||
				       sizeof(*&mnt->mnt_idmap) ==
					       sizeof(long)) ||
				      sizeof(*&mnt->mnt_idmap) ==
					      sizeof(long long)))
					__compiletime_assert_546();
			} while (0);
			(*(const volatile typeof(_Generic(
				(*&mnt->mnt_idmap),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 *&mnt->mnt_idmap)))
				   *)&(*&mnt->mnt_idmap));
		});
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_547(void) __attribute__((__error__(
				"Need native word sized stores/loads for atomicity.")));
			if (!((sizeof(*&mnt->mnt_idmap) == sizeof(char) ||
			       sizeof(*&mnt->mnt_idmap) == sizeof(short) ||
			       sizeof(*&mnt->mnt_idmap) == sizeof(int) ||
			       sizeof(*&mnt->mnt_idmap) == sizeof(long))))
				__compiletime_assert_547();
		} while (0);
		__asm__ __volatile__("" : : : "memory");
		___p1;
	});
}

extern int mnt_want_write(struct vfsmount *mnt);
extern int mnt_want_write_file(struct file *file);
extern void mnt_drop_write(struct vfsmount *mnt);
extern void mnt_drop_write_file(struct file *file);
extern void mntput(struct vfsmount *mnt);
extern struct vfsmount *mntget(struct vfsmount *mnt);
extern void mnt_make_shortterm(struct vfsmount *mnt);
extern struct vfsmount *mnt_clone_internal(const struct path *path);
extern bool __mnt_is_readonly(struct vfsmount *mnt);
extern bool mnt_may_suid(struct vfsmount *mnt);

extern struct vfsmount *clone_private_mount(const struct path *path);
int mnt_get_write_access(struct vfsmount *mnt);
void mnt_put_write_access(struct vfsmount *mnt);

extern struct vfsmount *fc_mount(struct fs_context *fc);
extern struct vfsmount *vfs_create_mount(struct fs_context *fc);
extern struct vfsmount *vfs_kern_mount(struct file_system_type *type, int flags,
				       const char *name, void *data);
extern struct vfsmount *vfs_submount(const struct dentry *mountpoint,
				     struct file_system_type *type,
				     const char *name, void *data);

extern void mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list);
extern void mark_mounts_for_expiry(struct list_head *mounts);

extern bool path_is_mountpoint(const struct path *path);

extern bool our_mnt(struct vfsmount *mnt);

extern struct vfsmount *kern_mount(struct file_system_type *);
extern void kern_unmount(struct vfsmount *mnt);
extern int may_umount_tree(struct vfsmount *);
extern int may_umount(struct vfsmount *);
extern long do_mount(const char *, const char *, const char *, unsigned long,
		     void *);
extern struct vfsmount *collect_mounts(const struct path *);
extern void drop_collected_mounts(struct vfsmount *);
extern int iterate_mounts(int (*)(struct vfsmount *, void *), void *,
			  struct vfsmount *);
extern void kern_unmount_array(struct vfsmount *mnt[], unsigned int num);

extern int cifs_root_data(char **dev, char **opts);

struct mnt_idmap;
struct user_namespace;

extern struct mnt_idmap nop_mnt_idmap;
extern struct mnt_idmap invalid_mnt_idmap;
extern struct user_namespace init_user_ns;

typedef struct {
	uid_t val;
} vfsuid_t;

typedef struct {
	gid_t val;
} vfsgid_t;

_Static_assert(sizeof(vfsuid_t) == sizeof(kuid_t),
	       "sizeof(vfsuid_t) == sizeof(kuid_t)");
_Static_assert(sizeof(vfsgid_t) == sizeof(kgid_t),
	       "sizeof(vfsgid_t) == sizeof(kgid_t)");
_Static_assert(__builtin_offsetof(vfsuid_t, val) ==
		       __builtin_offsetof(kuid_t, val),
	       "offsetof(vfsuid_t, val) == offsetof(kuid_t, val)");
_Static_assert(__builtin_offsetof(vfsgid_t, val) ==
		       __builtin_offsetof(kgid_t, val),
	       "offsetof(vfsgid_t, val) == offsetof(kgid_t, val)");

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) uid_t
__vfsuid_val(vfsuid_t uid)
{
	return uid.val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) gid_t
__vfsgid_val(vfsgid_t gid)
{
	return gid.val;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vfsuid_valid(vfsuid_t uid)
{
	return __vfsuid_val(uid) != (uid_t)-1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vfsgid_valid(vfsgid_t gid)
{
	return __vfsgid_val(gid) != (gid_t)-1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vfsuid_eq(vfsuid_t left, vfsuid_t right)
{
	return vfsuid_valid(left) && __vfsuid_val(left) == __vfsuid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vfsgid_eq(vfsgid_t left, vfsgid_t right)
{
	return vfsgid_valid(left) && __vfsgid_val(left) == __vfsgid_val(right);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vfsuid_eq_kuid(vfsuid_t vfsuid, kuid_t kuid)
{
	return vfsuid_valid(vfsuid) && __vfsuid_val(vfsuid) == __kuid_val(kuid);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vfsgid_eq_kgid(vfsgid_t vfsgid, kgid_t kgid)
{
	return vfsgid_valid(vfsgid) && __vfsgid_val(vfsgid) == __kgid_val(kgid);
}
int vfsgid_in_group_p(vfsgid_t vfsgid);

struct mnt_idmap *mnt_idmap_get(struct mnt_idmap *idmap);
void mnt_idmap_put(struct mnt_idmap *idmap);

vfsuid_t make_vfsuid(struct mnt_idmap *idmap, struct user_namespace *fs_userns,
		     kuid_t kuid);

vfsgid_t make_vfsgid(struct mnt_idmap *idmap, struct user_namespace *fs_userns,
		     kgid_t kgid);

kuid_t from_vfsuid(struct mnt_idmap *idmap, struct user_namespace *fs_userns,
		   vfsuid_t vfsuid);

kgid_t from_vfsgid(struct mnt_idmap *idmap, struct user_namespace *fs_userns,
		   vfsgid_t vfsgid);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vfsuid_has_fsmapping(struct mnt_idmap *idmap, struct user_namespace *fs_userns,
		     vfsuid_t vfsuid)
{
	return uid_valid(from_vfsuid(idmap, fs_userns, vfsuid));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vfsuid_has_mapping(struct user_namespace *userns, vfsuid_t vfsuid)
{
	return from_kuid(userns, (kuid_t){ __vfsuid_val(vfsuid) }) != (uid_t)-1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kuid_t
vfsuid_into_kuid(vfsuid_t vfsuid)
{
	return (kuid_t){ __vfsuid_val(vfsuid) };
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vfsgid_has_fsmapping(struct mnt_idmap *idmap, struct user_namespace *fs_userns,
		     vfsgid_t vfsgid)
{
	return gid_valid(from_vfsgid(idmap, fs_userns, vfsgid));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vfsgid_has_mapping(struct user_namespace *userns, vfsgid_t vfsgid)
{
	return from_kgid(userns, (kgid_t){ __vfsgid_val(vfsgid) }) != (gid_t)-1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kgid_t
vfsgid_into_kgid(vfsgid_t vfsgid)
{
	return (kgid_t){ __vfsgid_val(vfsgid) };
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kuid_t
mapped_fsuid(struct mnt_idmap *idmap, struct user_namespace *fs_userns)
{
	return from_vfsuid(idmap, fs_userns, (vfsuid_t){ __kuid_val((({
				   ({
					   do {
					   } while (0 && (!((1))));
					   ;
					   ((typeof(*(get_current()->cred)) *)((
						   get_current()->cred)));
				   })->fsuid;
			   }))) });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kgid_t
mapped_fsgid(struct mnt_idmap *idmap, struct user_namespace *fs_userns)
{
	return from_vfsgid(idmap, fs_userns, (vfsgid_t){ __kgid_val((({
				   ({
					   do {
					   } while (0 && (!((1))));
					   ;
					   ((typeof(*(get_current()->cred)) *)((
						   get_current()->cred)));
				   })->fsgid;
			   }))) });
}

enum rw_hint {
	WRITE_LIFE_NOT_SET = 0,
	WRITE_LIFE_NONE = 1,
	WRITE_LIFE_SHORT = 2,
	WRITE_LIFE_MEDIUM = 3,
	WRITE_LIFE_LONG = 4,
	WRITE_LIFE_EXTREME = 5,
} __attribute__((__packed__));

_Static_assert(sizeof(enum rw_hint) == 1, "sizeof(enum rw_hint) == 1");

struct file_clone_range {
	__s64 src_fd;
	__u64 src_offset;
	__u64 src_length;
	__u64 dest_offset;
};

struct fstrim_range {
	__u64 start;
	__u64 len;
	__u64 minlen;
};
struct fsuuid2 {
	__u8 len;
	__u8 uuid[16];
};

struct fs_sysfs_path {
	__u8 len;
	__u8 name[128];
};

struct file_dedupe_range_info {
	__s64 dest_fd;
	__u64 dest_offset;
	__u64 bytes_deduped;

	__s32 status;
	__u32 reserved;
};

struct file_dedupe_range {
	__u64 src_offset;
	__u64 src_length;
	__u16 dest_count;
	__u16 reserved1;
	__u32 reserved2;
	struct file_dedupe_range_info info[];
};

struct files_stat_struct {
	unsigned long nr_files;
	unsigned long nr_free_files;
	unsigned long max_files;
};

struct inodes_stat_t {
	long nr_inodes;
	long nr_unused;
	long dummy[5];
};

struct fsxattr {
	__u32 fsx_xflags;
	__u32 fsx_extsize;
	__u32 fsx_nextents;
	__u32 fsx_projid;
	__u32 fsx_cowextsize;
	unsigned char fsx_pad[8];
};
typedef int __kernel_rwf_t;
struct page_region {
	__u64 start;
	__u64 end;
	__u64 categories;
};
struct pm_scan_arg {
	__u64 size;
	__u64 flags;
	__u64 start;
	__u64 end;
	__u64 walk_end;
	__u64 vec;
	__u64 vec_len;
	__u64 max_pages;
	__u64 category_inverted;
	__u64 category_mask;
	__u64 category_anyof_mask;
	__u64 return_mask;
};

enum procmap_query_flags {
	PROCMAP_QUERY_VMA_READABLE = 0x01,
	PROCMAP_QUERY_VMA_WRITABLE = 0x02,
	PROCMAP_QUERY_VMA_EXECUTABLE = 0x04,
	PROCMAP_QUERY_VMA_SHARED = 0x08,
	PROCMAP_QUERY_COVERING_OR_NEXT_VMA = 0x10,
	PROCMAP_QUERY_FILE_BACKED_VMA = 0x20,
};
struct procmap_query {
	__u64 size;

	__u64 query_flags;

	__u64 query_addr;

	__u64 vma_start;
	__u64 vma_end;

	__u64 vma_flags;

	__u64 vma_page_size;

	__u64 vma_offset;

	__u64 inode;

	__u32 dev_major;
	__u32 dev_minor;
	__u32 vma_name_size;
	__u32 build_id_size;

	__u64 vma_name_addr;

	__u64 build_id_addr;
};

struct backing_dev_info;
struct bdi_writeback;
struct bio;
struct io_comp_batch;
struct export_operations;
struct fiemap_extent_info;
struct hd_geometry;
struct iovec;
struct kiocb;
struct kobject;
struct pipe_inode_info;
struct poll_table_struct;
struct kstatfs;
struct vm_area_struct;
struct vfsmount;
struct cred;
struct swap_info_struct;
struct seq_file;
struct workqueue_struct;
struct iov_iter;
struct fscrypt_inode_info;
struct fscrypt_operations;
struct fsverity_info;
struct fsverity_operations;
struct fsnotify_mark_connector;
struct fsnotify_sb_info;
struct fs_context;
struct fs_parameter_spec;
struct fileattr;
struct iomap_ops;

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
inode_init(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
inode_init_early(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
files_init(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
files_maxfiles_init(void);

extern unsigned long get_max_files(void);
extern unsigned int sysctl_nr_open;

typedef __kernel_rwf_t rwf_t;

struct buffer_head;
typedef int(get_block_t)(struct inode *inode, sector_t iblock,
			 struct buffer_head *bh_result, int create);
typedef int(dio_iodone_t)(struct kiocb *iocb, loff_t offset, ssize_t bytes,
			  void *private);
struct iattr {
	unsigned int ia_valid;
	umode_t ia_mode;
	union {
		kuid_t ia_uid;
		vfsuid_t ia_vfsuid;
	};
	union {
		kgid_t ia_gid;
		vfsgid_t ia_vfsgid;
	};
	loff_t ia_size;
	struct timespec64 ia_atime;
	struct timespec64 ia_mtime;
	struct timespec64 ia_ctime;

	struct file *ia_file;
};

typedef struct fs_disk_quota {
	__s8 d_version;
	__s8 d_flags;
	__u16 d_fieldmask;
	__u32 d_id;
	__u64 d_blk_hardlimit;
	__u64 d_blk_softlimit;
	__u64 d_ino_hardlimit;
	__u64 d_ino_softlimit;
	__u64 d_bcount;
	__u64 d_icount;
	__s32 d_itimer;

	__s32 d_btimer;
	__u16 d_iwarns;
	__u16 d_bwarns;
	__s8 d_itimer_hi;
	__s8 d_btimer_hi;
	__s8 d_rtbtimer_hi;
	__s8 d_padding2;
	__u64 d_rtb_hardlimit;
	__u64 d_rtb_softlimit;
	__u64 d_rtbcount;
	__s32 d_rtbtimer;
	__u16 d_rtbwarns;
	__s16 d_padding3;
	char d_padding4[8];
} fs_disk_quota_t;
typedef struct fs_qfilestat {
	__u64 qfs_ino;
	__u64 qfs_nblks;
	__u32 qfs_nextents;
} fs_qfilestat_t;

typedef struct fs_quota_stat {
	__s8 qs_version;
	__u16 qs_flags;
	__s8 qs_pad;
	fs_qfilestat_t qs_uquota;
	fs_qfilestat_t qs_gquota;
	__u32 qs_incoredqs;
	__s32 qs_btimelimit;
	__s32 qs_itimelimit;
	__s32 qs_rtbtimelimit;
	__u16 qs_bwarnlimit;
	__u16 qs_iwarnlimit;
} fs_quota_stat_t;
struct fs_qfilestatv {
	__u64 qfs_ino;
	__u64 qfs_nblks;
	__u32 qfs_nextents;
	__u32 qfs_pad;
};

struct fs_quota_statv {
	__s8 qs_version;
	__u8 qs_pad1;
	__u16 qs_flags;
	__u32 qs_incoredqs;
	struct fs_qfilestatv qs_uquota;
	struct fs_qfilestatv qs_gquota;
	struct fs_qfilestatv qs_pquota;
	__s32 qs_btimelimit;
	__s32 qs_itimelimit;
	__s32 qs_rtbtimelimit;
	__u16 qs_bwarnlimit;
	__u16 qs_iwarnlimit;
	__u16 qs_rtbwarnlimit;
	__u16 qs_pad3;
	__u32 qs_pad4;
	__u64 qs_pad2[7];
};

struct dquot;
struct kqid;

struct qtree_fmt_operations {
	void (*mem2disk_dqblk)(void *disk, struct dquot *dquot);
	void (*disk2mem_dqblk)(struct dquot *dquot, void *disk);
	int (*is_id)(void *disk, struct dquot *dquot);
};

struct qtree_mem_dqinfo {
	struct super_block *dqi_sb;
	int dqi_type;
	unsigned int dqi_blocks;
	unsigned int dqi_free_blk;
	unsigned int dqi_free_entry;
	unsigned int dqi_blocksize_bits;
	unsigned int dqi_entry_size;
	unsigned int dqi_usable_bs;
	unsigned int dqi_qtree_depth;
	const struct qtree_fmt_operations *dqi_ops;
};

int qtree_write_dquot(struct qtree_mem_dqinfo *info, struct dquot *dquot);
int qtree_read_dquot(struct qtree_mem_dqinfo *info, struct dquot *dquot);
int qtree_delete_dquot(struct qtree_mem_dqinfo *info, struct dquot *dquot);
int qtree_release_dquot(struct qtree_mem_dqinfo *info, struct dquot *dquot);
int qtree_entry_unused(struct qtree_mem_dqinfo *info, char *disk);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
qtree_depth(struct qtree_mem_dqinfo *info)
{
	unsigned int epb = info->dqi_usable_bs >> 2;
	unsigned long long entries = epb;
	int i;

	for (i = 1; entries < (1ULL << 32); i++)
		entries *= epb;
	return i;
}
int qtree_get_next_id(struct qtree_mem_dqinfo *info, struct kqid *qid);

struct user_namespace;
extern struct user_namespace init_user_ns;

typedef __kernel_uid32_t projid_t;

typedef struct {
	projid_t val;
} kprojid_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) projid_t
__kprojid_val(kprojid_t projid)
{
	return projid.val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
projid_eq(kprojid_t left, kprojid_t right)
{
	return __kprojid_val(left) == __kprojid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
projid_lt(kprojid_t left, kprojid_t right)
{
	return __kprojid_val(left) < __kprojid_val(right);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
projid_valid(kprojid_t projid)
{
	return !projid_eq(projid, (kprojid_t){ -1 });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) kprojid_t
make_kprojid(struct user_namespace *from, projid_t projid)
{
	return (kprojid_t){ projid };
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) projid_t
from_kprojid(struct user_namespace *to, kprojid_t kprojid)
{
	return __kprojid_val(kprojid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) projid_t
from_kprojid_munged(struct user_namespace *to, kprojid_t kprojid)
{
	projid_t projid = from_kprojid(to, kprojid);
	if (projid == (projid_t)-1)
		projid = 65534;
	return projid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kprojid_has_mapping(struct user_namespace *ns, kprojid_t projid)
{
	return true;
}
enum {
	QIF_BLIMITS_B = 0,
	QIF_SPACE_B,
	QIF_ILIMITS_B,
	QIF_INODES_B,
	QIF_BTIME_B,
	QIF_ITIME_B,
};
struct if_dqblk {
	__u64 dqb_bhardlimit;
	__u64 dqb_bsoftlimit;
	__u64 dqb_curspace;
	__u64 dqb_ihardlimit;
	__u64 dqb_isoftlimit;
	__u64 dqb_curinodes;
	__u64 dqb_btime;
	__u64 dqb_itime;
	__u32 dqb_valid;
};

struct if_nextdqblk {
	__u64 dqb_bhardlimit;
	__u64 dqb_bsoftlimit;
	__u64 dqb_curspace;
	__u64 dqb_ihardlimit;
	__u64 dqb_isoftlimit;
	__u64 dqb_curinodes;
	__u64 dqb_btime;
	__u64 dqb_itime;
	__u32 dqb_valid;
	__u32 dqb_id;
};
enum {
	DQF_ROOT_SQUASH_B = 0,
	DQF_SYS_FILE_B = 16,

	DQF_PRIVATE
};

struct if_dqinfo {
	__u64 dqi_bgrace;
	__u64 dqi_igrace;
	__u32 dqi_flags;
	__u32 dqi_valid;
};
enum {
	QUOTA_NL_C_UNSPEC,
	QUOTA_NL_C_WARNING,
	__QUOTA_NL_C_MAX,
};

enum {
	QUOTA_NL_A_UNSPEC,
	QUOTA_NL_A_QTYPE,
	QUOTA_NL_A_EXCESS_ID,
	QUOTA_NL_A_WARNING,
	QUOTA_NL_A_DEV_MAJOR,
	QUOTA_NL_A_DEV_MINOR,
	QUOTA_NL_A_CAUSED_ID,
	QUOTA_NL_A_PAD,
	__QUOTA_NL_A_MAX,
};

enum quota_type {
	USRQUOTA = 0,
	GRPQUOTA = 1,
	PRJQUOTA = 2,
};

typedef __kernel_uid32_t qid_t;
typedef long long qsize_t;

struct kqid {
	union {
		kuid_t uid;
		kgid_t gid;
		kprojid_t projid;
	};
	enum quota_type type;
};

extern bool qid_eq(struct kqid left, struct kqid right);
extern bool qid_lt(struct kqid left, struct kqid right);
extern qid_t from_kqid(struct user_namespace *to, struct kqid qid);
extern qid_t from_kqid_munged(struct user_namespace *to, struct kqid qid);
extern bool qid_valid(struct kqid qid);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kqid
make_kqid(struct user_namespace *from, enum quota_type type, qid_t qid)
{
	struct kqid kqid;

	kqid.type = type;
	switch (type) {
	case USRQUOTA:
		kqid.uid = make_kuid(from, qid);
		break;
	case GRPQUOTA:
		kqid.gid = make_kgid(from, qid);
		break;
	case PRJQUOTA:
		kqid.projid = make_kprojid(from, qid);
		break;
	default:
		do {
			({
				asm volatile(
					"548"
					": nop\n\t"
					".pushsection .discard.instr_begin\n\t"
					".long "
					"548"
					"b - .\n\t"
					".popsection\n\t"
					:
					: "i"(548));
			});
			do {
				asm __inline volatile(
					"1:\t"
					".byte 0x0f, 0x0b"
					"\n"
					".pushsection __bug_table,\"aw\"\n"
					"2:\t"
					".long "
					"1b"
					" - ."
					"\t# bug_entry::bug_addr\n"
					"\t"
					".long "
					"%c0"
					" - ."
					"\t# bug_entry::file\n"
					"\t.word %c1"
					"\t# bug_entry::line\n"
					"\t.word %c2"
					"\t# bug_entry::flags\n"
					"\t.org 2b+%c3\n"
					".popsection\n"
					""
					:
					: "i"("include/linux/quota.h"),
					  "i"(114), "i"(0),
					  "i"(sizeof(struct bug_entry)));
			} while (0);
			__builtin_unreachable();
		} while (0);
	}
	return kqid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kqid
make_kqid_invalid(enum quota_type type)
{
	struct kqid kqid;

	kqid.type = type;
	switch (type) {
	case USRQUOTA:
		kqid.uid = (kuid_t){ -1 };
		break;
	case GRPQUOTA:
		kqid.gid = (kgid_t){ -1 };
		break;
	case PRJQUOTA:
		kqid.projid = (kprojid_t){ -1 };
		break;
	default:
		do {
			({
				asm volatile(
					"549"
					": nop\n\t"
					".pushsection .discard.instr_begin\n\t"
					".long "
					"549"
					"b - .\n\t"
					".popsection\n\t"
					:
					: "i"(549));
			});
			do {
				asm __inline volatile(
					"1:\t"
					".byte 0x0f, 0x0b"
					"\n"
					".pushsection __bug_table,\"aw\"\n"
					"2:\t"
					".long "
					"1b"
					" - ."
					"\t# bug_entry::bug_addr\n"
					"\t"
					".long "
					"%c0"
					" - ."
					"\t# bug_entry::file\n"
					"\t.word %c1"
					"\t# bug_entry::line\n"
					"\t.word %c2"
					"\t# bug_entry::flags\n"
					"\t.org 2b+%c3\n"
					".popsection\n"
					""
					:
					: "i"("include/linux/quota.h"),
					  "i"(141), "i"(0),
					  "i"(sizeof(struct bug_entry)));
			} while (0);
			__builtin_unreachable();
		} while (0);
	}
	return kqid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kqid
make_kqid_uid(kuid_t uid)
{
	struct kqid kqid;
	kqid.type = USRQUOTA;
	kqid.uid = uid;
	return kqid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kqid
make_kqid_gid(kgid_t gid)
{
	struct kqid kqid;
	kqid.type = GRPQUOTA;
	kqid.gid = gid;
	return kqid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kqid
make_kqid_projid(kprojid_t projid)
{
	struct kqid kqid;
	kqid.type = PRJQUOTA;
	kqid.projid = projid;
	return kqid;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
qid_has_mapping(struct user_namespace *ns, struct kqid qid)
{
	return from_kqid(ns, qid) != (qid_t)-1;
}

extern spinlock_t dq_data_lock;
struct mem_dqblk {
	qsize_t dqb_bhardlimit;
	qsize_t dqb_bsoftlimit;
	qsize_t dqb_curspace;
	qsize_t dqb_rsvspace;
	qsize_t dqb_ihardlimit;
	qsize_t dqb_isoftlimit;
	qsize_t dqb_curinodes;
	time64_t dqb_btime;
	time64_t dqb_itime;
};

struct quota_format_type;

struct mem_dqinfo {
	struct quota_format_type *dqi_format;
	int dqi_fmt_id;

	struct list_head dqi_dirty_list;
	unsigned long dqi_flags;
	unsigned int dqi_bgrace;
	unsigned int dqi_igrace;
	qsize_t dqi_max_spc_limit;
	qsize_t dqi_max_ino_limit;
	void *dqi_priv;
};

struct super_block;

enum {
	DQF_INFO_DIRTY_B = DQF_PRIVATE,
};

extern void mark_info_dirty(struct super_block *sb, int type);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
info_dirty(struct mem_dqinfo *info)
{
	return ((__builtin_constant_p(DQF_INFO_DIRTY_B) &&
		 __builtin_constant_p((uintptr_t)(&info->dqi_flags) !=
				      (uintptr_t)((void *)0)) &&
		 (uintptr_t)(&info->dqi_flags) != (uintptr_t)((void *)0) &&
		 __builtin_constant_p(
			 *(const unsigned long *)(&info->dqi_flags))) ?
			const_test_bit(DQF_INFO_DIRTY_B, &info->dqi_flags) :
			_test_bit(DQF_INFO_DIRTY_B, &info->dqi_flags));
}

enum {
	DQST_LOOKUPS,
	DQST_DROPS,
	DQST_READS,
	DQST_WRITES,
	DQST_CACHE_HITS,
	DQST_ALLOC_DQUOTS,
	DQST_FREE_DQUOTS,
	DQST_SYNCS,
	_DQST_DQSTAT_LAST
};

struct dqstats {
	unsigned long stat[_DQST_DQSTAT_LAST];
	struct percpu_counter counter[_DQST_DQSTAT_LAST];
};

extern struct dqstats dqstats;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
dqstats_inc(unsigned int type)
{
	percpu_counter_inc(&dqstats.counter[type]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
dqstats_dec(unsigned int type)
{
	percpu_counter_dec(&dqstats.counter[type]);
}
struct dquot {
	struct hlist_node dq_hash;
	struct list_head dq_inuse;
	struct list_head dq_free;
	struct list_head dq_dirty;
	struct mutex dq_lock;
	spinlock_t dq_dqb_lock;
	atomic_t dq_count;
	struct super_block *dq_sb;
	struct kqid dq_id;
	loff_t dq_off;
	unsigned long dq_flags;
	struct mem_dqblk dq_dqb;
};

struct quota_format_ops {
	int (*check_quota_file)(struct super_block *sb, int type);
	int (*read_file_info)(struct super_block *sb, int type);
	int (*write_file_info)(struct super_block *sb, int type);
	int (*free_file_info)(struct super_block *sb, int type);
	int (*read_dqblk)(struct dquot *dquot);
	int (*commit_dqblk)(struct dquot *dquot);
	int (*release_dqblk)(struct dquot *dquot);
	int (*get_next_id)(struct super_block *sb, struct kqid *qid);
};

struct dquot_operations {
	int (*write_dquot)(struct dquot *);
	struct dquot *(*alloc_dquot)(struct super_block *, int);
	void (*destroy_dquot)(struct dquot *);
	int (*acquire_dquot)(struct dquot *);
	int (*release_dquot)(struct dquot *);
	int (*mark_dirty)(struct dquot *);
	int (*write_info)(struct super_block *, int);

	qsize_t *(*get_reserved_space)(struct inode *);
	int (*get_projid)(struct inode *, kprojid_t *);

	int (*get_inode_usage)(struct inode *, qsize_t *);

	int (*get_next_id)(struct super_block *sb, struct kqid *qid);
};

struct path;

struct qc_dqblk {
	int d_fieldmask;
	u64 d_spc_hardlimit;
	u64 d_spc_softlimit;
	u64 d_ino_hardlimit;
	u64 d_ino_softlimit;
	u64 d_space;
	u64 d_ino_count;
	s64 d_ino_timer;

	s64 d_spc_timer;
	int d_ino_warns;
	int d_spc_warns;
	u64 d_rt_spc_hardlimit;
	u64 d_rt_spc_softlimit;
	u64 d_rt_space;
	s64 d_rt_spc_timer;
	int d_rt_spc_warns;
};
struct qc_type_state {
	unsigned int flags;
	unsigned int spc_timelimit;

	unsigned int ino_timelimit;
	unsigned int rt_spc_timelimit;
	unsigned int spc_warnlimit;
	unsigned int ino_warnlimit;
	unsigned int rt_spc_warnlimit;
	unsigned long long ino;
	blkcnt_t blocks;
	blkcnt_t nextents;
};

struct qc_state {
	unsigned int s_incoredqs;
	struct qc_type_state s_state[3];
};

struct qc_info {
	int i_fieldmask;
	unsigned int i_flags;
	unsigned int i_spc_timelimit;

	unsigned int i_ino_timelimit;
	unsigned int i_rt_spc_timelimit;
	unsigned int i_spc_warnlimit;
	unsigned int i_ino_warnlimit;
	unsigned int i_rt_spc_warnlimit;
};

struct quotactl_ops {
	int (*quota_on)(struct super_block *, int, int, const struct path *);
	int (*quota_off)(struct super_block *, int);
	int (*quota_enable)(struct super_block *, unsigned int);
	int (*quota_disable)(struct super_block *, unsigned int);
	int (*quota_sync)(struct super_block *, int);
	int (*set_info)(struct super_block *, int, struct qc_info *);
	int (*get_dqblk)(struct super_block *, struct kqid, struct qc_dqblk *);
	int (*get_nextdqblk)(struct super_block *, struct kqid *,
			     struct qc_dqblk *);
	int (*set_dqblk)(struct super_block *, struct kqid, struct qc_dqblk *);
	int (*get_state)(struct super_block *, struct qc_state *);
	int (*rm_xquota)(struct super_block *, unsigned int);
};

struct quota_format_type {
	int qf_fmt_id;
	const struct quota_format_ops *qf_ops;
	struct module *qf_owner;
	struct quota_format_type *qf_next;
};
enum {
	_DQUOT_USAGE_ENABLED = 0,
	_DQUOT_LIMITS_ENABLED,
	_DQUOT_SUSPENDED,

	_DQUOT_STATE_FLAGS
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
dquot_state_flag(unsigned int flags, int type)
{
	return flags << type;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
dquot_generic_flag(unsigned int flags, int type)
{
	return (flags >> type) &
	       ((1 << _DQUOT_USAGE_ENABLED * 3) |
		(1 << _DQUOT_LIMITS_ENABLED * 3) | (1 << _DQUOT_SUSPENDED * 3));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned
dquot_state_types(unsigned flags, unsigned flag)
{
	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_550(void) __attribute__((__error__(
			"BUILD_BUG_ON failed: "
			"(flag) == 0 || (((flag) & ((flag) - 1)) != 0)")));
		if (!(!((flag) == 0 || (((flag) & ((flag)-1)) != 0))))
			__compiletime_assert_550();
	} while (0);
	return (flags / flag) & ((1 << 3) - 1);
}

extern void quota_send_warning(struct kqid qid, dev_t dev, const char warntype);
struct quota_info {
	unsigned int flags;
	struct rw_semaphore dqio_sem;
	struct inode *files[3];
	struct mem_dqinfo info[3];
	const struct quota_format_ops *ops[3];
};

void register_quota_format(struct quota_format_type *fmt);
void unregister_quota_format(struct quota_format_type *fmt);

struct quota_module_name {
	int qm_fmt_id;
	char *qm_mod_name;
};
enum positive_aop_returns {
	AOP_WRITEPAGE_ACTIVATE = 0x80000,
	AOP_TRUNCATED_PAGE = 0x80001,
};

struct page;
struct address_space;
struct writeback_control;
struct readahead_control;
struct kiocb {
	struct file *ki_filp;
	loff_t ki_pos;
	void (*ki_complete)(struct kiocb *iocb, long ret);
	void *private;
	int ki_flags;
	u16 ki_ioprio;
	union {
		struct wait_page_queue *ki_waitq;
		ssize_t (*dio_complete)(void *data);
	};
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_sync_kiocb(struct kiocb *kiocb)
{
	return kiocb->ki_complete == ((void *)0);
}

struct address_space_operations {
	int (*writepage)(struct page *page, struct writeback_control *wbc);
	int (*read_folio)(struct file *, struct folio *);

	int (*writepages)(struct address_space *, struct writeback_control *);

	bool (*dirty_folio)(struct address_space *, struct folio *);

	void (*readahead)(struct readahead_control *);

	int (*write_begin)(struct file *, struct address_space *mapping,
			   loff_t pos, unsigned len, struct folio **foliop,
			   void **fsdata);
	int (*write_end)(struct file *, struct address_space *mapping,
			 loff_t pos, unsigned len, unsigned copied,
			 struct folio *folio, void *fsdata);

	sector_t (*bmap)(struct address_space *, sector_t);
	void (*invalidate_folio)(struct folio *, size_t offset, size_t len);
	bool (*release_folio)(struct folio *, gfp_t);
	void (*free_folio)(struct folio *folio);
	ssize_t (*direct_IO)(struct kiocb *, struct iov_iter *iter);

	int (*migrate_folio)(struct address_space *, struct folio *dst,
			     struct folio *src, enum migrate_mode);
	int (*launder_folio)(struct folio *);
	bool (*is_partially_uptodate)(struct folio *, size_t from,
				      size_t count);
	void (*is_dirty_writeback)(struct folio *, bool *dirty, bool *wb);
	int (*error_remove_folio)(struct address_space *, struct folio *);

	int (*swap_activate)(struct swap_info_struct *sis, struct file *file,
			     sector_t *span);
	void (*swap_deactivate)(struct file *file);
	int (*swap_rw)(struct kiocb *iocb, struct iov_iter *iter);
};

extern const struct address_space_operations empty_aops;
struct address_space {
	struct inode *host;
	struct xarray i_pages;
	struct rw_semaphore invalidate_lock;
	gfp_t gfp_mask;
	atomic_t i_mmap_writable;

	struct rb_root_cached i_mmap;
	unsigned long nrpages;
	unsigned long writeback_index;
	const struct address_space_operations *a_ops;
	unsigned long flags;
	errseq_t wb_err;
	spinlock_t i_private_lock;
	struct list_head i_private_list;
	struct rw_semaphore i_mmap_rwsem;
	void *i_private_data;
} __attribute__((aligned(sizeof(long))));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
mapping_tagged(struct address_space *mapping, xa_mark_t tag)
{
	return xa_marked(&mapping->i_pages, tag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_mmap_lock_write(struct address_space *mapping)
{
	down_write(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
i_mmap_trylock_write(struct address_space *mapping)
{
	return down_write_trylock(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_mmap_unlock_write(struct address_space *mapping)
{
	up_write(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
i_mmap_trylock_read(struct address_space *mapping)
{
	return down_read_trylock(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_mmap_lock_read(struct address_space *mapping)
{
	down_read(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_mmap_unlock_read(struct address_space *mapping)
{
	up_read(&mapping->i_mmap_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_mmap_assert_locked(struct address_space *mapping)
{
	do {
		(void)(&mapping->i_mmap_rwsem);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_mmap_assert_write_locked(struct address_space *mapping)
{
	do {
		(void)(&mapping->i_mmap_rwsem);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mapping_mapped(struct address_space *mapping)
{
	return !(
		({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_551(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof((&mapping->i_mmap.rb_root)
						      ->rb_node) ==
					       sizeof(char) ||
				       sizeof((&mapping->i_mmap.rb_root)
						      ->rb_node) ==
					       sizeof(short) ||
				       sizeof((&mapping->i_mmap.rb_root)
						      ->rb_node) ==
					       sizeof(int) ||
				       sizeof((&mapping->i_mmap.rb_root)
						      ->rb_node) ==
					       sizeof(long)) ||
				      sizeof((&mapping->i_mmap.rb_root)
						     ->rb_node) ==
					      sizeof(long long)))
					__compiletime_assert_551();
			} while (0);
			(*(const volatile typeof(_Generic(
				((&mapping->i_mmap.rb_root)->rb_node),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 (&mapping->i_mmap
									   .rb_root)
									 ->rb_node)))
				   *)&((&mapping->i_mmap.rb_root)->rb_node));
		}) == ((void *)0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mapping_writably_mapped(struct address_space *mapping)
{
	return atomic_read(&mapping->i_mmap_writable) > 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mapping_map_writable(struct address_space *mapping)
{
	return atomic_inc_unless_negative(&mapping->i_mmap_writable) ? 0 : -1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mapping_unmap_writable(struct address_space *mapping)
{
	atomic_dec(&mapping->i_mmap_writable);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mapping_deny_writable(struct address_space *mapping)
{
	return atomic_dec_unless_positive(&mapping->i_mmap_writable) ? 0 : -16;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mapping_allow_writable(struct address_space *mapping)
{
	atomic_inc(&mapping->i_mmap_writable);
}
struct posix_acl;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct posix_acl *
uncached_acl_sentinel(struct task_struct *task)
{
	return (void *)task + 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_uncached_acl(struct posix_acl *acl)
{
	return (long)acl & 1;
}
struct inode {
	umode_t i_mode;
	unsigned short i_opflags;
	kuid_t i_uid;
	kgid_t i_gid;
	unsigned int i_flags;

	struct posix_acl *i_acl;
	struct posix_acl *i_default_acl;

	const struct inode_operations *i_op;
	struct super_block *i_sb;
	struct address_space *i_mapping;

	void *i_security;

	unsigned long i_ino;

	union {
		const unsigned int i_nlink;
		unsigned int __i_nlink;
	};
	dev_t i_rdev;
	loff_t i_size;
	time64_t i_atime_sec;
	time64_t i_mtime_sec;
	time64_t i_ctime_sec;
	u32 i_atime_nsec;
	u32 i_mtime_nsec;
	u32 i_ctime_nsec;
	u32 i_generation;
	spinlock_t i_lock;
	unsigned short i_bytes;
	u8 i_blkbits;
	enum rw_hint i_write_hint;
	blkcnt_t i_blocks;

	u32 i_state;

	struct rw_semaphore i_rwsem;

	unsigned long dirtied_when;
	unsigned long dirtied_time_when;

	struct hlist_node i_hash;
	struct list_head i_io_list;
	struct list_head i_lru;
	struct list_head i_sb_list;
	struct list_head i_wb_list;
	union {
		struct hlist_head i_dentry;
		struct callback_head i_rcu;
	};
	atomic64_t i_version;
	atomic64_t i_sequence;
	atomic_t i_count;
	atomic_t i_dio_count;
	atomic_t i_writecount;

	atomic_t i_readcount;

	union {
		const struct file_operations *i_fop;
		void (*free_inode)(struct inode *);
	};
	struct file_lock_context *i_flctx;
	struct address_space i_data;
	struct list_head i_devices;
	union {
		struct pipe_inode_info *i_pipe;
		struct cdev *i_cdev;
		char *i_link;
		unsigned i_dir_seq;
	};

	__u32 i_fsnotify_mask;

	struct fsnotify_mark_connector *i_fsnotify_marks;
	void *i_private;
};

struct wait_queue_head *inode_bit_waitqueue(struct wait_bit_queue_entry *wqe,
					    struct inode *inode, u32 bit);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_wake_up_bit(struct inode *inode, u32 bit)
{
	wake_up_var(((char *)&(inode)->i_state + (bit)));
}

struct timespec64 timestamp_truncate(struct timespec64 t, struct inode *inode);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
i_blocksize(const struct inode *node)
{
	return (1 << node->i_blkbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
inode_unhashed(struct inode *inode)
{
	return hlist_unhashed(&inode->i_hash);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_fake_hash(struct inode *inode)
{
	hlist_add_fake(&inode->i_hash);
}
enum inode_i_mutex_lock_class {
	I_MUTEX_NORMAL,
	I_MUTEX_PARENT,
	I_MUTEX_CHILD,
	I_MUTEX_XATTR,
	I_MUTEX_NONDIR2,
	I_MUTEX_PARENT2,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_lock(struct inode *inode)
{
	down_write(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_unlock(struct inode *inode)
{
	up_write(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_lock_shared(struct inode *inode)
{
	down_read(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_unlock_shared(struct inode *inode)
{
	up_read(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
inode_trylock(struct inode *inode)
{
	return down_write_trylock(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
inode_trylock_shared(struct inode *inode)
{
	return down_read_trylock(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
inode_is_locked(struct inode *inode)
{
	return rwsem_is_locked(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_lock_nested(struct inode *inode, unsigned subclass)
{
	down_write(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_lock_shared_nested(struct inode *inode, unsigned subclass)
{
	down_read(&inode->i_rwsem);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
filemap_invalidate_lock(struct address_space *mapping)
{
	down_write(&mapping->invalidate_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
filemap_invalidate_unlock(struct address_space *mapping)
{
	up_write(&mapping->invalidate_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
filemap_invalidate_lock_shared(struct address_space *mapping)
{
	down_read(&mapping->invalidate_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
filemap_invalidate_trylock_shared(struct address_space *mapping)
{
	return down_read_trylock(&mapping->invalidate_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
filemap_invalidate_unlock_shared(struct address_space *mapping)
{
	up_read(&mapping->invalidate_lock);
}

void lock_two_nondirectories(struct inode *, struct inode *);
void unlock_two_nondirectories(struct inode *, struct inode *);

void filemap_invalidate_lock_two(struct address_space *mapping1,
				 struct address_space *mapping2);
void filemap_invalidate_unlock_two(struct address_space *mapping1,
				   struct address_space *mapping2);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) loff_t
i_size_read(const struct inode *inode)
{
	return ({
		typeof(*&inode->i_size) ___p1 = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_552(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(*&inode->i_size) == sizeof(char) ||
				       sizeof(*&inode->i_size) ==
					       sizeof(short) ||
				       sizeof(*&inode->i_size) == sizeof(int) ||
				       sizeof(*&inode->i_size) ==
					       sizeof(long)) ||
				      sizeof(*&inode->i_size) ==
					      sizeof(long long)))
					__compiletime_assert_552();
			} while (0);
			(*(const volatile typeof(_Generic(
				(*&inode->i_size),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 *&inode->i_size)))
				   *)&(*&inode->i_size));
		});
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_553(void) __attribute__((__error__(
				"Need native word sized stores/loads for atomicity.")));
			if (!((sizeof(*&inode->i_size) == sizeof(char) ||
			       sizeof(*&inode->i_size) == sizeof(short) ||
			       sizeof(*&inode->i_size) == sizeof(int) ||
			       sizeof(*&inode->i_size) == sizeof(long))))
				__compiletime_assert_553();
		} while (0);
		__asm__ __volatile__("" : : : "memory");
		___p1;
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_size_write(struct inode *inode, loff_t i_size)
{
	do {
		do {
		} while (0);
		do {
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_554(void) __attribute__((__error__(
					"Need native word sized stores/loads for atomicity.")));
				if (!((sizeof(*&inode->i_size) == sizeof(char) ||
				       sizeof(*&inode->i_size) ==
					       sizeof(short) ||
				       sizeof(*&inode->i_size) == sizeof(int) ||
				       sizeof(*&inode->i_size) == sizeof(long))))
					__compiletime_assert_554();
			} while (0);
			__asm__ __volatile__("" : : : "memory");
			do {
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_555(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof(*&inode->i_size) ==
						       sizeof(char) ||
					       sizeof(*&inode->i_size) ==
						       sizeof(short) ||
					       sizeof(*&inode->i_size) ==
						       sizeof(int) ||
					       sizeof(*&inode->i_size) ==
						       sizeof(long)) ||
					      sizeof(*&inode->i_size) ==
						      sizeof(long long)))
						__compiletime_assert_555();
				} while (0);
				do {
					*(volatile typeof(*&inode->i_size) *)&(
						*&inode->i_size) = (i_size);
				} while (0);
			} while (0);
		} while (0);
	} while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned
iminor(const struct inode *inode)
{
	return ((unsigned int)((inode->i_rdev) & ((1U << 20) - 1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned
imajor(const struct inode *inode)
{
	return ((unsigned int)((inode->i_rdev) >> 20));
}

struct fown_struct {
	struct file *file;
	rwlock_t lock;
	struct pid *pid;
	enum pid_type pid_type;
	kuid_t uid, euid;
	int signum;
};
struct file_ra_state {
	unsigned long start;
	unsigned int size;
	unsigned int async_size;
	unsigned int ra_pages;
	unsigned int mmap_miss;
	loff_t prev_pos;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
ra_has_index(struct file_ra_state *ra, unsigned long index)
{
	return (index >= ra->start && index < ra->start + ra->size);
}
struct file {
	atomic_long_t f_count;
	spinlock_t f_lock;
	fmode_t f_mode;
	const struct file_operations *f_op;
	struct address_space *f_mapping;
	void *private_data;
	struct inode *f_inode;
	unsigned int f_flags;
	unsigned int f_iocb_flags;
	const struct cred *f_cred;

	struct path f_path;
	union {
		struct mutex f_pos_lock;

		u64 f_pipe;
	};
	loff_t f_pos;

	void *f_security;

	struct fown_struct *f_owner;
	errseq_t f_wb_err;
	errseq_t f_sb_err;

	struct hlist_head *f_ep;

	union {
		struct callback_head f_task_work;
		struct llist_node f_llist;
		struct file_ra_state f_ra;
		freeptr_t f_freeptr;
	};

} __attribute__((aligned(4)));

struct file_handle {
	__u32 handle_bytes;
	int handle_type;

	unsigned char f_handle[] __attribute__((__counted_by__(handle_bytes)));
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct file *
get_file(struct file *f)
{
	long prior = atomic_long_fetch_inc_relaxed(&f->f_count);
	({
		bool __ret_do_once = !!(!prior);
		if (({
			    static bool __attribute__((
				    __section__(".data.once"))) __already_done;
			    bool __ret_cond = !!(__ret_do_once);
			    bool __ret_once = false;
			    if (__builtin_expect(
					!!(__ret_cond && !__already_done), 0)) {
				    __already_done = true;
				    __ret_once = true;
			    }
			    __builtin_expect(!!(__ret_once), 0);
		    }))
			({
				int __ret_warn_on = !!(1);
				if (__builtin_expect(!!(__ret_warn_on), 0))
					do {
						({
							asm volatile(
								"556"
								": nop\n\t"
								".pushsection .discard.instr_begin\n\t"
								".long "
								"556"
								"b - .\n\t"
								".popsection\n\t"
								:
								: "i"(556));
						});
						__warn_printk(
							"struct file::f_count incremented from zero; use-after-free condition present!\n");
						do {
							__auto_type __flags =
								(1 << 0) |
								((1 << 3) |
								 ((9) << 8));
							({
								asm volatile(
									"557"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"557"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(557));
							});
							do {
								asm __inline volatile(
									"1:\t"
									".byte 0x0f, 0x0b"
									"\n"
									".pushsection __bug_table,\"aw\"\n"
									"2:\t"
									".long "
									"1b"
									" - ."
									"\t# bug_entry::bug_addr\n"
									"\t"
									".long "
									"%c0"
									" - ."
									"\t# bug_entry::file\n"
									"\t.word %c1"
									"\t# bug_entry::line\n"
									"\t.word %c2"
									"\t# bug_entry::flags\n"
									"\t.org 2b+%c3\n"
									".popsection\n"
									"998:\n\t"
									".pushsection .discard.reachable\n\t"
									".long 998b\n\t"
									".popsection\n\t"
									:
									: "i"("include/linux/fs.h"),
									  "i"(1082),
									  "i"(__flags),
									  "i"(sizeof(
										  struct bug_entry)));
							} while (0);
							({
								asm volatile(
									"558"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"558"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(558));
							});
						} while (0);
						({
							asm volatile(
								"559"
								": nop\n\t"
								".pushsection .discard.instr_end\n\t"
								".long "
								"559"
								"b - .\n\t"
								".popsection\n\t"
								:
								: "i"(559));
						});
					} while (0);
				__builtin_expect(!!(__ret_warn_on), 0);
			});
		__builtin_expect(!!(__ret_do_once), 0);
	});
	return f;
}

struct file *get_file_rcu(struct file **f);
struct file *get_file_active(struct file **f);
typedef void *fl_owner_t;

struct file_lock;
struct file_lease;

int file_f_owner_allocate(struct file *file);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct fown_struct *
file_f_owner(const struct file *file)
{
	return ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_560(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(file->f_owner) == sizeof(char) ||
			       sizeof(file->f_owner) == sizeof(short) ||
			       sizeof(file->f_owner) == sizeof(int) ||
			       sizeof(file->f_owner) == sizeof(long)) ||
			      sizeof(file->f_owner) == sizeof(long long)))
				__compiletime_assert_560();
		} while (0);
		(*(const volatile typeof(_Generic((file->f_owner),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (file->f_owner)))
			   *)&(file->f_owner));
	});
}

extern void send_sigio(struct fown_struct *fown, int fd, int band);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct inode *
file_inode(const struct file *f)
{
	return f->f_inode;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct dentry *
file_dentry(const struct file *file)
{
	struct dentry *dentry = file->f_path.dentry;

	({
		int __ret_warn_on = !!(d_inode(dentry) != file_inode(file));
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) |
						      ((1 << 1) | ((9) << 8));
				({
					asm volatile(
						"561"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"561"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(561));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/fs.h"),
						  "i"(1138), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"562"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"562"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(562));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	return dentry;
}

struct fasync_struct {
	rwlock_t fa_lock;
	int magic;
	int fa_fd;
	struct fasync_struct *fa_next;
	struct file *fa_file;
	struct callback_head fa_rcu;
};

extern int fasync_helper(int, struct file *, int, struct fasync_struct **);
extern struct fasync_struct *fasync_insert_entry(int, struct file *,
						 struct fasync_struct **,
						 struct fasync_struct *);
extern int fasync_remove_entry(struct file *, struct fasync_struct **);
extern struct fasync_struct *fasync_alloc(void);
extern void fasync_free(struct fasync_struct *);

extern void kill_fasync(struct fasync_struct **, int, int);

extern void __f_setown(struct file *filp, struct pid *, enum pid_type,
		       int force);
extern int f_setown(struct file *filp, int who, int force);
extern void f_delown(struct file *filp);
extern pid_t f_getown(struct file *filp);
extern int send_sigurg(struct file *file);
enum {
	SB_UNFROZEN = 0,
	SB_FREEZE_WRITE = 1,
	SB_FREEZE_PAGEFAULT = 2,
	SB_FREEZE_FS = 3,

	SB_FREEZE_COMPLETE = 4,
};

struct sb_writers {
	unsigned short frozen;
	int freeze_kcount;
	int freeze_ucount;
	struct percpu_rw_semaphore rw_sem[(SB_FREEZE_COMPLETE - 1)];
};

struct super_block {
	struct list_head s_list;
	dev_t s_dev;
	unsigned char s_blocksize_bits;
	unsigned long s_blocksize;
	loff_t s_maxbytes;
	struct file_system_type *s_type;
	const struct super_operations *s_op;
	const struct dquot_operations *dq_op;
	const struct quotactl_ops *s_qcop;
	const struct export_operations *s_export_op;
	unsigned long s_flags;
	unsigned long s_iflags;
	unsigned long s_magic;
	struct dentry *s_root;
	struct rw_semaphore s_umount;
	int s_count;
	atomic_t s_active;

	void *s_security;

	const struct xattr_handler *const *s_xattr;
	struct hlist_bl_head s_roots;
	struct list_head s_mounts;
	struct block_device *s_bdev;
	struct file *s_bdev_file;
	struct backing_dev_info *s_bdi;
	struct mtd_info *s_mtd;
	struct hlist_node s_instances;
	unsigned int s_quota_types;
	struct quota_info s_dquot;

	struct sb_writers s_writers;

	void *s_fs_info;

	u32 s_time_gran;

	time64_t s_time_min;
	time64_t s_time_max;

	u32 s_fsnotify_mask;
	struct fsnotify_sb_info *s_fsnotify_info;
	char s_id[32];
	uuid_t s_uuid;
	u8 s_uuid_len;

	char s_sysfs_name[36 + 1];

	unsigned int s_max_links;

	struct mutex s_vfs_rename_mutex;

	const char *s_subtype;

	const struct dentry_operations *s_d_op;

	struct shrinker *s_shrink;

	atomic_long_t s_remove_count;

	int s_readonly_remount;

	errseq_t s_wb_err;

	struct workqueue_struct *s_dio_done_wq;
	struct hlist_head s_pins;

	struct user_namespace *s_user_ns;

	struct list_lru s_dentry_lru;
	struct list_lru s_inode_lru;
	struct callback_head rcu;
	struct work_struct destroy_work;

	struct mutex s_sync_lock;

	int s_stack_depth;

	spinlock_t s_inode_list_lock __attribute__((__aligned__((1 << (6)))));
	struct list_head s_inodes;

	spinlock_t s_inode_wblist_lock;
	struct list_head s_inodes_wb;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct user_namespace *
i_user_ns(const struct inode *inode)
{
	return inode->i_sb->s_user_ns;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) uid_t
i_uid_read(const struct inode *inode)
{
	return from_kuid(i_user_ns(inode), inode->i_uid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) gid_t
i_gid_read(const struct inode *inode)
{
	return from_kgid(i_user_ns(inode), inode->i_gid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_uid_write(struct inode *inode, uid_t uid)
{
	inode->i_uid = make_kuid(i_user_ns(inode), uid);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_gid_write(struct inode *inode, gid_t gid)
{
	inode->i_gid = make_kgid(i_user_ns(inode), gid);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) vfsuid_t
i_uid_into_vfsuid(struct mnt_idmap *idmap, const struct inode *inode)
{
	return make_vfsuid(idmap, i_user_ns(inode), inode->i_uid);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
i_uid_needs_update(struct mnt_idmap *idmap, const struct iattr *attr,
		   const struct inode *inode)
{
	return ((attr->ia_valid & (1 << 1)) &&
		!vfsuid_eq(attr->ia_vfsuid, i_uid_into_vfsuid(idmap, inode)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_uid_update(struct mnt_idmap *idmap, const struct iattr *attr,
	     struct inode *inode)
{
	if (attr->ia_valid & (1 << 1))
		inode->i_uid =
			from_vfsuid(idmap, i_user_ns(inode), attr->ia_vfsuid);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) vfsgid_t
i_gid_into_vfsgid(struct mnt_idmap *idmap, const struct inode *inode)
{
	return make_vfsgid(idmap, i_user_ns(inode), inode->i_gid);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
i_gid_needs_update(struct mnt_idmap *idmap, const struct iattr *attr,
		   const struct inode *inode)
{
	return ((attr->ia_valid & (1 << 2)) &&
		!vfsgid_eq(attr->ia_vfsgid, i_gid_into_vfsgid(idmap, inode)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_gid_update(struct mnt_idmap *idmap, const struct iattr *attr,
	     struct inode *inode)
{
	if (attr->ia_valid & (1 << 2))
		inode->i_gid =
			from_vfsgid(idmap, i_user_ns(inode), attr->ia_vfsgid);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_fsuid_set(struct inode *inode, struct mnt_idmap *idmap)
{
	inode->i_uid = mapped_fsuid(idmap, i_user_ns(inode));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_fsgid_set(struct inode *inode, struct mnt_idmap *idmap)
{
	inode->i_gid = mapped_fsgid(idmap, i_user_ns(inode));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
fsuidgid_has_mapping(struct super_block *sb, struct mnt_idmap *idmap)
{
	struct user_namespace *fs_userns = sb->s_user_ns;
	kuid_t kuid;
	kgid_t kgid;

	kuid = mapped_fsuid(idmap, fs_userns);
	if (!uid_valid(kuid))
		return false;
	kgid = mapped_fsgid(idmap, fs_userns);
	if (!gid_valid(kgid))
		return false;
	return kuid_has_mapping(fs_userns, kuid) &&
	       kgid_has_mapping(fs_userns, kgid);
}

struct timespec64 current_time(struct inode *inode);
struct timespec64 inode_set_ctime_current(struct inode *inode);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) time64_t
inode_get_atime_sec(const struct inode *inode)
{
	return inode->i_atime_sec;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) long
inode_get_atime_nsec(const struct inode *inode)
{
	return inode->i_atime_nsec;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timespec64
inode_get_atime(const struct inode *inode)
{
	struct timespec64 ts = { .tv_sec = inode_get_atime_sec(inode),
				 .tv_nsec = inode_get_atime_nsec(inode) };

	return ts;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timespec64
inode_set_atime_to_ts(struct inode *inode, struct timespec64 ts)
{
	inode->i_atime_sec = ts.tv_sec;
	inode->i_atime_nsec = ts.tv_nsec;
	return ts;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timespec64
inode_set_atime(struct inode *inode, time64_t sec, long nsec)
{
	struct timespec64 ts = { .tv_sec = sec, .tv_nsec = nsec };

	return inode_set_atime_to_ts(inode, ts);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) time64_t
inode_get_mtime_sec(const struct inode *inode)
{
	return inode->i_mtime_sec;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) long
inode_get_mtime_nsec(const struct inode *inode)
{
	return inode->i_mtime_nsec;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timespec64
inode_get_mtime(const struct inode *inode)
{
	struct timespec64 ts = { .tv_sec = inode_get_mtime_sec(inode),
				 .tv_nsec = inode_get_mtime_nsec(inode) };
	return ts;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timespec64
inode_set_mtime_to_ts(struct inode *inode, struct timespec64 ts)
{
	inode->i_mtime_sec = ts.tv_sec;
	inode->i_mtime_nsec = ts.tv_nsec;
	return ts;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timespec64
inode_set_mtime(struct inode *inode, time64_t sec, long nsec)
{
	struct timespec64 ts = { .tv_sec = sec, .tv_nsec = nsec };
	return inode_set_mtime_to_ts(inode, ts);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) time64_t
inode_get_ctime_sec(const struct inode *inode)
{
	return inode->i_ctime_sec;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) long
inode_get_ctime_nsec(const struct inode *inode)
{
	return inode->i_ctime_nsec;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timespec64
inode_get_ctime(const struct inode *inode)
{
	struct timespec64 ts = { .tv_sec = inode_get_ctime_sec(inode),
				 .tv_nsec = inode_get_ctime_nsec(inode) };

	return ts;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timespec64
inode_set_ctime_to_ts(struct inode *inode, struct timespec64 ts)
{
	inode->i_ctime_sec = ts.tv_sec;
	inode->i_ctime_nsec = ts.tv_nsec;
	return ts;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct timespec64
inode_set_ctime(struct inode *inode, time64_t sec, long nsec)
{
	struct timespec64 ts = { .tv_sec = sec, .tv_nsec = nsec };

	return inode_set_ctime_to_ts(inode, ts);
}

struct timespec64 simple_inode_init_ts(struct inode *inode);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__sb_end_write(struct super_block *sb, int level)
{
	percpu_up_read(sb->s_writers.rw_sem + level - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__sb_start_write(struct super_block *sb, int level)
{
	percpu_down_read(sb->s_writers.rw_sem + level - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
__sb_start_write_trylock(struct super_block *sb, int level)
{
	return percpu_down_read_trylock(sb->s_writers.rw_sem + level - 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
__sb_write_started(const struct super_block *sb, int level)
{
	return (1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
sb_write_started(const struct super_block *sb)
{
	return __sb_write_started(sb, SB_FREEZE_WRITE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
sb_write_not_started(const struct super_block *sb)
{
	return __sb_write_started(sb, SB_FREEZE_WRITE) <= 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
file_write_started(const struct file *file)
{
	if (!(((file_inode(file)->i_mode) & 00170000) == 0100000))
		return true;
	return sb_write_started(file_inode(file)->i_sb);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
file_write_not_started(const struct file *file)
{
	if (!(((file_inode(file)->i_mode) & 00170000) == 0100000))
		return true;
	return sb_write_not_started(file_inode(file)->i_sb);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sb_end_write(struct super_block *sb)
{
	__sb_end_write(sb, SB_FREEZE_WRITE);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sb_end_pagefault(struct super_block *sb)
{
	__sb_end_write(sb, SB_FREEZE_PAGEFAULT);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sb_end_intwrite(struct super_block *sb)
{
	__sb_end_write(sb, SB_FREEZE_FS);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sb_start_write(struct super_block *sb)
{
	__sb_start_write(sb, SB_FREEZE_WRITE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
sb_start_write_trylock(struct super_block *sb)
{
	return __sb_start_write_trylock(sb, SB_FREEZE_WRITE);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sb_start_pagefault(struct super_block *sb)
{
	__sb_start_write(sb, SB_FREEZE_PAGEFAULT);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sb_start_intwrite(struct super_block *sb)
{
	__sb_start_write(sb, SB_FREEZE_FS);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
sb_start_intwrite_trylock(struct super_block *sb)
{
	return __sb_start_write_trylock(sb, SB_FREEZE_FS);
}

bool inode_owner_or_capable(struct mnt_idmap *idmap, const struct inode *inode);

int vfs_create(struct mnt_idmap *, struct inode *, struct dentry *, umode_t,
	       bool);
int vfs_mkdir(struct mnt_idmap *, struct inode *, struct dentry *, umode_t);
int vfs_mknod(struct mnt_idmap *, struct inode *, struct dentry *, umode_t,
	      dev_t);
int vfs_symlink(struct mnt_idmap *, struct inode *, struct dentry *,
		const char *);
int vfs_link(struct dentry *, struct mnt_idmap *, struct inode *,
	     struct dentry *, struct inode **);
int vfs_rmdir(struct mnt_idmap *, struct inode *, struct dentry *);
int vfs_unlink(struct mnt_idmap *, struct inode *, struct dentry *,
	       struct inode **);
struct renamedata {
	struct mnt_idmap *old_mnt_idmap;
	struct inode *old_dir;
	struct dentry *old_dentry;
	struct mnt_idmap *new_mnt_idmap;
	struct inode *new_dir;
	struct dentry *new_dentry;
	struct inode **delegated_inode;
	unsigned int flags;
};

int vfs_rename(struct renamedata *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
vfs_whiteout(struct mnt_idmap *idmap, struct inode *dir, struct dentry *dentry)
{
	return vfs_mknod(idmap, dir, dentry, 0020000 | 0, 0);
}

struct file *kernel_tmpfile_open(struct mnt_idmap *idmap,
				 const struct path *parentpath, umode_t mode,
				 int open_flag, const struct cred *cred);
struct file *kernel_file_open(const struct path *path, int flags,
			      const struct cred *cred);

int vfs_mkobj(struct dentry *, umode_t,
	      int (*f)(struct dentry *, umode_t, void *), void *);

int vfs_fchown(struct file *file, uid_t user, gid_t group);
int vfs_fchmod(struct file *file, umode_t mode);
int vfs_utimes(const struct path *path, struct timespec64 *times);

extern long vfs_ioctl(struct file *file, unsigned int cmd, unsigned long arg);

extern long compat_ptr_ioctl(struct file *file, unsigned int cmd,
			     unsigned long arg);

void inode_init_owner(struct mnt_idmap *idmap, struct inode *inode,
		      const struct inode *dir, umode_t mode);
extern bool may_open_dev(const struct path *path);
umode_t mode_strip_sgid(struct mnt_idmap *idmap, const struct inode *dir,
			umode_t mode);
bool in_group_or_capable(struct mnt_idmap *idmap, const struct inode *inode,
			 vfsgid_t vfsgid);
struct dir_context;
typedef bool (*filldir_t)(struct dir_context *, const char *, int, loff_t, u64,
			  unsigned);

struct dir_context {
	filldir_t actor;
	loff_t pos;
};
struct iov_iter;
struct io_uring_cmd;
struct offset_ctx;

typedef unsigned int fop_flags_t;

struct file_operations {
	struct module *owner;
	fop_flags_t fop_flags;
	loff_t (*llseek)(struct file *, loff_t, int);
	ssize_t (*read)(struct file *, char *, size_t, loff_t *);
	ssize_t (*write)(struct file *, const char *, size_t, loff_t *);
	ssize_t (*read_iter)(struct kiocb *, struct iov_iter *);
	ssize_t (*write_iter)(struct kiocb *, struct iov_iter *);
	int (*iopoll)(struct kiocb *kiocb, struct io_comp_batch *,
		      unsigned int flags);
	int (*iterate_shared)(struct file *, struct dir_context *);
	__poll_t (*poll)(struct file *, struct poll_table_struct *);
	long (*unlocked_ioctl)(struct file *, unsigned int, unsigned long);
	long (*compat_ioctl)(struct file *, unsigned int, unsigned long);
	int (*mmap)(struct file *, struct vm_area_struct *);
	int (*open)(struct inode *, struct file *);
	int (*flush)(struct file *, fl_owner_t id);
	int (*release)(struct inode *, struct file *);
	int (*fsync)(struct file *, loff_t, loff_t, int datasync);
	int (*fasync)(int, struct file *, int);
	int (*lock)(struct file *, int, struct file_lock *);
	unsigned long (*get_unmapped_area)(struct file *, unsigned long,
					   unsigned long, unsigned long,
					   unsigned long);
	int (*check_flags)(int);
	int (*flock)(struct file *, int, struct file_lock *);
	ssize_t (*splice_write)(struct pipe_inode_info *, struct file *,
				loff_t *, size_t, unsigned int);
	ssize_t (*splice_read)(struct file *, loff_t *,
			       struct pipe_inode_info *, size_t, unsigned int);
	void (*splice_eof)(struct file *file);
	int (*setlease)(struct file *, int, struct file_lease **, void **);
	long (*fallocate)(struct file *file, int mode, loff_t offset,
			  loff_t len);
	void (*show_fdinfo)(struct seq_file *m, struct file *f);

	ssize_t (*copy_file_range)(struct file *, loff_t, struct file *, loff_t,
				   size_t, unsigned int);
	loff_t (*remap_file_range)(struct file *file_in, loff_t pos_in,
				   struct file *file_out, loff_t pos_out,
				   loff_t len, unsigned int remap_flags);
	int (*fadvise)(struct file *, loff_t, loff_t, int);
	int (*uring_cmd)(struct io_uring_cmd *ioucmd, unsigned int issue_flags);
	int (*uring_cmd_iopoll)(struct io_uring_cmd *, struct io_comp_batch *,
				unsigned int poll_flags);
};
int wrap_directory_iterator(struct file *, struct dir_context *,
			    int (*)(struct file *, struct dir_context *));

struct inode_operations {
	struct dentry *(*lookup)(struct inode *, struct dentry *, unsigned int);
	const char *(*get_link)(struct dentry *, struct inode *,
				struct delayed_call *);
	int (*permission)(struct mnt_idmap *, struct inode *, int);
	struct posix_acl *(*get_inode_acl)(struct inode *, int, bool);

	int (*readlink)(struct dentry *, char *, int);

	int (*create)(struct mnt_idmap *, struct inode *, struct dentry *,
		      umode_t, bool);
	int (*link)(struct dentry *, struct inode *, struct dentry *);
	int (*unlink)(struct inode *, struct dentry *);
	int (*symlink)(struct mnt_idmap *, struct inode *, struct dentry *,
		       const char *);
	int (*mkdir)(struct mnt_idmap *, struct inode *, struct dentry *,
		     umode_t);
	int (*rmdir)(struct inode *, struct dentry *);
	int (*mknod)(struct mnt_idmap *, struct inode *, struct dentry *,
		     umode_t, dev_t);
	int (*rename)(struct mnt_idmap *, struct inode *, struct dentry *,
		      struct inode *, struct dentry *, unsigned int);
	int (*setattr)(struct mnt_idmap *, struct dentry *, struct iattr *);
	int (*getattr)(struct mnt_idmap *, const struct path *, struct kstat *,
		       u32, unsigned int);
	ssize_t (*listxattr)(struct dentry *, char *, size_t);
	int (*fiemap)(struct inode *, struct fiemap_extent_info *, u64 start,
		      u64 len);
	int (*update_time)(struct inode *, int);
	int (*atomic_open)(struct inode *, struct dentry *, struct file *,
			   unsigned open_flag, umode_t create_mode);
	int (*tmpfile)(struct mnt_idmap *, struct inode *, struct file *,
		       umode_t);
	struct posix_acl *(*get_acl)(struct mnt_idmap *, struct dentry *, int);
	int (*set_acl)(struct mnt_idmap *, struct dentry *, struct posix_acl *,
		       int);
	int (*fileattr_set)(struct mnt_idmap *idmap, struct dentry *dentry,
			    struct fileattr *fa);
	int (*fileattr_get)(struct dentry *dentry, struct fileattr *fa);
	struct offset_ctx *(*get_offset_ctx)(struct inode *inode);
} __attribute__((__aligned__((1 << (6)))));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
call_mmap(struct file *file, struct vm_area_struct *vma)
{
	return file->f_op->mmap(file, vma);
}

extern ssize_t vfs_read(struct file *, char *, size_t, loff_t *);
extern ssize_t vfs_write(struct file *, const char *, size_t, loff_t *);
extern ssize_t vfs_copy_file_range(struct file *, loff_t, struct file *, loff_t,
				   size_t, unsigned int);
int remap_verify_area(struct file *file, loff_t pos, loff_t len, bool write);
int __generic_remap_file_range_prep(struct file *file_in, loff_t pos_in,
				    struct file *file_out, loff_t pos_out,
				    loff_t *len, unsigned int remap_flags,
				    const struct iomap_ops *dax_read_ops);
int generic_remap_file_range_prep(struct file *file_in, loff_t pos_in,
				  struct file *file_out, loff_t pos_out,
				  loff_t *count, unsigned int remap_flags);
extern loff_t vfs_clone_file_range(struct file *file_in, loff_t pos_in,
				   struct file *file_out, loff_t pos_out,
				   loff_t len, unsigned int remap_flags);
extern int vfs_dedupe_file_range(struct file *file,
				 struct file_dedupe_range *same);
extern loff_t vfs_dedupe_file_range_one(struct file *src_file, loff_t src_pos,
					struct file *dst_file, loff_t dst_pos,
					loff_t len, unsigned int remap_flags);
enum freeze_holder {
	FREEZE_HOLDER_KERNEL = (1U << 0),
	FREEZE_HOLDER_USERSPACE = (1U << 1),
	FREEZE_MAY_NEST = (1U << 2),
};

struct super_operations {
	struct inode *(*alloc_inode)(struct super_block *sb);
	void (*destroy_inode)(struct inode *);
	void (*free_inode)(struct inode *);

	void (*dirty_inode)(struct inode *, int flags);
	int (*write_inode)(struct inode *, struct writeback_control *wbc);
	int (*drop_inode)(struct inode *);
	void (*evict_inode)(struct inode *);
	void (*put_super)(struct super_block *);
	int (*sync_fs)(struct super_block *sb, int wait);
	int (*freeze_super)(struct super_block *, enum freeze_holder who);
	int (*freeze_fs)(struct super_block *);
	int (*thaw_super)(struct super_block *, enum freeze_holder who);
	int (*unfreeze_fs)(struct super_block *);
	int (*statfs)(struct dentry *, struct kstatfs *);
	int (*remount_fs)(struct super_block *, int *, char *);
	void (*umount_begin)(struct super_block *);

	int (*show_options)(struct seq_file *, struct dentry *);
	int (*show_devname)(struct seq_file *, struct dentry *);
	int (*show_path)(struct seq_file *, struct dentry *);
	int (*show_stats)(struct seq_file *, struct dentry *);

	ssize_t (*quota_read)(struct super_block *, int, char *, size_t,
			      loff_t);
	ssize_t (*quota_write)(struct super_block *, int, const char *, size_t,
			       loff_t);
	struct dquot **(*get_dquots)(struct inode *);

	long (*nr_cached_objects)(struct super_block *,
				  struct shrink_control *);
	long (*free_cached_objects)(struct super_block *,
				    struct shrink_control *);
	void (*shutdown)(struct super_block *sb);
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
sb_rdonly(const struct super_block *sb)
{
	return sb->s_flags & ((((1UL))) << (0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
HAS_UNMAPPED_ID(struct mnt_idmap *idmap, struct inode *inode)
{
	return !vfsuid_valid(i_uid_into_vfsuid(idmap, inode)) ||
	       !vfsgid_valid(i_gid_into_vfsgid(idmap, inode));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
init_sync_kiocb(struct kiocb *kiocb, struct file *filp)
{
	*kiocb = (struct kiocb){
		.ki_filp = filp,
		.ki_flags = filp->f_iocb_flags,
		.ki_ioprio = get_current_ioprio(),
	};
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kiocb_clone(struct kiocb *kiocb, struct kiocb *kiocb_src, struct file *filp)
{
	*kiocb = (struct kiocb){
		.ki_filp = filp,
		.ki_flags = kiocb_src->ki_flags,
		.ki_ioprio = kiocb_src->ki_ioprio,
		.ki_pos = kiocb_src->ki_pos,
	};
}
extern void __mark_inode_dirty(struct inode *, int);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mark_inode_dirty(struct inode *inode)
{
	__mark_inode_dirty(inode, (((1 << 3) | (1 << 4)) | (1 << 5)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
mark_inode_dirty_sync(struct inode *inode)
{
	__mark_inode_dirty(inode, (1 << 3));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
inode_is_dirtytime_only(struct inode *inode)
{
	return (inode->i_state &
		((1 << 11) | (1 << 0) | (1 << 7) | (1 << 6))) == (1 << 11);
}

extern void inc_nlink(struct inode *inode);
extern void drop_nlink(struct inode *inode);
extern void clear_nlink(struct inode *inode);
extern void set_nlink(struct inode *inode, unsigned int nlink);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_inc_link_count(struct inode *inode)
{
	inc_nlink(inode);
	mark_inode_dirty(inode);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_dec_link_count(struct inode *inode)
{
	drop_nlink(inode);
	mark_inode_dirty(inode);
}

enum file_time_flags {
	S_ATIME = 1,
	S_MTIME = 2,
	S_CTIME = 4,
	S_VERSION = 8,
};

extern bool atime_needs_update(const struct path *, struct inode *);
extern void touch_atime(const struct path *);
int inode_update_time(struct inode *inode, int flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
file_accessed(struct file *file)
{
	if (!(file->f_flags & 01000000))
		touch_atime(&file->f_path);
}

extern int file_modified(struct file *file);
int kiocb_modified(struct kiocb *iocb);

int sync_inode_metadata(struct inode *inode, int wait);

struct file_system_type {
	const char *name;
	int fs_flags;

	int (*init_fs_context)(struct fs_context *);
	const struct fs_parameter_spec *parameters;
	struct dentry *(*mount)(struct file_system_type *, int, const char *,
				void *);
	void (*kill_sb)(struct super_block *);
	struct module *owner;
	struct file_system_type *next;
	struct hlist_head fs_supers;

	struct lock_class_key s_lock_key;
	struct lock_class_key s_umount_key;
	struct lock_class_key s_vfs_rename_key;
	struct lock_class_key s_writers_key[(SB_FREEZE_COMPLETE - 1)];

	struct lock_class_key i_lock_key;
	struct lock_class_key i_mutex_key;
	struct lock_class_key invalidate_lock_key;
	struct lock_class_key i_mutex_dir_key;
};

extern struct dentry *
mount_bdev(struct file_system_type *fs_type, int flags, const char *dev_name,
	   void *data, int (*fill_super)(struct super_block *, void *, int));
extern struct dentry *
mount_single(struct file_system_type *fs_type, int flags, void *data,
	     int (*fill_super)(struct super_block *, void *, int));
extern struct dentry *
mount_nodev(struct file_system_type *fs_type, int flags, void *data,
	    int (*fill_super)(struct super_block *, void *, int));
extern struct dentry *mount_subtree(struct vfsmount *mnt, const char *path);
void retire_super(struct super_block *sb);
void generic_shutdown_super(struct super_block *sb);
void kill_block_super(struct super_block *sb);
void kill_anon_super(struct super_block *sb);
void kill_litter_super(struct super_block *sb);
void deactivate_super(struct super_block *sb);
void deactivate_locked_super(struct super_block *sb);
int set_anon_super(struct super_block *s, void *data);
int set_anon_super_fc(struct super_block *s, struct fs_context *fc);
int get_anon_bdev(dev_t *);
void free_anon_bdev(dev_t);
struct super_block *
sget_fc(struct fs_context *fc,
	int (*test)(struct super_block *, struct fs_context *),
	int (*set)(struct super_block *, struct fs_context *));
struct super_block *sget(struct file_system_type *type,
			 int (*test)(struct super_block *, void *),
			 int (*set)(struct super_block *, void *), int flags,
			 void *data);
struct super_block *sget_dev(struct fs_context *fc, dev_t dev);
extern int register_filesystem(struct file_system_type *);
extern int unregister_filesystem(struct file_system_type *);
extern int vfs_statfs(const struct path *, struct kstatfs *);
extern int user_statfs(const char *, struct kstatfs *);
extern int fd_statfs(int, struct kstatfs *);
int freeze_super(struct super_block *super, enum freeze_holder who);
int thaw_super(struct super_block *super, enum freeze_holder who);
extern __attribute__((__format__(printf, 2, 3))) int
super_setup_bdi_name(struct super_block *sb, char *fmt, ...);
extern int super_setup_bdi(struct super_block *sb);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
super_set_uuid(struct super_block *sb, const u8 *uuid, unsigned len)
{
	if (({
		    int __ret_warn_on = !!(len > sizeof(sb->s_uuid));
		    if (__builtin_expect(!!(__ret_warn_on), 0))
			    do {
				    __auto_type __flags = (1 << 0) |
							  (((9) << 8));
				    ({
					    asm volatile(
						    "563"
						    ": nop\n\t"
						    ".pushsection .discard.instr_begin\n\t"
						    ".long "
						    "563"
						    "b - .\n\t"
						    ".popsection\n\t"
						    :
						    : "i"(563));
				    });
				    do {
					    asm __inline volatile(
						    "1:\t"
						    ".byte 0x0f, 0x0b"
						    "\n"
						    ".pushsection __bug_table,\"aw\"\n"
						    "2:\t"
						    ".long "
						    "1b"
						    " - ."
						    "\t# bug_entry::bug_addr\n"
						    "\t"
						    ".long "
						    "%c0"
						    " - ."
						    "\t# bug_entry::file\n"
						    "\t.word %c1"
						    "\t# bug_entry::line\n"
						    "\t.word %c2"
						    "\t# bug_entry::flags\n"
						    "\t.org 2b+%c3\n"
						    ".popsection\n"
						    "998:\n\t"
						    ".pushsection .discard.reachable\n\t"
						    ".long 998b\n\t"
						    ".popsection\n\t"
						    :
						    : "i"("include/linux/fs.h"),
						      "i"(2635), "i"(__flags),
						      "i"(sizeof(
							      struct bug_entry)));
				    } while (0);
				    ({
					    asm volatile(
						    "564"
						    ": nop\n\t"
						    ".pushsection .discard.instr_end\n\t"
						    ".long "
						    "564"
						    "b - .\n\t"
						    ".popsection\n\t"
						    :
						    : "i"(564));
				    });
			    } while (0);
		    __builtin_expect(!!(__ret_warn_on), 0);
	    }))
		len = sizeof(sb->s_uuid);
	sb->s_uuid_len = len;
	({
		const size_t __fortify_size = (size_t)(len);
		const size_t __p_size =
			(__builtin_dynamic_object_size(&sb->s_uuid, 0));
		const size_t __q_size =
			(__builtin_dynamic_object_size(uuid, 0));
		const size_t __p_size_field =
			(__builtin_dynamic_object_size(&sb->s_uuid, 1));
		const size_t __q_size_field =
			(__builtin_dynamic_object_size(uuid, 1));
		({
			bool __ret_do_once = !!(fortify_memcpy_chk(
				__fortify_size, __p_size, __q_size,
				__p_size_field, __q_size_field,
				FORTIFY_FUNC_memcpy));
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					int __ret_warn_on = !!(1);
					if (__builtin_expect(!!(__ret_warn_on),
							     0))
						do {
							({
								asm volatile(
									"565"
									": nop\n\t"
									".pushsection .discard.instr_begin\n\t"
									".long "
									"565"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(565));
							});
							__warn_printk(
								"memcpy"
								": detected field-spanning write (size %zu) of single %s (size %zu)\n",
								__fortify_size,
								"field \""
								"&sb->s_uuid"
								"\" at "
								"include/linux/fs.h"
								":"
								"2638",
								__p_size_field);
							do {
								__auto_type __flags =
									(1
									 << 0) |
									((1
									  << 3) |
									 ((9)
									  << 8));
								({
									asm volatile(
										"566"
										": nop\n\t"
										".pushsection .discard.instr_begin\n\t"
										".long "
										"566"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(566));
								});
								do {
									asm __inline volatile(
										"1:\t"
										".byte 0x0f, 0x0b"
										"\n"
										".pushsection __bug_table,\"aw\"\n"
										"2:\t"
										".long "
										"1b"
										" - ."
										"\t# bug_entry::bug_addr\n"
										"\t"
										".long "
										"%c0"
										" - ."
										"\t# bug_entry::file\n"
										"\t.word %c1"
										"\t# bug_entry::line\n"
										"\t.word %c2"
										"\t# bug_entry::flags\n"
										"\t.org 2b+%c3\n"
										".popsection\n"
										"998:\n\t"
										".pushsection .discard.reachable\n\t"
										".long 998b\n\t"
										".popsection\n\t"
										:
										: "i"("include/linux/fs.h"),
										  "i"(2638),
										  "i"(__flags),
										  "i"(sizeof(
											  struct bug_entry)));
								} while (0);
								({
									asm volatile(
										"567"
										": nop\n\t"
										".pushsection .discard.instr_end\n\t"
										".long "
										"567"
										"b - .\n\t"
										".popsection\n\t"
										:
										: "i"(567));
								});
							} while (0);
							({
								asm volatile(
									"568"
									": nop\n\t"
									".pushsection .discard.instr_end\n\t"
									".long "
									"568"
									"b - .\n\t"
									".popsection\n\t"
									:
									: "i"(568));
							});
						} while (0);
					__builtin_expect(!!(__ret_warn_on), 0);
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});
		__builtin_memcpy(&sb->s_uuid, uuid, __fortify_size);
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
super_set_sysfs_name_bdev(struct super_block *sb)
{
	snprintf(sb->s_sysfs_name, sizeof(sb->s_sysfs_name), "%pg", sb->s_bdev);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
super_set_sysfs_name_uuid(struct super_block *sb)
{
	({
		int __ret_warn_on = !!(sb->s_uuid_len != sizeof(sb->s_uuid));
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) | (((9) << 8));
				({
					asm volatile(
						"569"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"569"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(569));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/fs.h"),
						  "i"(2650), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"570"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"570"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(570));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	snprintf(sb->s_sysfs_name, sizeof(sb->s_sysfs_name), "%pU",
		 sb->s_uuid.b);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
super_set_sysfs_name_id(struct super_block *sb)
{
	sized_strscpy(sb->s_sysfs_name, sb->s_id,
		      sizeof(sb->s_sysfs_name) + ((int)(sizeof(struct {
			      int : (-!!((false)));
		      }))) + ((int)(sizeof(struct { int : (-!!((false))); }))));
}

__attribute__((__format__(printf, 2, 3))) static inline
	__attribute__((__gnu_inline__)) __attribute__((__unused__))
	__attribute__((no_instrument_function)) void
	super_set_sysfs_name_generic(struct super_block *sb, const char *fmt,
				     ...)
{
	va_list args;

	__builtin_va_start(args, fmt);
	vsnprintf(sb->s_sysfs_name, sizeof(sb->s_sysfs_name), fmt, args);
	__builtin_va_end(args);
}

extern int current_umask(void);

extern void ihold(struct inode *inode);
extern void iput(struct inode *);
int inode_update_timestamps(struct inode *inode, int flags);
int generic_update_time(struct inode *, int);

extern struct kobject *fs_kobj;

struct audit_names;
struct filename {
	const char *name;
	const char *uptr;
	atomic_t refcnt;
	struct audit_names *aname;
	const char iname[];
};
_Static_assert(__builtin_offsetof(struct filename, iname) % sizeof(long) == 0,
	       "offsetof(struct filename, iname) % sizeof(long) == 0");

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct mnt_idmap *
file_mnt_idmap(const struct file *file)
{
	return mnt_idmap(file->f_path.mnt);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_idmapped_mnt(const struct vfsmount *mnt)
{
	return mnt_idmap(mnt) != &nop_mnt_idmap;
}

extern long vfs_truncate(const struct path *, loff_t);
int do_truncate(struct mnt_idmap *, struct dentry *, loff_t start,
		unsigned int time_attrs, struct file *filp);
extern int vfs_fallocate(struct file *file, int mode, loff_t offset,
			 loff_t len);
extern long do_sys_open(int dfd, const char *filename, int flags, umode_t mode);
extern struct file *file_open_name(struct filename *, int, umode_t);
extern struct file *filp_open(const char *, int, umode_t);
extern struct file *file_open_root(const struct path *, const char *, int,
				   umode_t);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct file *
file_open_root_mnt(struct vfsmount *mnt, const char *name, int flags,
		   umode_t mode)
{
	return file_open_root(&(struct path){ .mnt = mnt,
					      .dentry = mnt->mnt_root },
			      name, flags, mode);
}
struct file *dentry_open(const struct path *path, int flags,
			 const struct cred *creds);
struct file *dentry_create(const struct path *path, int flags, umode_t mode,
			   const struct cred *cred);
struct path *backing_file_user_path(struct file *f);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const struct path *
file_user_path(struct file *f)
{
	if (__builtin_expect(!!(f->f_mode & ((fmode_t)(1 << 25))), 0))
		return backing_file_user_path(f);
	return &f->f_path;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const struct inode *
file_user_inode(struct file *f)
{
	if (__builtin_expect(!!(f->f_mode & ((fmode_t)(1 << 25))), 0))
		return d_inode(backing_file_user_path(f)->dentry);
	return file_inode(f);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct file *
file_clone_open(struct file *file)
{
	return dentry_open(&file->f_path, file->f_flags, file->f_cred);
}
extern int filp_close(struct file *, fl_owner_t id);

extern struct filename *getname_flags(const char *, int);
extern struct filename *getname_uflags(const char *, int);
extern struct filename *getname(const char *);
extern struct filename *getname_kernel(const char *);
extern void putname(struct filename *name);

extern int finish_open(struct file *file, struct dentry *dentry,
		       int (*open)(struct inode *, struct file *));
extern int finish_no_open(struct file *file, struct dentry *dentry);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
finish_open_simple(struct file *file, int error)
{
	if (error)
		return error;

	return finish_open(file, file->f_path.dentry, ((void *)0));
}

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
vfs_caches_init_early(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__))
vfs_caches_init(void);

extern struct kmem_cache *names_cachep;

extern struct super_block *blockdev_superblock;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
sb_is_blkdev_sb(struct super_block *sb)
{
	return 1 && sb == blockdev_superblock;
}

void emergency_thaw_all(void);
extern int sync_filesystem(struct super_block *);
extern const struct file_operations def_blk_fops;
extern const struct file_operations def_chr_fops;
extern int alloc_chrdev_region(dev_t *, unsigned, unsigned, const char *);
extern int register_chrdev_region(dev_t, unsigned, const char *);
extern int __register_chrdev(unsigned int major, unsigned int baseminor,
			     unsigned int count, const char *name,
			     const struct file_operations *fops);
extern void __unregister_chrdev(unsigned int major, unsigned int baseminor,
				unsigned int count, const char *name);
extern void unregister_chrdev_region(dev_t, unsigned);
extern void chrdev_show(struct seq_file *, off_t);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
register_chrdev(unsigned int major, const char *name,
		const struct file_operations *fops)
{
	return __register_chrdev(major, 0, 256, name, fops);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
unregister_chrdev(unsigned int major, const char *name)
{
	__unregister_chrdev(major, 0, 256, name);
}

extern void init_special_inode(struct inode *, umode_t, dev_t);

extern void make_bad_inode(struct inode *);
extern bool is_bad_inode(struct inode *);

extern int __attribute__((__warn_unused_result__))
file_fdatawait_range(struct file *file, loff_t lstart, loff_t lend);
extern int __attribute__((__warn_unused_result__))
file_check_and_advance_wb_err(struct file *file);
extern int __attribute__((__warn_unused_result__))
file_write_and_wait_range(struct file *file, loff_t start, loff_t end);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
file_write_and_wait(struct file *file)
{
	return file_write_and_wait_range(file, 0, ((long long)(~0ULL >> 1)));
}

extern int vfs_fsync_range(struct file *file, loff_t start, loff_t end,
			   int datasync);
extern int vfs_fsync(struct file *file, int datasync);

extern int sync_file_range(struct file *file, loff_t offset, loff_t nbytes,
			   unsigned int flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
iocb_is_dsync(const struct kiocb *iocb)
{
	return (iocb->ki_flags & (int)((__kernel_rwf_t)0x00000002)) ||
	       (((iocb->ki_filp->f_mapping->host)->i_sb->s_flags &
		 (((((1UL))) << (4)))) ||
		((iocb->ki_filp->f_mapping->host)->i_flags & (1 << 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ssize_t
generic_write_sync(struct kiocb *iocb, ssize_t count)
{
	if (iocb_is_dsync(iocb)) {
		int ret = vfs_fsync_range(
			iocb->ki_filp, iocb->ki_pos - count, iocb->ki_pos - 1,
			(iocb->ki_flags & (int)((__kernel_rwf_t)0x00000004)) ?
				0 :
				1);
		if (ret)
			return ret;
	}

	return count;
}

extern void emergency_sync(void);
extern void emergency_remount(void);

extern int bmap(struct inode *inode, sector_t *block);

int notify_change(struct mnt_idmap *, struct dentry *, struct iattr *,
		  struct inode **);
int inode_permission(struct mnt_idmap *, struct inode *, int);
int generic_permission(struct mnt_idmap *, struct inode *, int);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
file_permission(struct file *file, int mask)
{
	return inode_permission(file_mnt_idmap(file), file_inode(file), mask);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
path_permission(const struct path *path, int mask)
{
	return inode_permission(mnt_idmap(path->mnt), d_inode(path->dentry),
				mask);
}
int __check_sticky(struct mnt_idmap *idmap, struct inode *dir,
		   struct inode *inode);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
execute_ok(struct inode *inode)
{
	return (inode->i_mode & (00100 | 00010 | 00001)) ||
	       (((inode->i_mode) & 00170000) == 0040000);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
inode_wrong_type(const struct inode *inode, umode_t mode)
{
	return (inode->i_mode ^ mode) & 00170000;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
file_start_write(struct file *file)
{
	if (!(((file_inode(file)->i_mode) & 00170000) == 0100000))
		return;
	sb_start_write(file_inode(file)->i_sb);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
file_start_write_trylock(struct file *file)
{
	if (!(((file_inode(file)->i_mode) & 00170000) == 0100000))
		return true;
	return sb_start_write_trylock(file_inode(file)->i_sb);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
file_end_write(struct file *file)
{
	if (!(((file_inode(file)->i_mode) & 00170000) == 0100000))
		return;
	sb_end_write(file_inode(file)->i_sb);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kiocb_start_write(struct kiocb *iocb)
{
	struct inode *inode = file_inode(iocb->ki_filp);

	sb_start_write(inode->i_sb);

	percpu_rwsem_release(
		&(inode->i_sb)->s_writers.rw_sem[(SB_FREEZE_WRITE)-1], ({
			unsigned long __here;
			asm("lea 0(%%rip), %0" : "=r"(__here));
			__here;
		}));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kiocb_end_write(struct kiocb *iocb)
{
	struct inode *inode = file_inode(iocb->ki_filp);

	percpu_rwsem_acquire(
		&(inode->i_sb)->s_writers.rw_sem[(SB_FREEZE_WRITE)-1], 1, ({
			unsigned long __here;
			asm("lea 0(%%rip), %0" : "=r"(__here));
			__here;
		}));
	sb_end_write(inode->i_sb);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_write_access(struct inode *inode)
{
	return atomic_inc_unless_negative(&inode->i_writecount) ? 0 : -26;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
deny_write_access(struct file *file)
{
	struct inode *inode = file_inode(file);
	return atomic_dec_unless_positive(&inode->i_writecount) ? 0 : -26;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
put_write_access(struct inode *inode)
{
	atomic_dec(&inode->i_writecount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
allow_write_access(struct file *file)
{
	if (file)
		atomic_inc(&file_inode(file)->i_writecount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
inode_is_open_for_write(const struct inode *inode)
{
	return atomic_read(&inode->i_writecount) > 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_readcount_dec(struct inode *inode)
{
	do {
		if (__builtin_expect(
			    !!(atomic_dec_return(&inode->i_readcount) < 0), 0))
			do {
				({
					asm volatile(
						"571"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"571"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(571));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						""
						:
						: "i"("include/linux/fs.h"),
						  "i"(3037), "i"(0),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				__builtin_unreachable();
			} while (0);
	} while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
i_readcount_inc(struct inode *inode)
{
	atomic_inc(&inode->i_readcount);
}
extern int do_pipe_flags(int *, int);

extern ssize_t kernel_read(struct file *, void *, size_t, loff_t *);
ssize_t __kernel_read(struct file *file, void *buf, size_t count, loff_t *pos);
extern ssize_t kernel_write(struct file *, const void *, size_t, loff_t *);
extern ssize_t __kernel_write(struct file *, const void *, size_t, loff_t *);
extern struct file *open_exec(const char *);

extern bool is_subdir(struct dentry *, struct dentry *);
extern bool path_is_under(const struct path *, const struct path *);

extern char *file_path(struct file *, char *, int);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_dot_dotdot(const char *name, size_t len)
{
	return len && __builtin_expect(!!(name[0] == '.'), 0) &&
	       (len == 1 || (len == 2 && name[1] == '.'));
}

extern loff_t default_llseek(struct file *file, loff_t offset, int whence);

extern loff_t vfs_llseek(struct file *file, loff_t offset, int whence);

extern int inode_init_always_gfp(struct super_block *, struct inode *, gfp_t);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
inode_init_always(struct super_block *sb, struct inode *inode)
{
	return inode_init_always_gfp(
		sb, inode,
		(((gfp_t)(((((1UL))) << (___GFP_DIRECT_RECLAIM_BIT)) |
			  ((((1UL))) << (___GFP_KSWAPD_RECLAIM_BIT)))) |
		 ((gfp_t)((((1UL))) << (___GFP_IO_BIT)))));
}

extern void inode_init_once(struct inode *);
extern void address_space_init_once(struct address_space *mapping);
extern struct inode *igrab(struct inode *);
extern ino_t iunique(struct super_block *, ino_t);
extern int inode_needs_sync(struct inode *inode);
extern int generic_delete_inode(struct inode *inode);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
generic_drop_inode(struct inode *inode)
{
	return !inode->i_nlink || inode_unhashed(inode);
}
extern void d_mark_dontcache(struct inode *inode);

extern struct inode *ilookup5_nowait(struct super_block *sb,
				     unsigned long hashval,
				     int (*test)(struct inode *, void *),
				     void *data);
extern struct inode *ilookup5(struct super_block *sb, unsigned long hashval,
			      int (*test)(struct inode *, void *), void *data);
extern struct inode *ilookup(struct super_block *sb, unsigned long ino);

extern struct inode *inode_insert5(struct inode *inode, unsigned long hashval,
				   int (*test)(struct inode *, void *),
				   int (*set)(struct inode *, void *),
				   void *data);
struct inode *iget5_locked(struct super_block *, unsigned long,
			   int (*test)(struct inode *, void *),
			   int (*set)(struct inode *, void *), void *);
struct inode *iget5_locked_rcu(struct super_block *, unsigned long,
			       int (*test)(struct inode *, void *),
			       int (*set)(struct inode *, void *), void *);
extern struct inode *iget_locked(struct super_block *, unsigned long);
extern struct inode *find_inode_nowait(struct super_block *, unsigned long,
				       int (*match)(struct inode *,
						    unsigned long, void *),
				       void *data);
extern struct inode *find_inode_rcu(struct super_block *, unsigned long,
				    int (*)(struct inode *, void *), void *);
extern struct inode *find_inode_by_ino_rcu(struct super_block *, unsigned long);
extern int insert_inode_locked4(struct inode *, unsigned long,
				int (*test)(struct inode *, void *), void *);
extern int insert_inode_locked(struct inode *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
lockdep_annotate_inode_mutex_key(struct inode *inode) {};

extern void unlock_new_inode(struct inode *);
extern void discard_new_inode(struct inode *);
extern unsigned int get_next_ino(void);
extern void evict_inodes(struct super_block *sb);
void dump_mapping(const struct address_space *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_zero_ino(ino_t ino)
{
	return (u32)ino == 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
__iget(struct inode *inode)
{
	atomic_inc(&inode->i_count);
}

extern void iget_failed(struct inode *);
extern void clear_inode(struct inode *);
extern void __destroy_inode(struct inode *);
extern struct inode *new_inode_pseudo(struct super_block *sb);
extern struct inode *new_inode(struct super_block *sb);
extern void free_inode_nonrcu(struct inode *inode);
extern int setattr_should_drop_suidgid(struct mnt_idmap *, struct inode *);
extern int file_remove_privs_flags(struct file *file, unsigned int flags);
extern int file_remove_privs(struct file *);
int setattr_should_drop_sgid(struct mnt_idmap *idmap,
			     const struct inode *inode);

extern void __insert_inode_hash(struct inode *, unsigned long hashval);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
insert_inode_hash(struct inode *inode)
{
	__insert_inode_hash(inode, inode->i_ino);
}

extern void __remove_inode_hash(struct inode *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
remove_inode_hash(struct inode *inode)
{
	if (!inode_unhashed(inode) && !hlist_fake(&inode->i_hash))
		__remove_inode_hash(inode);
}

extern void inode_sb_list_add(struct inode *inode);
extern void inode_add_lru(struct inode *inode);

extern int sb_set_blocksize(struct super_block *, int);
extern int sb_min_blocksize(struct super_block *, int);

extern int generic_file_mmap(struct file *, struct vm_area_struct *);
extern int generic_file_readonly_mmap(struct file *, struct vm_area_struct *);
extern ssize_t generic_write_checks(struct kiocb *, struct iov_iter *);
int generic_write_checks_count(struct kiocb *iocb, loff_t *count);
extern int generic_write_check_limits(struct file *file, loff_t pos,
				      loff_t *count);
extern int generic_file_rw_checks(struct file *file_in, struct file *file_out);
ssize_t filemap_read(struct kiocb *iocb, struct iov_iter *to,
		     ssize_t already_read);
extern ssize_t generic_file_read_iter(struct kiocb *, struct iov_iter *);
extern ssize_t __generic_file_write_iter(struct kiocb *, struct iov_iter *);
extern ssize_t generic_file_write_iter(struct kiocb *, struct iov_iter *);
extern ssize_t generic_file_direct_write(struct kiocb *, struct iov_iter *);
ssize_t generic_perform_write(struct kiocb *, struct iov_iter *);
ssize_t direct_write_fallback(struct kiocb *iocb, struct iov_iter *iter,
			      ssize_t direct_written, ssize_t buffered_written);

ssize_t vfs_iter_read(struct file *file, struct iov_iter *iter, loff_t *ppos,
		      rwf_t flags);
ssize_t vfs_iter_write(struct file *file, struct iov_iter *iter, loff_t *ppos,
		       rwf_t flags);
ssize_t vfs_iocb_iter_read(struct file *file, struct kiocb *iocb,
			   struct iov_iter *iter);
ssize_t vfs_iocb_iter_write(struct file *file, struct kiocb *iocb,
			    struct iov_iter *iter);

ssize_t filemap_splice_read(struct file *in, loff_t *ppos,
			    struct pipe_inode_info *pipe, size_t len,
			    unsigned int flags);
ssize_t copy_splice_read(struct file *in, loff_t *ppos,
			 struct pipe_inode_info *pipe, size_t len,
			 unsigned int flags);
extern ssize_t iter_file_splice_write(struct pipe_inode_info *, struct file *,
				      loff_t *, size_t, unsigned int);

extern void file_ra_state_init(struct file_ra_state *ra,
			       struct address_space *mapping);
extern loff_t noop_llseek(struct file *file, loff_t offset, int whence);
extern loff_t vfs_setpos(struct file *file, loff_t offset, loff_t maxsize);
extern loff_t generic_file_llseek(struct file *file, loff_t offset, int whence);
extern loff_t generic_file_llseek_size(struct file *file, loff_t offset,
				       int whence, loff_t maxsize, loff_t eof);
loff_t generic_llseek_cookie(struct file *file, loff_t offset, int whence,
			     u64 *cookie);
extern loff_t fixed_size_llseek(struct file *file, loff_t offset, int whence,
				loff_t size);
extern loff_t no_seek_end_llseek_size(struct file *, loff_t, int, loff_t);
extern loff_t no_seek_end_llseek(struct file *, loff_t, int);
int rw_verify_area(int, struct file *, const loff_t *, size_t);
extern int generic_file_open(struct inode *inode, struct file *filp);
extern int nonseekable_open(struct inode *inode, struct file *filp);
extern int stream_open(struct inode *inode, struct file *filp);

typedef void(dio_submit_t)(struct bio *bio, struct inode *inode,
			   loff_t file_offset);

enum {

	DIO_LOCKING = 0x01,

	DIO_SKIP_HOLES = 0x02,
};

ssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
			     struct block_device *bdev, struct iov_iter *iter,
			     get_block_t get_block, dio_iodone_t end_io,
			     int flags);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ssize_t
blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,
		   struct iov_iter *iter, get_block_t get_block)
{
	return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev, iter,
				    get_block, ((void *)0),
				    DIO_LOCKING | DIO_SKIP_HOLES);
}

bool inode_dio_finished(const struct inode *inode);
void inode_dio_wait(struct inode *inode);
void inode_dio_wait_interruptible(struct inode *inode);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_dio_begin(struct inode *inode)
{
	atomic_inc(&inode->i_dio_count);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_dio_end(struct inode *inode)
{
	if (atomic_dec_and_test(&inode->i_dio_count))
		wake_up_var(&inode->i_dio_count);
}

extern void inode_set_flags(struct inode *inode, unsigned int flags,
			    unsigned int mask);

extern const struct file_operations generic_ro_fops;

extern int readlink_copy(char *, int, const char *);
extern int page_readlink(struct dentry *, char *, int);
extern const char *page_get_link(struct dentry *, struct inode *,
				 struct delayed_call *);
extern void page_put_link(void *);
extern int page_symlink(struct inode *inode, const char *symname, int len);
extern const struct inode_operations page_symlink_inode_operations;
extern void kfree_link(void *);
void generic_fillattr(struct mnt_idmap *, u32, struct inode *, struct kstat *);
void generic_fill_statx_attr(struct inode *inode, struct kstat *stat);
void generic_fill_statx_atomic_writes(struct kstat *stat, unsigned int unit_min,
				      unsigned int unit_max);
extern int vfs_getattr_nosec(const struct path *, struct kstat *, u32,
			     unsigned int);
extern int vfs_getattr(const struct path *, struct kstat *, u32, unsigned int);
void __inode_add_bytes(struct inode *inode, loff_t bytes);
void inode_add_bytes(struct inode *inode, loff_t bytes);
void __inode_sub_bytes(struct inode *inode, loff_t bytes);
void inode_sub_bytes(struct inode *inode, loff_t bytes);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) loff_t
__inode_get_bytes(struct inode *inode)
{
	return (((loff_t)inode->i_blocks) << 9) + inode->i_bytes;
}
loff_t inode_get_bytes(struct inode *inode);
void inode_set_bytes(struct inode *inode, loff_t bytes);
const char *simple_get_link(struct dentry *, struct inode *,
			    struct delayed_call *);
extern const struct inode_operations simple_symlink_inode_operations;

extern int iterate_dir(struct file *, struct dir_context *);

int vfs_fstatat(int dfd, const char *filename, struct kstat *stat, int flags);
int vfs_fstat(int fd, struct kstat *stat);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
vfs_stat(const char *filename, struct kstat *stat)
{
	return vfs_fstatat(-100, filename, stat, 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
vfs_lstat(const char *name, struct kstat *stat)
{
	return vfs_fstatat(-100, name, stat, 0x100);
}

extern const char *vfs_get_link(struct dentry *, struct delayed_call *);
extern int vfs_readlink(struct dentry *, char *, int);

extern struct file_system_type *get_filesystem(struct file_system_type *fs);
extern void put_filesystem(struct file_system_type *fs);
extern struct file_system_type *get_fs_type(const char *name);
extern void drop_super(struct super_block *sb);
extern void drop_super_exclusive(struct super_block *sb);
extern void iterate_supers(void (*)(struct super_block *, void *), void *);
extern void iterate_supers_type(struct file_system_type *,
				void (*)(struct super_block *, void *), void *);

extern int dcache_dir_open(struct inode *, struct file *);
extern int dcache_dir_close(struct inode *, struct file *);
extern loff_t dcache_dir_lseek(struct file *, loff_t, int);
extern int dcache_readdir(struct file *, struct dir_context *);
extern int simple_setattr(struct mnt_idmap *, struct dentry *, struct iattr *);
extern int simple_getattr(struct mnt_idmap *, const struct path *,
			  struct kstat *, u32, unsigned int);
extern int simple_statfs(struct dentry *, struct kstatfs *);
extern int simple_open(struct inode *inode, struct file *file);
extern int simple_link(struct dentry *, struct inode *, struct dentry *);
extern int simple_unlink(struct inode *, struct dentry *);
extern int simple_rmdir(struct inode *, struct dentry *);
void simple_rename_timestamp(struct inode *old_dir, struct dentry *old_dentry,
			     struct inode *new_dir, struct dentry *new_dentry);
extern int simple_rename_exchange(struct inode *old_dir,
				  struct dentry *old_dentry,
				  struct inode *new_dir,
				  struct dentry *new_dentry);
extern int simple_rename(struct mnt_idmap *, struct inode *, struct dentry *,
			 struct inode *, struct dentry *, unsigned int);
extern void simple_recursive_removal(struct dentry *,
				     void (*callback)(struct dentry *));
extern int noop_fsync(struct file *, loff_t, loff_t, int);
extern ssize_t noop_direct_IO(struct kiocb *iocb, struct iov_iter *iter);
extern int simple_empty(struct dentry *);
extern int simple_write_begin(struct file *file, struct address_space *mapping,
			      loff_t pos, unsigned len, struct folio **foliop,
			      void **fsdata);
extern const struct address_space_operations ram_aops;
extern int always_delete_dentry(const struct dentry *);
extern struct inode *alloc_anon_inode(struct super_block *);
extern int simple_nosetlease(struct file *, int, struct file_lease **, void **);
extern const struct dentry_operations simple_dentry_operations;

extern struct dentry *simple_lookup(struct inode *, struct dentry *,
				    unsigned int flags);
extern ssize_t generic_read_dir(struct file *, char *, size_t, loff_t *);
extern const struct file_operations simple_dir_operations;
extern const struct inode_operations simple_dir_inode_operations;
extern void make_empty_dir_inode(struct inode *inode);
extern bool is_empty_dir_inode(struct inode *inode);
struct tree_descr {
	const char *name;
	const struct file_operations *ops;
	int mode;
};
struct dentry *d_alloc_name(struct dentry *, const char *);
extern int simple_fill_super(struct super_block *, unsigned long,
			     const struct tree_descr *);
extern int simple_pin_fs(struct file_system_type *, struct vfsmount **mount,
			 int *count);
extern void simple_release_fs(struct vfsmount **mount, int *count);

extern ssize_t simple_read_from_buffer(void *to, size_t count, loff_t *ppos,
				       const void *from, size_t available);
extern ssize_t simple_write_to_buffer(void *to, size_t available, loff_t *ppos,
				      const void *from, size_t count);

struct offset_ctx {
	struct maple_tree mt;
	unsigned long next_offset;
};

void simple_offset_init(struct offset_ctx *octx);
int simple_offset_add(struct offset_ctx *octx, struct dentry *dentry);
void simple_offset_remove(struct offset_ctx *octx, struct dentry *dentry);
int simple_offset_empty(struct dentry *dentry);
int simple_offset_rename(struct inode *old_dir, struct dentry *old_dentry,
			 struct inode *new_dir, struct dentry *new_dentry);
int simple_offset_rename_exchange(struct inode *old_dir,
				  struct dentry *old_dentry,
				  struct inode *new_dir,
				  struct dentry *new_dentry);
void simple_offset_destroy(struct offset_ctx *octx);

extern const struct file_operations simple_offset_dir_operations;

extern int __generic_file_fsync(struct file *, loff_t, loff_t, int);
extern int generic_file_fsync(struct file *, loff_t, loff_t, int);

extern int generic_check_addressable(unsigned, u64);

extern void generic_set_sb_d_ops(struct super_block *sb);
extern int generic_ci_match(const struct inode *parent, const struct qstr *name,
			    const struct qstr *folded_name, const u8 *de_name,
			    u32 de_name_len);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
sb_has_encoding(const struct super_block *sb)
{
	return false;
}

int may_setattr(struct mnt_idmap *idmap, struct inode *inode,
		unsigned int ia_valid);
int setattr_prepare(struct mnt_idmap *, struct dentry *, struct iattr *);
extern int inode_newsize_ok(const struct inode *, loff_t offset);
void setattr_copy(struct mnt_idmap *, struct inode *inode,
		  const struct iattr *attr);

extern int file_update_time(struct file *file);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vma_is_dax(const struct vm_area_struct *vma)
{
	return vma->vm_file && ((vma->vm_file->f_mapping->host)->i_flags & 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vma_is_fsdax(struct vm_area_struct *vma)
{
	struct inode *inode;

	if (!0 || !vma->vm_file)
		return false;
	if (!vma_is_dax(vma))
		return false;
	inode = file_inode(vma->vm_file);
	if ((((inode->i_mode) & 00170000) == 0020000))
		return false;
	return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
iocb_flags(struct file *file)
{
	int res = 0;
	if (file->f_flags & 00002000)
		res |= (int)((__kernel_rwf_t)0x00000010);
	if (file->f_flags & 00040000)
		res |= (1 << 17);
	if (file->f_flags & 00010000)
		res |= (int)((__kernel_rwf_t)0x00000002);
	if (file->f_flags & 04000000)
		res |= (int)((__kernel_rwf_t)0x00000004);
	return res;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kiocb_set_rw_flags(struct kiocb *ki, rwf_t flags, int rw_type)
{
	int kiocb_flags = 0;

	do {
		__attribute__((__noreturn__)) extern void
		__compiletime_assert_572(void) __attribute__((__error__(
			"BUILD_BUG_ON failed: "
			"(__force int) RWF_SUPPORTED & IOCB_EVENTFD")));
		if (!(!((int)(((__kernel_rwf_t)0x00000001) |
			      ((__kernel_rwf_t)0x00000002) |
			      ((__kernel_rwf_t)0x00000004) |
			      ((__kernel_rwf_t)0x00000008) |
			      ((__kernel_rwf_t)0x00000010) |
			      ((__kernel_rwf_t)0x00000020) |
			      ((__kernel_rwf_t)0x00000040)) &
			(1 << 16))))
			__compiletime_assert_572();
	} while (0);

	if (!flags)
		return 0;
	if (__builtin_expect(!!(flags & ~(((__kernel_rwf_t)0x00000001) |
					  ((__kernel_rwf_t)0x00000002) |
					  ((__kernel_rwf_t)0x00000004) |
					  ((__kernel_rwf_t)0x00000008) |
					  ((__kernel_rwf_t)0x00000010) |
					  ((__kernel_rwf_t)0x00000020) |
					  ((__kernel_rwf_t)0x00000040))),
			     0))
		return -95;
	if (__builtin_expect(!!((flags & ((__kernel_rwf_t)0x00000010)) &&
				(flags & ((__kernel_rwf_t)0x00000020))),
			     0))
		return -22;

	if (flags & ((__kernel_rwf_t)0x00000008)) {
		if (!(ki->ki_filp->f_mode & ((fmode_t)(1 << 27))))
			return -95;
	}
	if (flags & ((__kernel_rwf_t)0x00000040)) {
		if (rw_type != 1)
			return -95;
		if (!(ki->ki_filp->f_mode & ((fmode_t)(1 << 7))))
			return -95;
	}
	kiocb_flags |= (int)(flags & (((__kernel_rwf_t)0x00000001) |
				      ((__kernel_rwf_t)0x00000002) |
				      ((__kernel_rwf_t)0x00000004) |
				      ((__kernel_rwf_t)0x00000008) |
				      ((__kernel_rwf_t)0x00000010) |
				      ((__kernel_rwf_t)0x00000020) |
				      ((__kernel_rwf_t)0x00000040)));
	if (flags & ((__kernel_rwf_t)0x00000004))
		kiocb_flags |= (int)((__kernel_rwf_t)0x00000002);

	if ((flags & ((__kernel_rwf_t)0x00000020)) &&
	    (ki->ki_flags & (int)((__kernel_rwf_t)0x00000010))) {
		if (((file_inode(ki->ki_filp))->i_flags & (1 << 2)))
			return -1;
		ki->ki_flags &= ~(int)((__kernel_rwf_t)0x00000010);
	}

	ki->ki_flags |= kiocb_flags;
	return 0;
}

struct simple_transaction_argresp {
	ssize_t size;
	char data[];
};

char *simple_transaction_get(struct file *file, const char *buf, size_t size);
ssize_t simple_transaction_read(struct file *file, char *buf, size_t size,
				loff_t *pos);
int simple_transaction_release(struct inode *inode, struct file *file);

void simple_transaction_set(struct file *file, size_t n);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__format__(printf, 1, 2))) void
__simple_attr_check_format(const char *fmt, ...)
{
}

int simple_attr_open(struct inode *inode, struct file *file,
		     int (*get)(void *, u64 *), int (*set)(void *, u64),
		     const char *fmt);
int simple_attr_release(struct inode *inode, struct file *file);
ssize_t simple_attr_read(struct file *file, char *buf, size_t len,
			 loff_t *ppos);
ssize_t simple_attr_write(struct file *file, const char *buf, size_t len,
			  loff_t *ppos);
ssize_t simple_attr_write_signed(struct file *file, const char *buf, size_t len,
				 loff_t *ppos);

struct ctl_table;
int __attribute__((__section__(".init.text"))) __attribute__((__cold__))
list_bdev_fs_names(char *buf, size_t size);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_sxid(umode_t mode)
{
	return mode & (0004000 | 0002000);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
check_sticky(struct mnt_idmap *idmap, struct inode *dir, struct inode *inode)
{
	if (!(dir->i_mode & 0001000))
		return 0;

	return __check_sticky(idmap, dir, inode);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
inode_has_no_xattr(struct inode *inode)
{
	if (!is_sxid(inode->i_mode) &&
	    (inode->i_sb->s_flags & ((((1UL))) << (28))))
		inode->i_flags |= (1 << 12);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_root_inode(struct inode *inode)
{
	return inode == inode->i_sb->s_root->d_inode;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
dir_emit(struct dir_context *ctx, const char *name, int namelen, u64 ino,
	 unsigned type)
{
	return ctx->actor(ctx, name, namelen, ctx->pos, ino, type);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
dir_emit_dot(struct file *file, struct dir_context *ctx)
{
	return ctx->actor(ctx, ".", 1, ctx->pos,
			  file->f_path.dentry->d_inode->i_ino, 4);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
dir_emit_dotdot(struct file *file, struct dir_context *ctx)
{
	return ctx->actor(ctx, "..", 2, ctx->pos,
			  d_parent_ino(file->f_path.dentry), 4);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
dir_emit_dots(struct file *file, struct dir_context *ctx)
{
	if (ctx->pos == 0) {
		if (!dir_emit_dot(file, ctx))
			return false;
		ctx->pos = 1;
	}
	if (ctx->pos == 1) {
		if (!dir_emit_dotdot(file, ctx))
			return false;
		ctx->pos = 2;
	}
	return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
dir_relax(struct inode *inode)
{
	inode_unlock(inode);
	inode_lock(inode);
	return !((inode)->i_flags & (1 << 4));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
dir_relax_shared(struct inode *inode)
{
	inode_unlock_shared(inode);
	inode_lock_shared(inode);
	return !((inode)->i_flags & (1 << 4));
}

extern bool path_noexec(const struct path *path);
extern void inode_nohighmem(struct inode *inode);

extern int vfs_fadvise(struct file *file, loff_t offset, loff_t len,
		       int advice);
extern int generic_fadvise(struct file *file, loff_t offset, loff_t len,
			   int advice);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
vfs_empty_path(int dfd, const char *path)
{
	char c;

	if (dfd < 0)
		return false;

	if (!path)
		return true;

	if (__builtin_expect(!!(({
		    might_fault();
		    ({
			    int __ret_gu;
			    register __typeof__(__builtin_choose_expr(
				    sizeof(*(path)) <= sizeof(char),
				    (unsigned char)0,
				    __builtin_choose_expr(
					    sizeof(*(path)) <= sizeof(short),
					    (unsigned short)0,
					    __builtin_choose_expr(
						    sizeof(*(path)) <=
							    sizeof(int),
						    (unsigned int)0,
						    __builtin_choose_expr(
							    sizeof(*(path)) <=
								    sizeof(long),
							    (unsigned long)0,
							    0ULL)))))
				    __val_gu asm("%"
						 "rdx");
			    (void)0;
			    asm volatile(
				    "call __"
				    "get_user"
				    "_%c[size]"
				    : "=a"(__ret_gu), "=r"(__val_gu),
				      "+r"(current_stack_pointer)
				    : "0"(path), [size] "i"(sizeof(*(path))));
			    ({
				    u64 __tmp = (u64)(__val_gu);
				    kmsan_unpoison_memory(&__tmp,
							  sizeof(__tmp));
				    __val_gu = __tmp;
			    });
			    (c) = (__typeof__(*(path)))__val_gu;
			    __builtin_expect(__ret_gu, 0);
		    });
	    })),
			     0))
		return false;

	return !c;
}

bool generic_atomic_write_valid(struct iov_iter *iter, loff_t pos);
typedef __kernel_ulong_t aio_context_t;

enum {
	IOCB_CMD_PREAD = 0,
	IOCB_CMD_PWRITE = 1,
	IOCB_CMD_FSYNC = 2,
	IOCB_CMD_FDSYNC = 3,

	IOCB_CMD_POLL = 5,
	IOCB_CMD_NOOP = 6,
	IOCB_CMD_PREADV = 7,
	IOCB_CMD_PWRITEV = 8,
};
struct io_event {
	__u64 data;
	__u64 obj;
	__s64 res;
	__s64 res2;
};

struct iocb {
	__u64 aio_data;

	__u32 aio_key;
	__kernel_rwf_t aio_rw_flags;
	__u16 aio_lio_opcode;
	__s16 aio_reqprio;
	__u32 aio_fildes;

	__u64 aio_buf;
	__u64 aio_nbytes;
	__s64 aio_offset;

	__u64 aio_reserved2;

	__u32 aio_flags;

	__u32 aio_resfd;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) void *
task_stack_page(const struct task_struct *task)
{
	return task->stack;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long *
end_of_stack(const struct task_struct *task)
{
	return task->stack;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
try_get_task_stack(struct task_struct *tsk)
{
	return refcount_inc_not_zero(&tsk->stack_refcount) ?
		       task_stack_page(tsk) :
		       ((void *)0);
}

extern void put_task_stack(struct task_struct *tsk);
void exit_task_stack_account(struct task_struct *tsk);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
object_is_on_stack(const void *obj)
{
	void *stack = task_stack_page(get_current());

	obj = kasan_reset_tag(obj);
	return (obj >= stack) && (obj < (stack + (((1UL) << 12) << (2 + 0))));
}

extern void thread_stack_cache_init(void);

unsigned long stack_not_used(struct task_struct *p);

extern void set_task_stack_end_magic(struct task_struct *tsk);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kstack_end(void *addr)
{
	return !(((unsigned long)addr + sizeof(void *) - 1) &
		 ((((1UL) << 12) << (2 + 0)) - sizeof(void *)));
}

struct user_i387_ia32_struct {
	u32 cwd;
	u32 swd;
	u32 twd;
	u32 fip;
	u32 fcs;
	u32 foo;
	u32 fos;
	u32 st_space[20];
};

struct user32_fxsr_struct {
	unsigned short cwd;
	unsigned short swd;
	unsigned short twd;
	unsigned short fop;
	int fip;
	int fcs;
	int foo;
	int fos;
	int mxcsr;
	int reserved;
	int st_space[32];
	int xmm_space[32];
	int padding[56];
};

struct user_regs_struct32 {
	__u32 ebx, ecx, edx, esi, edi, ebp, eax;
	unsigned short ds, __ds, es, __es;
	unsigned short fs, __fs, gs, __gs;
	__u32 orig_eax, eip;
	unsigned short cs, __cs;
	__u32 eflags, esp;
	unsigned short ss, __ss;
};

struct user32 {
	struct user_regs_struct32 regs;
	int u_fpvalid;

	struct user_i387_ia32_struct i387;

	__u32 u_tsize;
	__u32 u_dsize;
	__u32 u_ssize;
	__u32 start_code;
	__u32 start_stack;

	__u32 signal;
	int reserved;
	__u32 u_ar0;

	__u32 u_fpstate;
	__u32 magic;
	char u_comm[32];
	int u_debugreg[8];
};

typedef u16 compat_mode_t;

typedef u16 __compat_uid_t;
typedef u16 __compat_gid_t;

typedef u16 compat_dev_t;

typedef u16 compat_ipc_pid_t;

typedef u32 compat_size_t;
typedef s32 compat_ssize_t;
typedef s32 compat_clock_t;
typedef s32 compat_pid_t;
typedef u32 compat_ino_t;
typedef s32 compat_off_t;
typedef s64 compat_loff_t;
typedef s32 compat_daddr_t;
typedef s32 compat_timer_t;
typedef s32 compat_key_t;
typedef s16 compat_short_t;
typedef s32 compat_int_t;
typedef s32 compat_long_t;
typedef u16 compat_ushort_t;
typedef u32 compat_uint_t;
typedef u32 compat_ulong_t;
typedef u32 compat_uptr_t;
typedef u32 compat_caddr_t;
typedef u32 compat_aio_context_t;
typedef u32 compat_old_sigset_t;

typedef u32 __compat_uid32_t;
typedef u32 __compat_gid32_t;

typedef s64 __attribute__((aligned(4))) compat_s64;
typedef u64 __attribute__((aligned(4))) compat_u64;

typedef u32 compat_sigset_word;
typedef __kernel_fsid_t compat_fsid_t;
struct compat_ipc64_perm {
	compat_key_t key;
	__compat_uid32_t uid;
	__compat_gid32_t gid;
	__compat_uid32_t cuid;
	__compat_gid32_t cgid;
	compat_mode_t mode;
	unsigned char __pad1[4 - sizeof(compat_mode_t)];
	compat_ushort_t seq;
	compat_ushort_t __pad2;
	compat_ulong_t unused1;
	compat_ulong_t unused2;
};

struct compat_semid64_ds {
	struct compat_ipc64_perm sem_perm;
	compat_ulong_t sem_otime;
	compat_ulong_t sem_otime_high;
	compat_ulong_t sem_ctime;
	compat_ulong_t sem_ctime_high;
	compat_ulong_t sem_nsems;
	compat_ulong_t __unused3;
	compat_ulong_t __unused4;
};

struct compat_msqid64_ds {
	struct compat_ipc64_perm msg_perm;
	compat_ulong_t msg_stime;
	compat_ulong_t msg_stime_high;
	compat_ulong_t msg_rtime;
	compat_ulong_t msg_rtime_high;
	compat_ulong_t msg_ctime;
	compat_ulong_t msg_ctime_high;
	compat_ulong_t msg_cbytes;
	compat_ulong_t msg_qnum;
	compat_ulong_t msg_qbytes;
	compat_pid_t msg_lspid;
	compat_pid_t msg_lrpid;
	compat_ulong_t __unused4;
	compat_ulong_t __unused5;
};

struct compat_shmid64_ds {
	struct compat_ipc64_perm shm_perm;
	compat_size_t shm_segsz;
	compat_ulong_t shm_atime;
	compat_ulong_t shm_atime_high;
	compat_ulong_t shm_dtime;
	compat_ulong_t shm_dtime_high;
	compat_ulong_t shm_ctime;
	compat_ulong_t shm_ctime_high;
	compat_pid_t shm_cpid;
	compat_pid_t shm_lpid;
	compat_ulong_t shm_nattch;
	compat_ulong_t __unused4;
	compat_ulong_t __unused5;
};

typedef u16 compat_nlink_t;

struct compat_stat {
	u32 st_dev;
	compat_ino_t st_ino;
	compat_mode_t st_mode;
	compat_nlink_t st_nlink;
	__compat_uid_t st_uid;
	__compat_gid_t st_gid;
	u32 st_rdev;
	u32 st_size;
	u32 st_blksize;
	u32 st_blocks;
	u32 st_atime;
	u32 st_atime_nsec;
	u32 st_mtime;
	u32 st_mtime_nsec;
	u32 st_ctime;
	u32 st_ctime_nsec;
	u32 __unused4;
	u32 __unused5;
};

struct compat_statfs {
	int f_type;
	int f_bsize;
	int f_blocks;
	int f_bfree;
	int f_bavail;
	int f_files;
	int f_ffree;
	compat_fsid_t f_fsid;
	int f_namelen;
	int f_frsize;
	int f_flags;
	int f_spare[4];
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
in_x32_syscall(void)
{
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
in_32bit_syscall(void)
{
	return (1 && ((struct thread_info *)get_current())->status & 0x0002) ||
	       in_x32_syscall();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
in_compat_syscall(void)
{
	return in_32bit_syscall();
}

struct compat_siginfo;
extern long __x64_sys_ni_syscall(const struct pt_regs *regs);
extern long __ia32_sys_ni_syscall(const struct pt_regs *regs);
long __x64_sys_getcpu(const struct pt_regs *regs);
long __x64_sys_gettimeofday(const struct pt_regs *regs);
long __x64_sys_time(const struct pt_regs *regs);
struct compat_iovec {
	compat_uptr_t iov_base;
	compat_size_t iov_len;
};

typedef struct compat_sigaltstack {
	compat_uptr_t ss_sp;
	int ss_flags;
	compat_size_t ss_size;
} compat_stack_t;
typedef __compat_uid32_t compat_uid_t;
typedef __compat_gid32_t compat_gid_t;

struct compat_sel_arg_struct;
struct rusage;

struct old_itimerval32;

struct compat_tms {
	compat_clock_t tms_utime;
	compat_clock_t tms_stime;
	compat_clock_t tms_cutime;
	compat_clock_t tms_cstime;
};

typedef struct {
	compat_sigset_word sig[(64 / 32)];
} compat_sigset_t;

int set_compat_user_sigmask(const compat_sigset_t *umask, size_t sigsetsize);

struct compat_sigaction {
	compat_uptr_t sa_handler;
	compat_ulong_t sa_flags;

	compat_uptr_t sa_restorer;

	compat_sigset_t sa_mask __attribute__((__packed__));
};

typedef union compat_sigval {
	compat_int_t sival_int;
	compat_uptr_t sival_ptr;
} compat_sigval_t;

typedef struct compat_siginfo {
	int si_signo;

	int si_errno;
	int si_code;

	union {
		int _pad[128 / sizeof(int) - 3];

		struct {
			compat_pid_t _pid;
			__compat_uid32_t _uid;
		} _kill;

		struct {
			compat_timer_t _tid;
			int _overrun;
			compat_sigval_t _sigval;
		} _timer;

		struct {
			compat_pid_t _pid;
			__compat_uid32_t _uid;
			compat_sigval_t _sigval;
		} _rt;

		struct {
			compat_pid_t _pid;
			__compat_uid32_t _uid;
			int _status;
			compat_clock_t _utime;
			compat_clock_t _stime;
		} _sigchld;
		struct {
			compat_uptr_t _addr;

			union {
				int _trapno;

				short int _addr_lsb;

				struct {
					char _dummy_bnd[(
						__alignof__(compat_uptr_t) <
								sizeof(short) ?
							sizeof(short) :
							__alignof__(
								compat_uptr_t))];
					compat_uptr_t _lower;
					compat_uptr_t _upper;
				} _addr_bnd;

				struct {
					char _dummy_pkey[(
						__alignof__(compat_uptr_t) <
								sizeof(short) ?
							sizeof(short) :
							__alignof__(
								compat_uptr_t))];
					u32 _pkey;
				} _addr_pkey;

				struct {
					compat_ulong_t _data;
					u32 _type;
					u32 _flags;
				} _perf;
			};
		} _sigfault;

		struct {
			compat_long_t _band;
			int _fd;
		} _sigpoll;

		struct {
			compat_uptr_t _call_addr;
			int _syscall;
			unsigned int _arch;
		} _sigsys;
	} _sifields;
} compat_siginfo_t;

struct compat_rlimit {
	compat_ulong_t rlim_cur;
	compat_ulong_t rlim_max;
};

struct compat_flock {
	short l_type;
	short l_whence;
	compat_off_t l_start;
	compat_off_t l_len;

	compat_pid_t l_pid;
};

struct compat_flock64 {
	short l_type;
	short l_whence;
	compat_loff_t l_start;
	compat_loff_t l_len;
	compat_pid_t l_pid;

} __attribute__((packed));

struct compat_rusage {
	struct old_timeval32 ru_utime;
	struct old_timeval32 ru_stime;
	compat_long_t ru_maxrss;
	compat_long_t ru_ixrss;
	compat_long_t ru_idrss;
	compat_long_t ru_isrss;
	compat_long_t ru_minflt;
	compat_long_t ru_majflt;
	compat_long_t ru_nswap;
	compat_long_t ru_inblock;
	compat_long_t ru_oublock;
	compat_long_t ru_msgsnd;
	compat_long_t ru_msgrcv;
	compat_long_t ru_nsignals;
	compat_long_t ru_nvcsw;
	compat_long_t ru_nivcsw;
};

extern int put_compat_rusage(const struct rusage *, struct compat_rusage *);

struct compat_siginfo;
struct __compat_aio_sigset;

struct compat_dirent {
	u32 d_ino;
	compat_off_t d_off;
	u16 d_reclen;
	char d_name[256];
};

struct compat_ustat {
	compat_daddr_t f_tfree;
	compat_ino_t f_tinode;
	char f_fname[6];
	char f_fpack[6];
};

typedef struct compat_sigevent {
	compat_sigval_t sigev_value;
	compat_int_t sigev_signo;
	compat_int_t sigev_notify;
	union {
		compat_int_t _pad[((64 / sizeof(int)) - 3)];
		compat_int_t _tid;

		struct {
			compat_uptr_t _function;
			compat_uptr_t _attribute;
		} _sigev_thread;
	} _sigev_un;
} compat_sigevent_t;

struct compat_ifmap {
	compat_ulong_t mem_start;
	compat_ulong_t mem_end;
	unsigned short base_addr;
	unsigned char irq;
	unsigned char dma;
	unsigned char port;
};

struct compat_if_settings {
	unsigned int type;
	unsigned int size;
	compat_uptr_t ifs_ifsu;
};

struct compat_ifreq {
	union {
		char ifrn_name[16];
	} ifr_ifrn;
	union {
		struct sockaddr ifru_addr;
		struct sockaddr ifru_dstaddr;
		struct sockaddr ifru_broadaddr;
		struct sockaddr ifru_netmask;
		struct sockaddr ifru_hwaddr;
		short ifru_flags;
		compat_int_t ifru_ivalue;
		compat_int_t ifru_mtu;
		struct compat_ifmap ifru_map;
		char ifru_slave[16];
		char ifru_newname[16];
		compat_caddr_t ifru_data;
		struct compat_if_settings ifru_settings;
	} ifr_ifru;
};

struct compat_ifconf {
	compat_int_t ifc_len;
	compat_caddr_t ifcbuf;
};

struct compat_robust_list {
	compat_uptr_t next;
};

struct compat_robust_list_head {
	struct compat_robust_list list;
	compat_long_t futex_offset;
	compat_uptr_t list_op_pending;
};

struct compat_old_sigaction {
	compat_uptr_t sa_handler;
	compat_old_sigset_t sa_mask;
	compat_ulong_t sa_flags;
	compat_uptr_t sa_restorer;
};

struct compat_keyctl_kdf_params {
	compat_uptr_t hashname;
	compat_uptr_t otherinfo;
	__u32 otherinfolen;
	__u32 __spare[8];
};

struct compat_stat;
struct compat_statfs;
struct compat_statfs64;
struct compat_old_linux_dirent;
struct compat_linux_dirent;
struct linux_dirent64;
struct compat_msghdr;
struct compat_mmsghdr;
struct compat_sysinfo;
struct compat_sysctl_args;
struct compat_kexec_segment;
struct compat_mq_attr;
struct compat_msgbuf;

void copy_siginfo_to_external32(struct compat_siginfo *to,
				const struct kernel_siginfo *from);
int copy_siginfo_from_user32(kernel_siginfo_t *to,
			     const struct compat_siginfo *from);
int __copy_siginfo_to_user32(struct compat_siginfo *to,
			     const kernel_siginfo_t *from);

int get_compat_sigevent(struct sigevent *event,
			const struct compat_sigevent *u_event);

extern int get_compat_sigset(sigset_t *set, const compat_sigset_t *compat);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
put_compat_sigset(compat_sigset_t *compat, const sigset_t *set,
		  unsigned int size)
{
	return copy_to_user(compat, set, size) ? -14 : 0;
}
extern int compat_ptrace_request(struct task_struct *child,
				 compat_long_t request, compat_ulong_t addr,
				 compat_ulong_t data);

extern long compat_arch_ptrace(struct task_struct *child, compat_long_t request,
			       compat_ulong_t addr, compat_ulong_t data);

struct epoll_event;

int compat_restore_altstack(const compat_stack_t *uss);
int __compat_save_altstack(compat_stack_t *, unsigned long);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct old_timeval32
ns_to_old_timeval32(s64 nsec)
{
	struct __kernel_old_timeval tv;
	struct old_timeval32 ctv;

	tv = ns_to_kernel_old_timeval(nsec);
	ctv.tv_sec = tv.tv_sec;
	ctv.tv_usec = tv.tv_usec;

	return ctv;
}

int kcompat_sys_statfs64(const char *pathname, compat_size_t sz,
			 struct compat_statfs64 *buf);
int kcompat_sys_fstatfs64(unsigned int fd, compat_size_t sz,
			  struct compat_statfs64 *buf);
long compat_get_bitmap(unsigned long *mask, const compat_ulong_t *umask,
		       unsigned long bitmap_size);
long compat_put_bitmap(compat_ulong_t *umask, unsigned long *mask,
		       unsigned long bitmap_size);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void *
compat_ptr(compat_uptr_t uptr)
{
	return (void *)(unsigned long)uptr;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) compat_uptr_t
ptr_to_compat(void *uptr)
{
	return (u32)(unsigned long)uptr;
}
struct ucontext_ia32 {
	unsigned int uc_flags;
	unsigned int uc_link;
	compat_stack_t uc_stack;
	struct sigcontext_32 uc_mcontext;
	compat_sigset_t uc_sigmask;
};

struct stat64 {
	unsigned long long st_dev;
	unsigned char __pad0[4];

	unsigned int __st_ino;

	unsigned int st_mode;
	unsigned int st_nlink;

	unsigned int st_uid;
	unsigned int st_gid;

	unsigned long long st_rdev;
	unsigned char __pad3[4];

	long long st_size;
	unsigned int st_blksize;

	long long st_blocks;

	unsigned st_atime;
	unsigned st_atime_nsec;
	unsigned st_mtime;
	unsigned st_mtime_nsec;
	unsigned st_ctime;
	unsigned st_ctime_nsec;

	unsigned long long st_ino;
} __attribute__((packed));

extern bool __ia32_enabled;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) bool
ia32_enabled(void)
{
	return __ia32_enabled;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ia32_disable(void)
{
	__ia32_enabled = false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
ia32_enabled_verbose(void)
{
	bool enabled = ia32_enabled();

	if (1 && !enabled)
		({
			bool __ret_do_once = !!(true);
			if (({
				    static bool __attribute__((__section__(
					    ".data.once"))) __already_done;
				    bool __ret_cond = !!(__ret_do_once);
				    bool __ret_once = false;
				    if (__builtin_expect(!!(__ret_cond &&
							    !__already_done),
							 0)) {
					    __already_done = true;
					    __ret_once = true;
				    }
				    __builtin_expect(!!(__ret_once), 0);
			    }))
				({
					do {
					} while (0);
					_printk("\001"
						"5"
						"32-bit emulation disabled. You can reenable with ia32_emulation=on\n");
				});
			__builtin_expect(!!(__ret_do_once), 0);
		});

	return enabled;
}

struct user_i387_struct {
	unsigned short cwd;
	unsigned short swd;
	unsigned short twd;

	unsigned short fop;
	__u64 rip;
	__u64 rdp;
	__u32 mxcsr;
	__u32 mxcsr_mask;
	__u32 st_space[32];
	__u32 xmm_space[64];
	__u32 padding[24];
};

struct user_regs_struct {
	unsigned long r15;
	unsigned long r14;
	unsigned long r13;
	unsigned long r12;
	unsigned long bp;
	unsigned long bx;
	unsigned long r11;
	unsigned long r10;
	unsigned long r9;
	unsigned long r8;
	unsigned long ax;
	unsigned long cx;
	unsigned long dx;
	unsigned long si;
	unsigned long di;
	unsigned long orig_ax;
	unsigned long ip;
	unsigned long cs;
	unsigned long flags;
	unsigned long sp;
	unsigned long ss;
	unsigned long fs_base;
	unsigned long gs_base;
	unsigned long ds;
	unsigned long es;
	unsigned long fs;
	unsigned long gs;
};

struct user {
	struct user_regs_struct regs;

	int u_fpvalid;

	int pad0;
	struct user_i387_struct i387;

	unsigned long int u_tsize;
	unsigned long int u_dsize;
	unsigned long int u_ssize;
	unsigned long start_code;
	unsigned long start_stack;

	long int signal;
	int reserved;
	int pad1;
	unsigned long u_ar0;

	struct user_i387_struct *u_fpstate;
	unsigned long magic;
	char u_comm[32];
	unsigned long u_debugreg[8];
	unsigned long error_code;
	unsigned long fault_address;
};

struct user_ymmh_regs {
	__u32 ymmh_space[64];
};

struct user_xstate_header {
	__u64 xfeatures;
	__u64 reserved1[2];
	__u64 reserved2[5];
};
struct user_xstateregs {
	struct {
		__u64 fpx_space[58];
		__u64 xstate_fx_sw[6];
	} i387;
	struct user_xstate_header header;
	struct user_ymmh_regs ymmh;
};

extern unsigned long x86_fsbase_read_task(struct task_struct *task);
extern unsigned long x86_gsbase_read_task(struct task_struct *task);
extern void x86_fsbase_write_task(struct task_struct *task,
				  unsigned long fsbase);
extern void x86_gsbase_write_task(struct task_struct *task,
				  unsigned long gsbase);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
rdfsbase(void)
{
	unsigned long fsbase;

	asm volatile("rdfsbase %0" : "=r"(fsbase)::"memory");

	return fsbase;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) unsigned long
rdgsbase(void)
{
	unsigned long gsbase;

	asm volatile("rdgsbase %0" : "=r"(gsbase)::"memory");

	return gsbase;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
wrfsbase(unsigned long fsbase)
{
	asm volatile("wrfsbase %0" ::"r"(fsbase) : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
wrgsbase(unsigned long gsbase)
{
	asm volatile("wrgsbase %0" ::"r"(gsbase) : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
x86_fsbase_read_cpu(void)
{
	unsigned long fsbase;

	if ((__builtin_constant_p((9 * 32 + 0)) &&
			     (((((9 * 32 + 0)) >> 5) == (0) &&
			       (1UL << (((9 * 32 + 0)) & 31) &
				((1 << ((0 * 32 + 0) & 31)) |
				 (1 << ((0 * 32 + 3)) & 31) |
				 (1 << ((0 * 32 + 5) & 31)) |
				 (1 << ((0 * 32 + 6) & 31)) |
				 (1 << ((0 * 32 + 8) & 31)) |
				 (1 << ((0 * 32 + 13)) & 31) |
				 (1 << ((0 * 32 + 24) & 31)) |
				 (1 << ((0 * 32 + 15) & 31)) |
				 (1 << ((0 * 32 + 25) & 31)) |
				 (1 << ((0 * 32 + 26) & 31))))) ||
			      ((((9 * 32 + 0)) >> 5) == (1) &&
			       (1UL << (((9 * 32 + 0)) & 31) &
				((1 << ((1 * 32 + 29) & 31)) | 0))) ||
			      ((((9 * 32 + 0)) >> 5) == (2) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (3) &&
			       (1UL << (((9 * 32 + 0)) & 31) &
				((1 << ((3 * 32 + 20) & 31))))) ||
			      ((((9 * 32 + 0)) >> 5) == (4) &&
			       (1UL << (((9 * 32 + 0)) & 31) & (0))) ||
			      ((((9 * 32 + 0)) >> 5) == (5) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (6) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (7) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (8) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (9) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (10) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (11) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (12) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (13) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (14) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (15) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (16) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (17) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (18) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (19) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (20) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (21) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((int)(sizeof(
				      struct { int : (-!!(22 != 22)); }))) ||
			      ((int)(sizeof(
				      struct { int : (-!!(22 != 22)); })))) ?
		     1 :
		     arch_test_bit((9 * 32 + 0),
				   (unsigned long *)((&boot_cpu_data)
							     ->x86_capability))))
		fsbase = rdfsbase();
	else
		((fsbase) = native_read_msr((0xc0000100)));

	return fsbase;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
x86_fsbase_write_cpu(unsigned long fsbase)
{
	if ((__builtin_constant_p((9 * 32 + 0)) &&
			     (((((9 * 32 + 0)) >> 5) == (0) &&
			       (1UL << (((9 * 32 + 0)) & 31) &
				((1 << ((0 * 32 + 0) & 31)) |
				 (1 << ((0 * 32 + 3)) & 31) |
				 (1 << ((0 * 32 + 5) & 31)) |
				 (1 << ((0 * 32 + 6) & 31)) |
				 (1 << ((0 * 32 + 8) & 31)) |
				 (1 << ((0 * 32 + 13)) & 31) |
				 (1 << ((0 * 32 + 24) & 31)) |
				 (1 << ((0 * 32 + 15) & 31)) |
				 (1 << ((0 * 32 + 25) & 31)) |
				 (1 << ((0 * 32 + 26) & 31))))) ||
			      ((((9 * 32 + 0)) >> 5) == (1) &&
			       (1UL << (((9 * 32 + 0)) & 31) &
				((1 << ((1 * 32 + 29) & 31)) | 0))) ||
			      ((((9 * 32 + 0)) >> 5) == (2) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (3) &&
			       (1UL << (((9 * 32 + 0)) & 31) &
				((1 << ((3 * 32 + 20) & 31))))) ||
			      ((((9 * 32 + 0)) >> 5) == (4) &&
			       (1UL << (((9 * 32 + 0)) & 31) & (0))) ||
			      ((((9 * 32 + 0)) >> 5) == (5) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (6) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (7) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (8) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (9) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (10) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (11) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (12) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (13) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (14) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (15) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (16) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (17) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (18) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (19) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (20) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((((9 * 32 + 0)) >> 5) == (21) &&
			       (1UL << (((9 * 32 + 0)) & 31) & 0)) ||
			      ((int)(sizeof(
				      struct { int : (-!!(22 != 22)); }))) ||
			      ((int)(sizeof(
				      struct { int : (-!!(22 != 22)); })))) ?
		     1 :
		     arch_test_bit((9 * 32 + 0),
				   (unsigned long *)((&boot_cpu_data)
							     ->x86_capability))))
		wrfsbase(fsbase);
	else
		wrmsrl(0xc0000100, fsbase);
}

extern unsigned long x86_gsbase_read_cpu_inactive(void);
extern void x86_gsbase_write_cpu_inactive(unsigned long gsbase);
extern unsigned long x86_fsgsbase_read_task(struct task_struct *task,
					    unsigned short selector);

typedef unsigned long elf_greg_t;

typedef elf_greg_t
	elf_gregset_t[(sizeof(struct user_regs_struct) / sizeof(elf_greg_t))];

typedef struct user_i387_struct elf_fpregset_t;
struct vdso_image {
	void *data;
	unsigned long size;

	unsigned long alt, alt_len;
	unsigned long extable_base, extable_len;
	const void *extable;

	long sym_vvar_start;

	long sym_vvar_page;
	long sym_pvclock_page;
	long sym_hvclock_page;
	long sym_timens_page;
	long sym_VDSO32_NOTE_MASK;
	long sym___kernel_sigreturn;
	long sym___kernel_rt_sigreturn;
	long sym___kernel_vsyscall;
	long sym_int80_landing_pad;
	long sym_vdso32_sigreturn_landing_pad;
	long sym_vdso32_rt_sigreturn_landing_pad;
};

extern const struct vdso_image vdso_image_64;

extern const struct vdso_image vdso_image_32;

extern int __attribute__((__section__(".init.text"))) __attribute__((__cold__))
init_vdso_image(const struct vdso_image *image);

extern int map_vdso_once(const struct vdso_image *image, unsigned long addr);

extern bool fixup_vdso_exception(struct pt_regs *regs, int trapnr,
				 unsigned long error_code,
				 unsigned long fault_addr);

extern unsigned int vdso64_enabled;

extern unsigned int vdso32_enabled;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
elf_common_init(struct thread_struct *t, struct pt_regs *regs, const u16 ds)
{
	regs->bx = regs->cx = regs->dx = 0;
	regs->si = regs->di = regs->bp = 0;
	regs->r8 = regs->r9 = regs->r10 = regs->r11 = 0;
	regs->r12 = regs->r13 = regs->r14 = regs->r15 = 0;
	t->fsbase = t->gsbase = 0;
	t->fsindex = t->gsindex = 0;
	t->ds = t->es = ds;
}

void compat_start_thread(struct pt_regs *regs, u32 new_ip, u32 new_sp,
			 bool x32);

void set_personality_ia32(bool);
extern void set_personality_64bit(void);
extern int force_personality32;
extern u32 elf_hwcap2;
struct task_struct;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
mmap_is_ia32(void)
{
	return 0 ||
	       (1 &&
		test_ti_thread_flag(((struct thread_info *)get_current()), 29));
}

extern unsigned long task_size_32bit(void);
extern unsigned long task_size_64bit(int full_addr_space);
extern unsigned long get_mmap_base(int is_legacy);
extern bool mmap_address_hint_valid(unsigned long addr, unsigned long len);
extern unsigned long get_sigframe_size(void);
struct linux_binprm;

extern int arch_setup_additional_pages(struct linux_binprm *bprm,
				       int uses_interp);
extern int compat_arch_setup_additional_pages(struct linux_binprm *bprm,
					      int uses_interp, bool x32);

extern bool arch_syscall_is_vdso_sigreturn(struct pt_regs *regs);

enum align_flags {
	ALIGN_VA_32 = ((((1UL))) << (0)),
	ALIGN_VA_64 = ((((1UL))) << (1)),
};

struct va_alignment {
	int flags;
	unsigned long mask;
	unsigned long bits;
} __attribute__((__aligned__((1 << (6)))));

extern struct va_alignment va_align;

typedef __u32 Elf32_Addr;
typedef __u16 Elf32_Half;
typedef __u32 Elf32_Off;
typedef __s32 Elf32_Sword;
typedef __u32 Elf32_Word;

typedef __u64 Elf64_Addr;
typedef __u16 Elf64_Half;
typedef __s16 Elf64_SHalf;
typedef __u64 Elf64_Off;
typedef __s32 Elf64_Sword;
typedef __u32 Elf64_Word;
typedef __u64 Elf64_Xword;
typedef __s64 Elf64_Sxword;
typedef struct {
	Elf32_Sword d_tag;
	union {
		Elf32_Sword d_val;
		Elf32_Addr d_ptr;
	} d_un;
} Elf32_Dyn;

typedef struct {
	Elf64_Sxword d_tag;
	union {
		Elf64_Xword d_val;
		Elf64_Addr d_ptr;
	} d_un;
} Elf64_Dyn;
typedef struct elf32_rel {
	Elf32_Addr r_offset;
	Elf32_Word r_info;
} Elf32_Rel;

typedef struct elf64_rel {
	Elf64_Addr r_offset;
	Elf64_Xword r_info;
} Elf64_Rel;

typedef struct elf32_rela {
	Elf32_Addr r_offset;
	Elf32_Word r_info;
	Elf32_Sword r_addend;
} Elf32_Rela;

typedef struct elf64_rela {
	Elf64_Addr r_offset;
	Elf64_Xword r_info;
	Elf64_Sxword r_addend;
} Elf64_Rela;

typedef struct elf32_sym {
	Elf32_Word st_name;
	Elf32_Addr st_value;
	Elf32_Word st_size;
	unsigned char st_info;
	unsigned char st_other;
	Elf32_Half st_shndx;
} Elf32_Sym;

typedef struct elf64_sym {
	Elf64_Word st_name;
	unsigned char st_info;
	unsigned char st_other;
	Elf64_Half st_shndx;
	Elf64_Addr st_value;
	Elf64_Xword st_size;
} Elf64_Sym;

typedef struct elf32_hdr {
	unsigned char e_ident[16];
	Elf32_Half e_type;
	Elf32_Half e_machine;
	Elf32_Word e_version;
	Elf32_Addr e_entry;
	Elf32_Off e_phoff;
	Elf32_Off e_shoff;
	Elf32_Word e_flags;
	Elf32_Half e_ehsize;
	Elf32_Half e_phentsize;
	Elf32_Half e_phnum;
	Elf32_Half e_shentsize;
	Elf32_Half e_shnum;
	Elf32_Half e_shstrndx;
} Elf32_Ehdr;

typedef struct elf64_hdr {
	unsigned char e_ident[16];
	Elf64_Half e_type;
	Elf64_Half e_machine;
	Elf64_Word e_version;
	Elf64_Addr e_entry;
	Elf64_Off e_phoff;
	Elf64_Off e_shoff;
	Elf64_Word e_flags;
	Elf64_Half e_ehsize;
	Elf64_Half e_phentsize;
	Elf64_Half e_phnum;
	Elf64_Half e_shentsize;
	Elf64_Half e_shnum;
	Elf64_Half e_shstrndx;
} Elf64_Ehdr;

typedef struct elf32_phdr {
	Elf32_Word p_type;
	Elf32_Off p_offset;
	Elf32_Addr p_vaddr;
	Elf32_Addr p_paddr;
	Elf32_Word p_filesz;
	Elf32_Word p_memsz;
	Elf32_Word p_flags;
	Elf32_Word p_align;
} Elf32_Phdr;

typedef struct elf64_phdr {
	Elf64_Word p_type;
	Elf64_Word p_flags;
	Elf64_Off p_offset;
	Elf64_Addr p_vaddr;
	Elf64_Addr p_paddr;
	Elf64_Xword p_filesz;
	Elf64_Xword p_memsz;
	Elf64_Xword p_align;
} Elf64_Phdr;
typedef struct elf32_shdr {
	Elf32_Word sh_name;
	Elf32_Word sh_type;
	Elf32_Word sh_flags;
	Elf32_Addr sh_addr;
	Elf32_Off sh_offset;
	Elf32_Word sh_size;
	Elf32_Word sh_link;
	Elf32_Word sh_info;
	Elf32_Word sh_addralign;
	Elf32_Word sh_entsize;
} Elf32_Shdr;

typedef struct elf64_shdr {
	Elf64_Word sh_name;
	Elf64_Word sh_type;
	Elf64_Xword sh_flags;
	Elf64_Addr sh_addr;
	Elf64_Off sh_offset;
	Elf64_Xword sh_size;
	Elf64_Word sh_link;
	Elf64_Word sh_info;
	Elf64_Xword sh_addralign;
	Elf64_Xword sh_entsize;
} Elf64_Shdr;
typedef struct elf32_note {
	Elf32_Word n_namesz;
	Elf32_Word n_descsz;
	Elf32_Word n_type;
} Elf32_Nhdr;

typedef struct elf64_note {
	Elf64_Word n_namesz;
	Elf64_Word n_descsz;
	Elf64_Word n_type;
} Elf64_Nhdr;
extern Elf64_Dyn _DYNAMIC[];
struct file;
struct coredump_params;

extern int elf_coredump_extra_notes_size(void);
extern int elf_coredump_extra_notes_write(struct coredump_params *cprm);

struct gnu_property {
	u32 pr_type;
	u32 pr_datasz;
};

struct arch_elf_state;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
arch_parse_elf_property(u32 type, const void *data, size_t datasz, bool compat,
			struct arch_elf_state *arch)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
arch_elf_adjust_prot(int prot, const struct arch_elf_state *state,
		     bool has_interp, bool is_interp)
{
	return prot;
}

struct idr {
	struct xarray idr_rt;
	unsigned int idr_base;
	unsigned int idr_next;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned int
idr_get_cursor(const struct idr *idr)
{
	return ({
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_573(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(idr->idr_next) == sizeof(char) ||
			       sizeof(idr->idr_next) == sizeof(short) ||
			       sizeof(idr->idr_next) == sizeof(int) ||
			       sizeof(idr->idr_next) == sizeof(long)) ||
			      sizeof(idr->idr_next) == sizeof(long long)))
				__compiletime_assert_573();
		} while (0);
		(*(const volatile typeof(_Generic((idr->idr_next),
						 char: (char)0,
						 unsigned char: (unsigned char)0,
						 signed char: (signed char)0,
						 unsigned short: (
							  unsigned short)0,
						 signed short: (signed short)0,
						 unsigned int: (unsigned int)0,
						 signed int: (signed int)0,
						 unsigned long: (unsigned long)0,
						 signed long: (signed long)0,
						 unsigned long long: (
							  unsigned long long)0,
						 signed long long: (
							  signed long long)0,
						 default: (idr->idr_next)))
			   *)&(idr->idr_next));
	});
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
idr_set_cursor(struct idr *idr, unsigned int val)
{
	do {
		do {
			__attribute__((__noreturn__)) extern void
			__compiletime_assert_574(void) __attribute__((__error__(
				"Unsupported access size for {READ,WRITE}_ONCE().")));
			if (!((sizeof(idr->idr_next) == sizeof(char) ||
			       sizeof(idr->idr_next) == sizeof(short) ||
			       sizeof(idr->idr_next) == sizeof(int) ||
			       sizeof(idr->idr_next) == sizeof(long)) ||
			      sizeof(idr->idr_next) == sizeof(long long)))
				__compiletime_assert_574();
		} while (0);
		do {
			*(volatile typeof(idr->idr_next) *)&(idr->idr_next) =
				(val);
		} while (0);
	} while (0);
}
void idr_preload(gfp_t gfp_mask);

int idr_alloc(struct idr *, void *ptr, int start, int end, gfp_t);
int __attribute__((__warn_unused_result__))
idr_alloc_u32(struct idr *, void *ptr, u32 *id, unsigned long max, gfp_t);
int idr_alloc_cyclic(struct idr *, void *ptr, int start, int end, gfp_t);
void *idr_remove(struct idr *, unsigned long id);
void *idr_find(const struct idr *, unsigned long id);
int idr_for_each(const struct idr *, int (*fn)(int id, void *p, void *data),
		 void *data);
void *idr_get_next(struct idr *, int *nextid);
void *idr_get_next_ul(struct idr *, unsigned long *nextid);
void *idr_replace(struct idr *, void *, unsigned long id);
void idr_destroy(struct idr *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
idr_init_base(struct idr *idr, int base)
{
	xa_init_flags(&idr->idr_rt,
		      (((gfp_t)4) | (gfp_t)(1 << ((___GFP_LAST_BIT) + 0))));
	idr->idr_base = base;
	idr->idr_next = 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
idr_init(struct idr *idr)
{
	idr_init_base(idr, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
idr_is_empty(const struct idr *idr)
{
	return radix_tree_empty(&idr->idr_rt) &&
	       radix_tree_tagged(&idr->idr_rt, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
idr_preload_end(void)
{
	do {
		local_lock_release(({
			do {
				const void *__vpp_verify =
					(typeof((&radix_tree_preloads.lock) +
						0))((void *)0);
				(void)__vpp_verify;
			} while (0);
			({
				unsigned long tcp_ptr__ = ({
					u64 pfo_val__;
					asm("mov"
					    "q "
					    "%%"
					    "gs"
					    ":"
					    "%"
					    "[var]"
					    ", "
					    "%[val]"
					    : [val] "="
						    "r"(pfo_val__)
					    : [var] "m"((
						    *(typeof(*(&(this_cpu_off)))
							      *)(uintptr_t)(&(
							    this_cpu_off)))));
					(typeof(this_cpu_off))(unsigned long)
						pfo_val__;
				});
				tcp_ptr__ +=
					(unsigned long)(&radix_tree_preloads
								 .lock);
				(typeof(*(
					&radix_tree_preloads.lock)) *)tcp_ptr__;
			});
		}));
		do {
			__asm__ __volatile__("" : : : "memory");
			if (__builtin_expect(!!(__preempt_count_dec_and_test()),
					     0))
				do {
					static void *__attribute__((__used__))
					__attribute__((__section__(
						".discard.addressable")))
					__UNIQUE_ID___addressable___SCK__preempt_schedule575 =
						(void *)(uintptr_t)&__SCK__preempt_schedule;
					;
					asm volatile(
						"call "
						"__SCT__preempt_schedule"
						: "+r"(current_stack_pointer));
				} while (0);
		} while (0);
	} while (0);
}
struct ida_bitmap {
	unsigned long bitmap[(128 / sizeof(long))];
};

struct ida {
	struct xarray xa;
};
int ida_alloc_range(struct ida *, unsigned int min, unsigned int max, gfp_t);
void ida_free(struct ida *, unsigned int id);
void ida_destroy(struct ida *ida);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
ida_alloc(struct ida *ida, gfp_t gfp)
{
	return ida_alloc_range(ida, 0, ~0, gfp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
ida_alloc_min(struct ida *ida, unsigned int min, gfp_t gfp)
{
	return ida_alloc_range(ida, min, ~0, gfp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
ida_alloc_max(struct ida *ida, unsigned int max, gfp_t gfp)
{
	return ida_alloc_range(ida, 0, max, gfp);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
ida_init(struct ida *ida)
{
	xa_init_flags(&ida->xa, (((gfp_t)XA_LOCK_IRQ) |
				 (((gfp_t)4U) |
				  ((gfp_t)((1U << ___GFP_LAST_BIT)
					   << (unsigned)(((xa_mark_t)0U)))))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
ida_is_empty(const struct ida *ida)
{
	return xa_empty(&ida->xa);
}
struct file;
struct dentry;
struct iattr;
struct seq_file;
struct vm_area_struct;
struct vm_operations_struct;
struct super_block;
struct file_system_type;
struct poll_table_struct;
struct fs_context;

struct kernfs_fs_context;
struct kernfs_open_node;
struct kernfs_iattrs;
struct kernfs_global_locks {
	struct mutex open_file_mutex[(
		1 << (2 * ((__builtin_constant_p(64 < 32 ? 64 : 32) ?
				    ((64 < 32 ? 64 : 32) < 2 ?
					     0 :
					     63 - __builtin_clzll(
							  64 < 32 ? 64 : 32)) :
			    (sizeof(64 < 32 ? 64 : 32) <= 4) ?
				    __ilog2_u32(64 < 32 ? 64 : 32) :
				    __ilog2_u64(64 < 32 ? 64 : 32)))))];
};

enum kernfs_node_type {
	KERNFS_DIR = 0x0001,
	KERNFS_FILE = 0x0002,
	KERNFS_LINK = 0x0004,
};

enum kernfs_node_flag {
	KERNFS_ACTIVATED = 0x0010,
	KERNFS_NS = 0x0020,
	KERNFS_HAS_SEQ_SHOW = 0x0040,
	KERNFS_HAS_MMAP = 0x0080,
	KERNFS_LOCKDEP = 0x0100,
	KERNFS_HIDDEN = 0x0200,
	KERNFS_SUICIDAL = 0x0400,
	KERNFS_SUICIDED = 0x0800,
	KERNFS_EMPTY_DIR = 0x1000,
	KERNFS_HAS_RELEASE = 0x2000,
	KERNFS_REMOVING = 0x4000,
};

enum kernfs_root_flag {

	KERNFS_ROOT_CREATE_DEACTIVATED = 0x0001,
	KERNFS_ROOT_EXTRA_OPEN_PERM_CHECK = 0x0002,

	KERNFS_ROOT_SUPPORT_EXPORTOP = 0x0004,

	KERNFS_ROOT_SUPPORT_USER_XATTR = 0x0008,
};

struct kernfs_elem_dir {
	unsigned long subdirs;

	struct rb_root children;

	struct kernfs_root *root;

	unsigned long rev;
};

struct kernfs_elem_symlink {
	struct kernfs_node *target_kn;
};

struct kernfs_elem_attr {
	const struct kernfs_ops *ops;
	struct kernfs_open_node *open;
	loff_t size;
	struct kernfs_node *notify_next;
};
struct kernfs_node {
	atomic_t count;
	atomic_t active;
	struct kernfs_node *parent;
	const char *name;

	struct rb_node rb;

	const void *ns;
	unsigned int hash;
	unsigned short flags;
	umode_t mode;

	union {
		struct kernfs_elem_dir dir;
		struct kernfs_elem_symlink symlink;
		struct kernfs_elem_attr attr;
	};

	u64 id;

	void *priv;
	struct kernfs_iattrs *iattr;

	struct callback_head rcu;
};
struct kernfs_syscall_ops {
	int (*show_options)(struct seq_file *sf, struct kernfs_root *root);

	int (*mkdir)(struct kernfs_node *parent, const char *name,
		     umode_t mode);
	int (*rmdir)(struct kernfs_node *kn);
	int (*rename)(struct kernfs_node *kn, struct kernfs_node *new_parent,
		      const char *new_name);
	int (*show_path)(struct seq_file *sf, struct kernfs_node *kn,
			 struct kernfs_root *root);
};

struct kernfs_node *kernfs_root_to_node(struct kernfs_root *root);

struct kernfs_open_file {
	struct kernfs_node *kn;
	struct file *file;
	struct seq_file *seq_file;
	void *priv;

	struct mutex mutex;
	struct mutex prealloc_mutex;
	int event;
	struct list_head list;
	char *prealloc_buf;

	size_t atomic_write_len;
	bool mmapped : 1;
	bool released : 1;
	const struct vm_operations_struct *vm_ops;
};

struct kernfs_ops {
	int (*open)(struct kernfs_open_file *of);
	void (*release)(struct kernfs_open_file *of);
	int (*seq_show)(struct seq_file *sf, void *v);

	void *(*seq_start)(struct seq_file *sf, loff_t *ppos);
	void *(*seq_next)(struct seq_file *sf, void *v, loff_t *ppos);
	void (*seq_stop)(struct seq_file *sf, void *v);

	ssize_t (*read)(struct kernfs_open_file *of, char *buf, size_t bytes,
			loff_t off);
	size_t atomic_write_len;

	bool prealloc;
	ssize_t (*write)(struct kernfs_open_file *of, char *buf, size_t bytes,
			 loff_t off);

	__poll_t (*poll)(struct kernfs_open_file *of,
			 struct poll_table_struct *pt);

	int (*mmap)(struct kernfs_open_file *of, struct vm_area_struct *vma);
	loff_t (*llseek)(struct kernfs_open_file *of, loff_t offset,
			 int whence);
};

struct kernfs_fs_context {
	struct kernfs_root *root;
	void *ns_tag;
	unsigned long magic;

	bool new_sb_created;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) enum kernfs_node_type
kernfs_type(struct kernfs_node *kn)
{
	return kn->flags & 0x000f;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ino_t
kernfs_id_ino(u64 id)
{
	if (sizeof(ino_t) >= sizeof(u64))
		return id;
	else
		return (u32)id;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
kernfs_id_gen(u64 id)
{
	if (sizeof(ino_t) >= sizeof(u64))
		return 1;
	else
		return id >> 32;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ino_t
kernfs_ino(struct kernfs_node *kn)
{
	return kernfs_id_ino(kn->id);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) ino_t
kernfs_gen(struct kernfs_node *kn)
{
	return kernfs_id_gen(kn->id);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kernfs_enable_ns(struct kernfs_node *kn)
{
	({
		int __ret_warn_on = !!(kernfs_type(kn) != KERNFS_DIR);
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) |
						      ((1 << 1) | ((9) << 8));
				({
					asm volatile(
						"576"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"576"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(576));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/kernfs.h"),
						  "i"(381), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"577"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"577"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(577));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	({
		int __ret_warn_on = !!(!(
			({
				do {
					__attribute__((
						__noreturn__)) extern void
					__compiletime_assert_578(void)
						__attribute__((__error__(
							"Unsupported access size for {READ,WRITE}_ONCE().")));
					if (!((sizeof((&kn->dir.children)
							      ->rb_node) ==
						       sizeof(char) ||
					       sizeof((&kn->dir.children)
							      ->rb_node) ==
						       sizeof(short) ||
					       sizeof((&kn->dir.children)
							      ->rb_node) ==
						       sizeof(int) ||
					       sizeof((&kn->dir.children)
							      ->rb_node) ==
						       sizeof(long)) ||
					      sizeof((&kn->dir.children)
							     ->rb_node) ==
						      sizeof(long long)))
						__compiletime_assert_578();
				} while (0);
				(*(const volatile typeof(_Generic(
					((&kn->dir.children)->rb_node),
								 char: (char)0,
								 unsigned char: (
									 unsigned char)0,
								 signed char: (
									 signed char)0,
								 unsigned short: (
									 unsigned short)0,
								 signed short: (
									 signed short)0,
								 unsigned int: (
									 unsigned int)0,
								 signed int: (
									 signed int)0,
								 unsigned long: (
									 unsigned long)0,
								 signed long: (
									 signed long)0,
								 unsigned long long: (
									 unsigned long long)0,
								 signed long long: (
									 signed long long)0,
								 default: (
									 (&kn->dir.children)
										 ->rb_node)))
					   *)&((&kn->dir.children)->rb_node));
			}) == ((void *)0)));
		if (__builtin_expect(!!(__ret_warn_on), 0))
			do {
				__auto_type __flags = (1 << 0) |
						      ((1 << 1) | ((9) << 8));
				({
					asm volatile(
						"579"
						": nop\n\t"
						".pushsection .discard.instr_begin\n\t"
						".long "
						"579"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(579));
				});
				do {
					asm __inline volatile(
						"1:\t"
						".byte 0x0f, 0x0b"
						"\n"
						".pushsection __bug_table,\"aw\"\n"
						"2:\t"
						".long "
						"1b"
						" - ."
						"\t# bug_entry::bug_addr\n"
						"\t"
						".long "
						"%c0"
						" - ."
						"\t# bug_entry::file\n"
						"\t.word %c1"
						"\t# bug_entry::line\n"
						"\t.word %c2"
						"\t# bug_entry::flags\n"
						"\t.org 2b+%c3\n"
						".popsection\n"
						"998:\n\t"
						".pushsection .discard.reachable\n\t"
						".long 998b\n\t"
						".popsection\n\t"
						:
						: "i"("include/linux/kernfs.h"),
						  "i"(382), "i"(__flags),
						  "i"(sizeof(struct bug_entry)));
				} while (0);
				({
					asm volatile(
						"580"
						": nop\n\t"
						".pushsection .discard.instr_end\n\t"
						".long "
						"580"
						"b - .\n\t"
						".popsection\n\t"
						:
						: "i"(580));
				});
			} while (0);
		__builtin_expect(!!(__ret_warn_on), 0);
	});
	kn->flags |= KERNFS_NS;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
kernfs_ns_enabled(struct kernfs_node *kn)
{
	return kn->flags & KERNFS_NS;
}

int kernfs_name(struct kernfs_node *kn, char *buf, size_t buflen);
int kernfs_path_from_node(struct kernfs_node *root_kn, struct kernfs_node *kn,
			  char *buf, size_t buflen);
void pr_cont_kernfs_name(struct kernfs_node *kn);
void pr_cont_kernfs_path(struct kernfs_node *kn);
struct kernfs_node *kernfs_get_parent(struct kernfs_node *kn);
struct kernfs_node *kernfs_find_and_get_ns(struct kernfs_node *parent,
					   const char *name, const void *ns);
struct kernfs_node *kernfs_walk_and_get_ns(struct kernfs_node *parent,
					   const char *path, const void *ns);
void kernfs_get(struct kernfs_node *kn);
void kernfs_put(struct kernfs_node *kn);

struct kernfs_node *kernfs_node_from_dentry(struct dentry *dentry);
struct kernfs_root *kernfs_root_from_sb(struct super_block *sb);
struct inode *kernfs_get_inode(struct super_block *sb, struct kernfs_node *kn);

struct dentry *kernfs_node_dentry(struct kernfs_node *kn,
				  struct super_block *sb);
struct kernfs_root *kernfs_create_root(struct kernfs_syscall_ops *scops,
				       unsigned int flags, void *priv);
void kernfs_destroy_root(struct kernfs_root *root);

struct kernfs_node *kernfs_create_dir_ns(struct kernfs_node *parent,
					 const char *name, umode_t mode,
					 kuid_t uid, kgid_t gid, void *priv,
					 const void *ns);
struct kernfs_node *kernfs_create_empty_dir(struct kernfs_node *parent,
					    const char *name);
struct kernfs_node *__kernfs_create_file(struct kernfs_node *parent,
					 const char *name, umode_t mode,
					 kuid_t uid, kgid_t gid, loff_t size,
					 const struct kernfs_ops *ops,
					 void *priv, const void *ns,
					 struct lock_class_key *key);
struct kernfs_node *kernfs_create_link(struct kernfs_node *parent,
				       const char *name,
				       struct kernfs_node *target);
void kernfs_activate(struct kernfs_node *kn);
void kernfs_show(struct kernfs_node *kn, bool show);
void kernfs_remove(struct kernfs_node *kn);
void kernfs_break_active_protection(struct kernfs_node *kn);
void kernfs_unbreak_active_protection(struct kernfs_node *kn);
bool kernfs_remove_self(struct kernfs_node *kn);
int kernfs_remove_by_name_ns(struct kernfs_node *parent, const char *name,
			     const void *ns);
int kernfs_rename_ns(struct kernfs_node *kn, struct kernfs_node *new_parent,
		     const char *new_name, const void *new_ns);
int kernfs_setattr(struct kernfs_node *kn, const struct iattr *iattr);
__poll_t kernfs_generic_poll(struct kernfs_open_file *of,
			     struct poll_table_struct *pt);
void kernfs_notify(struct kernfs_node *kn);

int kernfs_xattr_get(struct kernfs_node *kn, const char *name, void *value,
		     size_t size);
int kernfs_xattr_set(struct kernfs_node *kn, const char *name,
		     const void *value, size_t size, int flags);

const void *kernfs_super_ns(struct super_block *sb);
int kernfs_get_tree(struct fs_context *fc);
void kernfs_free_fs_context(struct fs_context *fc);
void kernfs_kill_sb(struct super_block *sb);

void kernfs_init(void);

struct kernfs_node *kernfs_find_and_get_node_by_id(struct kernfs_root *root,
						   u64 id);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kernfs_path(struct kernfs_node *kn, char *buf, size_t buflen)
{
	return kernfs_path_from_node(kn, ((void *)0), buf, buflen);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kernfs_node *
kernfs_find_and_get(struct kernfs_node *kn, const char *name)
{
	return kernfs_find_and_get_ns(kn, name, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kernfs_node *
kernfs_walk_and_get(struct kernfs_node *kn, const char *path)
{
	return kernfs_walk_and_get_ns(kn, path, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kernfs_node *
kernfs_create_dir(struct kernfs_node *parent, const char *name, umode_t mode,
		  void *priv)
{
	return kernfs_create_dir_ns(parent, name, mode, (kuid_t){ 0 },
				    (kgid_t){ 0 }, priv, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kernfs_remove_by_name(struct kernfs_node *parent, const char *name)
{
	return kernfs_remove_by_name_ns(parent, name, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
kernfs_rename(struct kernfs_node *kn, struct kernfs_node *new_parent,
	      const char *new_name)
{
	return kernfs_rename_ns(kn, new_parent, new_name, ((void *)0));
}

struct sock;
struct kobject;

enum kobj_ns_type { KOBJ_NS_TYPE_NONE = 0, KOBJ_NS_TYPE_NET, KOBJ_NS_TYPES };
struct kobj_ns_type_operations {
	enum kobj_ns_type type;
	bool (*current_may_mount)(void);
	void *(*grab_current_ns)(void);
	const void *(*netlink_ns)(struct sock *sk);
	const void *(*initial_ns)(void);
	void (*drop_ns)(void *);
};

int kobj_ns_type_register(const struct kobj_ns_type_operations *ops);
int kobj_ns_type_registered(enum kobj_ns_type type);
const struct kobj_ns_type_operations *
kobj_child_ns_ops(const struct kobject *parent);
const struct kobj_ns_type_operations *kobj_ns_ops(const struct kobject *kobj);

bool kobj_ns_current_may_mount(enum kobj_ns_type type);
void *kobj_ns_grab_current(enum kobj_ns_type type);
const void *kobj_ns_netlink(enum kobj_ns_type type, struct sock *sk);
const void *kobj_ns_initial(enum kobj_ns_type type);
void kobj_ns_drop(enum kobj_ns_type type, void *ns);

struct kobject;
struct module;
struct bin_attribute;
enum kobj_ns_type;

struct attribute {
	const char *name;
	umode_t mode;
};
struct attribute_group {
	const char *name;
	umode_t (*is_visible)(struct kobject *, struct attribute *, int);
	umode_t (*is_bin_visible)(struct kobject *, struct bin_attribute *,
				  int);
	struct attribute **attrs;
	struct bin_attribute **bin_attrs;
};
struct file;
struct vm_area_struct;
struct address_space;

struct bin_attribute {
	struct attribute attr;
	size_t size;
	void *private;
	struct address_space *(*f_mapping)(void);
	ssize_t (*read)(struct file *, struct kobject *, struct bin_attribute *,
			char *, loff_t, size_t);
	ssize_t (*write)(struct file *, struct kobject *,
			 struct bin_attribute *, char *, loff_t, size_t);
	loff_t (*llseek)(struct file *, struct kobject *,
			 struct bin_attribute *, loff_t, int);
	int (*mmap)(struct file *, struct kobject *, struct bin_attribute *attr,
		    struct vm_area_struct *vma);
};
struct sysfs_ops {
	ssize_t (*show)(struct kobject *, struct attribute *, char *);
	ssize_t (*store)(struct kobject *, struct attribute *, const char *,
			 size_t);
};

int __attribute__((__warn_unused_result__))
sysfs_create_dir_ns(struct kobject *kobj, const void *ns);
void sysfs_remove_dir(struct kobject *kobj);
int __attribute__((__warn_unused_result__))
sysfs_rename_dir_ns(struct kobject *kobj, const char *new_name,
		    const void *new_ns);
int __attribute__((__warn_unused_result__))
sysfs_move_dir_ns(struct kobject *kobj, struct kobject *new_parent_kobj,
		  const void *new_ns);
int __attribute__((__warn_unused_result__))
sysfs_create_mount_point(struct kobject *parent_kobj, const char *name);
void sysfs_remove_mount_point(struct kobject *parent_kobj, const char *name);

int __attribute__((__warn_unused_result__))
sysfs_create_file_ns(struct kobject *kobj, const struct attribute *attr,
		     const void *ns);
int __attribute__((__warn_unused_result__))
sysfs_create_files(struct kobject *kobj, const struct attribute *const *attr);
int __attribute__((__warn_unused_result__))
sysfs_chmod_file(struct kobject *kobj, const struct attribute *attr,
		 umode_t mode);
struct kernfs_node *sysfs_break_active_protection(struct kobject *kobj,
						  const struct attribute *attr);
void sysfs_unbreak_active_protection(struct kernfs_node *kn);
void sysfs_remove_file_ns(struct kobject *kobj, const struct attribute *attr,
			  const void *ns);
bool sysfs_remove_file_self(struct kobject *kobj, const struct attribute *attr);
void sysfs_remove_files(struct kobject *kobj,
			const struct attribute *const *attr);

int __attribute__((__warn_unused_result__))
sysfs_create_bin_file(struct kobject *kobj, const struct bin_attribute *attr);
void sysfs_remove_bin_file(struct kobject *kobj,
			   const struct bin_attribute *attr);

int __attribute__((__warn_unused_result__))
sysfs_create_link(struct kobject *kobj, struct kobject *target,
		  const char *name);
int __attribute__((__warn_unused_result__))
sysfs_create_link_nowarn(struct kobject *kobj, struct kobject *target,
			 const char *name);
void sysfs_remove_link(struct kobject *kobj, const char *name);

int sysfs_rename_link_ns(struct kobject *kobj, struct kobject *target,
			 const char *old_name, const char *new_name,
			 const void *new_ns);

void sysfs_delete_link(struct kobject *dir, struct kobject *targ,
		       const char *name);

int __attribute__((__warn_unused_result__))
sysfs_create_group(struct kobject *kobj, const struct attribute_group *grp);
int __attribute__((__warn_unused_result__))
sysfs_create_groups(struct kobject *kobj,
		    const struct attribute_group **groups);
int __attribute__((__warn_unused_result__))
sysfs_update_groups(struct kobject *kobj,
		    const struct attribute_group **groups);
int sysfs_update_group(struct kobject *kobj, const struct attribute_group *grp);
void sysfs_remove_group(struct kobject *kobj,
			const struct attribute_group *grp);
void sysfs_remove_groups(struct kobject *kobj,
			 const struct attribute_group **groups);
int sysfs_add_file_to_group(struct kobject *kobj, const struct attribute *attr,
			    const char *group);
void sysfs_remove_file_from_group(struct kobject *kobj,
				  const struct attribute *attr,
				  const char *group);
int sysfs_merge_group(struct kobject *kobj, const struct attribute_group *grp);
void sysfs_unmerge_group(struct kobject *kobj,
			 const struct attribute_group *grp);
int sysfs_add_link_to_group(struct kobject *kobj, const char *group_name,
			    struct kobject *target, const char *link_name);
void sysfs_remove_link_from_group(struct kobject *kobj, const char *group_name,
				  const char *link_name);
int compat_only_sysfs_link_entry_to_kobj(struct kobject *kobj,
					 struct kobject *target_kobj,
					 const char *target_name,
					 const char *symlink_name);

void sysfs_notify(struct kobject *kobj, const char *dir, const char *attr);

int __attribute__((__warn_unused_result__)) sysfs_init(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sysfs_enable_ns(struct kernfs_node *kn)
{
	return kernfs_enable_ns(kn);
}

int sysfs_file_change_owner(struct kobject *kobj, const char *name, kuid_t kuid,
			    kgid_t kgid);
int sysfs_change_owner(struct kobject *kobj, kuid_t kuid, kgid_t kgid);
int sysfs_link_change_owner(struct kobject *kobj, struct kobject *targ,
			    const char *name, kuid_t kuid, kgid_t kgid);
int sysfs_groups_change_owner(struct kobject *kobj,
			      const struct attribute_group **groups,
			      kuid_t kuid, kgid_t kgid);
int sysfs_group_change_owner(struct kobject *kobj,
			     const struct attribute_group *groups, kuid_t kuid,
			     kgid_t kgid);
__attribute__((__format__(printf, 2, 3))) int sysfs_emit(char *buf,
							 const char *fmt, ...);
__attribute__((__format__(printf, 3, 4))) int
sysfs_emit_at(char *buf, int at, const char *fmt, ...);

ssize_t sysfs_bin_attr_simple_read(struct file *file, struct kobject *kobj,
				   struct bin_attribute *attr, char *buf,
				   loff_t off, size_t count);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
	__attribute__((__warn_unused_result__))
	sysfs_create_file(struct kobject *kobj, const struct attribute *attr)
{
	return sysfs_create_file_ns(kobj, attr, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sysfs_remove_file(struct kobject *kobj, const struct attribute *attr)
{
	sysfs_remove_file_ns(kobj, attr, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
sysfs_rename_link(struct kobject *kobj, struct kobject *target,
		  const char *old_name, const char *new_name)
{
	return sysfs_rename_link_ns(kobj, target, old_name, new_name,
				    ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sysfs_notify_dirent(struct kernfs_node *kn)
{
	kernfs_notify(kn);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kernfs_node *
sysfs_get_dirent(struct kernfs_node *parent, const char *name)
{
	return kernfs_find_and_get(parent, name);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kernfs_node *
sysfs_get(struct kernfs_node *kn)
{
	kernfs_get(kn);
	return kn;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
sysfs_put(struct kernfs_node *kn)
{
	kernfs_put(kn);
}
extern atomic64_t uevent_seqnum;
enum kobject_action {
	KOBJ_ADD,
	KOBJ_REMOVE,
	KOBJ_CHANGE,
	KOBJ_MOVE,
	KOBJ_ONLINE,
	KOBJ_OFFLINE,
	KOBJ_BIND,
	KOBJ_UNBIND,
};

struct kobject {
	const char *name;
	struct list_head entry;
	struct kobject *parent;
	struct kset *kset;
	const struct kobj_type *ktype;
	struct kernfs_node *sd;
	struct kref kref;

	unsigned int state_initialized : 1;
	unsigned int state_in_sysfs : 1;
	unsigned int state_add_uevent_sent : 1;
	unsigned int state_remove_uevent_sent : 1;
	unsigned int uevent_suppress : 1;
};

__attribute__((__format__(printf, 2, 3))) int
kobject_set_name(struct kobject *kobj, const char *name, ...);
__attribute__((__format__(printf, 2, 0))) int
kobject_set_name_vargs(struct kobject *kobj, const char *fmt, va_list vargs);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const char *
kobject_name(const struct kobject *kobj)
{
	return kobj->name;
}

void kobject_init(struct kobject *kobj, const struct kobj_type *ktype);
__attribute__((__format__(printf, 3, 4)))
__attribute__((__warn_unused_result__)) int
kobject_add(struct kobject *kobj, struct kobject *parent, const char *fmt, ...);
__attribute__((__format__(printf, 4, 5)))
__attribute__((__warn_unused_result__)) int
kobject_init_and_add(struct kobject *kobj, const struct kobj_type *ktype,
		     struct kobject *parent, const char *fmt, ...);

void kobject_del(struct kobject *kobj);

struct kobject *__attribute__((__warn_unused_result__))
kobject_create_and_add(const char *name, struct kobject *parent);

int __attribute__((__warn_unused_result__))
kobject_rename(struct kobject *, const char *new_name);
int __attribute__((__warn_unused_result__)) kobject_move(struct kobject *,
							 struct kobject *);

struct kobject *kobject_get(struct kobject *kobj);
struct kobject *__attribute__((__warn_unused_result__))
kobject_get_unless_zero(struct kobject *kobj);
void kobject_put(struct kobject *kobj);

const void *kobject_namespace(const struct kobject *kobj);
void kobject_get_ownership(const struct kobject *kobj, kuid_t *uid,
			   kgid_t *gid);
char *kobject_get_path(const struct kobject *kobj, gfp_t flag);

struct kobj_type {
	void (*release)(struct kobject *kobj);
	const struct sysfs_ops *sysfs_ops;
	const struct attribute_group **default_groups;
	const struct kobj_ns_type_operations *(*child_ns_type)(
		const struct kobject *kobj);
	const void *(*namespace)(const struct kobject *kobj);
	void (*get_ownership)(const struct kobject *kobj, kuid_t *uid,
			      kgid_t *gid);
};

struct kobj_uevent_env {
	char *argv[3];
	char *envp[64];
	int envp_idx;
	char buf[2048];
	int buflen;
};

struct kset_uevent_ops {
	int (*const filter)(const struct kobject *kobj);
	const char *(*const name)(const struct kobject *kobj);
	int (*const uevent)(const struct kobject *kobj,
			    struct kobj_uevent_env *env);
};

struct kobj_attribute {
	struct attribute attr;
	ssize_t (*show)(struct kobject *kobj, struct kobj_attribute *attr,
			char *buf);
	ssize_t (*store)(struct kobject *kobj, struct kobj_attribute *attr,
			 const char *buf, size_t count);
};

extern const struct sysfs_ops kobj_sysfs_ops;

struct sock;
struct kset {
	struct list_head list;
	spinlock_t list_lock;
	struct kobject kobj;
	const struct kset_uevent_ops *uevent_ops;
};

void kset_init(struct kset *kset);
int __attribute__((__warn_unused_result__)) kset_register(struct kset *kset);
void kset_unregister(struct kset *kset);
struct kset *__attribute__((__warn_unused_result__))
kset_create_and_add(const char *name, const struct kset_uevent_ops *u,
		    struct kobject *parent_kobj);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kset *
to_kset(struct kobject *kobj)
{
	return kobj ? ({
		void *__mptr = (void *)(kobj);
		_Static_assert(__builtin_types_compatible_p(
				       typeof(*(kobj)),
				       typeof(((struct kset *)0)->kobj)) ||
				       __builtin_types_compatible_p(
					       typeof(*(kobj)), typeof(void)),
			       "pointer type mismatch in container_of()");
		((struct kset *)(__mptr -
				 __builtin_offsetof(struct kset, kobj)));
	}) :
		      ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) struct kset *
kset_get(struct kset *k)
{
	return k ? to_kset(kobject_get(&k->kobj)) : ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
kset_put(struct kset *k)
{
	kobject_put(&k->kobj);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) const struct kobj_type *
get_ktype(const struct kobject *kobj)
{
	return kobj->ktype;
}

struct kobject *kset_find_obj(struct kset *, const char *);

extern struct kobject *kernel_kobj;

extern struct kobject *mm_kobj;

extern struct kobject *hypervisor_kobj;

extern struct kobject *power_kobj;

extern struct kobject *firmware_kobj;

int kobject_uevent(struct kobject *kobj, enum kobject_action action);
int kobject_uevent_env(struct kobject *kobj, enum kobject_action action,
		       char *envp[]);
int kobject_synth_uevent(struct kobject *kobj, const char *buf, size_t count);

__attribute__((__format__(printf, 2, 3))) int
add_uevent_var(struct kobj_uevent_env *env, const char *format, ...);
struct kernel_param;

enum { KERNEL_PARAM_OPS_FL_NOARG = (1 << 0) };

struct kernel_param_ops {
	unsigned int flags;

	int (*set)(const char *val, const struct kernel_param *kp);

	int (*get)(char *buffer, const struct kernel_param *kp);

	void (*free)(void *arg);
};

enum {
	KERNEL_PARAM_FL_UNSAFE = (1 << 0),
	KERNEL_PARAM_FL_HWPARAM = (1 << 1),
};

struct kernel_param {
	const char *name;
	struct module *mod;
	const struct kernel_param_ops *ops;
	const u16 perm;
	s8 level;
	u8 flags;
	union {
		void *arg;
		const struct kparam_string *str;
		const struct kparam_array *arr;
	};
};

extern const struct kernel_param __start___param[], __stop___param[];

struct kparam_string {
	unsigned int maxlen;
	char *string;
};

struct kparam_array {
	unsigned int max;
	unsigned int elemsize;
	unsigned int *num;
	const struct kernel_param_ops *ops;
	void *elem;
};
extern void kernel_param_lock(struct module *mod);
extern void kernel_param_unlock(struct module *mod);
extern bool parameq(const char *name1, const char *name2);
extern bool parameqn(const char *name1, const char *name2, size_t n);

typedef int (*parse_unknown_fn)(char *param, char *val, const char *doing,
				void *arg);

extern char *parse_args(const char *name, char *args,
			const struct kernel_param *params, unsigned num,
			s16 level_min, s16 level_max, void *arg,
			parse_unknown_fn unknown);

extern void destroy_params(const struct kernel_param *params, unsigned num);
extern const struct kernel_param_ops param_ops_byte;
extern int param_set_byte(const char *val, const struct kernel_param *kp);
extern int param_get_byte(char *buffer, const struct kernel_param *kp);

extern const struct kernel_param_ops param_ops_short;
extern int param_set_short(const char *val, const struct kernel_param *kp);
extern int param_get_short(char *buffer, const struct kernel_param *kp);

extern const struct kernel_param_ops param_ops_ushort;
extern int param_set_ushort(const char *val, const struct kernel_param *kp);
extern int param_get_ushort(char *buffer, const struct kernel_param *kp);

extern const struct kernel_param_ops param_ops_int;
extern int param_set_int(const char *val, const struct kernel_param *kp);
extern int param_get_int(char *buffer, const struct kernel_param *kp);

extern const struct kernel_param_ops param_ops_uint;
extern int param_set_uint(const char *val, const struct kernel_param *kp);
extern int param_get_uint(char *buffer, const struct kernel_param *kp);
int param_set_uint_minmax(const char *val, const struct kernel_param *kp,
			  unsigned int min, unsigned int max);

extern const struct kernel_param_ops param_ops_long;
extern int param_set_long(const char *val, const struct kernel_param *kp);
extern int param_get_long(char *buffer, const struct kernel_param *kp);

extern const struct kernel_param_ops param_ops_ulong;
extern int param_set_ulong(const char *val, const struct kernel_param *kp);
extern int param_get_ulong(char *buffer, const struct kernel_param *kp);

extern const struct kernel_param_ops param_ops_ullong;
extern int param_set_ullong(const char *val, const struct kernel_param *kp);
extern int param_get_ullong(char *buffer, const struct kernel_param *kp);

extern const struct kernel_param_ops param_ops_hexint;
extern int param_set_hexint(const char *val, const struct kernel_param *kp);
extern int param_get_hexint(char *buffer, const struct kernel_param *kp);

extern const struct kernel_param_ops param_ops_charp;
extern int param_set_charp(const char *val, const struct kernel_param *kp);
extern int param_get_charp(char *buffer, const struct kernel_param *kp);
extern void param_free_charp(void *arg);

extern const struct kernel_param_ops param_ops_bool;
extern int param_set_bool(const char *val, const struct kernel_param *kp);
extern int param_get_bool(char *buffer, const struct kernel_param *kp);

extern const struct kernel_param_ops param_ops_bool_enable_only;
extern int param_set_bool_enable_only(const char *val,
				      const struct kernel_param *kp);

extern const struct kernel_param_ops param_ops_invbool;
extern int param_set_invbool(const char *val, const struct kernel_param *kp);
extern int param_get_invbool(char *buffer, const struct kernel_param *kp);

extern const struct kernel_param_ops param_ops_bint;
extern int param_set_bint(const char *val, const struct kernel_param *kp);
enum hwparam_type {
	hwparam_ioport,
	hwparam_iomem,
	hwparam_ioport_or_iomem,
	hwparam_irq,
	hwparam_dma,
	hwparam_dma_addr,
	hwparam_other,
};
extern const struct kernel_param_ops param_array_ops;

extern const struct kernel_param_ops param_ops_string;
extern int param_set_copystring(const char *val, const struct kernel_param *);
extern int param_get_string(char *buffer, const struct kernel_param *kp);

struct module;

extern int module_param_sysfs_setup(struct module *mod,
				    const struct kernel_param *kparam,
				    unsigned int num_params);

extern void module_param_sysfs_remove(struct module *mod);

struct latch_tree_node {
	struct rb_node node[2];
};

struct latch_tree_root {
	seqcount_latch_t seq;
	struct rb_root tree[2];
};
struct latch_tree_ops {
	bool (*less)(struct latch_tree_node *a, struct latch_tree_node *b);
	int (*comp)(void *key, struct latch_tree_node *b);
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct latch_tree_node *
__lt_from_rb(struct rb_node *node, int idx)
{
	return ({
		void *__mptr = (void *)(node);
		_Static_assert(__builtin_types_compatible_p(
				       typeof(*(node)),
				       typeof(((struct latch_tree_node *)0)
						      ->node[idx])) ||
				       __builtin_types_compatible_p(
					       typeof(*(node)), typeof(void)),
			       "pointer type mismatch in container_of()");
		((struct latch_tree_node *)(__mptr -
					    __builtin_offsetof(
						    struct latch_tree_node,
						    node[idx])));
	});
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__lt_insert(struct latch_tree_node *ltn, struct latch_tree_root *ltr, int idx,
	    bool (*less)(struct latch_tree_node *a, struct latch_tree_node *b))
{
	struct rb_root *root = &ltr->tree[idx];
	struct rb_node **link = &root->rb_node;
	struct rb_node *node = &ltn->node[idx];
	struct rb_node *parent = ((void *)0);
	struct latch_tree_node *ltp;

	while (*link) {
		parent = *link;
		ltp = __lt_from_rb(parent, idx);

		if (less(ltn, ltp))
			link = &parent->rb_left;
		else
			link = &parent->rb_right;
	}

	rb_link_node_rcu(node, parent, link);
	rb_insert_color(node, root);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
__lt_erase(struct latch_tree_node *ltn, struct latch_tree_root *ltr, int idx)
{
	rb_erase(&ltn->node[idx], &ltr->tree[idx]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct latch_tree_node *
__lt_find(void *key, struct latch_tree_root *ltr, int idx,
	  int (*comp)(void *key, struct latch_tree_node *node))
{
	struct rb_node *node = ({
		typeof(ltr->tree[idx].rb_node) __UNIQUE_ID_rcu581 = ({
			do {
				__attribute__((__noreturn__)) extern void
				__compiletime_assert_582(void) __attribute__((__error__(
					"Unsupported access size for {READ,WRITE}_ONCE().")));
				if (!((sizeof(ltr->tree[idx].rb_node) ==
					       sizeof(char) ||
				       sizeof(ltr->tree[idx].rb_node) ==
					       sizeof(short) ||
				       sizeof(ltr->tree[idx].rb_node) ==
					       sizeof(int) ||
				       sizeof(ltr->tree[idx].rb_node) ==
					       sizeof(long)) ||
				      sizeof(ltr->tree[idx].rb_node) ==
					      sizeof(long long)))
					__compiletime_assert_582();
			} while (0);
			(*(const volatile typeof(_Generic(
				(ltr->tree[idx].rb_node),
							 char: (char)0,
							 unsigned char: (
								 unsigned char)0,
							 signed char: (
								 signed char)0,
							 unsigned short: (
								 unsigned short)0,
							 signed short: (
								 signed short)0,
							 unsigned int: (
								 unsigned int)0,
							 signed int: (
								 signed int)0,
							 unsigned long: (
								 unsigned long)0,
							 signed long: (
								 signed long)0,
							 unsigned long long: (
								 unsigned long long)0,
							 signed long long: (
								 signed long long)0,
							 default: (
								 ltr->tree[idx]
									 .rb_node)))
				   *)&(ltr->tree[idx].rb_node));
		});
		((typeof(*ltr->tree[idx].rb_node) *)(__UNIQUE_ID_rcu581));
	});
	struct latch_tree_node *ltn;
	int c;

	while (node) {
		ltn = __lt_from_rb(node, idx);
		c = comp(key, ltn);

		if (c < 0)
			node = ({
				typeof(node->rb_left) __UNIQUE_ID_rcu583 = ({
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_584(void)
							__attribute__((__error__(
								"Unsupported access size for {READ,WRITE}_ONCE().")));
						if (!((sizeof(node->rb_left) ==
							       sizeof(char) ||
						       sizeof(node->rb_left) ==
							       sizeof(short) ||
						       sizeof(node->rb_left) ==
							       sizeof(int) ||
						       sizeof(node->rb_left) ==
							       sizeof(long)) ||
						      sizeof(node->rb_left) ==
							      sizeof(long long)))
							__compiletime_assert_584();
					} while (0);
					(*(const volatile typeof(_Generic(
						(node->rb_left),
									 char: (char)0,
									 unsigned char: (
										 unsigned char)0,
									 signed char: (
										 signed char)0,
									 unsigned short: (
										 unsigned short)0,
									 signed short: (
										 signed short)0,
									 unsigned int: (
										 unsigned int)0,
									 signed int: (
										 signed int)0,
									 unsigned long: (
										 unsigned long)0,
									 signed long: (
										 signed long)0,
									 unsigned long long: (
										 unsigned long long)0,
									 signed long long: (
										 signed long long)0,
									 default: (
										 node->rb_left)))
						   *)&(node->rb_left));
				});
				((typeof(*node->rb_left) *)(__UNIQUE_ID_rcu583));
			});
		else if (c > 0)
			node = ({
				typeof(node->rb_right) __UNIQUE_ID_rcu585 = ({
					do {
						__attribute__((
							__noreturn__)) extern void
						__compiletime_assert_586(void)
							__attribute__((__error__(
								"Unsupported access size for {READ,WRITE}_ONCE().")));
						if (!((sizeof(node->rb_right) ==
							       sizeof(char) ||
						       sizeof(node->rb_right) ==
							       sizeof(short) ||
						       sizeof(node->rb_right) ==
							       sizeof(int) ||
						       sizeof(node->rb_right) ==
							       sizeof(long)) ||
						      sizeof(node->rb_right) ==
							      sizeof(long long)))
							__compiletime_assert_586();
					} while (0);
					(*(const volatile typeof(_Generic(
						(node->rb_right),
									 char: (char)0,
									 unsigned char: (
										 unsigned char)0,
									 signed char: (
										 signed char)0,
									 unsigned short: (
										 unsigned short)0,
									 signed short: (
										 signed short)0,
									 unsigned int: (
										 unsigned int)0,
									 signed int: (
										 signed int)0,
									 unsigned long: (
										 unsigned long)0,
									 signed long: (
										 signed long)0,
									 unsigned long long: (
										 unsigned long long)0,
									 signed long long: (
										 signed long long)0,
									 default: (
										 node->rb_right)))
						   *)&(node->rb_right));
				});
				((typeof(*node->rb_right)
					  *)(__UNIQUE_ID_rcu585));
			});
		else
			return ltn;
	}

	return ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
latch_tree_insert(struct latch_tree_node *node, struct latch_tree_root *root,
		  const struct latch_tree_ops *ops)
{
	raw_write_seqcount_latch(&root->seq);
	__lt_insert(node, root, 0, ops->less);
	raw_write_seqcount_latch(&root->seq);
	__lt_insert(node, root, 1, ops->less);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) __attribute__((__always_inline__)) void
latch_tree_erase(struct latch_tree_node *node, struct latch_tree_root *root,
		 const struct latch_tree_ops *ops)
{
	raw_write_seqcount_latch(&root->seq);
	__lt_erase(node, root, 0);
	raw_write_seqcount_latch(&root->seq);
	__lt_erase(node, root, 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function))
__attribute__((__always_inline__)) struct latch_tree_node *
latch_tree_find(void *key, struct latch_tree_root *root,
		const struct latch_tree_ops *ops)
{
	struct latch_tree_node *node;
	unsigned int seq;

	do {
		seq = raw_read_seqcount_latch(&root->seq);
		node = __lt_find(key, root, seq & 1, ops->comp);
	} while (raw_read_seqcount_latch_retry(&root->seq, seq));

	return node;
}

enum {
	EI_ETYPE_NULL,
	EI_ETYPE_ERRNO,
	EI_ETYPE_ERRNO_NULL,
	EI_ETYPE_TRUE,
};

struct error_injection_entry {
	unsigned long addr;
	int etype;
};

struct pt_regs;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
override_function_with_return(struct pt_regs *regs)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
within_error_injection_list(unsigned long addr)
{
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
get_injectable_error_type(unsigned long addr)
{
	return -95;
}

struct _ddebug {
	const char *modname;
	const char *function;
	const char *filename;
	const char *format;
	unsigned int lineno : 18;

	unsigned int class_id : 6;
	unsigned int flags : 8;

	union {
		struct static_key_true dd_key_true;
		struct static_key_false dd_key_false;
	} key;

} __attribute__((aligned(8)));

enum class_map_type {
	DD_CLASS_TYPE_DISJOINT_BITS,

	DD_CLASS_TYPE_LEVEL_NUM,

	DD_CLASS_TYPE_DISJOINT_NAMES,

	DD_CLASS_TYPE_LEVEL_NAMES,

};

struct ddebug_class_map {
	struct list_head link;
	struct module *mod;
	const char *mod_name;
	const char **class_names;
	const int length;
	const int base;
	enum class_map_type map_type;
};
struct _ddebug_info {
	struct _ddebug *descs;
	struct ddebug_class_map *classes;
	unsigned int num_descs;
	unsigned int num_classes;
};

struct ddebug_class_param {
	union {
		unsigned long *bits;
		unsigned int *lvl;
	};
	char flags[8];
	const struct ddebug_class_map *map;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
ddebug_dyndbg_module_param_cb(char *param, char *val, const char *modname)
{
	if (!strcmp(param, "dyndbg")) {
		({
			do {
			} while (0);
			_printk("\001"
				"4"
				"dyndbg param is supported only in "
				"CONFIG_DYNAMIC_DEBUG builds\n");
		});

		return 0;
	}
	return -22;
}

struct kernel_param;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
param_set_dyndbg_classes(const char *instr, const struct kernel_param *kp)
{
	return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) int
param_get_dyndbg_classes(char *buffer, const struct kernel_param *kp)
{
	return 0;
}

extern const struct kernel_param_ops param_ops_dyndbg_classes;

struct mod_arch_specific {
	unsigned int num_orcs;
	int *orc_unwind_ip;
	struct orc_entry *orc_unwind;
};

struct modversion_info {
	unsigned long crc;
	char name[(64 - sizeof(unsigned long))];
};

struct module;
struct exception_table_entry;

struct module_kobject {
	struct kobject kobj;
	struct module *mod;
	struct kobject *drivers_dir;
	struct module_param_attrs *mp;
	struct completion *kobj_completion;
};

struct module_attribute {
	struct attribute attr;
	ssize_t (*show)(struct module_attribute *, struct module_kobject *,
			char *);
	ssize_t (*store)(struct module_attribute *, struct module_kobject *,
			 const char *, size_t count);
	void (*setup)(struct module *, const char *);
	int (*test)(struct module *);
	void (*free)(struct module *);
};

struct module_version_attribute {
	struct module_attribute mattr;
	const char *module_name;
	const char *version;
};

extern ssize_t __modver_version_show(struct module_attribute *,
				     struct module_kobject *, char *);

extern struct module_attribute module_uevent;

extern int init_module(void);
extern void cleanup_module(void);
struct notifier_block;

extern int modules_disabled;

void *__symbol_get(const char *symbol);
void *__symbol_get_gpl(const char *symbol);

struct module_use {
	struct list_head source_list;
	struct list_head target_list;
	struct module *source, *target;
};

enum module_state {
	MODULE_STATE_LIVE,
	MODULE_STATE_COMING,
	MODULE_STATE_GOING,
	MODULE_STATE_UNFORMED,
};

struct mod_tree_node {
	struct module *mod;
	struct latch_tree_node node;
};

enum mod_mem_type {
	MOD_TEXT = 0,
	MOD_DATA,
	MOD_RODATA,
	MOD_RO_AFTER_INIT,
	MOD_INIT_TEXT,
	MOD_INIT_DATA,
	MOD_INIT_RODATA,

	MOD_MEM_NUM_TYPES,
	MOD_INVALID = -1,
};
struct module_memory {
	void *base;
	unsigned int size;

	struct mod_tree_node mtn;
};
struct mod_kallsyms {
	Elf64_Sym *symtab;
	unsigned int num_symtab;
	char *strtab;
	char *typetab;
};
struct module {
	enum module_state state;

	struct list_head list;

	char name[(64 - sizeof(unsigned long))];

	struct module_kobject mkobj;
	struct module_attribute *modinfo_attrs;
	const char *version;
	const char *srcversion;
	struct kobject *holders_dir;

	const struct kernel_symbol *syms;
	const s32 *crcs;
	unsigned int num_syms;
	struct mutex param_lock;

	struct kernel_param *kp;
	unsigned int num_kp;

	unsigned int num_gpl_syms;
	const struct kernel_symbol *gpl_syms;
	const s32 *gpl_crcs;
	bool using_gplonly_symbols;

	bool async_probe_requested;

	unsigned int num_exentries;
	struct exception_table_entry *extable;

	int (*init)(void);

	struct module_memory mem[MOD_MEM_NUM_TYPES]
		__attribute__((__aligned__((1 << (6)))));

	struct mod_arch_specific arch;

	unsigned long taints;

	unsigned num_bugs;
	struct list_head bug_list;
	struct bug_entry *bug_table;

	struct mod_kallsyms *kallsyms;
	struct mod_kallsyms core_kallsyms;

	struct module_sect_attrs *sect_attrs;

	struct module_notes_attrs *notes_attrs;

	char *args;

	void *percpu;
	unsigned int percpu_size;

	void *noinstr_text_start;
	unsigned int noinstr_text_size;

	unsigned int num_tracepoints;
	tracepoint_ptr_t *tracepoints_ptrs;

	unsigned int num_srcu_structs;
	struct srcu_struct **srcu_struct_ptrs;
	struct jump_entry *jump_entries;
	unsigned int num_jump_entries;

	unsigned int num_trace_bprintk_fmt;
	const char **trace_bprintk_fmt_start;

	struct trace_event_call **trace_events;
	unsigned int num_trace_events;
	struct trace_eval_map **trace_evals;
	unsigned int num_trace_evals;

	void *kprobes_text_start;
	unsigned int kprobes_text_size;
	unsigned long *kprobe_blacklist;
	unsigned int num_kprobe_blacklist;

	int num_static_call_sites;
	struct static_call_site *static_call_sites;
	struct list_head source_list;

	struct list_head target_list;

	void (*exit)(void);

	atomic_t refcnt;
} __attribute__((__aligned__((1 << (6)))));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) unsigned long
kallsyms_symbol_value(const Elf64_Sym *sym)
{
	return sym->st_value;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
module_is_live(struct module *mod)
{
	return mod->state != MODULE_STATE_GOING;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
module_is_coming(struct module *mod)
{
	return mod->state == MODULE_STATE_COMING;
}

struct module *__module_text_address(unsigned long addr);
struct module *__module_address(unsigned long addr);
bool is_module_address(unsigned long addr);
bool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr);
bool is_module_percpu_address(unsigned long addr);
bool is_module_text_address(unsigned long addr);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
within_module_mem_type(unsigned long addr, const struct module *mod,
		       enum mod_mem_type type)
{
	unsigned long base, size;

	base = (unsigned long)mod->mem[type].base;
	size = mod->mem[type].size;
	return addr - base < size;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
within_module_core(unsigned long addr, const struct module *mod)
{
	for (enum mod_mem_type(type) = 0; (type) < MOD_MEM_NUM_TYPES; (type)++)
		if ((!((type) == MOD_INIT_TEXT || (type) == MOD_INIT_DATA ||
		       (type) == MOD_INIT_RODATA))) {
			if (within_module_mem_type(addr, mod, type))
				return true;
		}
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
within_module_init(unsigned long addr, const struct module *mod)
{
	for (enum mod_mem_type(type) = 0; (type) < MOD_MEM_NUM_TYPES; (type)++)
		if (((type) == MOD_INIT_TEXT || (type) == MOD_INIT_DATA ||
		     (type) == MOD_INIT_RODATA)) {
			if (within_module_mem_type(addr, mod, type))
				return true;
		}
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
within_module(unsigned long addr, const struct module *mod)
{
	return within_module_init(addr, mod) || within_module_core(addr, mod);
}

struct module *find_module(const char *name);

extern void __attribute__((__noreturn__))
__module_put_and_kthread_exit(struct module *mod, long code);

int module_refcount(struct module *mod);
void __symbol_put(const char *symbol);

void symbol_put_addr(void *addr);

extern void __module_get(struct module *module);
extern bool try_module_get(struct module *module);
extern void module_put(struct module *module);
void *dereference_module_function_descriptor(struct module *mod, void *ptr);

int register_module_notifier(struct notifier_block *nb);
int unregister_module_notifier(struct notifier_block *nb);

extern void print_modules(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
module_requested_async_probing(struct module *module)
{
	return module && module->async_probe_requested;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_livepatch_module(struct module *mod)
{
	return false;
}

void set_module_sig_enforced(void);
extern struct kset *module_kset;
extern const struct kobj_type module_ktype;
void module_bug_finalize(const Elf64_Ehdr *, const Elf64_Shdr *,
			 struct module *);
void module_bug_cleanup(struct module *);
extern bool retpoline_module_ok(bool has_retpoline);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
is_module_sig_enforced(void)
{
	return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) bool
module_sig_ok(struct module *module)
{
	return true;
}

int module_kallsyms_on_each_symbol(const char *modname,
				   int (*fn)(void *, const char *,
					     unsigned long),
				   void *data);

int module_address_lookup(unsigned long addr, unsigned long *symbolsize,
			  unsigned long *offset, char **modname,
			  const unsigned char **modbuildid, char *namebuf);
int lookup_module_symbol_name(unsigned long addr, char *symname);
int lookup_module_symbol_attrs(unsigned long addr, unsigned long *size,
			       unsigned long *offset, char *modname,
			       char *name);

int module_get_kallsym(unsigned int symnum, unsigned long *value, char *type,
		       char *name, char *module_name, int *exported);

unsigned long module_kallsyms_lookup_name(const char *name);

unsigned long find_kallsyms_symbol_value(struct module *mod, const char *name);

static const u32 SHA256_K[] = {
	0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5, 0x3956c25b, 0x59f111f1,
	0x923f82a4, 0xab1c5ed5, 0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,
	0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174, 0xe49b69c1, 0xefbe4786,
	0x0fc19dc6, 0x240ca1cc, 0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,
	0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7, 0xc6e00bf3, 0xd5a79147,
	0x06ca6351, 0x14292967, 0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,
	0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85, 0xa2bfe8a1, 0xa81a664b,
	0xc24b8b70, 0xc76c51a3, 0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,
	0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5, 0x391c0cb3, 0x4ed8aa4a,
	0x5b9cca4f, 0x682e6ff3, 0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,
	0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
Ch(u32 x, u32 y, u32 z)
{
	return z ^ (x & (y ^ z));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) u32
Maj(u32 x, u32 y, u32 z)
{
	return (x & y) | (z & (x | y));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
LOAD_OP(int I, u32 *W, const u8 *input)
{
	W[I] = get_unaligned_be32((__u32 *)input + I);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__))
__attribute__((no_instrument_function)) void
BLEND_OP(int I, u32 *W)
{
	W[I] = (ror32(W[I - 2], 17) ^ ror32(W[I - 2], 19) ^ (W[I - 2] >> 10)) +
	       W[I - 7] +
	       (ror32(W[I - 15], 7) ^ ror32(W[I - 15], 18) ^ (W[I - 15] >> 3)) +
	       W[I - 16];
}
static void sha256_transform(u32 *state, const u8 *input, u32 *W)
{
	u32 a, b, c, d, e, f, g, h;
	int i;

	for (i = 0; i < 16; i += 8) {
		LOAD_OP(i + 0, W, input);
		LOAD_OP(i + 1, W, input);
		LOAD_OP(i + 2, W, input);
		LOAD_OP(i + 3, W, input);
		LOAD_OP(i + 4, W, input);
		LOAD_OP(i + 5, W, input);
		LOAD_OP(i + 6, W, input);
		LOAD_OP(i + 7, W, input);
	}

	for (i = 16; i < 64; i += 8) {
		BLEND_OP(i + 0, W);
		BLEND_OP(i + 1, W);
		BLEND_OP(i + 2, W);
		BLEND_OP(i + 3, W);
		BLEND_OP(i + 4, W);
		BLEND_OP(i + 5, W);
		BLEND_OP(i + 6, W);
		BLEND_OP(i + 7, W);
	}

	a = state[0];
	b = state[1];
	c = state[2];
	d = state[3];
	e = state[4];
	f = state[5];
	g = state[6];
	h = state[7];

	for (i = 0; i < 64; i += 8) {
		do {
			u32 t1, t2;
			t1 = h + (ror32(e, 6) ^ ror32(e, 11) ^ ror32(e, 25)) +
			     Ch(e, f, g) + SHA256_K[i + 0] + W[i + 0];
			t2 = (ror32(a, 2) ^ ror32(a, 13) ^ ror32(a, 22)) +
			     Maj(a, b, c);
			d += t1;
			h = t1 + t2;
		} while (0);
		do {
			u32 t1, t2;
			t1 = g + (ror32(d, 6) ^ ror32(d, 11) ^ ror32(d, 25)) +
			     Ch(d, e, f) + SHA256_K[i + 1] + W[i + 1];
			t2 = (ror32(h, 2) ^ ror32(h, 13) ^ ror32(h, 22)) +
			     Maj(h, a, b);
			c += t1;
			g = t1 + t2;
		} while (0);
		do {
			u32 t1, t2;
			t1 = f + (ror32(c, 6) ^ ror32(c, 11) ^ ror32(c, 25)) +
			     Ch(c, d, e) + SHA256_K[i + 2] + W[i + 2];
			t2 = (ror32(g, 2) ^ ror32(g, 13) ^ ror32(g, 22)) +
			     Maj(g, h, a);
			b += t1;
			f = t1 + t2;
		} while (0);
		do {
			u32 t1, t2;
			t1 = e + (ror32(b, 6) ^ ror32(b, 11) ^ ror32(b, 25)) +
			     Ch(b, c, d) + SHA256_K[i + 3] + W[i + 3];
			t2 = (ror32(f, 2) ^ ror32(f, 13) ^ ror32(f, 22)) +
			     Maj(f, g, h);
			a += t1;
			e = t1 + t2;
		} while (0);
		do {
			u32 t1, t2;
			t1 = d + (ror32(a, 6) ^ ror32(a, 11) ^ ror32(a, 25)) +
			     Ch(a, b, c) + SHA256_K[i + 4] + W[i + 4];
			t2 = (ror32(e, 2) ^ ror32(e, 13) ^ ror32(e, 22)) +
			     Maj(e, f, g);
			h += t1;
			d = t1 + t2;
		} while (0);
		do {
			u32 t1, t2;
			t1 = c + (ror32(h, 6) ^ ror32(h, 11) ^ ror32(h, 25)) +
			     Ch(h, a, b) + SHA256_K[i + 5] + W[i + 5];
			t2 = (ror32(d, 2) ^ ror32(d, 13) ^ ror32(d, 22)) +
			     Maj(d, e, f);
			g += t1;
			c = t1 + t2;
		} while (0);
		do {
			u32 t1, t2;
			t1 = b + (ror32(g, 6) ^ ror32(g, 11) ^ ror32(g, 25)) +
			     Ch(g, h, a) + SHA256_K[i + 6] + W[i + 6];
			t2 = (ror32(c, 2) ^ ror32(c, 13) ^ ror32(c, 22)) +
			     Maj(c, d, e);
			f += t1;
			b = t1 + t2;
		} while (0);
		do {
			u32 t1, t2;
			t1 = a + (ror32(f, 6) ^ ror32(f, 11) ^ ror32(f, 25)) +
			     Ch(f, g, h) + SHA256_K[i + 7] + W[i + 7];
			t2 = (ror32(b, 2) ^ ror32(b, 13) ^ ror32(b, 22)) +
			     Maj(b, c, d);
			e += t1;
			a = t1 + t2;
		} while (0);
	}

	state[0] += a;
	state[1] += b;
	state[2] += c;
	state[3] += d;
	state[4] += e;
	state[5] += f;
	state[6] += g;
	state[7] += h;
}

static void sha256_transform_blocks(struct sha256_state *sctx, const u8 *input,
				    int blocks)
{
	u32 W[64];

	do {
		sha256_transform(sctx->state, input, W);
		input += 64;
	} while (--blocks);

	memzero_explicit(W, sizeof(W));
}

void sha256_update(struct sha256_state *sctx, const u8 *data, unsigned int len)
{
	lib_sha256_base_do_update(sctx, data, len, sha256_transform_blocks);
}
extern typeof(sha256_update) sha256_update;
static void *__attribute__((__used__)) __attribute__((__section__(
	".discard.addressable"))) __UNIQUE_ID___addressable_sha256_update587 =
	(void *)(uintptr_t)&sha256_update;
asm(".section \".export_symbol\",\"a\" ; __export_symbol_sha256_update: ; .asciz \"\" ; .asciz \"\" ; .balign 8 ; .quad sha256_update ; .previous");

static void __sha256_final(struct sha256_state *sctx, u8 *out, int digest_size)
{
	lib_sha256_base_do_finalize(sctx, sha256_transform_blocks);
	lib_sha256_base_finish(sctx, out, digest_size);
}

void sha256_final(struct sha256_state *sctx, u8 *out)
{
	__sha256_final(sctx, out, 32);
}
extern typeof(sha256_final) sha256_final;
static void *__attribute__((__used__)) __attribute__((__section__(
	".discard.addressable"))) __UNIQUE_ID___addressable_sha256_final588 =
	(void *)(uintptr_t)&sha256_final;
asm(".section \".export_symbol\",\"a\" ; __export_symbol_sha256_final: ; .asciz \"\" ; .asciz \"\" ; .balign 8 ; .quad sha256_final ; .previous");

void sha224_final(struct sha256_state *sctx, u8 *out)
{
	__sha256_final(sctx, out, 28);
}
extern typeof(sha224_final) sha224_final;
static void *__attribute__((__used__)) __attribute__((__section__(
	".discard.addressable"))) __UNIQUE_ID___addressable_sha224_final589 =
	(void *)(uintptr_t)&sha224_final;
asm(".section \".export_symbol\",\"a\" ; __export_symbol_sha224_final: ; .asciz \"\" ; .asciz \"\" ; .balign 8 ; .quad sha224_final ; .previous");

void sha256(const u8 *data, unsigned int len, u8 *out)
{
	struct sha256_state sctx;

	sha256_init(&sctx);
	sha256_update(&sctx, data, len);
	sha256_final(&sctx, out);
}
extern typeof(sha256) sha256;
static void *__attribute__((__used__)) __attribute__((__section__(
	".discard.addressable"))) __UNIQUE_ID___addressable_sha256590 =
	(void *)(uintptr_t)&sha256;
asm(".section \".export_symbol\",\"a\" ; __export_symbol_sha256: ; .asciz \"\" ; .asciz \"\" ; .balign 8 ; .quad sha256 ; .previous");

static const char __UNIQUE_ID_description591[] __attribute__((__used__))
__attribute__((__section__(".modinfo")))
__attribute__((__aligned__(1))) = "libsha256"
				  "."
				  "description"
				  "="
				  "SHA-256 Algorithm";
static const char __UNIQUE_ID_file592[] __attribute__((__used__))
__attribute__((__section__(".modinfo")))
__attribute__((__aligned__(1))) = "libsha256"
				  "."
				  "file"
				  "="
				  "lib/crypto/libsha256";
static const char __UNIQUE_ID_license593[] __attribute__((__used__))
__attribute__((__section__(".modinfo")))
__attribute__((__aligned__(1))) = "libsha256"
				  "."
				  "license"
				  "="
				  "GPL";
